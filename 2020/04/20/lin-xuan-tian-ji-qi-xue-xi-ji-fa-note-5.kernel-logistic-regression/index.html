<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>林轩田《机器学习技法》Note——5.Kernel Logistic Regression | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">HOME</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">HOME</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">林轩田《机器学习技法》Note——5.Kernel Logistic Regression</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 20, 2020&nbsp;&nbsp;9:47:37</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Notes/">Notes</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程：</p>
<ul>
<li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li>
</ul>
<p>参考笔记：</p>
<ul>
<li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li>
</ul>
</blockquote>
<h1 id="Lecture-5-Kernel-Logistic-Regression"><a href="#Lecture-5-Kernel-Logistic-Regression" class="headerlink" title="Lecture 5: Kernel Logistic Regression"></a>Lecture 5: Kernel Logistic Regression</h1><p>上节课我们主要介绍了Soft-Margin SVM，即如果允许有分类错误的点存在，那么在原来的Hard-Margin SVM中添加新的惩罚因子$C$，修正原来的公式，得到新的$\alpha_n$值。最终的到的$\alpha_n$有个上界，上界就是$C$。Soft-Margin SVM权衡了large-margin和error point之前的关系，目的是在尽可能犯更少错误的前提下，得到最大分类边界。本节课将把Soft-Margin SVM和我们之前介绍的Logistic Regression联系起来，研究如何使用kernel技巧来解决更多的问题。</p>
<h2 id="1-Soft-Margin-SVM-as-Regularized-Model"><a href="#1-Soft-Margin-SVM-as-Regularized-Model" class="headerlink" title="1. Soft-Margin SVM as Regularized Model"></a>1. Soft-Margin SVM as Regularized Model</h2><p>因为Soft-Margin Dual SVM更加灵活、便于调整参数，所以在实际应用中，使用Soft-Margin Dual SVM来解决分类问题的情况更多一些。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-11.png" alt="img"></p>
<p>Soft-Margin Dual SVM有两个应用非常广泛的工具包，分别是Libsvm和Liblinear。</p>
<p>下面我们再来回顾一下Soft-Margin SVM的主要内容。我们的出发点是用$\xi_n$来表示margin violation，即犯错值的大小，没有犯错对应的$\xi_n=0$。然后将有条件问题转化为对偶dual形式，使用QP来得到最佳化的解。</p>
<p>从另外一个角度来看，$\xi_n$描述的是点$(x_n,y_n)$距离$y_n(w^Tz_n+b)=1$的边界有多远：</p>
<ol>
<li>第一种情况是violating margin，即不满足$y_n(w^Tz_n+b)\geq1$。那么$\xi_n$可表示为：$\xi_n=1-y_n(w^Tz_n+b)\gt0$。</li>
<li>第二种情况是not violating margin，满足$y_n(w^Tz_n+b)\geq1$的条件，此时$\xi_n=0$。</li>
</ol>
<p>我们可以将两种情况整合到一个表达式中，对任意点：</p>
<script type="math/tex; mode=display">
\xi_n=\max(1-y_n(w^Tz_n+b),0)</script><p>上式表明，如果有voilating margin，则$1-y_n(w^Tz_n+b)\gt0$，$\xi_n=1-y_n(w^Tz_n+b)$；如果not violating margin，则$1-y_n(w^Tz_n+b)\lt0$，$\xi_n=0$。整合之后，我们可以把Soft-Margin SVM的最小化问题写成如下形式：</p>
<script type="math/tex; mode=display">
\frac12w^Tw+C\sum_{n=1}^N\max(1-y_n(w^Tz_n+b),0)</script><p>经过这种转换之后，表征犯错误值大小的变量$\xi_n$就被消去了，转而由一个$\max$操作代替。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-10.png" alt="img"></p>
<p>为什么要将把Soft-Margin SVM转换为这种unconstrained form呢？我们再来看一下转换后的形式，其中包含两项，第一项是$w$的内积，第二项关于$y$和$w,b,z$的表达式，似乎有点像一种错误估计$\hat{err}$，则类似这样的形式：</p>
<script type="math/tex; mode=display">
\min \frac12w^Tw+C\sum\hat{err}</script><p>看到这样的形式我们应该很熟悉，因为之前介绍的L2 Regularization中最优化问题的表达式跟这个是类似的：</p>
<script type="math/tex; mode=display">
\min \frac{\lambda}{N}w^Tw+\frac1N\sum err</script><p>既然unconstrained form SVM与L2 Regularization的形式是一致的，而且L2 Regularization的解法我们之前也介绍过，那么为什么不直接利用这种方法来解决unconstrained form SVM的问题呢？有两个原因：</p>
<ol>
<li>这种无条件的最优化问题<strong>无法通过QP解决</strong>，即对偶推导和kernel都无法使用；</li>
<li>这种形式中包含的max()项可能造成函数<strong>并不是处处可导</strong>，这种情况难以用微分方法解决。</li>
</ol>
<p>Hard-Margin SVM与Regularization Model是有关系的：</p>
<ol>
<li>Regularization的目标是最小化$E_{in}$，条件是$w^Tw\leq C$</li>
<li>Hard-Margin SVM的目标是最小化$w^Tw$，条件是$E_{in}=0$</li>
</ol>
<p>即它们的<strong>最小化目标和限制条件是相互对调的</strong>。对于L2 Regularization来说，条件和最优化问题结合起来，整体形式写成：</p>
<script type="math/tex; mode=display">
\frac{\lambda}{N}w^Tw+E_{in}</script><p>而对于Soft-Margin SVM来说，条件和最优化问题结合起来，整体形式写成：</p>
<script type="math/tex; mode=display">
\frac12w^Tw+CN\hat{E_{in}}</script><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-8.png" alt="img"></p>
<p>通过对比，我们发现L2 Regularization和Soft-Margin SVM的形式是相同的，两个式子分别包含了参数$\lambda$和$C$。Soft-Margin SVM中的large margin对应着L2 Regularization中的short $w$，也就是都让hyperplanes更简单一些。我们使用特别的$\hat{err}$来代表可以容忍犯错误的程度，即soft margin。L2 Regularization中的$\lambda$和Soft-Margin SVM中的$C$也是相互对应的，$\lambda$越大，$w$会越小，Regularization的程度就越大；$C$越小，$\hat{E_{in}}$会越大，相应的margin就越大。</p>
<p>所以说<strong>增大$C$，或者减小$\lambda$，效果是一致的，Large-Margin等同于Regularization，都起到了防止过拟合的作用</strong>。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-8.png" alt="img"></p>
<p>建立了Regularization和Soft-Margin SVM的关系，接下来我们将尝试看看是否能把SVM作为一个regularized的模型进行扩展，来解决其它一些问题。</p>
<h2 id="2-SVM-versus-Logistic-Regression"><a href="#2-SVM-versus-Logistic-Regression" class="headerlink" title="2. SVM versus Logistic Regression"></a>2. SVM versus Logistic Regression</h2><p>上一小节，我们已经把Soft-Margin SVM转换成无条件的形式：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-8.png" alt="img"></p>
<p>上式中第二项的$\max(1-y_n(w^Tz_n+b),0)$被设置为$\hat{err}$。下面来看$\hat{err}$与之前再二元分类中介绍过的$err_{0/1}$有什么关系。</p>
<ol>
<li>对于$err_{0/1}$，linear score $s=w^Tz_n+b$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1$，呈阶梯状。</li>
<li>对于$\hat{err}$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1-ys$，呈折线状。通常把$\hat{err}_{svm}$称为<strong>hinge error measure</strong>（合页损失函数）。</li>
</ol>
<p>比较两条error曲线，我们发现$\hat{err}_{svm}$始终在$err_{0/1}$的上面，则$\hat{err}_{svm}$可作为$err_{0/1}$的上界。所以，可以使用$\hat{err}_{svm}$来代替$err_{0/1}$，解决二元线性分类问题，而且$\hat{err}_{svm}$是一个凸函数，使它在最佳化问题中有更好的性质。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-9.png" alt="img"></p>
<p>紧接着，我们再来看一下logistic regression中的error function。逻辑回归中，$err_{sce}=\log_2(1+\exp(-ys))$，当$ys=0$时，$err_{sce}=1$。它的err曲线如下所示。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-9.png" alt="img"></p>
<p>很明显，$err_{sce}$也是$err_{0/1}$的上界，而$err_{sce}$与$\hat{err}_{svm}$也是比较相近的。因为当$ys$趋向正无穷大的时候，$err_{sce}$与$\hat{err}_{svm}$都趋向于零；当$ys$趋向负无穷大的时候，$err_{sce}$与$\hat{err}_{svm}$都趋向于正无穷大。正因为二者的这种相似性，我们<strong>可以把SVM看成是L2-regularized logistic regression</strong>。</p>
<p>总结一下，我们已经介绍过几种Binary Classification的Linear Models，包括PLA，Logistic Regression和Soft-Margin SVM。</p>
<ol>
<li>PLA是相对简单的一个模型，对应的是$err_{0/1}$，通过不断修正错误的点来获得最佳分类线。它的优点是简单快速，缺点是只对线性可分的情况有用，线性不可分的情况需要用到pocket算法。</li>
<li>Logistic Regression对应的是$err_{sce}$，通常使用GD/SGD算法求解最佳分类线。它的优点是凸函数$err_{sce}$便于最优化求解，而且有regularization作为避免过拟合的保证；缺点是$err_{sce}$作为$err_{0/1}$的上界，当$ys$很小（负值）时，上界变得更宽松，不利于最优化求解。</li>
<li>Soft-Margin SVM对应的是$\hat{err}_{svm}$，通常使用QP求解最佳分类线。它的优点和Logistic Regression一样，凸优化问题计算简单而且分类线比较“粗壮”一些；缺点也和Logistic Regression一样，当$ys$很小（负值）时，上界变得过于宽松。其实，Logistic Regression和Soft-Margin SVM都是在最佳化$err_{0/1}$的上界而已。</li>
</ol>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-9.png" alt="img"></p>
<p>至此，可以看出，<strong>求解regularized logistic regression的问题相当于求解soft-margin SVM的问题</strong>。反过来，如果我们求解了一个soft-margin SVM的问题，那这个解能否直接为regularized logistic regression所用？来预测结果是正类的几率是多少，就像regularized logistic regression做的一样。我们下一小节将来解答这个问题。</p>
<h2 id="3-SVM-for-Soft-Binary-Classification"><a href="#3-SVM-for-Soft-Binary-Classification" class="headerlink" title="3. SVM for Soft Binary Classification"></a>3. SVM for Soft Binary Classification</h2><p>接下来，我们探讨如何将SVM的结果应用在Soft Binary Classification中，得到是正类的概率值。</p>
<ol>
<li>第一种简单的方法是先得到SVM的解$(b_{svm},w_{svm})$，然后直接代入到logistic regression中，得到$g(x)=\theta(w_{svm}^Tx+b_{svm})$。这种方法直接使用了SVM和logistic regression的相似性，一般情况下表现还不错。但是，这种形式过于简单，与logistic regression的关联不大，没有使用到logistic regression中好的性质和方法。</li>
<li>第二种简单的方法是同样先得到SVM的解$(b_{svm},w_{svm})$，然后把$(b_{svm},w_{svm})$作为logistic regression的<strong>初始值</strong>，再进行迭代训练修正，速度比较快，最后，将得到的$b$和$w$代入到$g(x)$中。这种做法有点显得多此一举，因为并没有比直接使用logistic regression快捷多少。</li>
</ol>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-9.png" alt="img"></p>
<p>这两种方法都没有融合SVM和logistic regression各自的优势，下面构造一个模型，融合了二者的优势。构造的模型g(x)表达式为：</p>
<script type="math/tex; mode=display">
g(x)=\theta(A\cdot(w_{svm}^T\Phi(x)+b_{svm})+B)</script><p>与上述第一种简单方法不同，我们<strong>额外增加了放缩因子$A$和平移因子$B$。</strong>首先利用SVM的解$(b_{svm},w_{svm})$来构造这个模型，放缩因子$A$和平移因子$B$是待定系数。然后再用通用的logistic regression优化算法，通过迭代优化，得到最终的$A$和$B$。一般来说，如果$(b_{svm},w_{svm})$较为合理的话，满足$A$&gt;0且$B\approx0$。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-8.png" alt="img"></p>
<p>那么，新的logistic regression表达式为：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-8.png" alt="img"></p>
<p>这个表达式看上去很复杂，其实其中的$(b_{svm},w_{svm})$已经在SVM中解出来了，实际上的未知参数只有$A$和$B$两个。归纳一下，这种Probabilistic SVM的做法分为三个步骤：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-8.png" alt="img"></p>
<p>这种soft binary classifier方法得到的结果跟直接使用SVM classifier得到的结果可能不一样，这是因为我们引入了系数$A$和$B$。一般来说，soft binary classifier效果更好。至于logistic regression的解法，可以选择GD、SGD等等。</p>
<h2 id="4-Kernel-Logistic-Regression"><a href="#4-Kernel-Logistic-Regression" class="headerlink" title="4. Kernel Logistic Regression"></a>4. Kernel Logistic Regression</h2><p>上一小节我们介绍的是通过kernel SVM在$z$空间中求得logistic regression的近似解。如果我们希望直接在$z$空间中直接求解logistic regression，通过引入kernel，来解决最优化问题，又该怎么做呢？SVM中使用kernel，转化为QP问题，进行求解，但是logistic regression却不是个QP问题，看似好像没有办法利用kernel来解决。</p>
<p>我们先来看看之前介绍的kernel trick为什么会work，kernel trick就是把$z$空间的内积转换到$x$空间中比较容易计算的函数。如果$w$可以表示为$z$的线性组合，即$w_<em>=\sum_{n=1}^N\beta_nz_n$的形式，那么乘积项$w_</em>^Tz=\sum_{n=1}^N\beta_nz_n^Tz=\sum_{n=1}^N\beta_nK(x_n,x)$，即其中包含了$z$的内积。也就是$w$可以表示为$z$的线性组合是kernel trick可以work的关键。</p>
<p>我们之前介绍过SVM、PLA、logistic regression都可以表示成$z$的线性组合，这也提供了一种可能，就是将kernel应用到这些问题中去，简化z空间的计算难度。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-8.png" alt="img"></p>
<p>claim：对于L2-regularized linear model，若它的最小化问题形式为如下的话，那么最优解$w_*=\sum_{n=1}^N\beta_nz_n$。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-7.png" alt="img"></p>
<blockquote>
<p>下面给出简单的证明：</p>
<p>假如最优解$w_*=w_{||}+w_{\bot}$。其中，$w_{||}$和$w_{\bot}$分别是平行$z$空间和垂直$z$空间的部分。我们需要证明的是$w_{\bot}=0$。</p>
<p>利用反证法，假如$w_{\bot}\neq0$，考虑$w_*$与$w_{||}$的比较。</p>
<ol>
<li>比较第二项：$err(y,w_*^Tz_n)=err(y_n,(w_{||}+w_{\bot})^Tz_n=err(y_n,w_{||}^Tz_n)$，即第二项是相等的。</li>
<li>比较第一项：$w_<em>^Tw_</em>=w_{||}^Tw_{||}+2w_{||}^Tw_{\bot}+w_{\bot}^Tw_{\bot}\gt w_{||}^Tw_{||}$，即$w_<em>$对应的L2-regularized linear model值要比$w_{||}$大，这就说明$w_</em>$并不是最优解，从而证明$w_{\bot}$必然等于零，即$w_<em>=\sum_{n=1}^N\beta_nz_n$一定成立，$w_</em>$一定可以写成$z$的线性组合形式。</li>
</ol>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-7.png" alt="img"></p>
</blockquote>
<p>经过证明和分析，我们得到了结论是<strong>任何L2-regularized linear model都可以使用kernel来解决</strong>。</p>
<p>现在，我们来看看如何把kernel应用在L2-regularized logistic regression上。上面我们已经证明了$w_<em>$一定可以写成$z$的线性组合形式，即$w_</em>=\sum_{n=1}^N\beta_nz_n$。那么我们就无需一定求出$w_<em>$，而只要求出其中的$\beta_n$就行了。怎么求呢？直接将$w_</em>=\sum_{n=1}^N\beta_nz_n$代入到L2-regularized logistic regression最小化问题中，得到：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-8.png" alt="img"></p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-6.png" alt="img"></p>
<p>上式中，所有的$w$项都换成$\beta_n$来表示了，变成了没有条件限制的最优化问题。我们把这种问题称为kernel logistic regression，即引入kernel，将求w的问题转换为求$\beta_n$的问题。</p>
<p>从另外一个角度来看Kernel Logistic Regression（KLR）：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-3.png" alt="img"></p>
<p>上式中log项里的$\sum_{m=1}^N\beta_mK(x_m,x_n)$可以看成是变量$\beta$和$K(x_m,x_n)$的内积。上式第一项中的$\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(x_n,x_m)$可以看成是关于$\beta$的正则化项$\beta^TK\beta$。所以，KLR是$\beta$的线性组合，其中包含了kernel内积项和kernel regularizer。这与SVM是相似的形式。</p>
<p>但值得一提的是，KLR中的$\beta_n$与SVM中的$\alpha_n$是有区别的。SVM中的$\alpha_n$大部分为零，SV的个数通常是比较少的；而KLR中的$\beta_n$通常都是非零值。</p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了Kernel Logistic Regression。</p>
<p>首先把Soft-Margin SVM解释成Regularized Model，建立二者之间的联系，其实Soft-Margin SVM就是一个L2-regularization，对应着hinge error messure。</p>
<p>然后利用它们之间的相似性，讨论了如何利用SVM的解来得到Soft Binary Classification。方法是先得到SVM的解，再在logistic regression中引入参数$A$和$B$，迭代训练，得到最佳解。</p>
<p>最后介绍了Kernel Logistic Regression，证明L2-regularized logistic regression中，最佳解$w_*$一定可以写成$z$的线性组合形式，从而可以将kernel引入logistic regression中，使用kernel思想在$z$空间直接求解L2-regularized logistic regression问题。</p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-5.kernel-logistic-regression/">http://xjw924.github.io/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-5.kernel-logistic-regression/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Notes/"># Notes</a>
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/22/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-7.blending-and-bagging/">林轩田《机器学习技法》Note——7.Blending and Bagging</a>
            
            
            <a class="next" rel="next" href="/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-6.support-vector-regression/">林轩田《机器学习技法》Note——6.Support Vector Regression</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
