<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>林轩田《机器学习技法》Note——2.Dual SVM | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">HOME</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">HOME</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">林轩田《机器学习技法》Note——2.Dual SVM</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 19, 2020&nbsp;&nbsp;12:11:37</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Notes/">Notes</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程：</p>
<ul>
<li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li>
</ul>
<p>参考笔记：</p>
<ul>
<li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li>
</ul>
</blockquote>
<h1 id="Lecture-2-Dual-SVM"><a href="#Lecture-2-Dual-SVM" class="headerlink" title="Lecture 2: Dual SVM"></a>Lecture 2: Dual SVM</h1><p>上节课我们主要介绍了线性支持向量机（Linear Support Vector Machine）。Linear SVM的目标是找出最“胖”的分割线进行正负类的分离，方法是使用二次规划来求出分类线。本节课将从另一个方面入手，研究对偶支持向量机（Dual Support Vector Machine），尝试从新的角度计算得出分类线，推广SVM的应用范围。</p>
<h2 id="1-Motivation-of-Dual-SVM"><a href="#1-Motivation-of-Dual-SVM" class="headerlink" title="1. Motivation of Dual SVM"></a>1. Motivation of Dual SVM</h2><p>对于非线性SVM，我们通常可以使用非线性变换将变量从$x$域转换到$z$域中。然后，在$z$域中，根据上一节课的内容，使用线性SVM解决问题即可。上一节课我们说过，使用SVM得到large-margin，减少了有效的VC Dimension，限制了模型复杂度；另一方面，使用特征转换，目的是让模型更复杂，减小$E_{in}$。所以说，<strong>非线性SVM是把这两者目的结合起来，平衡这两者的关系</strong>。</p>
<p>那么，特征转换下，求解QP问题在$z$域中的维度设为$\hat d +1$，如果模型越复杂，则$\hat d +1$越大，相应求解这个QP问题也变得很困难。当$\hat d$无限大的时候，问题将会变得难以求解，那么有没有什么办法可以解决这个问题呢？一种方法就是<strong>使SVM的求解过程不依赖$\hat d$</strong>，这就是我们本节课所要讨论的主要内容。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-7.png" alt="img"></p>
<p>比较一下，我们上一节课所讲的Original SVM二次规划问题的变量个数是$\hat d +1$，有$N$个限制条件；而本节课，我们把问题转化为对偶问题（’Equivalent’ SVM），同样是二次规划，只不过变量个数变成$N$个，有$N+1$个限制条件。<strong>这种对偶SVM的好处就是问题只跟$N$有关，与$\hat d$无关</strong>，这样就不存在上文提到的当$\hat d$无限大时难以求解的情况。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-6.png" alt="img"></p>
<p>如何把问题转化为对偶问题（’Equivalent’ SVM），其中的数学推导非常复杂，本文不做详细数学论证，但是会从概念和原理上进行简单的推导。</p>
<p>还记得我们在《机器学习基石》课程中介绍的Regularization中，在最小化$E_{in}$的过程中，也添加了限制条件：$w^Tw\leq C$。我们的求解方法是引入拉格朗日因子$\lambda$，将有条件的最小化问题转换为无条件的最小化问题：$\min\ E_{aug}(w)=E_{in}(w)+\frac{\lambda}{N}w^Tw $，最终得到的$w$的最优化解为：</p>
<script type="math/tex; mode=display">
\nabla E_{in}(w)+\frac{2\lambda}{N}w=0</script><p>所以，在regularization问题中，$\lambda$是已知常量，求解过程变得容易。那么，对于dual SVM问题，同样可以引入$\lambda$，将条件问题转换为非条件问题，只不过$\lambda$是未知参数，且个数是$N$，需要对其进行求解。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-4.png" alt="img"></p>
<p>如何将条件问题转换为非条件问题？</p>
<p>上一节课我们介绍的SVM中，目标是：$\min \frac12w^Tw$，条件是：$y_n(w^Tz_n+b)\geq 1,\ for\ n=1,2,\cdots,N$。首先，我们令拉格朗日因子为$\alpha_n$（区别于regularization），构造一个函数：</p>
<script type="math/tex; mode=display">
L(b,w,\alpha)=\frac12w^Tw+\sum_{n=1}^N\alpha_n(1-y_n(w^Tz_n+b))</script><p>这个函数右边第一项是SVM的目标，第二项是SVM的条件和拉格朗日因子$\alpha_n$的乘积。我们把这个函数称为拉格朗日函数，其中包含三个参数：$b，w，\alpha_n$。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-4.png" alt="img"></p>
<p>下面，我们利用拉格朗日函数，把SVM构造成一个非条件问题：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-4.png" alt="img"></p>
<p>该最小化问题中包含了最大化问题，怎么解释呢？首先我们规定拉格朗日因子$\alpha_n\geq0$，根据SVM的限定条件可得：$1-y_n(w^Tz_n+b)\leq0$。</p>
<ol>
<li>如果没有达到最优解，即有不满足$1-y_n(w^Tz_n+b)\leq0$的情况，因为$\alpha_n\geq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\geq0$。对于这种大于零的情况，其最大值是无解的。</li>
<li>如果对于所有的点，均满足$1-y_n(w^Tz_n+b)\leq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\leq0$，则当$\sum_n\alpha_n(1-y_n(w^Tz_n+b))=0$时，其有最大值，最大值就是我们SVM的目标：$\frac12w^Tw$。因此，这种转化为非条件的SVM构造函数的形式是可行的。</li>
</ol>
<h2 id="2-Lagrange-Dual-SVM"><a href="#2-Lagrange-Dual-SVM" class="headerlink" title="2. Lagrange Dual SVM"></a>2. Lagrange Dual SVM</h2><p>现在，我们已经将SVM问题转化为与拉格朗日因子$\alpha_n$有关的最大最小值形式。已知$\alpha_n\geq0$，那么对于任何固定的$\alpha’$，且$\alpha_n’\geq0$，一定有如下不等式成立：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-4.png" alt="img"></p>
<p>对上述不等式右边取最大值，不等式同样成立：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-5.png" alt="img"></p>
<p>上述不等式表明，我们对SVM的min和max做了对调，满足这样的关系，这叫做<strong>Lagrange dual problem</strong>。</p>
<p>不等式右边是SVM问题的下界，我们接下来的目的就是求出这个下界。</p>
<p>已知$\geq$是一种弱对偶关系，在二次规划QP问题中，如果满足以下三个条件：</p>
<ul>
<li>函数是凸的（convex primal）</li>
<li>函数有解（feasible primal）</li>
<li>条件是线性的（linear constraints）</li>
</ul>
<p>那么，上述不等式关系就变成强对偶关系，$\geq$变成=，即一定存在满足条件的解$(b,w,\alpha)$，使等式左边和右边都成立，SVM的解就转化为右边的形式。</p>
<p>经过推导，SVM对偶问题的解已经转化为无条件形式：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-5.png" alt="img"></p>
<p>其中，上式括号里面的是对拉格朗日函数$L(b,w,\alpha)$计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。</p>
<p>首先，令$L(b,w,\alpha)$对参数$b$的梯度为零：</p>
<script type="math/tex; mode=display">
\frac{\partial L(b,w,\alpha)}{\partial b}=0=-\sum_{n=1}^N\alpha_ny_n</script><p>也就是说，最优解一定满足$\sum_{n=1}^N\alpha_ny_n=0$。那么，我们把这个条件代入计算$\max$条件中（与$\alpha_n\geq0$同为条件），并进行化简：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-5.png" alt="img"></p>
<p>这样，SVM表达式消去了$b$，问题化简了一些。</p>
<p>然后，再根据最小值思想，令$L(b,w,\alpha)$对参数$w$的梯度为零：</p>
<script type="math/tex; mode=display">
\frac{\partial L(b,w,\alpha)}{\partial w}=0=w-\sum_{n=1}^N\alpha_ny_nz_n</script><p>即得到：</p>
<script type="math/tex; mode=display">
w=\sum_{n=1}^N\alpha_ny_nz_n</script><p>也就是说，最优解一定满足$w=\sum_{n=1}^N\alpha_ny_nz_n$。那么，同样我们把这个条件代入并进行化简：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-5.png" alt="img"></p>
<p>这样，SVM表达式消去了w，问题更加简化了。这时候的条件有3个：</p>
<ol>
<li>all $\alpha_n\geq0$</li>
<li>$\sum_{n=1}^N\alpha_ny_n=0$</li>
<li>$w=\sum_{n=1}^N\alpha_ny_nz_n$</li>
</ol>
<p>SVM简化为只有$\alpha_n$的最佳化问题，即计算满足上述三个条件下，函数$-\frac12||\sum_{n=1}^N\alpha_ny_nz_n||^2+\sum_{n=1}^N\alpha_n$最大值时对应的$\alpha_n$是多少。</p>
<p>总结一下，SVM最佳化形式转化为只与$\alpha_n$有关：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-4.png" alt="img"></p>
<p>其中，满足最佳化的条件称之为<strong>Karush-Kuhn-Tucker(KKT)</strong>：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-4.png" alt="img"></p>
<p>在下一部分中，我们将利用KKT条件来计算最优化问题中的$\alpha$，进而得到$b$和$w$。</p>
<h2 id="3-Solving-Dual-SVM"><a href="#3-Solving-Dual-SVM" class="headerlink" title="3. Solving Dual SVM"></a>3. Solving Dual SVM</h2><p>上面我们已经得到了dual SVM的简化版了，接下来，我们继续对它进行一些优化。首先，将max问题转化为min问题，再做一些条件整理和推导，得到：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-3.png" alt="img"></p>
<p>显然，这是一个convex的QP问题，且有N个变量$\alpha_n$，限制条件有$N+1$个。则根据上一节课讲的QP解法，找到$Q,p,A,c$对应的值，用软件工具包进行求解即可。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-4.png" alt="img"></p>
<p>求解过程很清晰，但是值得注意的是，$q_{n,m}=y_ny_mz^T_nz_m$，大部分值是非零的，称为dense。当$N$很大的时候，例如$N=30000$，那么对应的$Q_D$的计算量将会很大，存储空间也很大。所以一般情况下，对dual SVM问题的矩阵$Q_D$，需要使用一些特殊的方法，这部分内容就不再赘述了。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-3.png" alt="img"></p>
<p>得到$\alpha_n$之后，再根据之前的KKT条件，就可以计算出$w$和$b$了。首先利用条件$w=\sum\alpha_ny_nz_n$得到$w$，然后利用条件$\alpha_n(1-y_n(w^Tz_n+b))=0$，取任一$\alpha_n\neq0$即$\alpha_n&gt;0$的点，得到$1-y_n(w^Tz_n+b)=0$，进而求得$b=y_n-w^Tz_n$。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-3.png" alt="img"></p>
<p>值得注意的是，计算$b$值，$\alpha_n&gt;0$时，有$y_n(w^Tz_n+b)=1$成立。$y_n(w^Tz_n+b)=1$正好表示的是该点在SVM分类线上，即fat boundary。也就是说，<strong>满足$\alpha_n&gt;0$的点一定落在fat boundary上，这些点就是Support Vector</strong>。这是一个非常有趣的特性。</p>
<h2 id="4-Messages-behind-Dual-SVM"><a href="#4-Messages-behind-Dual-SVM" class="headerlink" title="4. Messages behind Dual SVM"></a>4. Messages behind Dual SVM</h2><p>把位于分类线边界上的点：support vector（candidates）。</p>
<p>$\alpha_n&gt;0$的点：一定落在分类线边界上，这些点为support vector（注意没有candidates）。</p>
<p>也就是说分类线上的点不一定都是支持向量，但是满足$\alpha_n&gt;0$的点，一定是支持向量。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-4.png" alt="img"></p>
<p>SV只由$\alpha_n&gt;0$的点决定，根据上一部分推导的$w$和$b$的计算公式，我们发现，$w$和$b$仅由SV的点决定，简化了计算量。这跟我们上一节课介绍的分类线只由“胖”边界上的点所决定是一个道理。</p>
<p>也就是说，样本点可以分成两类：</p>
<ol>
<li>一类是support vectors，通过support vectors可以求得fattest hyperplane；</li>
<li>另一类不是support vectors，对我们求得fattest hyperplane没有影响。</li>
</ol>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-2.png" alt="img"></p>
<p>回过头来，我们来比较一下SVM和PLA的w公式：</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-1.png" alt="img"></p>
<p>我们发现，二者在形式上是相似的。$w_{SVM}$由fattest hyperplane边界上所有的SV决定，$w_{PLA}$由所有当前分类错误的点决定。$w_{SVM}$和$w_{PLA}$都是原始数据点$y_nz_n$的线性组合形式，是原始数据的代表。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/20-1.png" alt="img"></p>
<p>总结一下，本节课和上节课主要介绍了两种形式的SVM：</p>
<ol>
<li>Primal Hard-Margin SVM有$\hat d+1$个参数，有$N$个限制条件。当$\hat d+1$很大时，求解困难。</li>
<li>Dual Hard_Margin SVM有$N$个参数，有$N+1$个限制条件。当数据量$N$很大时，也同样会增大计算难度。</li>
</ol>
<p>两种形式都能得到$w$和$b$，求得fattest hyperplane。通常情况下，如果$N$不是很大，一般使用Dual SVM来解决问题。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/21-1.png" alt="img"></p>
<p>这节课提出的Dual SVM的目的是为了避免计算过程中对$\hat d$的依赖，而只与$N$有关。但是，Dual SVM是否真的消除了对$\hat d$的依赖呢？其实并没有。因为在计算$q_{n,m}=y_ny_mz_n^Tz_m$的过程中，由$z$向量引入了$\hat d$，实际上复杂度已经隐藏在计算过程中了。所以，我们的目标并没有实现。下一节课我们将继续研究探讨如何消除对$\hat d$的依赖。</p>
<p><img src="http://47.94.229.135/wp-content/uploads/2018/07/22.png" alt="img"></p>
<h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了SVM的另一种形式：Dual SVM。我们这样做的出发点是为了移除计算过程对$\hat d$的依赖。Dual SVM的推导过程是通过引入拉格朗日因子$\alpha$，将SVM转化为新的非条件形式。然后，利用QP，得到最佳解的拉格朗日因子$\alpha$。再通过KKT条件，计算得到对应的$w$和$b$。最终求得fattest hyperplane。下一节课，我们将解决Dual SVM计算过程中对$\hat d$的依赖问题。</p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-2.dual-svm/">http://xjw924.github.io/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-2.dual-svm/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Notes/"># Notes</a>
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-3.kernel-svm/">林轩田《机器学习技法》Note——3.Kernel SVM</a>
            
            
            <a class="next" rel="next" href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-1.linear-svm/">林轩田《机器学习技法》Note——1.Linear SVM</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
