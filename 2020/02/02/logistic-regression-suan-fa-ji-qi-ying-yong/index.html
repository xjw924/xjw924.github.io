<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>Logistic Regression 算法基础及实现 | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">xjw924</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">xjw924</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Logistic Regression 算法基础及实现</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">February 2, 2020&nbsp;&nbsp;10:51:11</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/MachineLearning/">MachineLearning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><p>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到二分类的目的。</p>
<h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>逻辑回归是在线性回归的基础上，利用sigmoid函数（或称为logistic函数）</p>
<script type="math/tex; mode=display">
g(z)=\frac{1}{1+e^{-z}}</script><p>进行映射，代入线性回归部分</p>
<script type="math/tex; mode=display">
z=\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n</script><p>得到二元逻辑回归模型的一般形式：</p>
<script type="math/tex; mode=display">
h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的$h_\theta(x)$就是逻辑回归返回的值，介于0到1之间，可以将其当做样本取正类的“概率”。因此对于样本$x$分类结果为正类1和负类0的概率分别为</p>
<script type="math/tex; mode=display">
\begin{cases}
P(y=1|x;\theta)=h_\theta(x)\\
P(y=0|x;\theta)=1-h_\theta(x)
\end{cases}</script><p>对$h_\theta(x)$进行变换可以得到对数几率的表达式</p>
<script type="math/tex; mode=display">
ln\frac{h_\theta(x)}{1-h_\theta(x)}=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})=ln(\frac{1}{e^{-\theta^Tx}})=ln(e^{\theta^Tx})=\theta^Tx</script><p>从上式可以看出，<strong>逻辑回归的本质是在对线性回归模型的预测去逼近真实标记的对数几率</strong>。求解的关注点在于求解参数$\theta$上，通常使用极大似然估计的方法对$\theta$进行估计。</p>
<p>令</p>
<script type="math/tex; mode=display">
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的似然函数为</p>
<script type="math/tex; mode=display">
L(\theta)=\prod_{i=1}^{m}P(y^i|x^i;\theta)=\prod_{i=1}^{m}h_\theta(x^i)^{y^i}*(1-h_\theta(x^i))^{1-y^i}</script><p>其中，$x^i$为第$i$个样本的特征做构成的向量（每个向量$n+1$维，共$m$个向量），$y^i$为第$i$个样本的标签，$m$为样本量。实际中为了简化计算，同时防止连乘所造成的浮点数下溢，通常会转化为对数似然函数</p>
<script type="math/tex; mode=display">
l(\theta)=\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>逻辑回归所要解决的问题即为找到参数$\theta$，使得对数似然函数达到最大。</p>
<p>令损失函数为（忽略正则化项）</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}l(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>利用梯度下降求解参数</p>
<script type="math/tex; mode=display">
\begin{align}\frac{∂J(\theta)}{∂\theta_j}
& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{h_\theta(x^i)}-\frac{1-y^i}{1-h_\theta(x^i)}]\frac{∂h_\theta(x^i)}{∂\theta_j}\\
& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]\frac{∂g(\theta^Tx^i)}{∂\theta_j}\\
& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))\frac{∂\theta^Tx^i}{∂\theta_j}\\
& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))x^i_j\\
& =-\frac{1}{m}\sum_{i=1}^{m}[y^i(1-g(\theta^Tx^i))-(1-y^i)g(\theta^Tx^i)]x^i_j\\
& =-\frac{1}{m}\sum_{i=1}^{m}[y^i-g(\theta^Tx^i)]x^i_j\\
& =\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j\\\end{align}</script><p>因此最终得到参数迭代式</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>（参考：<a href="https://www.cnblogs.com/Luv-GEM/p/10674719.html" target="_blank" rel="noopener">Logistic回归（逻辑回归）和softmax回归</a>）</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点：</strong></p>
<ol>
<li><p>速度快，在时间和内存需求上相当高效，它可以应用于分布式数据和在线算法实现，用较少的资源处理大型数据</p>
</li>
<li><p>对线性分类问题拟合很好</p>
</li>
<li>简单易于理解，直接看到各个特征的权重</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>分类精度可能不高，在非线性分类问题上表现不好</li>
<li>数据特征有缺失或者特征空间很大时表现效果并不好，受异常值影响大</li>
</ol>
<h2 id="与线性回归的异同"><a href="#与线性回归的异同" class="headerlink" title="与线性回归的异同"></a>与线性回归的异同</h2><p>本质是线性的，只是特征到结果映射用的是sigmoid函数，属于广义线性模型（GLM）</p>
<ul>
<li><p>相同</p>
<ol>
<li><p>都使用极大似然估计对训练样本进行建模；求解超参数时都可以使用梯度下降。</p>
</li>
<li><p>都是广义线性模型，逻辑回归本质上是一个线性回归模型，LR是以线性回归为理论支持的。</p>
</li>
</ol>
</li>
<li><p>不同</p>
<ol>
<li>本质：逻辑回归是分类，线性回归是回归，逻辑回归中$y$是因变量而非$\frac{p}{1-p}$，因变量是离散而非连续</li>
<li>LR形式上是线性回归，实质上是在求取输入空间到输出空间的非线性函数映射（对率函数起到将线性回归模型的预测值与真实标记联系起来的作用）</li>
<li>LR是直接对分类可能性进行建模，无需事先假设数据分布，而线性回归需要假设数据分布</li>
</ol>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数是它的极大似然函数取对数再除以样本量的相反数</strong></p>
<p>极大似然函数：</p>
<script type="math/tex; mode=display">
L_\theta(x)=\prod_{i=1}^{m}h_\theta(x^i;\theta)^{y^i}*(1-h_\theta(x^i;\theta))^{1-y^i}</script><p>损失函数：</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]</script><p>除以样本量$m$并不改变最终求导极值结果，通过除以$m$可以得到<strong>平均损失值</strong>，避免<strong>样本数量对于损失值的影响</strong></p>
<p>（但是也有不除以样本量的，比如sklearn中的损失函数就不除以样本量）</p>
<p>（乘上样本量的倒数也并不影响梯度下降的过程┓( ´∀` )┏ ）</p>
<blockquote>
<p><strong>Q：为什么要用极大似然函数作为损失函数？</strong></p>
<p>损失函数一般有四种：平方损失函数，对数损失函数，HingeLoss损失函数，绝对值损失函数。</p>
<p>将极大似然函数取对数以后等同于对数损失函数。</p>
<p><strong>在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</strong></p>
<p>梯度更新公式：</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>这个式子的更新速度只和$x^i_j$和$y^i$相关，和sigmoid函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 </p>
<p><strong>Q：为什么不选平方损失函数？</strong></p>
<p>其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmoid函数本身的梯度是很相关的。</p>
<script type="math/tex; mode=display">
θ_j=θ_j-2(sigmoid(x)*(1-sigmoid(x)))x^i_j</script><p>sigmoid函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p>
<p>实际上也可以用最小二乘，但是最小二乘得到的权重效果比较差。</p>
<p>如果用最小二乘法，目标函数就是差值的平方和，<strong>是非凸的，不容易求解，很容易陷入到局部最优</strong>。</p>
<p>如果用极大似然估计，目标函数就是对数似然函数，是关于$(w,b)$的高阶<strong>连续可导凸函数</strong>，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。</p>
</blockquote>
<h2 id="参数求解方法"><a href="#参数求解方法" class="headerlink" title="参数求解方法"></a>参数求解方法</h2><ul>
<li>梯度下降法</li>
</ul>
<p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼近最优解。</p>
<blockquote>
<p>梯度下降：随机梯度下降，批梯度下降，small-batch梯度下降</p>
<p>Q：三种方式的优劣以及如何选择最合适的梯度下降方式</p>
<ol>
<li><p>批梯度下降(BGD)：每次迭代使用所有样本来进行梯度的更新，能得到全局最优解。缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p>
</li>
<li><p>随机梯度下降(SGD)：每次迭代随机使用一个样本来对参数进行更新，优点是每一轮参数的更新速度大大加快，缺点是准确度下降，可能会收敛到局部最优（单个样本不能代表全体样本的趋势）。</p>
</li>
<li><p>小批量梯度下降：结合了BGD和SGD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p>
</li>
</ol>
</blockquote>
<ul>
<li><p>牛顿法</p>
<p>(待更新)</p>
</li>
<li><p>拟牛顿法</p>
<p>(待更新)</p>
</li>
</ul>
<blockquote>
<p>牛顿法与梯度下降法求解参数的区别：</p>
<p>两种方法不同在于牛顿法中<strong>多了一项二阶导数</strong>，这项二阶导数对参数更新的影响主要体现在<strong>改变参数更新方向上</strong>。如下图所示，红色是牛顿法参数更新的方向，绿色为梯度下降法参数更新方向，因为牛顿法考虑了二阶导数，因而可以<strong>找到更优的参数更新方向</strong>，在每次更新的步幅相同的情况下，可以<strong>比梯度下降法节省很多的迭代次数</strong>。</p>
<p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/zdow4EB72J3r.png" alt="mark"></p>
</blockquote>
<h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>作用：需要一个单调可微的函数，把分类任务的真实标记与线性回归模型的预测值联系起来。</p>
<p>对于二分类问题，由线性回归得来的启发是根据特征的加权平均进行预测。很自然地想到设定一个阈值，如果加权平均大于该阈值就判为正类，反之判为负类。但<strong>阶跃函数不可导</strong>，所以<strong>引入Sigmoid函数，将样本的加权平均代入函数得到的值就是样本属于正类的概率，即将输入空间到输出空间作非线性函数映射</strong>。</p>
<p>Sigmoid函数形式：</p>
<script type="math/tex; mode=display">
g(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid函数是一个S型的函数，函数图像：</p>
<p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p>
<p>当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0。</p>
<p>它能够<strong>将任何实数映射到(0,1)区间</strong>（开区间，不可等于0或1），使其可用于将任意值函数转换为更适合二分类的函数。 </p>
<p>因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。</p>
<p>区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p>
<h2 id="共线性问题"><a href="#共线性问题" class="headerlink" title="共线性问题"></a>共线性问题</h2><p>对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。</p>
<p>通常做法为：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF， VIF越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。</p>
<h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><ol>
<li><p>多项逻辑回归(Softmax Regression)</p>
<p>（二分类逻辑回归在多标签分类下的一种拓展）</p>
<p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/4HnDRP6ijvH6.png" alt="mark"></p>
</li>
<li><p>one v.s. rest</p>
<p>k个二分类LR分类器，把标签重新整理为“第i类标签”与“非第i类标签”</p>
</li>
</ol>
<h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a> </p>
<blockquote>
<p><em>class</em> <code>sklearn.linear_model.LogisticRegression</code>(<em>penalty=’l2’</em>, <em>dual=False</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>random_state=None</em>, <em>solver=’lbfgs’</em>, <em>max_iter=100</em>, <em>multi_class=’auto’</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>n_jobs=None</em>, <em>l1_ratio=None</em>) </p>
</blockquote>
<h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数"><a href="#1-基本模型参数" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul>
<li><p><strong>fit_intercept</strong>：bool，指定是否将截距项添加到线性回归部分中。默认True。</p>
</li>
<li><p><strong>intercept_scaling</strong>：float，默认1。仅在solver=’liblinear’且fit_intercept=True时有用。 在这种情况下原本的向量是[x]就变成[x,intercept_scaling]，即具有等于设定的intercept_scaling值的“合成”特征会被添加到实例矢量。截距会变为intercept_scaling * synthetic_feature_weight(合成特征权重)。synthetic_feature_weight会与其他特征经历l1和l2正则化，为减小正则化对synthetic_feature_weight（并因此对截距）的影响，必须增加intercept_scaling。</p>
<blockquote>
<p>因为本身截距项是不需要进行正则化的，当采用fit_intcept时相当于人造一个特征出来，特征恒为1，权重为b。在计算正则化项的时候，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 (O_o)??</p>
</blockquote>
</li>
<li><p><strong>multi_class</strong>：str，’auto’（默认）/‘ovr’/‘multinomial’，表示要预测的分类是二分类或一对多形式的多分类问题，还是多对多形式的多分类问题。</p>
<ul>
<li><p><strong>‘auto’</strong>：表示自动选择，会根据数据的分类情况和其他参数确定模型要处理的分类问题的类型。</p>
<blockquote>
<p>根据源码得到判定方法如下：</p>
<p>step 1：if solver = ‘liblinear’: multi_class = ‘ovr’</p>
<p>step 2：elif n_classes &gt; 2: multi_class = ‘multinomial’</p>
<p>step 3：else: multi_class = ‘ovr’</p>
</blockquote>
</li>
<li><p><strong>‘ovr’</strong>：表示当前处理的是二分类或一对多形式的多分类问题</p>
</li>
<li><p><strong>‘multinomial’</strong>：表示当前处理的是多对多形式的多分类问题</p>
</li>
</ul>
</li>
<li><p><strong>class_weight</strong>：None（默认）/‘balanced’/dict，标签(label)的权重。</p>
<ul>
<li><p><strong>None</strong>：所有的label持有相同的权重， 所有类别的权值为1 </p>
</li>
<li><p><strong>‘balanced’</strong>：自动调整与样本中类频率成反比的权重，即<code>n_samples/(n_classes*np.bincount(y))</code></p>
<blockquote>
<p><strong>‘balanced’如何计算class_weight？</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line"></span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment"># 标签值，一共16个样本</span></span><br><span class="line">np.bincount(y)</span><br><span class="line"><span class="comment"># array([8, 6, 2], dtype=int64) 计算每个类别的样本数量，顺序按类别的出现次序</span></span><br><span class="line">class_weight = <span class="string">'balanced'</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight) <span class="comment"># [0.66666667 0.88888889 2.66666667]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">8</span>))  <span class="comment">#输出 0.6666666666666666</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">6</span>))  <span class="comment">#输出 0.8888888888888888</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">2</span>))  <span class="comment">#输出 2.6666666666666665</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>dict类型</strong></p>
<p>对于二分类问题，可定义class_weight = {0:0.9, 1:0.1}，这样类别0的权重为0.9，类别1的权重为0.1。</p>
<p>对于多分类问题，定义的权重必须具体到每个标签下的每个类，其中类是key-value中的key，权重是value。</p>
<blockquote>
<p><strong>dict类型如何计算class_weight？</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment">#标签值，一共16个样本</span></span><br><span class="line"></span><br><span class="line">class_weight = &#123;<span class="number">0</span>:<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>,<span class="number">2</span>:<span class="number">5</span>&#125;   <span class="comment"># 设置</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight)   <span class="comment"># 输出：[1. 3. 5.]，也就是字典中设置的值</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<blockquote>
<p><strong>class_weight如何体现在逻辑回归的损失函数上？</strong></p>
</blockquote>
<p>  class_weight给每个类别分别设置不同的<strong>惩罚参数C</strong>。</p>
<p>  惩罚项C会相应的放大或者缩小某一类的损失，如果某一类C越大，这一类的损失也被（相对于其他类来说）放大，那么系统会把本次学习重点放在这一类上，使得系统尽可能的预测对这一类的输入，所以惩罚项C不会影响计算的损失，但反向学习时会相应的放大或缩小损失，间接影响学习的方向。</p>
<p>  (参考：<a href="https://www.zhihu.com/question/265420166/answer/293896934" target="_blank" rel="noopener">https://www.zhihu.com/question/265420166/answer/293896934</a>)</p>
<blockquote>
<p><strong>源码关于class_weight与sample_weight在LR损失函数上的具体计算方式？</strong></p>
</blockquote>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sample_weight *= class_weight_[le.fit_transform(y_bin)] </span><br><span class="line"><span class="comment"># 将class_weight乘到每个样本的sample_weight上</span></span><br><span class="line"><span class="comment"># sample_weight : shape (n_samples,)</span></span><br><span class="line"><span class="comment"># le即LabelEncoder，将标签标准化为0/1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">out = -np.sum(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure>
<h3 id="2-求解算法参数"><a href="#2-求解算法参数" class="headerlink" title="2. 求解算法参数"></a>2. 求解算法参数</h3><ul>
<li><p><strong>solver</strong>：str，用于求解模型最优化问题的算法，可选{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认’lbfgs’。</p>
<ul>
<li><strong>‘liblinear’</strong>：使用坐标轴下降法来迭代优化损失函数。</li>
<li><strong>‘lbfgs’</strong>：拟牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。</li>
<li><strong>‘newton-cg’</strong>：牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。？</li>
<li><strong>‘sag’</strong>：随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅用一部分的样本来计算梯度，适用于样本量大的情况。</li>
<li><strong>‘saga’</strong>：线性收敛的随机优化算法的变种。</li>
</ul>
<blockquote>
<ul>
<li><p>对于数据量大小方面，’liblinear’仅限于处理二分类和一对多（OvR）问题，适用于小型数据集。’sag’和’saga’对于大型数据集来说更快（快速收敛仅在量纲大致相同的数据上得到保证），’sag’每次仅使用了部分样本进行梯度迭代，样本量少时不适合。</p>
</li>
<li><p>对于多分类问题来说，’liblinear’只能用于一对多（OvR），其它算法还可处理多对多（MvM），而多对多一般比一对多分类相对更准确一些。</p>
</li>
<li><p>对于正则化方法来说，’newton-cg’,’sag’,’lbfgs’这三种算法计算时都需要涉及到损失函数的一阶导或二阶导，因此不能用于没有连续导数的l1正则化，只能用于l2正则化。其他两种算法均可使用l1和l2正则化。</p>
</li>
</ul>
</blockquote>
<p>（这部分还不是很了解……待补充）</p>
</li>
<li><p><strong>dual</strong>：bool，是否使用对偶或原始计算方式。对偶方式仅在solver=’liblinear’与penalty=’l2’连用的情况下有小。如果样本量大于特征的数目，这个参数设置为False会更好。（逻辑回归的对偶形式是什么？……待补充）</p>
</li>
</ul>
<h3 id="3-正则化参数"><a href="#3-正则化参数" class="headerlink" title="3. 正则化参数"></a>3. 正则化参数</h3><p>损失函数</p>
<script type="math/tex; mode=display">
\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>其中$C$为正则化参数（$\lambda\ge0$)，$\alpha$为l1正则化的占比（$\alpha\in[0,1]$）。</p>
<blockquote>
<p>这里用的是<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" rel="noopener">sklearn官网</a>给出的损失函数形式， sklearn中假设y正负label定义为1和-1，与之前1/0不一样</p>
<p>损失函数除去正则化和求和部分剩余部分为：$\log(\exp(- y_i (X_i^T w + c)) + 1)$</p>
<p>当$y_i=+1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp(- (X_i^T w + c)) + 1)$</p>
<p>当$y_i=-1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp((X_i^T w + c)) + 1)$</p>
<p>上式难以继续化简，因此对比之前的损失函数形式，观测结果是否一致</p>
<p>之前的表达式对应的部分为$-[y^i\log(h_\theta(x^i;\theta))+(1-y^i)\log(1-h_\theta(x^i;\theta))]$</p>
<p>当$y_i=1$时，$-y^i\log(h_\theta(x^i;\theta))=-\log(\frac{1}{1+\exp(-(X_i^T w + c))})=\log(\exp(-(X_i^T w + c))+1)$</p>
<p>当$y_i=0$时，$-(1-y^i)\log(1-h_\theta(x^i;\theta))=-\log(1-\frac{1}{\exp(-(X_i^T w + c))+1})=\log(\exp((X_i^T w + c)) + 1)$</p>
<p>从而证明这两种表达形式是等价的。</p>
<p>（注意sklearn损失函数里没有除以样本量）</p>
</blockquote>
<ul>
<li><strong>penalty</strong>：str，指定正则化策略 ， {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}，默认’l2’。’elasticnet’同时包含‘l1’和‘l2’正则化。</li>
<li><strong>C</strong>：float，正则化系数$\lambda$的倒数（乘在损失函数的前面，与乘在正则化部分前效果相同，均用来平衡两个部分的比重），必须是一个大于0的浮点数，默认值1.0，即默认正则项与损失函数的比值是1:1。</li>
<li><strong>l1_ratio</strong>：float，l1正则化的占比$\rho$，取值范围[0,1]，默认为None。仅当penalty=’elasticnet’时使用，对于0&lt; l1_ratio &lt;1，惩罚是l1和l2正则化的组合。</li>
</ul>
<h3 id="4-控制迭代次数参数"><a href="#4-控制迭代次数参数" class="headerlink" title="4. 控制迭代次数参数"></a>4. 控制迭代次数参数</h3><ul>
<li><strong>max_iter</strong>：int，控制梯度下降的迭代次数（仅适用于solver=’newton-cg’, ‘lbfgs’, ‘sag’）。默认值为100。值过小损失函数可能会没有收敛到最小值，值过大会使得梯度下降迭代次数过多，模型运行时间缓慢。</li>
<li><strong>tol</strong>：float，让迭代停下的最小值。默认1e-4。数字越大迭代越早停下。</li>
</ul>
<h3 id="5-其他参数"><a href="#5-其他参数" class="headerlink" title="5. 其他参数"></a>5. 其他参数</h3><ul>
<li><strong>random_state</strong>：int(可选)，随机数种子，可选参数（仅适用于solver=’liblinear’, ‘sag’）。默认为无。</li>
<li><strong>verbose</strong>：int，日志冗长度。对于solver=’liblinear’, ‘lbfgs’，当设置为大于等于1的任何整数时，输出训练的详细过程 。默认为0，不输出训练过程。</li>
<li><strong>warm_start</strong>：bool，是否进行热启动，默认为False。若设置为True，则以上一次fit的结果作为此次的初始化，如果”solver”参数为”liblinear”时无效。 </li>
<li><strong>n_jobs</strong>：int，并行数。int类型，默认为1。等于1时用CPU的一个内核运行程序，等于-1时用所有CPU的内核运行程序。 </li>
</ul>
<h2 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h2><ul>
<li><strong>coef_</strong>：预测函数中特征对应的系数$w$。</li>
<li><strong>intercept_</strong>：预测函数中的截距$c$。</li>
<li><strong>n_iter_</strong>：实际迭代次数。</li>
</ul>
<h2 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h2><ul>
<li><strong>fit(x,y[,sample_weight])</strong>：训练模型。</li>
<li><strong>predict(x)</strong>：用模型进行训练，返回预测值。</li>
<li><strong>predict_log_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率的对数值。</li>
<li><strong>predict_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率值。</li>
<li><strong>score(x,y[,sample_weight])</strong>：返回在(x,y)上预测的准确率。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line">clf.predict(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本的类别</span></span><br><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本属于各个类的概率</span></span><br><span class="line">clf.score(X, y) <span class="comment"># 返回准确率</span></span><br><span class="line"></span><br><span class="line">clf.coef_ <span class="comment"># 系数</span></span><br><span class="line">clf.intercept_ <span class="comment">#截距</span></span><br><span class="line">clf.n_iter_ <span class="comment"># 迭代次数</span></span><br></pre></td></tr></table></figure>
<h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression</a> </p>
<p><em>class</em> <code>pyspark.ml.classification.LogisticRegression</code>(<em>featuresCol=’features’</em>, <em>labelCol=’label’</em>, <em>predictionCol=’prediction’</em>, <em>maxIter=100</em>, <em>regParam=0.0</em>, <em>elasticNetParam=0.0</em>, <em>tol=1e-06</em>, <em>fitIntercept=True</em>, <em>threshold=0.5</em>, <em>thresholds=None</em>, <em>probabilityCol=’probability’</em>, <em>rawPredictionCol=’rawPrediction’</em>, <em>standardization=True</em>, <em>weightCol=None</em>, <em>aggregationDepth=2</em>, <em>family=’auto’</em>, <em>lowerBoundsOnCoefficients=None</em>, <em>upperBoundsOnCoefficients=None</em>, <em>lowerBoundsOnIntercepts=None</em>, <em>upperBoundsOnIntercepts=None</em>) </p>
<h2 id="参数列表-1"><a href="#参数列表-1" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数-1"><a href="#1-基本模型参数-1" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul>
<li><p><strong>fitIntercept</strong>：bool，是否包含截距项，默认True。</p>
</li>
<li><p><strong>family</strong>：str，表示分类是二分类还是多分类，默认’auto’，还可选’binomial’和’multinomial’。</p>
<blockquote>
<p>spark中处理多分类问题的’multinomial’使用的是<strong>softmax回归</strong></p>
<p>(参考：<a href="https://blog.csdn.net/u013855234/article/details/84343963" target="_blank" rel="noopener">spark 2.x 源码分析 之 Logistic Regression 逻辑回归</a>)</p>
<p>在多分类问题中，假设有$C$个类，即类别标签$y\in\{1,2,…,C\}$，则给定一个样本$x$，softmax回归预测样本$x$属于类别$c$的后验概率为</p>
<script type="math/tex; mode=display">
P(y=c|x;\theta)=\frac{\exp(\theta^T_cx)}{\sum_{c=1}^{C}\exp(\theta^T_cx)}</script><p>其中$\theta^T_c$是第$c$类的权重向量，则样本$x$属于每个类别的概率可以由向量表示，向量的第$c$个元素就是样本被预测为第$c$类的概率。</p>
</blockquote>
</li>
<li><p><strong>threshold</strong>：float，分类中的阈值，默认为0.5。</p>
</li>
<li><p><strong>thresholds</strong>：list，分类中的阈值，默认为0.5。多元分类中的thresholds是为了调整预测每个类别时的概率。数组长度必须和类别数目相等，且值都大于0。若thresholds长度为2（即对于二分类问题），要满足<code>threshold = 1/(1+threholds[0]/threholds[1])</code></p>
<blockquote>
<p>这两个阈值看上去很迷惑，因此直接进行测试……</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类的情况</span></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThreshold(<span class="number">0.8</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment"># 会认为Thresholds = [1,6], Threshold = 0.8</span></span><br><span class="line"><span class="comment"># 报错，因为不满足threshold = 1/(1+threholds[0]/threholds[1])</span></span><br><span class="line"></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>]).setThreshold(<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># 会认为Threshold = 0.8, 忽略Thresholds(被覆盖，之前定义无效)</span></span><br><span class="line"><span class="comment"># 不报错，将超过0.8的类概率预测为该类</span></span><br></pre></td></tr></table></figure>
<p>对于多分类的情况比较简单，无论thresholds和threshold如何设定，仍会按照概率最高类进行预测（迷惑行为）</p>
<p>（参考：<a href="https://stackoverflow.com/questions/47325607/set-thresholds-in-pyspark-multinomial-logistic-regression" target="_blank" rel="noopener">Set thresholds in PySpark multinomial logistic regression</a>）</p>
</li>
<li><p><strong>standardization</strong>：bool，是否在训练模型之前对特征进行标准化，默认True。</p>
<blockquote>
<p>这里体现出spark与python的不同，spark会默认对特征进行标准化。</p>
<p>但如果设置<code>standardization = False</code>，仍会将数据进行标准化以提高收敛速度（又一迷惑行为），从而获得相同效果的目标函数。</p>
<p>这里的标准化是直接除以变量的标准差，没有减去均值的部分。</p>
</blockquote>
<p><a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L683" target="_blank" rel="noopener">源码标准化部分</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> standardizationParam = $(standardization)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regParamL1Fun</span> </span>= (index: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">// Remove the L1 penalization on the intercept</span></span><br><span class="line">  <span class="keyword">val</span> isIntercept = $(fitIntercept) &amp;&amp; index &gt;= numFeatures * numCoefficientSets</span><br><span class="line">  <span class="keyword">if</span> (isIntercept) &#123;</span><br><span class="line">    <span class="number">0.0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (standardizationParam) &#123;</span><br><span class="line">      regParamL1</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> featureIndex = index / numCoefficientSets</span><br><span class="line">      <span class="comment">// If `standardization` is false, we still standardize the data</span></span><br><span class="line">      <span class="comment">// to improve the rate of convergence; as a result, we have to</span></span><br><span class="line">      <span class="comment">// perform this reverse standardization by penalizing each component</span></span><br><span class="line">      <span class="comment">// differently to get effectively the same objective function when</span></span><br><span class="line">      <span class="comment">// the training dataset is not standardized.</span></span><br><span class="line">      <span class="keyword">if</span> (featuresStd(featureIndex) != <span class="number">0.0</span>) &#123;</span><br><span class="line">        regParamL1 / featuresStd(featureIndex)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="number">0.0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>aggregationDepth</strong>：int，树聚合所建议的深度，默认为2</p>
</li>
<li><p><strong>lowerBoundsOnCoefficients(upperBoundsOnCoefficients)</strong>：？</p>
</li>
<li><p><strong>lowerBoundsOnIntercepts(upperBoundsOnIntercepts)</strong>：？</p>
</li>
</ul>
<h3 id="2-正则化参数"><a href="#2-正则化参数" class="headerlink" title="2. 正则化参数"></a>2. 正则化参数</h3><p>正则化部分</p>
<script type="math/tex; mode=display">
\lambda[\frac{1 - \rho}{2}w^T w + \rho \|w\|_1], \rho\in[0,1], \lambda\ge0</script><p>其中$\lambda$为regParam，$\rho$为elasticNetParam。</p>
<ul>
<li><p><strong>regParam</strong>：float，正则化参数，默认0.0，即不进行正则化。</p>
</li>
<li><p><strong>elasticNetParam</strong>：float，正则化范式比，即l1正则化的占比。默认0.0，即只使用l2正则化。</p>
<blockquote>
<p>与sklearn的正则化参数$C$不同，这里的$\lambda$是乘在正则化部分，而$C$乘在损失部分</p>
<p>仅表达方式不同，改变了参数的位置</p>
<p>以l2正则为例</p>
<script type="math/tex; mode=display">
J(\theta)+\lambda L_2 \Longleftrightarrow CJ(\theta)+L_2</script><p>$\lambda$越大$C$越小，正则项的地位越高，优化时集中优化$L_2$，从而使参数$\theta$中的元素尽量小</p>
</blockquote>
</li>
</ul>
<h3 id="3-控制迭代次数参数"><a href="#3-控制迭代次数参数" class="headerlink" title="3. 控制迭代次数参数"></a>3. 控制迭代次数参数</h3><ul>
<li><strong>maxIter</strong>：int，最大迭代次数，默认100。（与sklearn完全一致）</li>
<li><strong>tol</strong>：float，让迭代停下的最小值，数字越大迭代越早停下，默认1e-6。（sklearn默认1e-4)</li>
</ul>
<h3 id="4-设定列名参数"><a href="#4-设定列名参数" class="headerlink" title="4. 设定列名参数"></a>4. 设定列名参数</h3><p>这部分参数仅用于指定列名和设置输出的列名</p>
<ul>
<li><p><strong>featuresCol</strong>：str，输入的数据集中的特征列名（一个合并后的列名），默认’features’</p>
<blockquote>
<p>spark输入到模型训练的数据需要把多列的特征合并成一列（测试集也一样）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">model_train_df = model_train.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line">model_test_df = model_test.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line"><span class="comment"># 这里标签列均在最后一列</span></span><br><span class="line"><span class="comment"># 然后就可以在训练模型时令参数featuresCol = 'features'（也可以不用设定，默认值即为'features'）</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>labelCol</strong>：str，输入的数据集中的标签列名（同featuresCol），默认’label’</p>
</li>
<li><p><strong>predictionCol</strong>：str，输出的模型预测结果中样本预测类的列名，默认’prediction’</p>
</li>
<li><p><strong>rawPredictionCol</strong>：str，输出的模型预测结果中原始概率的列名，默认’rawPrediction’</p>
</li>
<li><p><strong>probabilityCol</strong>：str，输出的模型预测结果中最终预测概率的列名，默认’probability’</p>
</li>
<li><p><strong>weightCol</strong>：str，样本权重列名，默认None，即所有样本的权重均视为等权重。</p>
</li>
</ul>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression, LogisticRegressionModel</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">df = sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line"><span class="comment"># 转换为features和label两列的形式</span></span><br><span class="line">model_df = df.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>])</span><br><span class="line"><span class="comment"># 划分训练集、测试集</span></span><br><span class="line">train, test = model_df.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>], seed = <span class="number">123</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lrModel = lr.fit(train)</span><br><span class="line"><span class="comment"># 查看模型系数和截距</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'coefficient'</span>:list(lrModel.coefficientMatrix.toArray()[<span class="number">0</span>])&#125;, index = df.columns[:<span class="number">-1</span>]).sort_values(by = <span class="string">'coefficient'</span>, ascending = <span class="literal">False</span>)</span><br><span class="line">lrModel.interceptVector</span><br><span class="line"><span class="comment"># 在测试集上预测</span></span><br><span class="line">lr_test = lrModel.transform(test)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator()</span><br><span class="line">print(evaluator.evaluate(lr_test, &#123;evaluator.matricName: <span class="string">'areaUnderROC'</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>模型输出结果（仅取前两行为例）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>features</th>
<th>label</th>
<th>rawPrediction</th>
<th>probability</th>
<th>prediction</th>
</tr>
</thead>
<tbody>
<tr>
<td>[4.9,3.1,1.5,0.1]</td>
<td>0</td>
<td>[60.297,-7.393,-52.905]</td>
<td>[1,0,0]</td>
<td>0</td>
</tr>
<tr>
<td>[5.0,3.2,1.2,0.2]</td>
<td>0</td>
<td>[64.815,-8.896,-55.919]</td>
<td>[1,0,0]</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p>该模型为多分类情况，其中rawPrediction为线性回归模型输出结果，probability为经过softmax过后得到的逻辑回归结果。具体来说，以第一行为例，根据rawPrediction输出probability，并以最大值对应的类作为最终预测的类</p>
<script type="math/tex; mode=display">
\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 1\\
\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0\\
\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0</script><h1 id="集群scala实现"><a href="#集群scala实现" class="headerlink" title="集群scala实现"></a>集群scala实现</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.param.<span class="type">ParamMap</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.&#123;<span class="type">Vector</span>, <span class="type">Vectors</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// basic variable settings</span></span><br><span class="line"><span class="keyword">val</span> train_tbl = <span class="string">"test.sklearn_dataset_iris_train"</span></span><br><span class="line"><span class="keyword">val</span> test_tbl = <span class="string">"test.sklearn_dataset_iris_test"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hc = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">//data processing</span></span><br><span class="line"><span class="keyword">val</span> train_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$train_tbl</span>"</span>).cache())</span><br><span class="line"><span class="keyword">val</span> test_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$test_tbl</span>"</span>).cache())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> featureCols = train_dataset.columns.filter(x=&gt;x.split(<span class="string">"_"</span>)(x.split(<span class="string">"_"</span>).length<span class="number">-1</span>)!=<span class="string">"label"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> assembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>().setInputCols(featureCols).setOutputCol(<span class="string">"features"</span>))</span><br><span class="line"><span class="keyword">val</span> train_data = assembler.transform(train_dataset)</span><br><span class="line"><span class="keyword">val</span> test_data = assembler.transform(test_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line">lr.setMaxIter(<span class="number">10</span>).setRegParam(<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">val</span> lr_model = lr.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model evaluation</span></span><br><span class="line"><span class="keyword">val</span> auc_calculator = lr_model.transform(test_data)</span><br><span class="line"><span class="keyword">val</span> evaluator = (<span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>())</span><br><span class="line"><span class="keyword">val</span> auc = evaluator.evaluate(auc_calculator)</span><br></pre></td></tr></table></figure>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>xjw924</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/">http://xjw924.github.io/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        <a href="/tags/Classification/"># Classification</a>
                    
                        <a href="/tags/LogisticRegression/"># LogisticRegression</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/">WOE与IV理论介绍及python和pyspark实现</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
