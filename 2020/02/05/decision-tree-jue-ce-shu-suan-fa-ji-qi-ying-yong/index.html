<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>Decision Tree 决策树算法及其应用 | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">xjw924</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">xjw924</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Decision Tree 决策树算法及其应用</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">February 5, 2020&nbsp;&nbsp;22:16:59</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/MachineLearning/">MachineLearning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="分类树理论"><a href="#分类树理论" class="headerlink" title="分类树理论"></a>分类树理论</h1><p>递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。</p>
<ul>
<li><strong>信息熵</strong>：表示随机变量不确定性的度量</li>
</ul>
<script type="math/tex; mode=display">
Info(D)=-\sum_{i=1}^{n}{p_ilog_2p_i}</script><p>​        其中$p_i$指样本集合$D$中第$i$类样本所占比例。</p>
<p>​        信息熵描述样本集合$D$携带的信息量。 信息量越大（值变化越多），则越不确定，越不容易被预测。</p>
<ul>
<li><strong>信息熵特点</strong>： <ol>
<li>不同类别的概率分布越均匀，信息熵越大</li>
<li>类别个数越多，信息熵越大</li>
<li>信息熵越大，越不容易被预测（变化个数多，变化之间区分小，则越不容易被预测；对于确定性问题，信息熵为0）</li>
</ol>
</li>
</ul>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p>根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分此时决策树停止生长。</p>
<p>有三种选择最优特征的标准：信息增益、增益率和基尼指数，分别对应了三种决策树算法：ID3，C4.5，CART。</p>
<h3 id="ID3算法：信息增益"><a href="#ID3算法：信息增益" class="headerlink" title="ID3算法：信息增益"></a>ID3算法：信息增益</h3><p>计算每个特征的信息增益，并比较它们的大小，每一次都选择使得<strong>信息增益最大</strong>的特征进行分裂，递归地构建决策树。信息增益越大，意味着使用某个特征进行划分所获得的纯度的提升越大。</p>
<p>信息增益$Gain(A)$：由于特征$A$使数据集$D$的分类不确定性减少的程度</p>
<script type="math/tex; mode=display">
Gain(A)=Info(D)-Info_A(D)</script><p>缺点：</p>
<ol>
<li><p>选择取值较多的特征往往会具有较大的信息增益（取值越多意味着确定性更高，条件熵越小，信息增益越大），所以<strong>ID3偏向于选择取值较多的特征</strong>。</p>
</li>
<li><p>仅支持分类不支持回归，不支持连续型变量，只有树的生成没有剪枝（容易过拟合），没有缺失值处理方法。</p>
</li>
</ol>
<h3 id="C4-5算法：信息增益率"><a href="#C4-5算法：信息增益率" class="headerlink" title="C4.5算法：信息增益率"></a>C4.5算法：信息增益率</h3><p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征</p>
<p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p>
<p>信息增益率$GainRate(A)$：特征$A$的信息增益$Gain(A)$与训练数据集$D$关于特征$A$的值的熵$SplitInfo(A)$之比</p>
<script type="math/tex; mode=display">
GainRate(A)=\frac{Gain(A)}{SplitInfo(A)}</script><p>其中$SplitInfo(A)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$，其中$n$是特征$A$取值的个数。</p>
<p>特征数越多的特征对应的特征熵越大，它作为分母，一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升泛化能力。</p>
<p>信息增益率准则<strong>对可取值数目较少</strong>的特征有所偏好，因此C4.5算法不是直接选取增益率最大的候选划分特征，<strong>而是先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的</strong>。</p>
<p>过拟合策略：C4.5引入了正则化系数进行剪枝</p>
<h3 id="CART算法：基尼指数"><a href="#CART算法：基尼指数" class="headerlink" title="CART算法：基尼指数"></a>CART算法：基尼指数</h3><p>基尼指数也是度量数据集纯度的指标，CART是使用基尼指数来选择最优特征的。基尼指数越小，代表数据集的纯度越高，这与信息增益（率）相反。</p>
<p>假设有$K$个类，第$k$个类别的概率为$p_k$，</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^{K}p_k^2</script><p>代表集合D的不确定性</p>
<p>特征A的基尼指数为</p>
<script type="math/tex; mode=display">
Gini(D,A)=\sum_{k=1}^{K}\frac{|D_k|}{|D|}Gini(D^k)</script><p>表示经过特征A分割之后集合D的不确定性</p>
<p>选<strong>基尼指数最小</strong>的特征的作为最优划分特征。</p>
<p>其实不同的决策树学习算法只是它们<strong>选择特征的依据不同</strong>，决策树的生成过程都是一样的</p>
<p><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同评估标准，从而衍生出不同的决策树算法。</p>
<p>特征选择的关键是如何选择最优特征对数据集进行划分，随着划分过程的进行，希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的纯度越来越高。</p>
<p>由以下两步组成：</p>
<p>（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</p>
<p>（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，使用损失函数最小作为剪枝标准</p>
<p>特征选择</p>
<p>​    回归树：平方误差最小化准则</p>
<p>​    分类树：基尼指数最小化准则</p>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>为缓解决策树过拟合，需要对决策树进行剪枝。分为预剪枝和后剪枝</p>
<p>往往通过极小化决策树整体的损失函数或代价函数来实现</p>
<p>决策树生成学习局部的模型，而决策树剪枝学习整体的模型</p>
<h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p>在生成决策树的过程中提前停止树的增长。核心思想是在树中节点进行扩展之前，先计算当前的划分是否能带来模型泛化性能的提升，如果不能则不再继续生成子树。</p>
<p>前剪枝停止决策树生长的几种方法：</p>
<p>(1) 当树达到一定深度时停止生长。</p>
<p>(2) 当到达当前节点的样本数量小于某个阈值时停止生长。</p>
<p>(3) 计算每次分类对测试集的准确率提升，当小于某个阈值时停止生长。</p>
<p>前剪枝的优缺点：</p>
<p>优点：简单高效，适合解决大规模问题。</p>
<p>缺点：</p>
<p>(1) 深度和阈值这些值很难准确估计，针对不同问题会有很大差别。</p>
<p>(2) 前剪枝存在一定局限性，有<strong>欠拟合的风险</strong>，虽然当前的划分会导致测试集准确率下降，可能在后面会有显著上升。</p>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>在已生成的过拟合决策树上进行剪枝。核心思想是让算法生成一棵完全生长的决策树，然后从底层向上计算是否剪枝。(剪枝是将子树删除，用一个叶子节点代替，节点类别按多数投票)如果剪枝之后准确率有提升，则剪枝。</p>
<p>后剪枝的优缺点：</p>
<p>优点：通常可以得到泛化能力更强的决策树。</p>
<p>缺点：<strong>时间开销大。</strong></p>
<h1 id="ID3-C4-5-CART-对比"><a href="#ID3-C4-5-CART-对比" class="headerlink" title="ID3/C4.5/CART 对比"></a>ID3/C4.5/CART 对比</h1><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼指数，均方误差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
</div>
<p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。</p>
<p>回归树：用平方误差最小化作为选择特征的准则</p>
<p>分类树：采用基尼指数最小化原则进行特征选择，递归地生成二叉树。</p>
<p>也提供了优化的剪枝策略</p>
<p><strong>从样本类型的角度：</strong></p>
<p>ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。</p>
<p>C4.5会排序找到切分点，将连续变量转换为多个取值区间的离散型变量；</p>
<p>CART每次会对特征进行二值划分，适用于连续变量。</p>
<p><strong>从应用角度：</strong></p>
<p>ID3和C4.5只能用于分类，CART树可以用于分类和回归。</p>
<p><strong>从细节、优化过程角度：</strong></p>
<p>ID3对样本特征缺失值比较敏感，而C4.5和CART树都可以对缺失值进行不同方式的处理。</p>
<p>ID3和C4.5可以产生多叉分支，且每个特征在层级之间不会复用。CART树是二叉树，<strong>每个特征可以被重复利用</strong>。</p>
<p>ID3和C4.5通过剪枝来权衡树的准确性和泛化性能，CART树枝节利用全部数据发现所有可能的树结构进行对比。</p>
<h1 id="Sklearn中树模型输出的特征重要程度"><a href="#Sklearn中树模型输出的特征重要程度" class="headerlink" title="Sklearn中树模型输出的特征重要程度"></a>Sklearn中树模型输出的特征重要程度</h1><p>决策树中节点分裂不纯度的改变量的归一化值</p>
<h1 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h1><p>优点：</p>
<p>易于理解和解释，可视化分析，容易提取出规则</p>
<p>可同时处理分类型和数值型变量</p>
<p>缺点：</p>
<p>容易过拟合</p>
<p>通常情况下精确度不如其他算法好</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>xjw924</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/">http://xjw924.github.io/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        <a href="/tags/Classification/"># Classification</a>
                    
                        <a href="/tags/DecisionTree/"># DecisionTree</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/">白板推导系列6-支持向量机SVM</a>
            
            
            <a class="next" rel="next" href="/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/">白板推导系列5——降维</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
