<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>Decision Tree 决策树算法及其应用 | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">HOME</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">HOME</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Decision Tree 决策树算法及其应用</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">February 5, 2020&nbsp;&nbsp;22:16:59</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/MachineLearning/">MachineLearning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="分类树理论"><a href="#分类树理论" class="headerlink" title="分类树理论"></a>分类树理论</h1><p>递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。</p>
<ul>
<li><strong>信息熵</strong>：表示随机变量不确定性的度量</li>
</ul>
<script type="math/tex; mode=display">
Info(D)=-\sum_{i=1}^{n}{p_ilog_2p_i}</script><p>​        其中$p_i$指样本集合$D$中第$i$类样本所占比例。</p>
<p>​        信息熵描述样本集合$D$携带的信息量。 信息量越大（值变化越多），则越不确定，越不容易被预测。</p>
<ul>
<li><strong>信息熵特点</strong>： <ol>
<li>不同类别的概率分布越均匀，信息熵越大</li>
<li>类别个数越多，信息熵越大</li>
<li>信息熵越大，越不容易被预测（变化个数多，变化之间区分小，则越不容易被预测；对于确定性问题，信息熵为0）</li>
</ol>
</li>
</ul>
<h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何该选择特征有着很多不同评估标准，从而衍生出不同的决策树算法。</p>
<p>特征选择的关键是如何选择最优特征对数据集进行划分，随着划分过程的进行，希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的纯度越来越高。</p>
<p>根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分此时决策树停止生长。</p>
<p>有三种选择最优特征的标准：信息增益、增益率和基尼指数，分别对应了三种决策树算法：ID3，C4.5，CART。</p>
<h3 id="ID3算法：信息增益"><a href="#ID3算法：信息增益" class="headerlink" title="ID3算法：信息增益"></a>ID3算法：信息增益</h3><p>计算每个特征的信息增益，并比较它们的大小，每一次都选择使得<strong>信息增益最大</strong>的特征进行分裂，递归地构建决策树。信息增益越大，意味着使用某个特征进行划分所获得的纯度的提升越大。</p>
<p>信息增益$Gain(A)$：由于特征$A$使数据集$D$的分类不确定性减少的程度</p>
<script type="math/tex; mode=display">
Gain(A)=Info(D)-Info_A(D)</script><p>缺点：</p>
<ol>
<li><p>选择取值较多的特征往往会具有较大的信息增益（取值越多意味着确定性更高，条件熵越小，信息增益越大），所以<strong>ID3偏向于选择取值较多的特征</strong>。</p>
</li>
<li><p>仅支持分类不支持回归、不支持连续型变量、只有树的生成没有剪枝（容易过拟合）、没有缺失值处理方法。</p>
</li>
</ol>
<h3 id="C4-5算法：信息增益率"><a href="#C4-5算法：信息增益率" class="headerlink" title="C4.5算法：信息增益率"></a>C4.5算法：信息增益率</h3><p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征</p>
<p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p>
<p>信息增益率$GainRate(A)$：特征$A$的信息增益$Gain(A)$与训练数据集$D$关于特征$A$的值的熵$SplitInfo(A)$之比</p>
<script type="math/tex; mode=display">
GainRate(A)=\frac{Gain(A)}{SplitInfo(A)}</script><p>其中$SplitInfo(A)=-\sum_{k=1}^{K}\frac{|D_k|}{|D|}\log_2\frac{|D_k|}{|D|}$，其中$K$是特征$A$取值的个数，$|D|$表示样本$D$的样本个数。</p>
<p>特征数越多的特征对应的特征熵越大，它作为分母，一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升泛化能力。</p>
<p>信息增益率准则<strong>对可取值数目较少</strong>的特征有所偏好，因此C4.5算法不是直接选取增益率最大的候选划分特征，<strong>而是先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的</strong>。</p>
<p>过拟合策略：C4.5引入了正则化系数进行剪枝</p>
<h3 id="CART算法：基尼指数"><a href="#CART算法：基尼指数" class="headerlink" title="CART算法：基尼指数"></a>CART算法：基尼指数</h3><p>基尼指数也是度量数据集纯度的指标，CART是使用基尼指数来选择最优特征的。基尼指数越小，代表数据集的纯度越高。</p>
<p>假设有$K$个类，第$k$个类别的概率为$p_k$，对于样本$D$，基尼指数定义为</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^{K}p_k^2</script><p>对于样本$D$，样本个数为$|D|$。根据特征$A$的某个值$a$，把$D$分成$D_1$和$D_2$，则在特征$A$的条件下，样本$D$的基尼系数表达式为</p>
<script type="math/tex; mode=display">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>表示经过特征$A$分割之后集合$D$的不确定性，因此选<strong>基尼指数最小</strong>的特征的作为最优划分特征。</p>
<p>CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样<strong>CART分类树算法所建立起来的是二叉树，而不是多叉树</strong>。</p>
<blockquote>
<p><strong>CART分类树对连续特征和离散特征的处理</strong></p>
<ul>
<li>CART分类树对连续特征的处理：连续特征离散化</li>
</ul>
<p>$m$个样本的连续特征$A$从小到大排列$a_1,a_2,\cdots,a_m$，CART取相邻两样本值的平均数作为划分点，一共有$m-1$个，其中第$i$个划分点$T_i$表示为：$T_i=(a_i+a_{i+1})/2$。</p>
<p>分别计算这$m-1$个划分点作为二元分类点时的基尼系数，选择基尼系数最小的点为该连续特征的划分点。比如取到使基尼系数最小的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样就做到了连续特征的离散化。</p>
<ul>
<li>CART分类树对离散特征的处理：不停地二分离散特征</li>
</ul>
<p>在ID3、C4.5算法中，比如特征$A$被选取建立决策树节点，若它有三个取值$A_1,A_2,A_3$，会在决策树上建立一个三叉点，这样的决策树是多叉树。</p>
<p>CART采用的是不停地二分。对于特征$A$会考虑把其分成$\{A_1\}$和$\{A_2,A_3\}$、$\{A_2\}$和$\{A_1,A_3\}$、$\{A_3\}$和$\{A_1,A_2\}$三种情况，找到基尼系数最小的组合，比如$\{A_1\}$和$\{A_2,A_3\}$，一个节点是特征$A$取值为$A_1$对应的样本，另一个节点是取值为$A_2$或$A_3$对应的样本。</p>
<p>由于CART分类树是二叉树，与ID3和C4.5不同，在对某特征进行划分后，该特征在后面还可以参与子节点的划分过程。</p>
</blockquote>
<h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>剪枝是将子树删除，用一个叶子结点代替，节点类别取多数类。为缓解决策树过拟合，需要对决策树进行剪枝。往往通过极小化决策树整体的损失函数或代价函数来实现。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p>
<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>在生成决策树的过程中提前停止树的增长。核心思想是在树中节点进行扩展之前，先计算当前的划分是否能带来模型泛化性能的提升，如果不能则不再继续生成子树。</p>
<p>预剪枝停止决策树生长的几种方法：</p>
<ol>
<li><p>当树达到一定深度时停止生长。</p>
</li>
<li><p>当到达当前节点的样本数量小于某个阈值时停止生长。</p>
</li>
<li><p>计算每次分类对测试集的准确率提升，当小于某个阈值时停止生长。</p>
</li>
</ol>
<p>预剪枝的优缺点：</p>
<p>优点：简单高效，适合解决大规模问题。</p>
<p>缺点：深度和阈值这些参数很难准确估计，针对不同问题会有很大差别。前剪枝存在一定局限性，有<strong>欠拟合的风险</strong>。</p>
<h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>核心思想是让算法生成一棵完全生长的决策树，然后从底层向上计算是否剪枝。如果剪枝之后准确率有提升，则剪枝。</p>
<p>后剪枝的优缺点：</p>
<p>优点：通常可以得到泛化能力更强的决策树。</p>
<p>缺点：时间开销大。</p>
<h4 id="代价复杂性剪枝算法-CCP"><a href="#代价复杂性剪枝算法-CCP" class="headerlink" title="代价复杂性剪枝算法(CCP)"></a>代价复杂性剪枝算法(CCP)</h4><p>CART决策树所采用的剪枝方法，是后剪枝方法的一个实例。</p>
<p>算法分为两步：</p>
<ol>
<li>先从自由生成的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\{T_0,T_1,\cdots,T_n\}$。</li>
<li>通过交叉验证的方式，在验证集中对这$n$个树序列进行评价，选择最优的子树作为最终剪枝结果。</li>
</ol>
<p>因此关键问题在于：如何构造步骤1中的子树序列$\{T_0,T_1,\cdots,T_n\}$。</p>
<p>首先给出子树$T$的损失函数</p>
<script type="math/tex; mode=display">
C_\alpha(T)=C(T)+\alpha|T|</script><p>其中$C(T)$为训练集的预测误差，误差率的计算是认为较少的一部分样本数量作为误差样本（对每个叶子节点中的两个分类，个数少的是误差），$|T|$为叶节点个数，$\alpha$为剪枝系数（作为平衡代价和复杂度两者的参数），$\alpha=0$时，损失函数等于预测误差，相当于不进行剪枝；$\alpha$越大，惩罚越大，会得到更简单的树，即剪枝幅度更大。</p>
<p><strong>对于固定的$\alpha$，一定存在一个使得损失函数达到最小的子树$T_\alpha$，即对于$\alpha$序列$\{\alpha_0,\alpha_1,\cdots,\alpha_n\}$，有最优子树序列$\{T_0,T_1,\cdots,T_n\}$与其一一对应</strong>，因此我们只需要在最优子树系列中寻找交叉验证集效果最好的那个作为最终的剪枝结果即可。因此子树序列$\{T_0,T_1,\cdots,T_n\}$的构造可以转变为$\alpha$序列$\{\alpha_0,\alpha_1,\cdots,\alpha_n\}$的构造。</p>
<p>将$\alpha$序列构造为递增的序列，则子树序列$T$是满树到根节点树的递减树序列。 </p>
<p>首先令$\alpha_0=0$，决策树本身为$T_0$，在$T_0$上进行第一次剪枝（构造$\alpha_1$），只需要将问题聚焦于节点$t$以及对应的子树$T_t$上：</p>
<ul>
<li><p>若在节点$t$处进行剪枝，则节点$t$就变成了单节点树，对应的损失函数为</p>
<script type="math/tex; mode=display">
C_\alpha(t)=C(t)+\alpha</script></li>
<li><p>若不进行剪枝，节点$t$及以下部分构成的子树$T_t$的损失函数为</p>
<script type="math/tex; mode=display">
C_\alpha(T_t)=C(T_t)+\alpha|T_t|</script></li>
</ul>
<p>已知剪枝后的损失函数$\le$剪枝前的损失函数，则有</p>
<script type="math/tex; mode=display">
\begin{align}
C_\alpha(t) &\le C_\alpha(T_t)\\
C(t)+\alpha &\le C(T_t)+\alpha|T_t|\\
\alpha &\ge \frac{C(t)-C(T_t)}{|T_t|-1}
\end{align}</script><p>因此对于$T_0$，能够在节点$t$处剪枝的最小$\alpha$为$\alpha_{min}=\frac{C(t)-C(T_t)}{|T_t|-1}$；当$\alpha\in[0,\frac{C(t)-C(T_t)}{|T_t|-1})$时，此时进行剪枝会使损失函数增大，不能进行剪枝。</p>
<p>自下而上地对每个内部节点$t$计算可以剪枝的最小$\alpha$（即$\alpha_{min}$），取$\alpha_{min}$中最小的值作为$\alpha_1$（$\alpha$序列逐渐增大，树序列逐渐更简单；最小的$\alpha_{min}$</p>
<p>|NTt|表示子树包含的叶子节点个数</p>
<p>R(T)训练数据的预测误差（如基尼指数）=节点 t 上的数据占所有数据的比例*节点 t 的误差率</p>
<p>R(Tt)R(Tt)是子树TtTt的预测误差=子树TtTt上所有叶子节点的预测误差之和；</p>
<h1 id="Sklearn中树模型输出的特征重要程度"><a href="#Sklearn中树模型输出的特征重要程度" class="headerlink" title="Sklearn中树模型输出的特征重要程度"></a>Sklearn中树模型输出的特征重要程度</h1><p>决策树中节点分裂不纯度的改变量的归一化值</p>
<h1 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h1><p>优点：</p>
<p>易于理解和解释，可视化分析，容易提取出规则</p>
<p>可同时处理分类型和数值型变量</p>
<p>缺点：</p>
<p>容易过拟合</p>
<p>通常情况下精确度不如其他算法好</p>
<h1 id="ID3-C4-5-CART对比"><a href="#ID3-C4-5-CART对比" class="headerlink" title="ID3/C4.5/CART对比"></a>ID3/C4.5/CART对比</h1><div class="table-container">
<table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼指数，均方误差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
</div>
<p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。</p>
<p>回归树：用平方误差最小化作为选择特征的准则</p>
<p>分类树：采用基尼指数最小化原则进行特征选择，递归地生成二叉树。</p>
<p>也提供了优化的剪枝策略</p>
<p><strong>从样本类型的角度：</strong></p>
<p>ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。</p>
<p>C4.5会排序找到切分点，将连续变量转换为多个取值区间的离散型变量；</p>
<p>CART每次会对特征进行二值划分，适用于连续变量。</p>
<p><strong>从应用角度：</strong></p>
<p>ID3和C4.5只能用于分类，CART树可以用于分类和回归。</p>
<p><strong>从细节、优化过程角度：</strong></p>
<p>ID3对样本特征缺失值比较敏感，而C4.5和CART树都可以对缺失值进行不同方式的处理。</p>
<p>ID3和C4.5可以产生多叉分支，且每个特征在层级之间不会复用。CART树是二叉树，<strong>每个特征可以被重复利用</strong>。</p>
<p>ID3和C4.5通过剪枝来权衡树的准确性和泛化性能，CART树枝节利用全部数据发现所有可能的树结构进行对比。</p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/">http://xjw924.github.io/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        <a href="/tags/Classification/"># Classification</a>
                    
                        <a href="/tags/DecisionTree/"># DecisionTree</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/">白板推导系列6——支持向量机SVM</a>
            
            
            <a class="next" rel="next" href="/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/">白板推导系列5——降维</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
