<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>林轩田《机器学习基石》Note——Part2：Why Can Machines Learn? | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">HOME</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/MachineLearning">Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">Data Analysis</a>
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Category</a>
                
                    <a class="menu-item" href="/tag">Tag</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">HOME</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/MachineLearning">Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">Data Analysis</a>
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Category</a>
                
                    <a class="menu-item" href="/tag">Tag</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">林轩田《机器学习基石》Note——Part2：Why Can Machines Learn?</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 8, 2020&nbsp;&nbsp;17:06:37</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Notes/">Notes</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>课程：</p>
<ul>
<li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li>
</ul>
<p>参考笔记：</p>
<ul>
<li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li>
<li><a href="https://beader.me/mlnotebook/index.html" target="_blank" rel="noopener">https://beader.me/mlnotebook/index.html</a></li>
<li><a href="https://me.csdn.net/github_36324732" target="_blank" rel="noopener">https://me.csdn.net/github_36324732</a></li>
<li><a href="https://zhuanlan.zhihu.com/ml-note" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/ml-note</a></li>
</ul>
</blockquote>
<h1 id="Why-Can-Machines-Learn"><a href="#Why-Can-Machines-Learn" class="headerlink" title="Why Can Machines Learn?"></a>Why Can Machines Learn?</h1><h2 id="VC-Dimension"><a href="#VC-Dimension" class="headerlink" title="VC Dimension"></a>VC Dimension</h2><h3 id="Dichotomy"><a href="#Dichotomy" class="headerlink" title="Dichotomy"></a>Dichotomy</h3><p>从中任意选取一个方程，让这个对进行二元分类，输出一个结果向量，比如对4个点进行预测，输出，这样的一个输出向量我们称它为一个dichotomy。</p>
<p>我们把一个dichotomy对应的所有直线方程视为一类，则effective number of lines=dichotomy的数量。显然这个dichotomy的数量小于等于所有数据点的排列组合数的，例如下图中画大叉的那幅图对应的排列组合，就不能成为一个dichotomy，因为它们无法由任何一条直线方程产生。如果存在三点共线的情况，则dichotomy的数量会更少。</p>
<p>2D Perceptrons：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/4points14lines.png" alt="img"></p>
<h3 id="Growth-Function"><a href="#Growth-Function" class="headerlink" title="Growth Function"></a>Growth Function</h3><p>成长函数：有效线的数量，描述的是“<strong>最多</strong>”能产生的dichotomy种数。是有限的，以$2^N$为上界。</p>
<script type="math/tex; mode=display">
m_\mathcal H(N)=\max_{x_1,x_2,\cdots,x_N\in\mathcal X}|\mathcal H(x_1,x_2,\cdots,x_N)|</script><p>在确定的情况下，growth function是一个与N相关的函数。以下是几种常见的Hypothesis Set的成长函数。</p>
<ol>
<li><p>Positive Rays</p>
<p><img src="https://beader.me/mlnotebook/section2/images/postive_rays.png" alt="img"></p>
<p>输入空间为一维实数空间。大于threshold a的预测+1，否则预测-1。</p>
<p>当N=4时，共能产生5个不同的dichotomies。如下图：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/positive_rays_dichotomies.png" alt="img"></p>
<p>成长函数为</p>
<script type="math/tex; mode=display">
m_\mathcal H(N)=N+1</script></li>
<li><p>Positive Intervals</p>
<p><img src="https://beader.me/mlnotebook/section2/images/positive_intervals.png" alt="img"></p>
<p>当N=4时，共能产生11种dichotomies。</p>
<p><img src="https://beader.me/mlnotebook/section2/images/positive_intervals_dichotomies.png" alt="img"></p>
<p>成长函数为</p>
<script type="math/tex; mode=display">
m_\mathcal H(N)=C_{N+1}^{2}+1=\frac{1}{2}N^2+\frac{1}{2}N+1</script></li>
<li><p>Convex Sets</p>
<p><img src="https://beader.me/mlnotebook/section2/images/convex_sets.png" alt="img"></p>
<p>任选k个点，在这k个点组成的convex多边形包围内的所有点都预测+1，否则预测-1。这N个点的任意一种排列组合都能成为一个dichotomy。因此Convex Sets的成长函数为：</p>
<script type="math/tex; mode=display">
m_\mathcal H(N)=2^N</script><p>当作用于有N个inputs时，产生的dichotomies数量等于这N个点的排列组合数时（也就是上述这种情况），我们就称这N个inputs被shatter掉了，或者说产生的dichotomies把这N个点的排列组合给shatter了。</p>
<blockquote>
<p>shatter 的原意是「打碎」，在此指「N個點的所有(碎片般的)可能情形都被產生了」。</p>
<p>从打游戏过关的角度去理解：“是把散弹枪，在每个关卡(level N)中，他可以有发小子弹（每发小子弹对应一种dichotomy），而你面临的是个敌人。你得一枪打出去shatter掉所有人。对于这把散弹枪来说，第一关和第二关都还好，第三关6发小子弹shatter不掉8个人，于是它就break了。”</p>
</blockquote>
</li>
</ol>
<h3 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h3><p>第一个小于$2^N$的点以及之后的点均是break point。令$k$为最小的那个break point。</p>
<ol>
<li>Positive Rays成长函数的break point为2。</li>
<li>Positive Intervals成长函数的break point为3。</li>
<li>Convex Sets不管N多大都可以去shatter掉那N个点，因此它的成长函数没有break point。</li>
<li>2D Perceptrons的break point为4，当N=4时，它不能够shatter，最多只能产生14种dichotomies（而非16种），因此2D Perceptrons成长函数的break point为4。</li>
</ol>
<p>因此结合break point和成长函数，猜测：</p>
<ul>
<li>没有break point：$m_\mathcal H(N)=2^N$，肯定正确</li>
<li>k(min break point)：$m_\mathcal H(N)=O(N^{k-1})$，可能是多项式而非指数！（后验证猜想是正确的）</li>
</ul>
<h3 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h3><ul>
<li>限制：有些成长函数很容易找到，比如前面说到的Positive Rays、Positive Intervals以及Convex Sets；有些则没有那么容易，比如2D perceptrons，我们无法直接看出它的成长函数是什么。</li>
<li><p>考虑：能否利用break point得到成长函数的upper bound呢？</p>
</li>
<li><p>先用例子来看看，当我们完全不知道是什么，只知道它的break point k时，作用于“<strong>最多最多</strong>”可以产生多少个dichotomies。注意这里我用了两个“<strong>最多</strong>”，由于我们无法确切知道成长函数，因此我们用这个break point推算出的这个dichotomies的数量仍然是个高估值，这个高估值实际上是任何break point为k的作用于所真实产生的dichotomies数量的上界 (upper bound)。</p>
</li>
<li><p>举例说明，假设我们不知道某个的成长函数，但知道它的break point k=2，那么作用于N=3的时，“<strong>最多最多</strong>”能产生多少种dichotomies？</p>
<ul>
<li><p>从k=2我们可以知道，任意2个数据点都不能被shatter，意思就是我产生的dichotomies不能完全包含任何2个数据点所有的排列组合。</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n3k2d4s.png" alt="img"></p>
<p>上图右边两列被shatter了。k=2，即任意2个点不能被shatter，因此不可能产生这4种dichotomies。那我们换一个dichotomy试试看。</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n3k2d4.png" alt="img"></p>
<p>换了一个dichotomy之后就行了，检查任意的两个点都没有被shatter，看来这4种dichotomies是可以的。</p>
</li>
<li><p>5个dichotomies的情形这里就不再画出来了，很容易看出不管增加怎样的dichotomy进去，都会有两个点被shatter掉。因此这里“<strong>最多最多</strong>”只能有4种dichotomies。因此，k=2时的upper bound是4。</p>
</li>
</ul>
</li>
<li><p>我们用Bounding Function $B(N,k)$来表示：break point为$k$时，任意的作用于size为$N$时，所能产生的dichotomies的数量的上限（“<strong>最多最多</strong>”）。比如刚刚得出的结论可以表示为，当$k=2,N=3$时，$B(N,k)=B(3,2)=4$。</p>
</li>
<li><p>从以上可以看出，虽然很多时候我们无法直接得到成长函数，但如果我们知道它的break point是多少，我们似乎还是有办法算出这个的上界的。于是乎我们就有了新的目标，不去直接研究成长函数，转而去研究$B(N,k)$。</p>
</li>
</ul>
<h3 id="Bounding-Function"><a href="#Bounding-Function" class="headerlink" title="Bounding Function"></a>Bounding Function</h3><p>考察$B(N,k)$的取值情况：</p>
<ul>
<li>$k=1$时，1个点（2种排列组合）都没有办法shatter，因此恒等于1。</li>
<li>$k&gt;N$时，一定能shatter掉$2^N$个点，此时无任何条件限制。</li>
<li>$k=N$时，从所有的排列组合中移除掉一个（否则会shatter掉），剩下的都可以作为dichotomies，因此它产生的dichotomies的数量“<strong>最多最多</strong>”可以是$2^N-1$。</li>
</ul>
<p>  因此我们可以得到下表：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/bnk_table.png" alt="img"></p>
<p>如何求取其他部分？</p>
<ul>
<li><p>以$B(4,3)$为例，采用穷举法得到所有可能的dichotomies：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n4k3d11.png" alt="img"></p>
<p>整理一下后得到：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n4k3d11_ordered2.png" alt="img"></p>
<p>其中橙色的部分是成对的，紫色的部分是单个出现的。</p>
<p>去掉$x_4$并去重，可以得到：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n4k3d7.png" alt="img"></p>
<p>$\alpha+\beta$部分可以成为这3个点的dichotomies，因为对于$B(4,3)$任3个点不能够被shatter，所以$\alpha+\beta$部分这3个点也不能够被shatter，从而有</p>
<script type="math/tex; mode=display">
\alpha+\beta\le B(3,3)</script><p>再单独观察$\alpha$部分：</p>
<p><img src="https://beader.me/mlnotebook/section2/images/n4k2_alpha.png" alt="img"></p>
<p>因为对于$B(4,3)$任3个点不能够被shatter，所以$\alpha$部分这3个点中任2个点也不能够被shatter，从而有</p>
<script type="math/tex; mode=display">
\alpha\le B(3,2)</script></li>
</ul>
<p>  结合以上两个结论，可以得出</p>
<script type="math/tex; mode=display">
  B(4,3)=2\alpha+\beta\le B(3,3)+B(3,2)</script><p>  这样就能够把前面那张表给填完整：</p>
<p>  <img src="https://beader.me/mlnotebook/section2/images/bnk_table_full.png" alt="img"></p>
<p>  从而得到了Bounding Function的upper bound</p>
<ul>
<li><p>可以推广上述结论，得到</p>
<script type="math/tex; mode=display">
B(N,k)\le B(N-1,k)+B(N-1,k-1)</script><p>（可以证明上述不等号其实可以取等号）</p>
<blockquote>
<p>思路可以归纳为：</p>
<p>成长函数（Growth Function）&lt;上限函数（Bounding Function）&lt;上限的上限（Bounding Function的upper bound）（最高次项为$N^{k-1}$的多项式）</p>
<p>从而证明霍夫丁不等式中的M是有上限的。</p>
</blockquote>
</li>
<li><p>思路总结：上一篇说的learning的可行性，讲到：如果遇上bad sample，与就会差很多，此时learning不可行。遇到bad sample的概率与中方程的数量以及中的数据量有关。然而中方程的数量往往是无穷的（比如2D Perceptrons的是平面上所有的直线），本篇则继续阐述，方程的数量看上去是无穷的，但真正有效(effective)的方程的数量却是有限的，我们可以用成长函数来描述作用于会产生多少种有效的方程。但实际上我们很难确切知道各种的成长函数究竟长什么样子，我们只好通过break point去寻找成长函数的upper bound。</p>
</li>
</ul>
<h3 id="VC-Bound"><a href="#VC-Bound" class="headerlink" title="VC Bound"></a>VC Bound</h3><p>我们设想利用有限的$m_{\mathcal{H}}(N)$来替换无限的大$M$，得到$\mathcal{H}$遇到Bad Sample的概率上界：</p>
<script type="math/tex; mode=display">
\mathbb{P}_\mathcal{D}[BAD\ D]=\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon]\leq 2m_{\mathcal{H}}(N)\cdot exp(-2\epsilon ^2N)</script><p>其中$\mathbb{P}_\mathcal{D}[BAD\ D]$是$\mathcal{H}$中所有有效的方程(Effective Hypotheses)遇到Bad Sample的联合概率，即$\mathcal{H}$中存在一个方程遇上bad sample，则说$\mathcal{H}$遇上bad sample。</p>
<p>但事实上上面的不等式是不严谨的，为什么呢？</p>
<p>$m_{\mathcal{H}}(N)$描述的是$\mathcal{H}$作用于数据量为$N$的资料$\mathcal{D}$，有效的方程数，因此$\mathcal{H}$当中每一个$h$作用于$\mathcal{D}$都能算出一个$E_{in}$来，$E_{in}$的可能取值是一个有限的数。</p>
<p>但在out of sample的世界里(总体)，往往存在无限多个点，平面中任意一条直线，随便转一转就能产生一个不同的$E_{out}$来。<strong>$E_{in}$的可能取值是有限个的，而$E_{out}$的可能取值是无限的，无法直接套用union bound，我们得先把上面那个无限多种可能的$E_{out}$换掉。</strong>那么如何把$E_{out}$变成有限个呢？</p>
<ul>
<li>Step 1: 用$E_{in}^{’}$替换$E_{out}$</li>
</ul>
<p>假设我们能从总体当中再获得一份$N$个样本的验证资料(verification set)$\mathcal{D}’$，对于任何一个$h$我们可以算出它作用于$\mathcal{D}’$上的$E_{in}^{’}$。<strong>由于$\mathcal{D}’$也是总体的一个样本，因此如果$E_{in}$和$E_{out}$离很远，有非常大的可能$E_{in}$和$E_{in}^{’}$也会离得比较远。</strong></p>
<p><img src="https://beader.me/imgs/vc-dimension-two/pdf_of_ein.png" alt="img"></p>
<p>事实上，当$N$很大的时候，$E_{in}$和$E_{in}^{’}$可以看做服从以$E_{out}$为中心的近似正态分布(Gaussian)，如上图。</p>
<p>$[|E_{in}-E_{out}|\text{ is large}]$这个事件取决于$\mathcal{D}$，如果$[|E_{in}-E_{out}|\text{ is large}]$，则如果我们从总体中再抽一份$\mathcal{D}^{‘}$出来，有50%左右的可能性会发生$[|E_{in}-E_{in}^{‘}|\text{ is large}]$，还有大约50%的可能$[|E_{in}-E_{in}^{’}|\text{ is not large}]$。</p>
<p>从而得到下式（没有进行严格的推导）：</p>
<script type="math/tex; mode=display">
\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 2\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]</script><p>这样一来我们就把无限多种的$E_{out}$换成了有限多种的$E_{in}^{‘}$。</p>
<ul>
<li>Step 2：重新构成$\mathcal{H}$</li>
</ul>
<p>因为$\mathcal{D}$与$\mathcal{D}^{’}$的大小相等，都为$N$，因此我们手中一共有$2N$笔数据，这样$\mathcal{H}$作用于$\mathcal{D}+\mathcal{D}^{’}$最多能产生$m_{\mathcal{H}}(2N)$种dichotomies，此时又可以使用union bound了。</p>
<p><img src="https://i.loli.net/2020/03/13/d8VGeU7pDJL3tsn.png" alt="微信截图_20200313124748.png" style="zoom:67%;"></p>
<p>用固定的$h$来看$E_{in}$和$E_{in}^{’}$之间的差别：</p>
<script type="math/tex; mode=display">
2\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}] \le 2m_{\mathcal{H}}(2N) \mathbb{P}[\text{fixed h s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]</script><ul>
<li>Step 3：使用Hoeffding inequality</li>
</ul>
<p>前面的动作相当于先从总体中抽出$2N$笔数据，把这$2N$笔数据当成一个比较小的bin，然后在这个bin中抽取$N$笔作为$\mathcal{D}$，剩下的$N$笔作为$\mathcal{D}^{’}$，$\mathcal{D}$和$\mathcal{D}^{’}$之间是没有交集的。在我们想象出来的这个small bin当中，整个bin的错误率为$\frac{E_{in}+E_{out}}{2}$，又因为：</p>
<script type="math/tex; mode=display">
|E_{in}-E_{in}^{'}|\gt \frac{\epsilon}{2} \Leftrightarrow |E_{in} - \frac{E_{in}+E_{in}^{'}}{2}|\gt \frac{\epsilon}{4}</script><p>所以不等式就又可以使用Hoeffding inequality了：</p>
<script type="math/tex; mode=display">
2m_{\mathcal{H}}(2N) \mathbb{P}[\text{fixed h s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]\le2m_{\mathcal{H}}(2N)·2\exp(-2(\frac{\epsilon}{4})
^2N)</script><p>最终整理得到：</p>
<script type="math/tex; mode=display">
\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4m_{\mathcal{H}}(2N)\cdot \exp(-\frac{1}{8}\varepsilon ^2N)</script><p>上式右侧千辛万苦得出来的这个bound就叫做VC Bound。</p>
<h3 id="VC-Dimension-1"><a href="#VC-Dimension-1" class="headerlink" title="VC Dimension"></a>VC Dimension</h3><p>VC Bound所描述的是在给定数据量N以及给定的Hypothesis Set的条件下，遇到坏事情的概率的上界，即$E_{in}$与$E_{out}$差很远的概率，最多是多少。</p>
<p>因为寻找所有Hypothesis Set的成长函数是困难的，因此我们再利用$N^{d_{vc}}$来bound住所有VC Dimension为$d_{vc}$的Hypothesis Set的成长函数。所以对于任意一个从$\mathcal{H}$中的$g$来说，如果$k$存在，有：</p>
<script type="math/tex; mode=display">
\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4(2N)^{k-1}\cdot \exp(-\frac{1}{8}\varepsilon ^2N)</script><p>因此说想让机器真正学到东西，并且学得好，有三个条件：</p>
<ol>
<li>$\mathcal{H}$的$d_{vc}$是有限的，即存在break point $k$，这样VC Bound才存在。(good $\mathcal{H}$)</li>
<li>$N$足够大(对于特定的$d_{vc}$而言)，这样才能保证上面不等式的bound不会太大。(good $\mathcal{D}$)</li>
<li>算法$\mathcal{A}$有办法在$\mathcal{H}$中顺利地挑选一个使得$E_{in}$最小的方程$g$。(good $\mathcal{A}$)</li>
</ol>
<blockquote>
<p>为什么要费那么大的力气来讲这个VC Bound和VC Dimension呢？因为对于初学者来说，最常犯的错误就是只考虑到了第3点，而忽略掉了前两点，往往能在training set上得到极好的表现，但是在test set中表现却很烂。关于算法$\mathcal{A}$的部分会在后续的笔记当中整理，目前我们只关心前面两点。</p>
</blockquote>
<p>假设空间$\mathcal H$的VC Dimension记为$d_{VC}(\mathcal H)$，是这个假设空间最多能够shatter掉的点的数量。</p>
<ul>
<li>如果该假设空间中存在$N$个点能够shatter他们，则$d_{VC}(\mathcal H)\ge N$。</li>
<li>如果该假设空间中任意$N$个点都不能够shatter他们，则$d_{VC}(\mathcal H)&lt; N$。</li>
<li>不难看出与break point k的关系，有$k=d_{vc}+1$</li>
</ul>
<p>因此我们用下式来描述成长函数的上界（$N\ge 2,d_{VC}\ge 2）$：</p>
<script type="math/tex; mode=display">
m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i}</script><p>上式右边事实上是最高项为$d_{vc}$的多项式，利用数学归纳法可得：</p>
<script type="math/tex; mode=display">
m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i} \leq N^{d_{vc}}+1</script><p>对于以下几个$\mathcal{H}$，由于之前我们已经知道了他们的成长函数，因此可以根据$m_{\mathcal{H}}(N)\leq N^{d_{vc}}$，直接得到他们的VC Dimension：</p>
<ul>
<li>positive rays: $m_{\mathcal{H}}(N)=N+1$，看N的最高次项的次数，知道$d_{vc}=1$</li>
<li>positive intervals: $m_{\mathcal{H}}(N)=\frac{1}{2}N^2+\frac{1}{2}N+1$，$d_{vc}=2$</li>
<li>convex sets: $m_{\mathcal{H}}(N)=2^N$，$d_{vc}=\infty$</li>
<li>2D Perceptrons: $m_{\mathcal{H}}(N)\leq N^3\;for\;N\geq 2$，所以$d_{vc}=3$</li>
</ul>
<p>由于convex sets的$d_{vc}=\infty$，不满足上面所说的第1个条件，因此不能用convex sets这个$\mathcal{H}$来学习（不是好的假设空间）。</p>
<p>但这里要回归本意，通过成长函数来求得$d_{vc}$没有太大的意义，引入$d_{vc}$很大的一部分原因是，我们想要得到某个Hypothesis Set的成长函数是困难的，希望用$N^{d_{vc}}$来bound住对应的$m_{\mathcal{H}}(N)$。对于陌生的$\mathcal{H}$，如何求解它的$d_{vc}$呢？下面用自由度的角度进行解释。</p>
<p>对于比较简单的模型，可以从它最多能够shatter的点的数量，得到$d_{vc}$，但对于一些较为复杂的模型，寻找能够shatter掉的点的数量，就不太容易了。此时我们可以通过模型的自由度，来近似的得到模型的$d_{vc}$。</p>
<p>定义自由度是：<strong>模型当中可以自由变动的参数的个数</strong>，即我们的机器需要通过学习来决定模型参数的个数。</p>
<p>譬如：</p>
<ul>
<li>Positive Rays，需要确定1个threshold，这个threshold就是机器需要根据$\mathcal{D}$来确定的一个参数，则Positive Rays中自由的参数个数为1，$d_{vc}=1$</li>
<li>Positive Intervals，需要确定左右2个thresholds，则可以由机器自由决定的参数的个数为2，$d_{vc}=2$</li>
<li>d-D Perceptrons，$d$维的感知机，可以由机器通过学习自由决定的参数的个数为$d+1$（别忘了还有个$w_0$），$d_{vc}=d+1$</li>
</ul>
<h3 id="Model-Complexity"><a href="#Model-Complexity" class="headerlink" title="Model Complexity"></a>Model Complexity</h3><p>learning的问题应该关注的两个最重要的问题是：</p>
<ol>
<li>能不能使$E_{in}$与$E_{out}$很接近；</li>
<li>能不能使$E_{in}$足够小。</li>
</ol>
<ul>
<li>对于相同的$\mathcal{D}$而言，$d_{vc}$小的模型，其VC Bound比较小，比较容易保证$E_{in}$与$E_{out}$很接近，但较难做到小的$E_{in}$。试想，对于2D Perceptron，如果规定它一定要过原点($d_{vc}=2$)，则它就比没有规定要过原点($d_{vc}=3$)的直线更难实现小的$E_{in}$，因为可选的方程更少。2维平面的直线，就比双曲线($d_{vc}=6$)，更难实现小的$E_{in}$。</li>
<li>对于相同的$\mathcal{D}$而言，$d_{vc}$大的模型，比较容易实现小的$E_{in}$，但是其VC Bound就会很大，很难保证模型对$\mathcal{D}$之外的世界也能有同样强的预测能力。</li>
</ul>
<p>令之前得到的VC Bound为$\delta$，坏事情$[|E_{in}(g)-E_{out}(g)|\gt \varepsilon]$发生的概率小于$\delta$，则好事情$[|E_{in}(g)-E_{out}(g)|\leq \varepsilon]$发生的概率就大于$1-\delta$，这个$1-\delta$在统计学中又被称为置信度，或置信水平。</p>
<script type="math/tex; mode=display">
\mathbb{P}[|E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4(2N)^{d_{vc}}\cdot \exp(-\frac{1}{8}\varepsilon ^2N)=\delta</script><p>因此$E_{in}$、$E_{out}$又有下面的关系：</p>
<script type="math/tex; mode=display">
|E_{in}(h)-E_{out}(h)|\le\sqrt{\frac{8}{N}\ln(\frac{4(2N)^{d_{vc}}}{\delta})}</script><p>令$\Omega (N,\mathcal{H},\delta)=\sqrt{…}$，即上式的根号项为来自模型复杂度的，模型越复杂，$E_{in}$与$E_{out}$离得越远。</p>
<p><img src="https://beader.me/imgs/vc-dimension-three/model_complexity_curve.png" alt="model_complexity_curve.png"></p>
<p>随着$d_{vc}$的上升，$E_{in}$不断降低，而$\Omega$项不断上升，他们的上升与下降的速度在每个阶段都是不同的，因此我们能够寻找一个二者兼顾的，比较合适的$d_{vc}^{*}$，用来决定应该使用多复杂的模型。</p>
<p>反过来，如果我们需要使用$d_{vc}=3$这种复杂程度的模型，并且想保证$\varepsilon = 0.1$，置信度$1-\delta =90\%$，我们也可以通过VC Bound来求得大致需要的数据量$N$。通过简单的计算可以得到理论上，我们需要$N\approx 10,000d_{vc}$笔数据。</p>
<p>但VC Bound事实上是一个极为宽松的bound，因为它对于任何演算法$\mathcal{A}$，任何分布的数据，任何目标函数$f$都成立，所以经验上，常常认为$N\approx 10d_{vc}$就可以有不错的结果。</p>
<h2 id="Noise-and-Error"><a href="#Noise-and-Error" class="headerlink" title="Noise and Error"></a>Noise and Error</h2><p>首先我们认为存在一个未知的真理$f$，认为 $\mathcal{D}$中的$y$就是$f$作用于$x$产生的，因此虽然无法直接得到$f$，但若能找到一个和$f$表现差不多的函数，也算是能学到东西。</p>
<p>但在现实世界中，我们拿到的$\mathcal{D}$并不是完美的，会有noise的存在。在Learning中，noise主要表现为以下几种形式：</p>
<p>在Learning中，noise主要表现为以下几种形式：</p>
<ul>
<li>noise in y : 本来应该是圈圈的，却被标记为叉叉</li>
<li>noise in y : 输入$\mathcal{X}$完全相同的点，既有被标记圈圈的，也有被标记叉叉的</li>
<li>noise in x : 输入$\mathcal{X}$本身就存在问题，譬如100被写成了100万</li>
</ul>
<p>$f$是一个“确定性”(deterministic)的模型，但$noise$是一个随机发生的东西，他们两个共同作用的结果，就成了一个“概率性”(probabilistic)的东西：</p>
<p>对于某个样本 $x$，理想状态下，应该有$y=f(x)=+1$，但由于某种noise的存在，该noise会有30%的概率会转换$f(x)$的结果(把+1变成-1或把-1变成+1)。因此在$\mathcal{D}$中，该样本有70%的概率表现出$y=+1$，30%的概率表现出。</p>
<h3 id="Error-Measure"><a href="#Error-Measure" class="headerlink" title="Error Measure"></a>Error Measure</h3><p>在把learning的工作交给机器的时候，必须让机器明白你学习的目标，譬如你想让什么什么最大化，或者什么什么最小化。通常的做法是把每一个预测值与真实值之间的误差(error)看成一种成本，机器要做的，就是在$\mathcal{H}$中，挑选一个能使总成本最低的函数。</p>
<p>之前一直说的$E_{in}(h)$，就是$h$作用于$\mathcal{D}$中每一笔数据，所产生的成本之和：</p>
<script type="math/tex; mode=display">
E_{out}(g)=\varepsilon[[g(x)\neq f(x)]]</script><p>这种误差衡量方式称为”pointwise measure”，即对每个点记录误差，总误差为所有点产生的误差之和。</p>
<p>针对不同的问题与不同的使用环境，我们可以设计不同的误差衡量方法，下面是常见的pointwise误差的定义：</p>
<ul>
<li><p>0/1 error ，通常用于分类问题：</p>
<script type="math/tex; mode=display">
err(\widetilde{y},y)=[\widetilde{y}\neq y]</script></li>
<li><p>squared error，通常用于回归问题：</p>
<script type="math/tex; mode=display">
err(\widetilde{y},y)=(\widetilde{y}-y)^2</script></li>
<li><p>absolute error：</p>
<script type="math/tex; mode=display">
err(\widetilde{y},y)=|\widetilde{y}-y|</script></li>
</ul>
<p>之前提到的二元分类问题，就是对判断错误的点，记误差为1，判断正确的点，记误差为0；不管是把$y=+1$的猜错成$-1$，或是把$y=-1$的猜错成$+1$，其产生的误差都为1。</p>
<p>但在实际应用中，这个误差的定义可以很灵活，例如下面指纹验证的例子：</p>
<ul>
<li><p>中情局的门禁系统，利用指纹判断是内部工作人员，才允许进入。这种情形下，若是把好人当坏人，代价并不高，无非就是请工作人员多按一次指纹的功夫，但如果把坏人当好人，损失可就大了。针对这种需求，下面这个error的衡量办法可能更加合理。</p>
<p><img src="https://i.loli.net/2020/03/13/InbO4kXmYZaxE7R.png" alt="微信截图_20200313175204.png" style="zoom:50%;"></p>
<p>其中权重1000可以视为：将$y=-1$时将数据复制了1000倍（过采样），从而也可以转化为等权重的问题</p>
<p>（随机地重复选取这些$y=-1$的样本1000次）</p>
</li>
</ul>
<p>总结一下，先根据问题的不同选择合适的误差衡量方式，0/1 error还是squared error或者是其他针对某一场景特殊设计的error？把$h$作用于$\mathcal{D}$中所有点的error加总起来就成了一个cost function，也就是$E_{in}(h)$，接着要设计一个最优化算法$\mathcal{A}$，它能够从$\mathcal{H}$中挑选出能够使$E_{in}$最小的方程$g$，learning就完成了。</p>
<p>对于不同类型的cost function，通常会使用不同的最优化算法。对于某些cost function，很容易实现$E_{in}$最小，但对于某些cost function，寻找最小的$E_{in}$是困难的，比如用0/1 error来衡量误差，要minimize $E_{in}$就是个NP Hard问题。可以将问题转化为求解闭式解、凸优化的问题。</p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part2/">http://xjw924.github.io/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part2/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/MachineLearning/"># MachineLearning</a>
                    
                        <a href="/tags/Notes/"># Notes</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part1/">林轩田《机器学习基石》Note——Part1：When Can Machines Learn?</a>
            
            
            <a class="next" rel="next" href="/2020/02/14/jian-zhi-offer-ti-mu-python-jie-da/">剑指offer题目Python解答</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
