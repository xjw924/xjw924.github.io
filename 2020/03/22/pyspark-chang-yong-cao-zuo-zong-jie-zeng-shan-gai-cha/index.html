<!DOCTYPE html>
<html lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xjw924">


    <meta name="subtitle" content="Hi, there~">


    <meta name="description" content="统计|数据分析|机器学习|大数据">



<title>PySpark常用操作总结——增删改查 | 小徐小徐不断学习</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">HOME</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>

        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">HOME</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/MachineLearning">▶Machine Learning</a>
                
                    <a class="menu-item" href="/DataAnalysis">▶Data Analysis</a>
                
                    <a class="menu-item" href="/Hadoop">▶Hadoop</a>
                
                    <a class="menu-item" href="/Spark">▶Spark</a>
                
                    <a class="menu-item" href="/archives">▶Posts</a>
                
                    <a class="menu-item" href="/category">▶Category</a>
                
                    <a class="menu-item" href="/tag">▶Tag</a>
                
                    <a class="menu-item" href="/about">▶About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">PySpark常用操作总结——增删改查</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xjw924</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">March 22, 2020&nbsp;&nbsp;15:01:14</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Spark/">Spark</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="配置-amp-启动"><a href="#配置-amp-启动" class="headerlink" title="配置&amp;启动"></a>配置&amp;启动</h1><p>测试环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br><span class="line">--master yarn-client</span><br><span class="line">--driver-cores <span class="number">1</span></span><br><span class="line">--driver-memory <span class="number">2</span>g</span><br><span class="line">--num-executors <span class="number">2</span></span><br><span class="line">--executor-cores <span class="number">1</span></span><br><span class="line">--executor-memory <span class="number">3</span>g</span><br><span class="line"><span class="comment">#--queue ...</span></span><br><span class="line"><span class="comment">#--name ...</span></span><br></pre></td></tr></table></figure>
<p>可适当调大</p>
<p>建议：executor的cores:memory = 1:3<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext, SparkSession, Row, DataFrame</span><br><span class="line"></span><br><span class="line">sc = SparkContext()</span><br></pre></td></tr></table></figure></p>
<h1 id="格式转换-amp-保存"><a href="#格式转换-amp-保存" class="headerlink" title="格式转换&amp;保存"></a>格式转换&amp;保存</h1><p>spark dataframe持久化为hive表（若表名重复则替换旧表）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.write.saveAsTable(tableName)</span><br></pre></td></tr></table></figure>
<p>spark dataframe存成临时表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark_df.registerTempTable(<span class="string">"tableName"</span>) </span><br><span class="line">sql_result = spark.sql(<span class="string">"select * from tableName"</span>) <span class="comment"># 对临时表进行查询，可简化嵌套查询</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用registerTempTable注册表是一个临时表，生命周期只在所定义的sqlContext或hiveContext实例之中。换而言之，在一个sqlontext（或hiveContext）中registerTempTable的表不能在另一个sqlContext（或hiveContext）中使用。</p>
<p>而saveAsTable则是永久的，只要连接存在，spark再启的时候，这个表还是在的。</p>
</blockquote>
<p>spark dataframe转化为pandas dataframe</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas_df = spark_df.toPandas()</span><br></pre></td></tr></table></figure>
<p>注：数据量大会非常缓慢，数据量小适用</p>
<p>pandas dataframe转spark dataframe</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df = sqlContext.createDataFrame(pandas_df)</span><br></pre></td></tr></table></figure>
<p>本地csv转spark dataframe</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">raw_rdd = sc.text_file(<span class="string">"data.csv"</span>)</span><br><span class="line">df_spark = spark.createDataFrame(raw_rdd)</span><br></pre></td></tr></table></figure>
<h1 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h1><h2 id="查"><a href="#查" class="headerlink" title="查"></a>查</h2><h3 id="行元素查询操作"><a href="#行元素查询操作" class="headerlink" title="行元素查询操作"></a>行元素查询操作</h3><p>像SQL那样打印列表前20元素<br>show函数内可用int类型指定要打印的行数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.show()</span><br><span class="line">df.show(<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<p>以树的形式打印概要</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>
<p>获取头几行到本地：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">list = df.head(<span class="number">3</span>)   <span class="comment"># Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]</span></span><br><span class="line">list = df.take(<span class="number">5</span>)   <span class="comment"># Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]</span></span><br></pre></td></tr></table></figure></p>
<p>查询总行数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.count()</span><br></pre></td></tr></table></figure></p>
<p>取别名<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(df.age.alias(<span class="string">'age_value'</span>),<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure></p>
<p>查询某列为null的行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">df = df.filter(isnull(<span class="string">"col_a"</span>))</span><br></pre></td></tr></table></figure></p>
<p>输出list类型，list中每个元素是Row类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list = df.collect()</span><br></pre></td></tr></table></figure>
<p>注：此方法将所有数据全部导入到本地，返回一个Array对象</p>
<p>查询某列为null的行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">df = df.filter(isnull(<span class="string">"col_a"</span>))</span><br></pre></td></tr></table></figure></p>
<p>输出list类型，list中每个元素是Row类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list = df.collect()</span><br></pre></td></tr></table></figure></p>
<p>注：此方法将所有数据全部导入到本地，返回一个Array对象</p>
<p>去重set操作<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.select(<span class="string">'columns'</span>).distinct().show()</span><br></pre></td></tr></table></figure></p>
<p>跟py中的set一样，可以distinct()一下去重，同时也可以.count()计算剩余个数</p>
<p>随机抽样<br>随机抽样有两种方式，一种是在HIVE里面查数随机；另一种是在pyspark之中。</p>
<p>HIVE里面查数随机<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql = <span class="string">"select * from data order by rand()  limit 2000"</span></span><br></pre></td></tr></table></figure></p>
<p>pyspark之中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample = result.sample(<span class="literal">False</span>,<span class="number">0.5</span>,<span class="number">0</span>) <span class="comment"># randomly select 50% of lines</span></span><br></pre></td></tr></table></figure></p>
<h3 id="列元素操作"><a href="#列元素操作" class="headerlink" title="列元素操作"></a>列元素操作</h3><p>获取Row元素的所有列名：</p>
<p>r = Row(age=11, name=’Alice’)<br>print r.columns    #  [‘age’, ‘name’]<br>1<br>2<br>选择一列或多列：select<br>df[“age”]<br>df.age<br>df.select(“name”)<br>df.select(df[‘name’], df[‘age’]+1)<br>df.select(df.a, df.b, df.c)    # 选择a、b、c三列<br>df.select(df[“a”], df[“b”], df[“c”])    # 选择a、b、c三列</p>
<p>重载的select方法：<br>jdbcDF.select(jdbcDF( “id” ), jdbcDF( “id”) + 1 ).show( false)<br>1<br>会同时显示id列 + id + 1列</p>
<p>还可以用where按条件选择<br>jdbcDF .where(“id = 1 or c1 = ‘b’” ).show()<br>1<br>— 1.3 排序 —<br>orderBy和sort：按指定字段排序，默认为升序</p>
<p>train.orderBy(train.Purchase.desc()).show(5)<br>Output:<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|<br>|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|<br>|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|<br>|1005848| P00119342|     M|51-55|        20|            A|                         0|             1|                10|                13|              null|   23960|<br>|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|                16|              null|   23960|<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>only showing top 5 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>按指定字段排序。加个-表示降序排序</p>
<p>— 1.4 抽样 —<br>sample是抽样函数</p>
<p>t1 = train.sample(False, 0.2, 42)<br>t2 = train.sample(False, 0.2, 43)<br>t1.count(),t2.count()<br>Output:<br>(109812, 109745)<br>1<br>2<br>3<br>4<br>5<br>withReplacement = True or False代表是否有放回。<br>fraction = x, where x = .5，代表抽取百分比</p>
<p>— 1.5 按条件筛选when / between —<br>when(condition, value1).otherwise(value2)联合使用：<br>那么：当满足条件condition的指赋值为values1,不满足条件的则赋值为values2.<br>otherwise表示，不满足条件的情况下，应该赋值为啥。</p>
<p>demo1</p>
<blockquote>
<blockquote>
<blockquote>
<p>from pyspark.sql import functions as F<br>df.select(df.name, F.when(df.age &gt; 4, 1).when(df.age &lt; 3, -1).otherwise(0)).show()<br>+——-+——————————————————————————————+<br>| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0 END|<br>+——-+——————————————————————————————+<br>|Alice|                                                          -1|<br>|  Bob|                                                           1|<br>+——-+——————————————————————————————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>demo 2:多个when串联</p>
</blockquote>
</blockquote>
</blockquote>
<p>df = df.withColumn(‘mod_val_test1’,F.when(df[‘rand’] &lt;= 0.35,1).when(df[‘rand’] &lt;= 0.7, 2).otherwise(3))<br>1<br>between(lowerBound, upperBound)<br>筛选出某个范围内的值，返回的是TRUE or FALSE</p>
<blockquote>
<blockquote>
<blockquote>
<p>df.select(df.name, df.age.between(2, 4)).show()<br>+——-+—————————————-+<br>| name|((age &gt;= 2) AND (age &lt;= 4))|<br>+——-+—————————————-+<br>|Alice|                       true|<br>|  Bob|                      false|<br>+——-+—————————————-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>选择dataframe中间的特定行数<br>而我使用的dataframe前两种方法都没法解决。特点如下：</p>
</blockquote>
</blockquote>
</blockquote>
<p>特定列中的内容为字符串，并非数值，不能直接比较大小。<br>所选取数据为中间行，如第10~20行，不能用函数直接选取。<br>最终的解决方法如下：</p>
<p>首先添加行索引，然后选择特定区间内的行索引，从而选取特定中间行。<br>第一步，添加行索引。</p>
<p>from pyspark.sql.functions import monotonically_increasing_id</p>
<p>dfWithIndex = df.withColumn(“id”,monotonically_increasing_id())<br>1<br>2<br>3<br>第二步，筛选特定行。</p>
<p>dfWithIndex.select(dfWithIndex.name, dfWithIndex.id.between(50, 100)).show()<br>1<br>2、———— 增、改 ————<br>— 2.1 新建数据 —<br>有这么两种常规的新建数据方式：createDataFrame、.toDF()</p>
<p>sqlContext.createDataFrame(pd.dataframe())<br>1<br>是把pandas的dataframe转化为spark.dataframe格式，所以可以作为两者的格式转化</p>
<p>from pyspark.sql import Row<br>row = Row(“spe_id”, “InOther”)<br>x = [‘x1’,’x2’]<br>y = [‘y1’,’y2’]<br>new_df = sc.parallelize([row(x[i], y[i]) for i in range(2)]).toDF()<br>1<br>2<br>3<br>4<br>5<br>Row代表的是该数据集的列名。</p>
<p>— 2.2 新增数据列 withColumn—<br>withColumn是通过添加或替换与现有列有相同的名字的列，返回一个新的DataFrame</p>
<p>result3.withColumn(‘label’, 0)<br>1<br>或者案例</p>
<p>train.withColumn(‘Purchase_new’, train.Purchase /2.0).select(‘Purchase’,’Purchase_new’).show(5)<br>Output:<br>+————+——————+<br>|Purchase|Purchase_new|<br>+————+——————+<br>|    8370|      4185.0|<br>|   15200|      7600.0|<br>|    1422|       711.0|<br>|    1057|       528.5|<br>|    7969|      3984.5|<br>+————+——————+<br>only showing top 5 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br><strong>报错：</strong>AssertionError: col should be Column，一定要指定某现有列</p>
<p>有两种方式可以实现：</p>
<p>一种方式通过functions<br>from pyspark.sql import functions<br>result3 = result3.withColumn(‘label’,  functions.lit(0))<br>1<br>2<br>但是！！ 如何新增一个特别List??(参考：王强的知乎回复)<br>python中的list不能直接添加到dataframe中，需要先将list转为新的dataframe,然后新的dataframe和老的dataframe进行join操作, 下面的例子会先新建一个dataframe，然后将list转为dataframe，然后将两者join起来。</p>
<p>from pyspark.sql.functions import lit</p>
<p>df = sqlContext.createDataFrame(<br>    [(1, “a”, 23.0), (3, “B”, -23.0)], (“x1”, “x2”, “x3”))<br>from pyspark.sql.functions import monotonically_increasing_id<br>df = df.withColumn(“id”, monotonically_increasing_id())<br>df.show()<br>+—-+—-+——-+—-+<br>| x1| x2|   x3| id|<br>+—-+—-+——-+—-+<br>|  1|  a| 23.0|  0|<br>|  3|  B|-23.0|  1|<br>+—-+—-+——-+—-+<br>from pyspark.sql import Row<br>l = [‘jerry’, ‘tom’]<br>row = Row(“pid”, “name”)<br>new_df = sc.parallelize([row(i, l[i]) for i in range(0,len(l))]).toDF()<br>new_df.show()<br>+—-+——-+<br>|pid| name|<br>+—-+——-+<br>|  0|jerry|<br>|  1|  tom|<br>+—-+——-+<br>join_df = df.join(new_df, df.id==new_df.pid)<br>join_df.show()<br>+—-+—-+——-+—-+—-+——-+<br>| x1| x2|   x3| id|pid| name|<br>+—-+—-+——-+—-+—-+——-+<br>|  1|  a| 23.0|  0|  0|jerry|<br>|  3|  B|-23.0|  1|  1|  tom|<br>+—-+—-+——-+—-+—-+——-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32</p>
<h5 id="坑啊！！！其中，monotonically-increasing-id-生成的ID保证是单调递增和唯一的，但不是连续的。"><a href="#坑啊！！！其中，monotonically-increasing-id-生成的ID保证是单调递增和唯一的，但不是连续的。" class="headerlink" title="坑啊！！！其中，monotonically_increasing_id()生成的ID保证是单调递增和唯一的，但不是连续的。"></a><strong>坑啊！！！</strong>其中，monotonically_increasing_id()生成的ID保证是单调递增和唯一的，但不是连续的。</h5><p>所以，有可能，单调到1-140000，到了第144848个，就变成一长串：8845648744563，所以千万要注意！！</p>
<p>另一种方式通过另一个已有变量：<br>result3 = result3.withColumn(‘label’,  df.result*0 )<br>1<br>修改原有df[“xx”]列的所有值：<br>df = df.withColumn(“xx”, 1)<br>1<br>修改列的类型（类型投射）：<br>df = df.withColumn(“year2”, df[“year1”].cast(“Int”))<br>1<br>修改列名<br>jdbcDF.withColumnRenamed( “id” , “idx” )<br>1<br>— 2.3 过滤数据—</p>
<h5 id="过滤数据（filter和where方法相同）："><a href="#过滤数据（filter和where方法相同）：" class="headerlink" title="过滤数据（filter和where方法相同）："></a>过滤数据（filter和where方法相同）：</h5><p>df = df.filter(df[‘age’]&gt;21)<br>df = df.where(df[‘age’]&gt;21)<br>1<br>2<br>多个条件jdbcDF .filter(“id = 1 or c1 = ‘b’” ).show()</p>
<h5 id="对null或nan数据进行过滤："><a href="#对null或nan数据进行过滤：" class="headerlink" title="对null或nan数据进行过滤："></a>对null或nan数据进行过滤：</h5><p>from pyspark.sql.functions import isnan, isnull<br>df = df.filter(isnull(“a”))  # 把a列里面数据为null的筛选出来（代表python的None类型）<br>df = df.filter(isnan(“a”))  # 把a列里面数据为nan的筛选出来（Not a Number，非数字数据）<br>1<br>2<br>3<br>3、———— 合并 join / union ————<br>3.1 横向拼接rbind<br>result3 = result1.union(result2)<br>jdbcDF.unionALL(jdbcDF.limit(1)) # unionALL<br>1<br>2<br>— 3.2 Join根据条件 —<br>单字段Join<br>合并2个表的join方法：</p>
<p> df_join = df_left.join(df_right, df_left.key == df_right.key, “inner”)<br>1<br>其中，方法可以为：inner, outer, left_outer, right_outer, leftsemi.<br>其中注意，一般需要改为：left_outer</p>
<p>多字段join<br>joinDF1.join(joinDF2, Seq(“id”, “name”)）<br>1<br>混合字段<br>joinDF1.join(joinDF2 , joinDF1(“id” ) === joinDF2( “t1_id”))<br>1<br>跟pandas 里面的left_on,right_on</p>
<p>— 3.2 求并集、交集 —<br>来看一个例子，先构造两个dataframe：</p>
<p>sentenceDataFrame = spark.createDataFrame((<br>      (1, “asf”),<br>      (2, “2143”),<br>      (3, “rfds”)<br>    )).toDF(“label”, “sentence”)<br>sentenceDataFrame.show()</p>
<p>sentenceDataFrame1 = spark.createDataFrame((<br>      (1, “asf”),<br>      (2, “2143”),<br>      (4, “f8934y”)<br>    )).toDF(“label”, “sentence”)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12</p>
<h1 id="差集"><a href="#差集" class="headerlink" title="差集"></a>差集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).subtract(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p>
<p>+————+<br>|sentence|<br>+————+<br>|  f8934y|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9</p>
<h1 id="交集"><a href="#交集" class="headerlink" title="交集"></a>交集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).intersect(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p>
<p>+————+<br>|sentence|<br>+————+<br>|     asf|<br>|    2143|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</p>
<h1 id="并集"><a href="#并集" class="headerlink" title="并集"></a>并集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).union(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p>
<p>+————+<br>|sentence|<br>+————+<br>|     asf|<br>|    2143|<br>|  f8934y|<br>|     asf|<br>|    2143|<br>|    rfds|<br>+————+</p>
<p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15</p>
<h1 id="并集-去重"><a href="#并集-去重" class="headerlink" title="并集 + 去重"></a>并集 + 去重</h1><p>newDF = sentenceDataFrame1.select(“sentence”).union(sentenceDataFrame.select(“sentence”)).distinct()<br>newDF.show()</p>
<p>+————+<br>|sentence|<br>+————+<br>|    rfds|<br>|     asf|<br>|    2143|<br>|  f8934y|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>— 3.3 分割：行转列 —<br>有时候需要根据某个字段内容进行分割，然后生成多行，这时可以使用explode方法<br>　　下面代码中，根据c3字段中的空格将字段内容进行分割，分割的内容存储在新的字段c3_中，如下所示</p>
<p>jdbcDF.explode( “c3” , “c3_” ){time: String =&gt; time.split( “ “ )}<br>1</p>
<p>4 ———— 统计 ————<br>— 4.1 频数统计与筛选 ——<br>jdbcDF.stat.freqItems(Seq (“c1”) , 0.3).show()<br>1<br>根据c4字段，统计该字段值出现频率在30%以上的内容</p>
<p>— 4.2 分组统计—<br>交叉分析<br>train.crosstab(‘Age’, ‘Gender’).show()<br>Output:<br>+—————+——-+———+<br>|Age_Gender|    F|     M|<br>+—————+——-+———+<br>|      0-17| 5083| 10019|<br>|     46-50|13199| 32502|<br>|     18-25|24628| 75032|<br>|     36-45|27170| 82843|<br>|       55+| 5083| 16421|<br>|     51-55| 9894| 28607|<br>|     26-35|50752|168835|<br>+—————+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>groupBy方法整合：<br>train.groupby(‘Age’).agg({‘Purchase’: ‘mean’}).show()<br>Output:<br>+——-+————————-+<br>|  Age|    avg(Purchase)|<br>+——-+————————-+<br>|51-55|9534.808030960236|<br>|46-50|9208.625697468327|<br>| 0-17|8933.464640444974|<br>|36-45|9331.350694917874|<br>|26-35|9252.690632869888|<br>|  55+|9336.280459449405|<br>|18-25|9169.663606261289|<br>+——-+————————-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>另外一些demo：</p>
<p>df[‘x1’].groupby(df[‘x2’]).count().reset_index(name=’x1’)<br>1<br>分组汇总</p>
<p>train.groupby(‘Age’).count().show()<br>Output:<br>+——-+———+<br>|  Age| count|<br>+——-+———+<br>|51-55| 38501|<br>|46-50| 45701|<br>| 0-17| 15102|<br>|36-45|110013|<br>|26-35|219587|<br>|  55+| 21504|<br>|18-25| 99660|<br>+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>应用多个函数：</p>
<p>from pyspark.sql import functions<br>df.groupBy(“A”).agg(functions.avg(“B”), functions.min(“B”), functions.max(“B”)).show()<br>1<br>2<br>整合后GroupedData类型可用的方法（均返回DataFrame类型）：<br>avg(<em>cols)     ——   计算每组中一列或多列的平均值<br>count()          ——   计算每组中一共有多少行，返回DataFrame有2列，一列为分组的组名，另一列为行总数<br>max(</em>cols)    ——   计算每组中一列或多列的最大值<br>mean(<em>cols)  ——  计算每组中一列或多列的平均值<br>min(</em>cols)     ——  计算每组中一列或多列的最小值<br>sum(*cols)    ——   计算每组中一列或多列的总和<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>— 4.3 apply 函数 —<br>将df的每一列应用函数f：</p>
<p>df.foreach(f) 或者 df.rdd.foreach(f)<br>1<br>将df的每一块应用函数f：</p>
<p>df.foreachPartition(f) 或者 df.rdd.foreachPartition(f)<br>1<br>—— 4.4 【Map和Reduce应用】返回类型seqRDDs ——<br>map函数应用<br>可以参考：Spark Python API函数学习：pyspark API(1)</p>
<p>train.select(‘User_ID’).rdd.map(lambda x:(x,1)).take(5)<br>Output:<br>[(Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000002), 1)]<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>其中map在spark2.0就移除了，所以只能由rdd.调用。</p>
<p>data.select(‘col’).rdd.map(lambda l: 1 if l in [‘a’,’b’] else 0 ).collect()</p>
<p>print(x.collect())<br>print(y.collect())</p>
<p>[1, 2, 3]<br>[(1, 1), (2, 4), (3, 9)]<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>还有一种方式mapPartitions：</p>
<p>def _map_to_pandas(rdds):<br>    “”” Needs to be here due to pickling issues “””<br>    return [pd.DataFrame(list(rdds))]</p>
<p>data.rdd.mapPartitions(_map_to_pandas).collect()<br>1<br>2<br>3<br>4<br>5<br>返回的是list。</p>
<p>udf 函数应用</p>
<p>from pyspark.sql.functions import udf<br>from pyspark.sql.types import StringType<br>import datetime</p>
<h1 id="定义一个-udf-函数"><a href="#定义一个-udf-函数" class="headerlink" title="定义一个 udf 函数"></a>定义一个 udf 函数</h1><p>def today(day):<br>    if day==None:<br>        return datetime.datetime.fromtimestamp(int(time.time())).strftime(‘%Y-%m-%d’)<br>    else:<br>        return day</p>
<h1 id="返回类型为字符串类型"><a href="#返回类型为字符串类型" class="headerlink" title="返回类型为字符串类型"></a>返回类型为字符串类型</h1><p>udfday = udf(today, StringType())</p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>df.withColumn(‘day’, udfday(df.day))</p>
<p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>有点类似apply,定义一个 udf 方法, 用来返回今天的日期(yyyy-MM-dd):</p>
<p>———— 5、删除 ————<br>df.drop(‘age’).collect()<br>df.drop(df.age).collect()<br>1<br>2<br>dropna函数：</p>
<p>df = df.na.drop()  # 扔掉任何列包含na的行<br>df = df.dropna(subset=[‘col_name1’, ‘col_name2’])  # 扔掉col1或col2中任一一列包含na的行</p>
<p>1<br>2<br>3<br>ex:</p>
<p>train.dropna().count()<br>Output:<br>166821<br>1<br>2<br>3<br>填充NA包括fillna</p>
<p>train.fillna(-1).show(2)<br>Output:<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|<br>|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>only showing top 2 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>———— 6、去重 ————<br>6.1 distinct：返回一个不包含重复记录的DataFrame<br>返回当前DataFrame中不重复的Row记录。该方法和接下来的dropDuplicates()方法不传入指定字段时的结果相同。<br>　　示例：</p>
<p>jdbcDF.distinct()<br>1<br>6.2 dropDuplicates：根据指定字段去重<br>根据指定字段去重。类似于select distinct a, b操作<br>示例：</p>
<p>train.select(‘Age’,’Gender’).dropDuplicates().show()<br>Output:<br>+——-+———+<br>|  Age|Gender|<br>+——-+———+<br>|51-55|     F|<br>|51-55|     M|<br>|26-35|     F|<br>|26-35|     M|<br>|36-45|     F|<br>|36-45|     M|<br>|46-50|     F|<br>|46-50|     M|<br>|  55+|     F|<br>|  55+|     M|<br>|18-25|     F|<br>| 0-17|     F|<br>|18-25|     M|<br>| 0-17|     M|<br>+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>———— 7、 格式转换 ————<br>pandas-spark.dataframe互转<br>Pandas和Spark的DataFrame两者互相转换：</p>
<p>pandas_df = spark_df.toPandas()<br>spark_df = sqlContext.createDataFrame(pandas_df)<br>1<br>2<br>转化为pandas，但是该数据要读入内存，如果数据量大的话，很难跑得动</p>
<p>两者的异同：</p>
<p>Pyspark DataFrame是在分布式节点上运行一些数据操作，而pandas是不可能的；<br>Pyspark DataFrame的数据反映比较缓慢，没有Pandas那么及时反映；<br>Pyspark DataFrame的数据框是不可变的，不能任意添加列，只能通过合并进行；<br>pandas比Pyspark DataFrame有更多方便的操作以及很强大<br>转化为RDD<br>与Spark RDD的相互转换：</p>
<p>rdd_df = df.rdd<br>df = rdd_df.toDF()<br>1<br>2<br>———— 8、SQL操作 ————<br>DataFrame注册成SQL的表：</p>
<p>df.createOrReplaceTempView(“TBL1”)<br>1<br>进行SQL查询（返回DataFrame）：</p>
<p>conf = SparkConf()<br>ss = SparkSession.builder.appName(“APP_NAME”).config(conf=conf).getOrCreate()</p>
<p>df = ss.sql(“SELECT name, age FROM TBL1 WHERE age &gt;= 13 AND age &lt;= 19″)<br>1<br>2<br>3<br>4<br>———— 9、读写csv ————<br>在Python中，我们也可以使用SQLContext类中 load/save函数来读取和保存CSV文件：</p>
<p>from pyspark.sql import SQLContext<br>sqlContext = SQLContext(sc)<br>df = sqlContext.load(source=”com.databricks.spark.csv”, header=”true”, path = “cars.csv”)<br>df.select(“year”, “model”).save(“newcars.csv”, “com.databricks.spark.csv”,header=”true”)<br>1<br>2<br>3<br>4<br>其中，header代表是否显示表头。<br>其中主函数：</p>
<p>save(path=None, format=None, mode=None, partitionBy=None, **options)[source]<br>1<br>Parameters:</p>
<p>path – the path in a Hadoop supported file system</p>
<p>format – the format used to save</p>
<p>mode –</p>
<p>specifies the behavior of the save operation when data already<br>exists.</p>
<p>append: Append contents of this DataFrame to existing data.</p>
<p>overwrite: Overwrite existing data.</p>
<p>ignore: Silently ignore this operation if data already exists.</p>
<p>error (default case): Throw an exception if data already exists.</p>
<p>partitionBy – names of partitioning columns</p>
<p>options – all other string options</p>
<p>延伸一：去除两个表重复的内容<br>场景是要，依据B表与A表共有的内容，需要去除这部分共有的。<br>使用的逻辑是merge两张表，然后把匹配到的删除即可。</p>
<p>from pyspark.sql import functions<br>def LeftDeleteRight(test_left,test_right,left_col = ‘user_pin’,right_col = ‘user_pin’):<br>    print(‘right data process …’)<br>    columns_right = test_right.columns<br>    test_right = test_right.withColumn(‘user_pin_right’, test_right[right_col])<br>    test_right = test_right.withColumn(‘notDelete’,  functions.lit(0))</p>
<pre><code># 删除其余的
for col in columns_right:
    test_right = test_right.drop(col)
# 合并
print(&#39;rbind left and right data ...&#39;)
test_left = test_left.join(test_right, test_left[left_col] == test_right[&#39;user_pin_right&#39;], &quot;left&quot;)
test_left = test_left.fillna(1)
test_left = test_left.where(&#39;notDelete =1&#39;)
# 去掉多余的字段
for col in [&#39;user_pin_right&#39;,&#39;notDelete&#39;]:
    test_left = test_left.drop(col)
return test_left
</code></pre><p>%time  test_left = LeftDeleteRight(test_b,test_a,left_col = ‘user_pin’,right_col = ‘user_pin’)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>延伸二：报错<br>Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure: Lost task 3.3 in</p>
<p>1<br>2<br>解决方案</p>
<p>这里遇到的问题主要是因为数据源数据量过大，而机器的内存无法满足需求，导致长时间执行超时断开的情况，数据无法有效进行交互计算，因此有必要增加内存</p>
<p>参考：Spark常见问题汇总：<a href="https://my.oschina.net/tearsky/blog/629201" target="_blank" rel="noopener">https://my.oschina.net/tearsky/blog/629201</a><br>————————————————<br>版权声明：本文为CSDN博主「悟乙己」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/sinat_26917383/article/details/80500349" target="_blank" rel="noopener">https://blog.csdn.net/sinat_26917383/article/details/80500349</a></p>

        </div>

        
            <section class="post-copyright">
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://xjw924.github.io/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-zeng-shan-gai-cha/">http://xjw924.github.io/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-zeng-shan-gai-cha/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>如有错误，还望指正，谢谢！</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/Spark/"># Spark</a>
                    
                        <a href="/tags/Notes/"># Notes</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/30/pyspark-chang-yong-cao-zuo-zong-jie-mo-xing-ping-gu/">PySpark常用操作总结——模型评估</a>
            
            
            <a class="next" rel="next" href="/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-yu-chu-li/">PySpark常用操作总结——预处理</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© xjw924 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
