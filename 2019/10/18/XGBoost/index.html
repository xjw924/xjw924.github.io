<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="MachineLearning,Ensemble,Classification,XGBoost,">










<meta name="description" content="XGBoost面试题总结">
<meta name="keywords" content="MachineLearning,Ensemble,Classification,XGBoost">
<meta property="og:type" content="article">
<meta property="og:title" content="XGBoost">
<meta property="og:url" content="http://yoursite.com/2019/10/18/XGBoost/index.html">
<meta property="og:site_name" content="小徐小徐不断学习">
<meta property="og:description" content="XGBoost面试题总结">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-10-18T13:37:20.224Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="XGBoost">
<meta name="twitter:description" content="XGBoost面试题总结">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/18/XGBoost/">





  <title>XGBoost | 小徐小徐不断学习</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小徐小徐不断学习</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/18/XGBoost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xjw924">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小徐小徐不断学习">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">XGBoost</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-18T21:17:32+08:00">
                2019-10-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MachineLearning/" itemprop="url" rel="index">
                    <span itemprop="name">MachineLearning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          
              <div class="post-description">
                  XGBoost面试题总结
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="xgboost的原理"><a href="#xgboost的原理" class="headerlink" title="xgboost的原理"></a>xgboost的原理</h2><h2 id="XGBoost与GBDT的区别"><a href="#XGBoost与GBDT的区别" class="headerlink" title="XGBoost与GBDT的区别"></a>XGBoost与GBDT的区别</h2><p><strong>损失函数的改变：（导数和正则项的认识）</strong></p>
<ol>
<li><p><strong>正则项</strong>：XGBoost显式地<strong>加入了正则项</strong>来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。正则项里包含树的叶子节点个数、每个叶子节点上输出的score的平方和。</p>
</li>
<li><p><strong>几阶导数</strong>：传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对损失函数<strong>进行了二阶泰勒展开</strong>，得到一阶和二阶导数</p>
</li>
<li><p><strong>列采样</strong>：传统的GBDT在每轮迭代的时候使用全部的数据，XGBoost则采用与RF相类似的策略，支持对数据进行采样，同时还支持列抽样，不仅能降低过拟合，还能减少计算。</p>
</li>
<li><p><strong>缺失值</strong>：传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略</p>
</li>
<li><p><strong>基学习器</strong>：传统的GBDT以CART树作为基学习器，XGBoost支持多种类型的基分类器，比如线性分类器</p>
</li>
<li><p>xgboost工具支持自定义损失函数，只要函数可一阶和二阶求导。</p>
</li>
<li><p><strong>并行（计算特征的增益）</strong>：xgboost工具支持并行。注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。<strong>xgboost的并行是在特征粒度上的</strong>。因为决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，<strong>预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。</strong>这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
</li>
<li><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p>
</li>
<li><p>树节点在进行分裂时，需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p>
</li>
</ol>
<h2 id="xgboost有什么优势"><a href="#xgboost有什么优势" class="headerlink" title="xgboost有什么优势"></a>xgboost有什么优势</h2><ol>
<li><p>正则化</p>
<p>使用许多策略防止过拟合，如：正则化项、支持列抽样和Shrinkage等</p>
</li>
<li><p>并行处理</p>
<p>支持并行化，虽然树与树之间是串行关系，但是同层级节点可并行。具体地，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么<strong>各个特征的增益计算就可以开多线程进行</strong></p>
</li>
<li><p>高度的灵活性</p>
<p>XGBoost 允许用户定义自定义优化目标和评价标准</p>
</li>
<li><p>缺失值处理</p>
<p>XGBoost内置处理缺失值的规则，XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法</p>
</li>
<li><p>剪枝</p>
<p>当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。</p>
<p>XGBoost会一直分裂到指定的最大深度，然后回过头来剪枝。</p>
</li>
<li><p>内置交叉验证</p>
<p>XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数</p>
</li>
<li><p>在已有的模型基础上继续</p>
<p>XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势</p>
</li>
</ol>
<h2 id="怎么缓解过拟合的"><a href="#怎么缓解过拟合的" class="headerlink" title="怎么缓解过拟合的"></a>怎么缓解过拟合的</h2><p>​    正则化项</p>
<p>​    通过early_stopping提前停止训练</p>
<p>​    通过控制树的深度（max_depth）、树的个数、叶子结点数等参数</p>
<h2 id="xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？"><a href="#xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？" class="headerlink" title="xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？"></a>xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？</h2><p>一句话的解释：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</p>
<p>随机森林和GBDT都是属于集成学习的范畴。</p>
<p>对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差, 因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差,所以我们会采用深度很深甚至不剪枝的决策树。</p>
<p>对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差,所以对于每个基分类器来说，问题就在于如何选择方差更小的分类器，即更简单的分类器，所以选择了深度很浅的决策树。</p>
<h2 id="为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？"><a href="#为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？" class="headerlink" title="为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？"></a>为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？</h2><p>主要从三个方面来回答这个问题。</p>
<p>​    理论模型 （站在 vc-dimension 的角度）</p>
<p>​    实际数据</p>
<p>​    系统的实现 （主要基于xgboost）</p>
<p>通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。</p>
<p><strong>1） 站在理论模型的角度</strong></p>
<p>一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件：</p>
<p>​    模型在我们的训练数据上的表现要不错，也就是训练误差要足够小。</p>
<p>​    模型的自由度不能太大，以防过拟合。 </p>
<p>为什么 tree-ensemble 在实际中的效果很好呢？区别就在于 “模型的可控性”。</p>
<p>先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。</p>
<p>我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。</p>
<p>对于LR这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使模型在训练数据上取得足够高的正确率。而对于 tree-ensemble来说，解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。</p>
<p><strong>在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在LR中，初始的维数设定，或者说特征的高次转换对于vc-dimension的影响都是更大的。</strong>换句话说，tree-ensemble总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于LR这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.</p>
<p><strong>2) 站在数据的角度</strong></p>
<p>除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而<strong>基于树的算法通常抗噪能力更强。</strong>比如在树模型中，<strong>很容易对缺失值进行处理。除此之外，基于树的模型对于categorical feature也更加友好</strong>。</p>
<p>除了数据噪音之外，<strong>feature的多样性</strong>也是tree-ensemble模型能够取得更好效果的原因之一。通常在一个kaggle任务中，可能有年龄、收入特征、性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义样本之间的 kernel 或者 similarity（对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的分布通常并不敏感。</p>
<p><strong>3) 站在系统实现的角度</strong></p>
<p>除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征：</p>
<p>​    正确高效的实现某种模型。</p>
<p>​    系统具有灵活、深度的定制功能</p>
<p>​    系统简单易用</p>
<p>​    系统具有可扩展性, 可以从容处理更大的数据。</p>
<p>到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package.</p>
<p>在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop，cv等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。</p>
<p>最后，在可扩展性方面，xgboost提供了分布式训练（底层采用rabit接口），并且其分布式版本可以跑在各种平台之上，例如mpi, yarn, spark等等。</p>
<h2 id="XGBoost如何输出概率？算法上怎么输出概率的？"><a href="#XGBoost如何输出概率？算法上怎么输出概率的？" class="headerlink" title="XGBoost如何输出概率？算法上怎么输出概率的？"></a>XGBoost如何输出概率？算法上怎么输出概率的？</h2><p>sklearn接口的xgboost 分类器：</p>
<p>xgboost.predict() ：输出预测的类别，默认采用0.5做阈值</p>
<p>xgboost.predict_proba()：输出概率</p>
<p>XGBoost库的分类器：</p>
<p>predict() 只会输出概率</p>
<h2 id="XGBoost怎么处理缺失值？"><a href="#XGBoost怎么处理缺失值？" class="headerlink" title="XGBoost怎么处理缺失值？"></a>XGBoost怎么处理缺失值？</h2><p>缺失数据会被分到左子树和右子树分别计算损失，选择较优的那一个。</p>
<p>如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分到右子树。</p>
<h2 id="XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？"><a href="#XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？" class="headerlink" title="XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？"></a>XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？</h2><p>gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算</p>
<h2 id="XGBoost如何寻找最优特征？是有放回还是无放回的呢？"><a href="#XGBoost如何寻找最优特征？是有放回还是无放回的呢？" class="headerlink" title="XGBoost如何寻找最优特征？是有放回还是无放回的呢？"></a>XGBoost如何寻找最优特征？是有放回还是无放回的呢？</h2><p>XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性。XGB属于boosting集成学习方法，样本是不放回的，每轮计算样本不重复。</p>
<h2 id="XGBoost-v-s-Boosting"><a href="#XGBoost-v-s-Boosting" class="headerlink" title="XGBoost v.s. Boosting"></a>XGBoost v.s. Boosting</h2><p>是在GBDT的基础上对boosting算法进行的改进：</p>
<p>GBDT是用模型在数据上的负梯度作为残差的近似值，从而拟合残差；XGBoost也是拟合的在数据上的残差，但是它是用泰勒展式对模型损失残差的近似</p>
<p>同时XGBoost对模型的损失函数进行的改进，并加入了模型复杂度的正则项。</p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>可按照max_depth，min_child_weight，colsamplt_bytree，eta的顺序一个一个调，每次调的时候其他参数保持不变</p>
<p>（参考<a href="https://blog.csdn.net/han_xiaoyang/article/details/52665396#commentBox）" target="_blank" rel="noopener">https://blog.csdn.net/han_xiaoyang/article/details/52665396#commentBox）</a></p>
<p><strong>XGBoost的作者把所有的参数分成了三类：</strong></p>
<p>​    通用参数：宏观函数控制。</p>
<p>​    Booster参数：控制每一步的booster(tree/regression)。</p>
<p>​    学习目标参数：控制训练目标的表现。</p>
<p><strong>通用参数</strong></p>
<p>Booster [默认gbtree]</p>
<p>gbtree：基于树的模型</p>
<p>gbliner：线性模型</p>
<p>silent[默认0]：当这个参数值为1时，静默模式开启，不会输出任何信息。</p>
<p>nthread[默认值为最大可能的线程数]</p>
<p><strong>Booster参数</strong></p>
<p>尽管有两种booster可供选择，这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</p>
<p>eta学习率 [默认0.3]典型值为0.01-0.2</p>
<p>min_child_weight[默认1]决定最小叶子节点样本权重和。</p>
<p>这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</p>
<p>max_depth[默认6]树的最大深度，用来避免过拟合的。典型值：3-10</p>
<p>max_leaf_nodes树上最大的节点或叶子的数量。可以替代max_depth的作用，因为如果生成的是二叉树，一个深度为n的树最多生成n^2个叶子</p>
<p>gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了<strong>节点分裂所需的最小损失函数下降值</strong>。这个参数的值和损失函数息息相关，所以是需要调整的</p>
<p>subsample[默认1]控制对于<strong>每棵树随机采样的比例</strong>。减小这个参数的值，算法会更加保守，避免过拟合。但如果这个值设置得过小，可能会导致欠拟合。典型值：0.5-1</p>
<p>colsample_bytree[默认1]用来控制每棵随机<strong>采样的列数</strong>的占比(每一列是一个特征)。典型值：0.5-1</p>
<p>lambda[默认1]L2正则化项系数</p>
<p>alpha[默认1]L1正则化项系数，可以应用在很高维度的情况下，使得算法的速度更快。</p>
<p>scale_pos_weight[默认1]在各类别样本十分不平衡时，把这个参数设定为一个正值，可使算法更快收敛</p>
<p><strong>学习目标参数</strong></p>
<p>这些参数用来控制理想的优化目标和每一步结果的度量方法</p>
<p><strong>objective[默认reg:linear]</strong></p>
<p>这个参数定义需要被最小化的损失函数。最常用的值有：</p>
<p>​    binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。</p>
<p>​    multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。</p>
<p>​    multi:softprob和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</p>
<p><strong>eval_metric[默认值取决于objective参数的取值]</strong></p>
<p>对于回归问题，默认值是rmse，对于分类问题，默认值是error</p>
<p>常用值：</p>
<p>​    rmse 均方根误差</p>
<p>​    mae 平均绝对误差</p>
<p>​    logloss 负对数似然函数值</p>
<p>​    error 二分类错误率(阈值为0.5)</p>
<p>​    merror 多分类错误率</p>
<p>​    mlogloss 多分类logloss损失函数</p>
<p>​    auc 曲线下面积</p>
<p>​    seed(默认0)</p>
<p>设置它可以复现随机数据的结果，也可以用于调整参数</p>
<h2 id="XGB为什么要用二阶信息不用一阶-：更快更准确"><a href="#XGB为什么要用二阶信息不用一阶-：更快更准确" class="headerlink" title="XGB为什么要用二阶信息不用一阶 ：更快更准确"></a>XGB为什么要用二阶信息不用一阶 ：更快更准确</h2><p>​    由于之前求最优解的过程只是对平方损失函数进行的，一阶导数是残差，二阶是常数，当损失函数是其它函数时，展开就没有这种形式了，为了能够有个统一的形式，使用泰勒二阶展开。为了统一损失函数求导的形式以支持自定义损失函数。</p>
<p>​    <strong>二阶信息本身能够让梯度收敛的更快更准确，可以简单认为一阶导数引导梯度方向，二阶导数引导梯度方向如何变化。</strong></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a>
          
            <a href="/tags/Ensemble/" rel="tag"><i class="fa fa-tag"></i> Ensemble</a>
          
            <a href="/tags/Classification/" rel="tag"><i class="fa fa-tag"></i> Classification</a>
          
            <a href="/tags/XGBoost/" rel="tag"><i class="fa fa-tag"></i> XGBoost</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/18/RandomForest/" rel="next" title="Random Forest 随机森林">
                <i class="fa fa-chevron-left"></i> Random Forest 随机森林
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/18/Ensemble/" rel="prev" title="Ensemble">
                Ensemble <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">xjw924</p>
              <p class="site-description motion-element" itemprop="description">今天不学习！明天变垃圾！</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/xjw924" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost的原理"><span class="nav-number">1.</span> <span class="nav-text">xgboost的原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost与GBDT的区别"><span class="nav-number">2.</span> <span class="nav-text">XGBoost与GBDT的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost有什么优势"><span class="nav-number">3.</span> <span class="nav-text">xgboost有什么优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#怎么缓解过拟合的"><span class="nav-number">4.</span> <span class="nav-text">怎么缓解过拟合的</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？"><span class="nav-number">5.</span> <span class="nav-text">xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？"><span class="nav-number">6.</span> <span class="nav-text">为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost如何输出概率？算法上怎么输出概率的？"><span class="nav-number">7.</span> <span class="nav-text">XGBoost如何输出概率？算法上怎么输出概率的？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost怎么处理缺失值？"><span class="nav-number">8.</span> <span class="nav-text">XGBoost怎么处理缺失值？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？"><span class="nav-number">9.</span> <span class="nav-text">XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost如何寻找最优特征？是有放回还是无放回的呢？"><span class="nav-number">10.</span> <span class="nav-text">XGBoost如何寻找最优特征？是有放回还是无放回的呢？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGBoost-v-s-Boosting"><span class="nav-number">11.</span> <span class="nav-text">XGBoost v.s. Boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#调参"><span class="nav-number">12.</span> <span class="nav-text">调参</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGB为什么要用二阶信息不用一阶-：更快更准确"><span class="nav-number">13.</span> <span class="nav-text">XGB为什么要用二阶信息不用一阶 ：更快更准确</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xjw924</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
