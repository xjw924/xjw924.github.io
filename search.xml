<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hive基础知识及调优</title>
      <link href="/2020/02/09/hive-ji-chu-zhi-shi-ji-diao-you/"/>
      <url>/2020/02/09/hive-ji-chu-zhi-shi-ji-diao-you/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：尚硅谷大数据技术之Hive（<a href="https://www.bilibili.com/video/av65556024?from=search&amp;seid=1273196552526002153" target="_blank" rel="noopener">b站教学视频</a>）</p></blockquote><h1 id="Hive入门"><a href="#Hive入门" class="headerlink" title="Hive入门"></a>Hive入门</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><ul><li><p>概述：Hive是<strong>基于Hadoop</strong>的一个<strong>数据仓库工具</strong>，可以将结构化的数据文件映射为一张表，并提供<strong>类SQL</strong>查询功能</p></li><li><p>本质：<strong>将HiveQL转化为MapReduce程序</strong>，是一个分析引擎，相当于一个客户端</p><p>SQL与MapReduce的关系：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200209/QhHPHWrRFcws.png" alt="mark"></p></li><li><p>Hive是基于Hadoop的体现在：</p><ol><li>数据存储在HDFS上</li><li>数据底层实现用MapReduce（默认使用MR，也可以使用Spark）</li><li>执行程序运行在Yarn上</li></ol></li></ul><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>操作接口采用<strong>类SQL语法</strong>，<strong>提供快速开发的能力</strong>（简单、容易上手）。提供 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行，使不熟悉MapReduce 的用户也能很方便地对数据进行查询、汇总、分析。</li><li>Hive优势在于处理大数据，因此Hive常用于数据分析，对实时性要求不高的场合。  </li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li><p><strong>Hive的HQL表达能力有限</strong>。迭代式算法无法表达，因此数据挖掘方面不擅长（需要不断地对结果进行迭代，但MR在迭代方面很慢）。</p></li><li><p><strong>Hive的效率比较低</strong>。Hive自动生成的MapReduce作业，通常情况下不够智能化；Hive调优（包含SQL代码调优、资源调优）比较困难，粒度较粗（依赖模板，无法像MR精细化管理）；Hive因为Hive的执行延迟比较高，对于处理小数据没有优势。</p></li></ol><h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><blockquote><p><strong>Hive与SQL的区别</strong></p><p>Hive是一种基于Hadoop的数据仓库架构，定义了简单的类SQL查询语句（HQL），当输入HQL时，Hive会处理SQL将其转换为MapReduce。</p><p>Hive的表其实是HDFS的目录，Hive的数据对应目录下的文件。</p><p>SQL是一种查询语言的标准，Hive是基于Hadoop的数据仓库架构，提供了类SQL的查询接口。</p></blockquote><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><ul><li>Hive是建立在 Hadoop 之上的，所有Hive 的数据都是存储在 HDFS 中的。</li><li>数据库可以将数据保存在块设备或者本地文件系统中。</li></ul><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><ul><li>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</li><li>数据库中的数据通常是需要经常进行修改的，需要实时地进行增删改查。</li></ul><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ul><li>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。但由于 MapReduce 的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。</li><li>数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。</li></ul><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><ul><li>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。</li><li>数据库通常有自己的执行引擎。</li></ul><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><ul><li>Hive在查询数据的时候，由于没有索引需要扫描整个表，因此延迟较高。另外一个导致Hive执行延迟高的因素是 MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。</li><li>数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</li></ul><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><ul><li>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的。</li><li>数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。</li></ul><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><ul><li>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据。</li><li>数据库可以支持的数据规模较小。</li></ul><h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><div class="table-container"><table><thead><tr><th>Hive数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>2byte有符号整数</td><td>20</td></tr><tr><td><strong>INT</strong></td><td>4byte有符号整数</td><td>20</td></tr><tr><td><strong>BIGINT</strong></td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td><strong>DOUBLE</strong></td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td><strong>STRING </strong></td><td>字符系列。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td>字节数组</td></tr></tbody></table></div><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table></div><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>Hive的原子数据类型是可以进行隐式转换的，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，</p><p>但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><ul><li><p><strong>隐式类型转换规则如下</strong></p><ol><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li><li>TINYINT、SMALLINT、INT都可以转换为FLOAT。</li><li>BOOLEAN类型不可以转换为任何其它的类型。</li></ol></li><li><p><strong>可以使用CAST()操作显式地进行数据类型转换</strong></p><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；</p><p>如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式<strong>返回空值 NULL</strong>。</p></li></ul><h1 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h1><p>DDL(Data Definition Language)，数据定义语言</p><p>适用范围：对数据库中的某些对象（例如: database,table）进行管理（不会对具体的数据进行操作），如Create,Alter, Drop</p><ul><li><p><strong>DDL的操作对象</strong>： 包括数据库本身，以及数据库对象，如表、视图等等 </p></li><li><p><strong>DDL的主要语句</strong>： </p><ul><li>Create语句：可以创建数据库和数据库的一些对象。</li><li>Drop语句：可以删除数据表、索引、触发程序、条件约束以及数据表的权限等。</li><li>Alter语句：修改数据表定义及属性。</li></ul></li></ul><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>默认存储路径是/user/hive/warehouse/*.db </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive*'</span>;</span><br><span class="line">desc database db_hive;</span><br><span class="line">desc database extended db_hive;  <span class="comment">--显示数据库详细信息</span></span><br></pre></td></tr></table></figure><h2 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h2><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。</p><p><strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20170830'</span>);</span><br></pre></td></tr></table></figure><h2 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">--为表和列添加注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">--创建分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">--创建分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">--不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>] <span class="comment">--列分隔符</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">--指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">--指定表在HDFS上的存储位置</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="内部表与外部表"><a href="#内部表与外部表" class="headerlink" title="内部表与外部表"></a>内部表与外部表</h3><p>默认创建的表都是内部表。<br>Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(如/user/hive/warehouse)所定义的目录的子目录下。<br>当我们删除一个内部表时，Hive也会删除这个表中数据。内部表不适合和其他工具共享数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;  <span class="comment">--以'\t'结尾的行格式分隔字段</span></span><br></pre></td></tr></table></figure><p><strong>被external修饰的为外部表（external table）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">EXTERNAL</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>  <span class="comment">--以'\t'结尾的行格式分隔字段 </span></span><br><span class="line">location <span class="string">'/user/t2'</span>;</span><br></pre></td></tr></table></figure><p><strong>区别：</strong> </p><ol><li>内部表数据由Hive自身管理，外部表数据由HDFS管理 </li><li>内部表数据存储的位置默认是/user/hive/warehouse，会将数据移动到数据仓库指向的路径；外部表数据的存储位置由自己制定，仅记录数据所在的路径，不移动数据</li><li>删除内部表会直接删除元数据及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除，这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ol><p><strong>相互转换：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>); <span class="comment">--转为外部表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>); <span class="comment">--转为内部表</span></span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，按分区键的列值存储在表目录的子目录中，针对的是<strong>数据的存储路径</strong>，提供一个隔离数据和优化查询的便利方式。</p><p>Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集（通常会按照时间日/月进行分区）。</p><p>好处：<strong>可以更快地执行查询</strong>。使用分区列的名称创建一个子目录，当使用where子句进行查询时，<strong>只扫描特定子目录，而不是扫描整个表</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition   </span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">dname <span class="keyword">string</span>, </span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2</span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">    dname <span class="keyword">string</span>, </span><br><span class="line">    loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.dept_partition <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 增加（多个）分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看分区</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment">--查看分区表结构</span></span><br><span class="line">desc formatted dept_partition;</span><br></pre></td></tr></table></figure><h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><p>将表中记录按分桶键的哈希值分散进多个文件中，针对的是<strong>数据文件</strong>，将数据集分解成更容易管理的若干部分</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置参数（否则导入数据后不分桶）</span></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 使用分桶抽样查询（对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，<strong>决定抽样的比例</strong>。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>x表示<strong>从哪个bucket开始抽取</strong>，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>注意：x的值必须小于等于y的值，否则报错</p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><blockquote><p><strong>分区与分桶的区别</strong></p><p>分区和分桶的区别除了存储的格式不同外，最主要的是作用：</p><ul><li>分区表：细化数据管理，缩小MapReduce程序需要<strong>扫描的数据量</strong>。</li><li>分桶表：<strong>提高join查询的效率</strong>，在一份数据会被经常用来做连接查询的时候建立分桶，分桶字段就是连接字段，从而<strong>提高采样的效率</strong>。</li></ul></blockquote><h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 重命名表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> old_table <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">columns</span>(deptdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">change</span> <span class="keyword">column</span> dept_old dept_new <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">replace</span> <span class="keyword">columns</span>(deptno <span class="keyword">string</span>, dname</span><br><span class="line"> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> dept_partition;</span><br></pre></td></tr></table></figure><h1 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h1><p>DML(Data Manipulation Language，数据操控语言)</p><p>用于操作数据库对象中包含的数据，也就是说操作的单位是记录。</p><ul><li><p><strong>DML的操作对象：记录</strong></p></li><li><p><strong>DML的主要语句</strong>：</p><ul><li>Insert：向数据表张插入一条记录。</li><li>Delete：删除数据表中的一条或多条记录，也可以删除数据表中的所有记录，但操作对象仍是记录。</li><li>Update：用于修改已存在表中的记录的内容。</li></ul></li></ul><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><ul><li><strong>方法1：使用load语句向表中装载数据</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'/opt/module/datas/student.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> student [<span class="keyword">partition</span> (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><ol><li>load data：表示加载数据</li><li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li><li>inpath：表示加载数据的路径</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>into table：表示加载到哪张表</li><li>student：表示具体的表</li><li>partition：表示上传到指定分区</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建一张空表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(<span class="keyword">id</span> <span class="keyword">string</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 加载本地文件到hive</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure><ul><li><strong>方法2：通过查询语句向表中插入数据（Insert）</strong> </li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建一张分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基本插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基本模式插入（根据单张表查询结果）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多插入模式（根据多张表查询结果）</span></span><br><span class="line">from student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>方法3：创建表时通过Location指定加载数据路径</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建表，并指定在hdfs上的位置</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student5'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 上传数据到上述hdfs的指定位置</span></span><br><span class="line">dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</span><br></pre></td></tr></table></figure><ul><li><p><strong>方法4：Import数据到指定Hive表中</strong>  </p><p>注意：先用export导出后，再将数据导入</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><ul><li><strong>方法1：Hive Shell 命令导出</strong>  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在linux环境下执行</span></span><br><span class="line">hive -e <span class="string">'select * from default.student;'</span> &gt;/opt/module/datas/<span class="built_in">export</span>/student4.txt</span><br></pre></td></tr></table></figure><ul><li><strong>方法2：Insert导出</strong>  </li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 三种导出方式：</span></span><br><span class="line"><span class="comment">-- 将查询的结果导出到本地</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/export/student'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将查询的结果格式化导出到本地</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/export/student1'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将查询的结果导出到HDFS上(没有local)</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/user/atguigu/student2'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li><strong>方法3：Hadoop命令导出到本地</strong>  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><ul><li><strong>方法4：Export导出到HDFS上</strong>  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table default.student to &apos;/user/hive/warehouse/export/student&apos;;</span><br></pre></td></tr></table></figure><h2 id="清除表中数据"><a href="#清除表中数据" class="headerlink" title="清除表中数据"></a>清除表中数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><blockquote><h3 id="DROP、TRUNCATE和DELETE的区别"><a href="#DROP、TRUNCATE和DELETE的区别" class="headerlink" title="DROP、TRUNCATE和DELETE的区别"></a>DROP、TRUNCATE和DELETE的区别</h3><ol><li><p>TRUNCATE和DELETE只删除数据，DROP则删除整个表（结构和数据）。</p></li><li><p>表和索引所占空间。当表被TRUNCATE后，这个表和索引所占用的空间会恢复到初始大小；DELETE操作不会减少表或索引所占用的空间；DROP语句将表所占用的空间全释放掉。</p></li><li><p>DELETE语句为DML，这个操作会被放到rollback segment中，事务提交后才生效。如果有相应的tigger，执行的时候将被触发；TRUNCATE、DROP是DDL，操作立即生效，原数据不放到rollback segment中，不能回滚。</p></li><li><p>在没有备份情况下，谨慎使用DROP与TRUNCATE。删除部分数据行采用DELETE时，要注意结合where来约束影响范围。删除整个表用DROP。若想保留表而将表中数据删除，用TRUNCATE即可实现。</p></li><li><p><code>TRUNCATE TABLE 表名</code>速度快，而且效率高，因为TRUNCATE TABLE在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 </p></li></ol></blockquote><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*] </span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">  | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><h2 id="基本查询"><a href="#基本查询" class="headerlink" title="基本查询"></a>基本查询</h2><p>注意：</p><ol><li>SQL 语言大小写不敏感</li><li>SQL 可以写在一行或者多行</li><li>关键字不能被缩写也不能分行</li><li>各子句一般要分行写，使用缩进提高语句的可读性</li></ol><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A\</td><td>B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table></div><h3 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h3><div class="table-container"><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;(=)B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值<strong>大于等于</strong>B而且<strong>小于或等于</strong>C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS [NOT] NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table></div><h3 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h3><ol><li>使用LIKE运算选择类似的值。</li><li>选择条件可以包含字符或数字：% 代表零个或多个字符(任意个字符)，_ 代表一个字符。</li><li>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查找薪水中含有2的员工信息</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">RLIKE</span> <span class="string">'[2]'</span>;</span><br></pre></td></tr></table></figure><h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h3 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h3><p>having与where不同点：</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li><li>where后面不能写分组函数，而having后面可以使用分组函数。</li><li>having只用于group by分组统计语句。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="keyword">avg</span>(sal) avg_sal </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> deptno </span><br><span class="line"><span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><p>Hive支持通常的SQL JOIN语句，但是：</p><ul><li><p>只支持等值连接，<strong>不支持非等值连接</strong></p></li><li><p>连接谓词中<strong>不支持or</strong>，如</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line"><span class="keyword">from</span> emp e </span><br><span class="line"><span class="keyword">join</span> dept d </span><br><span class="line"><span class="keyword">on</span> e.deptno= d.deptno <span class="keyword">or</span> e.ename=d.ename; <span class="comment">--错误</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="多种连接"><a href="#多种连接" class="headerlink" title="多种连接"></a>多种连接</h3><ol><li><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。    </p></li><li><p>左（右）外连接：JOIN操作符左（右）边表中符合WHERE子句的所有记录将会被返回。</p></li><li><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p></li></ol><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1.创建位置表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.location</span><br><span class="line">(</span><br><span class="line">    loc <span class="built_in">int</span>,  </span><br><span class="line">    loc_name <span class="keyword">string</span>  </span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/location.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.location;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.多表连接查询</span></span><br><span class="line"><span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line"><span class="keyword">FROM</span> emp e </span><br><span class="line"><span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> d.deptno = e.deptno </span><br><span class="line"><span class="keyword">JOIN</span> location l</span><br><span class="line"><span class="keyword">ON</span> d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p><p>注意：为什么不是表d和表l先进行连接操作呢？这是因为<strong>Hive总是按照从左到右的顺序执行的</strong>。</p><h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><p>笛卡尔集会在下面条件下产生：</p><ol><li>省略连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接</li></ol><p><strong>在大型数据集上使用笛卡尔积会造成非常严重的生产事故！</strong></p><p>可使用以下选项进行限制（切换严格模式）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.mode=<span class="keyword">strict</span>;</span><br></pre></td></tr></table></figure><blockquote><p>严格模式：</p><p>防止用户执行那些可能意想不到的不好的影响的查询，开启严格模式可以禁止3种类型的查询。</p><ol><li><strong>对于分区表，分区表必须指定要查询的分区，否则不允许执行。</strong>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li><li><strong>对于使用了order by语句的查询，要求必须使用limit语句。</strong>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li><li><strong>限制笛卡尔积的查询。</strong></li></ol></blockquote><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h3><p>Order By：全局排序，一个Reducer</p><ul><li><p>ASC（ascend）: 升序（默认）</p></li><li><p>DESC（descend）: 降序</p></li></ul><p>ORDER BY 子句在SELECT语句的<strong>结尾</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 可以按照别名排序：按照员工薪水的2倍降序排序</span></span><br><span class="line"><span class="keyword">select</span> ename, sal*<span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多个列排序：按照部门和工资升序排序</span></span><br><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal;</span><br></pre></td></tr></table></figure><h3 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h3><p>Sort By：每个Reducer内部进行排序，对全局结果集来说不是排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1.设置reduce个数(可以通过set mapreduce.job.reduces;查看设定的reduce个数)</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.根据部门编号降序查看员工信息</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.将查询结果导入到文件中（按照部门编号降序排序）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/sortby-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 先按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/distribute-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h3><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定的数值，可以是20号和30号部门分到一个分区里面去。</p><blockquote><p><strong>order by，sort by，distribute by，cluster by 的区别</strong></p><ul><li><p>order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，会导致当输入规模较大时，消耗较长的计算时间。 </p></li><li><p>sort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则<strong>sort by只会保证同一个reducer的输出有序，并不保证全局有序</strong>。sort by不同于order by，它不受hive.mapred.mode属性的影响。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序，即可得到全部结果。</p></li><li><p>distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by再为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。  </p><ul><li><p>Distribute by和sort by的使用场景：</p><p>Map输出的文件大小不均/Reduce输出文件大小不均/小文件过多/文件超大</p></li></ul><ul><li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。 如果distribute by和sort by中所指定的列相同，可以缩写为cluster by该列以便同时指定两者所用的列。</li></ul></li></ul></blockquote><h2 id="其他查询"><a href="#其他查询" class="headerlink" title="其他查询"></a>其他查询</h2><h3 id="union和union-all"><a href="#union和union-all" class="headerlink" title="union和union all"></a>union和union all</h3><p>1、对<strong>重复</strong>结果的处理：union在进行表链接后会筛选掉重复的记录，union all不会去除重复记录。</p><p>2、对<strong>排序</strong>的处理：union将会按照字段的顺序进行排序；union all只是简单的将两个结果合并后就返回。</p><p>3、从<strong>效率</strong>上说，union all要比union快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用union all。 </p><h3 id="count-count-1-和count-字段-的区别"><a href="#count-count-1-和count-字段-的区别" class="headerlink" title="count(*), count(1)和count(字段)的区别"></a>count(*), count(1)和count(字段)的区别</h3><ul><li><p><strong>count(1)和count(*)</strong>：都会对全表进行扫描，统计所有记录的条数，包括那些为null的记录，count(1)会比count(*)更快，查询结果是完全一致的。</p></li><li><p><strong>count(1) and count(字段)</strong>：</p><ul><li>count(1)会统计表中的所有的记录数，包含字段为null的记录</li><li>count(字段)会统计该字段在表中出现的次数，不统计字段为null的记录。</li></ul></li></ul><h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h2><p>NVL：给值为NULL的数据赋值，它的格式是<code>NVL(string1, replace_with)</code>。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询：如果员工的comm为NULL，则用-1代替</span></span><br><span class="line"><span class="keyword">select</span> nvl(comm,<span class="number">-1</span>) <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询：如果员工的comm为NULL，则用领导id代替</span></span><br><span class="line"><span class="keyword">select</span> nvl(comm,mgr) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure><h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><p><code>CONCAT(string A/col, string B/col…)</code>：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p><code>CONCAT_WS(separator, str1, str2,...)</code>：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p><code>COLLECT_SET(col)</code>：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。（COLLECT_LIST类似，不去重）</p><p><strong>e.g. </strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">','</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th>base</th><th>name</th></tr></thead><tbody><tr><td>射手座,A</td><td>大海,凤姐</td></tr><tr><td>白羊座,A</td><td>孙悟空,猪八戒</td></tr><tr><td>白羊座,B</td><td>宋宋</td></tr></tbody></table></div><h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><p><code>EXPLODE(col)</code>：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：<code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code></p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p><strong>e.g.</strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie  to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑</td></tr><tr><td>《疑犯追踪》</td><td>动作</td></tr><tr><td>《疑犯追踪》</td><td>科幻</td></tr><tr><td>《疑犯追踪》</td><td>剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑</td></tr><tr><td>《Lie to me》</td><td>警匪</td></tr><tr><td>《Lie to me》</td><td>动作</td></tr><tr><td>《Lie to me》</td><td>心理</td></tr><tr><td>《Lie to me》</td><td>剧情</td></tr><tr><td>《战狼2》</td><td>战争</td></tr><tr><td>《战狼2》</td><td>动作</td></tr><tr><td>《战狼2》</td><td>灾难</td></tr></tbody></table></div><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p><p>CURRENT ROW：当前行</p><p>n PRECEDING：往前n行数据</p><p>n FOLLOWING：往后n行数据</p><p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>LAG(col,n)：往前第n行数据</p><p>LEAD(col,n)：往后第n行数据</p><p>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  orderdate,</span><br><span class="line">  <span class="keyword">cost</span>, </span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">  business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看顾客上次购买时间</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  orderdate,</span><br><span class="line">  <span class="keyword">cost</span>, </span><br><span class="line">  lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1,     </span><br><span class="line">  lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询前20%时间的订单信息</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">  <span class="keyword">from</span></span><br><span class="line">    business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">  sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="rank函数"><a href="#rank函数" class="headerlink" title="rank函数"></a>rank函数</h2><p>RANK()：排序相同时会重复，总数不会变</p><p>DENSE_RANK()：排序相同时会重复，总数会减少</p><p>ROW_NUMBER()：会根据顺序计算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">score;</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>name</th><th>subject</th><th>score</th><th>rank</th><th>dense_rank</th><th>row_number</th></tr></thead><tbody><tr><td>宋宋</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>1</strong></td></tr><tr><td>大海</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>2</strong></td></tr><tr><td>婷婷</td><td>英语</td><td>78</td><td><strong>3</strong></td><td><strong>2</strong></td><td><strong>3</strong></td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td><td><strong>4</strong></td><td><strong>3</strong></td><td><strong>4</strong></td></tr></tbody></table></div><h1 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h1><h2 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h2><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：<code>SELECT * FROM employees;</code>在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=more;</span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>有时Hive的输入数据量是非常小的。在这种情况下，<strong>为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多</strong>。对于大多数这种情况，<strong>Hive可以通过本地模式在单台机器上处理所有的任务</strong>。对于小数据集，执行时间可以明显被缩短。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启本地MR</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MR的最大输入数据量，当输入数据量小于这个值时采用local MR的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h2><h3 id="小表、大表join"><a href="#小表、大表join" class="headerlink" title="小表、大表join"></a>小表、大表join</h3><p><strong>将key相对分散，并且数据量小的表放在join的左边</strong>，这样可以有效减少内存溢出错误发生的几率；</p><p>再进一步，可以使用MapJoin让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p><strong>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</strong></p><h3 id="大表join小表"><a href="#大表join小表" class="headerlink" title="大表join小表"></a>大表join小表</h3><h4 id="空KEY过滤"><a href="#空KEY过滤" class="headerlink" title="空KEY过滤"></a>空KEY过滤</h4><p>有时join超时是因为<strong>某些key对应的数据太多</strong>，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> nullidtable <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> ) n  <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id; <span class="comment">-- 更快</span></span><br></pre></td></tr></table></figure><h4 id="空KEY转换"><a href="#空KEY转换" class="headerlink" title="空KEY转换"></a>空KEY转换</h4><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以<strong>表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上</strong>。  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure><h3 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。<strong>可以用MapJoin把小表全部加载到内存在map端进行join</strong>，避免reducer处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置自动选择Mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 大表小表的阈值设置（默认25M一下认为是小表）</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure><blockquote><p>MapJoin工作机制</p></blockquote><h3 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>并不是所有的聚合操作都需要在Reduce端完成，<strong>很多聚合操作都可以先在Map端进行部分聚合</strong>，最后在Reduce端得出最终结果。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 是否在Map端进行聚合，默认为True</span></span><br><span class="line">hive.map.aggr = true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在Map端进行聚合操作的条目数目</span></span><br><span class="line">hive.groupby.mapaggr.checkinterval = 100000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 有数据倾斜的时候进行负载均衡（默认是false）</span></span><br><span class="line">hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure><p>当选项设定为 true，生成的查询计划会有两个MR Job。</p><p>第一个MR Job中，<strong>Map的输出结果会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的Reduce中</strong>，从而达到负载均衡的目的；</p><p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证<strong>相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。  </p><h3 id="Count-Distinct"><a href="#Count-Distinct" class="headerlink" title="Count(Distinct)"></a>Count(Distinct)</h3><p>数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用<strong>先GROUP BY再COUNT</strong>的方式替换  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 原始</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 改进</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span><br></pre></td></tr></table></figure><p>虽然会多用一个Job来完成，但在数据量大的情况下，绝对是值得的。</p><h3 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h3><p>对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启动态分区功能（默认true，开启）</span></span><br><span class="line">hive.exec.dynamic.partition=true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</span></span><br><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在所有执行MR的节点上，最大一共可以创建多少个动态分区。</span></span><br><span class="line">hive.exec.max.dynamic.partitions=1000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</span></span><br><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure><h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><p>数据倾斜：由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p><p>主要表现：任务进度长时间维持在99%的附近，只有少量reduce子任务未完成，因为其处理的数据量和其他的 reduce 差异过大。 </p><p>数据倾斜的原因：</p><ul><li>key 分布不均匀</li><li>业务数据本身的特性（小表join大表）</li><li>建表考虑不周全</li><li>某些 HQL 语句本身就存在数据倾斜（count(distinct)，group by不和聚集函数搭配使用的时候）</li></ul><p><strong>目的：使map的输出数据更均匀的分布到reduce中去</strong></p><p><strong>在hive中产生数据倾斜的原因和解决方法：</strong></p><ul><li><strong>group by</strong><ul><li>使用Hive对数据做一些类型统计的时候遇到过<strong>某种类型的数据量特别多，而其他类型数据的数据量特别少</strong>。当按照类型进行group by的时候，会将相同的group by字段的reduce任务<strong>需要的数据拉取到同一个节点进行聚合</strong>，而当其中每一组的数据量过大时，会出现其他组的计算已经完成而这里还没计算完成，其他节点的一直等待这个节点的任务执行完成，所以会看到一直map 100% reduce 99%的情况。</li><li>解决方法：设置参数<code>set hive.map.aggr=true; set hive.groupby.skewindata=true;</code></li><li>原理：<code>set hive.map.aggr=true;</code>这个配置项代表<strong>是否在map端进行聚合</strong>。<code>set hive.groupby.skwindata=true;</code> 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，<strong>Map 的输出结果集合会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的</strong>；第二个 MR Job 再根据预处理的数据结果按照 Group By Key分布到 Reduce 中（这个过程可以<strong>保证相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。</li></ul></li><li><strong>map和reduce优化</strong></li><li>当出现小文件过多，需要合并小文件。可以通过<code>set hive.merge.mapfiles=true</code>来解决。<ul><li>单个文件大小稍稍大于配置的block块的大写，此时需要适当增加map的个数。解决方法：set mapred.map.tasks个数</li></ul></li><li>文件大小适中，但map端计算量非常大，如select id,count(*),sum(case when…),sum(case when…)…需要增加map个数。解决方法：set mapred.map.tasks个数，set mapred.reduce.tasks个数<ul><li>大表和小表join。解决方法：使用<strong>MapJoin</strong> 将小表加载到内存中（在Map阶段进行表之间的连接。而不需要进入到Reduce阶段才进行连接。这样就节省了在Shuffle阶段时要进行的大量数据传输。从而起到了优化作业的作用）。set hive.auto.convert.join=true;</li></ul></li><li><p><strong>count(distinct)</strong></p><ul><li>如果数据量非常大，执行如<code>select a, count(distinct b) from t group by a;</code>类型的SQL时，会出现数据倾斜的问题。</li><li>解决方法：使用sum…group by代替。如select a, sum(1) from (select a, b from t group by a, b) group by a;</li></ul></li><li><p><strong>遇到需要进行join的但是关联字段有数据为空</strong></p><ul><li>解决方法1：id为空的不参与关联</li><li>解决方法2：给空值分配随机的key值，其核心是将这些引起倾斜的值随机分发到Reduce</li></ul></li></ul><h3 id="合理设置Map数"><a href="#合理设置Map数" class="headerlink" title="合理设置Map数"></a>合理设置Map数</h3><ul><li><p>如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。因此需要<strong>减少map数</strong>。</p></li><li><p>比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。因此<strong>增加map数</strong>。</p></li><li><p>复杂文件增加Map数</p><ul><li>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</li><li>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</li></ul></li></ul><h3 id="合理设置Reduce数"><a href="#合理设置Reduce数" class="headerlink" title="合理设置Reduce数"></a>合理设置Reduce数</h3><ul><li><p>调整reduce个数方法一</p><ul><li>每个Reduce处理的数据量默认是256MB。<code>hive.exec.reducers.bytes.per.reducer=256000000</code></li><li>每个任务最大的reduce数默认为1009。<code>hive.exec.reducers.max=1009</code></li><li>计算reducer数的公式：N=min(参数2，总输入数据量/参数1)</li></ul></li><li><p>调整reduce个数方法二</p><ul><li>在hadoop的mapred-default.xml文件中修改</li><li>设置每个job的Reduce个数<code>set mapreduce.job.reduces = 15;</code></li></ul></li><li><p>reduce个数并不是越多越好</p><ul><li>过多的启动和初始化reduce也会消耗时间和资源；</li><li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li><li>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</li></ul></li></ul><h3 id="小文件进行合并"><a href="#小文件进行合并" class="headerlink" title="小文件进行合并"></a>小文件进行合并</h3><p>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p><strong>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。</strong></p><p>不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。在系统资源比较空闲的时候才有优势。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;       <span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>; <span class="comment">-- 同一个sql允许最大并行度，默认为8。</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop,Hive,Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列3-线性回归</title>
      <link href="/2020/02/07/bai-ban-tui-dao-xi-lie-3-xian-xing-hui-gui/"/>
      <url>/2020/02/07/bai-ban-tui-dao-xi-lie-3-xian-xing-hui-gui/</url>
      
        <content type="html"><![CDATA[<ul><li>最小二乘法（矩阵表达，几何意义）</li><li>概率角度：最小二乘法$\Longleftrightarrow$noise为正态分布的MLE</li><li>正则化（L1:Lasso；L2：Ridge）</li></ul><h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>Data：$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in R,i=1,\cdots,N$</p><p>$X_{N<em>p}=(x_1,x_2,…,x_N)^T,Y_{N</em>1}=(y_1,y_2,…,y_N)^T$</p><p>定义最小二乘法的损失函数</p><script type="math/tex; mode=display">\begin{align}L(w)&=\sum_{i=1}^{N}(w^Tx_i-y_i)^2\\&=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\\&=[w^T(x_1,\cdots,x_N)-(y_1,\cdots,y_N)][w^T(x_1,\cdots,x_N)-(y_1,\cdots,y_N)]^T\\&=(w^TX^T-Y^T)(Xw-Y)\\&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY\\&=w^TX^TXw-2w^TX^TY+Y^TY\end{align}</script><p>求解参数$w$</p><script type="math/tex; mode=display">\hat{w}=\arg\min L(w)\\\frac{\partial L(w)}{\partial w}=2X^TXw-2X^TY=0\\\Rightarrow w=(X^TX)^{-1}X^TY</script><h2 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a>几何意义</h2><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200208/UJ34aJcsDTUb.jpg" alt="mark"></p><h2 id="概率角度"><a href="#概率角度" class="headerlink" title="概率角度"></a>概率角度</h2><p>假定</p><script type="math/tex; mode=display">\varepsilon \sim N(0,\sigma^2)\\y=f(w)+\varepsilon=w^Tx+\varepsilon\\y|x;w\sim N(w^Tx,\sigma^2)\\P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\frac{-(y_i-w^Tx_i)^2}{2\sigma^2}}</script><p>定义对数似然函数为</p><script type="math/tex; mode=display">l(w)=\log P(Y|X;w)=\log \prod_{i=1}^{N}P(y_i|x_i;w)=\sum_{i=1}^{N}\log P(y_i|x_i;w)\\=\sum_{i=1}^{N}[\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}]</script><script type="math/tex; mode=display">\begin{align}\hat{w}&=\arg\max \limits_{w}l(w)\\&=\arg\max \limits_{w}\sum_{i=1}^{N}[-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}]\\&=\arg\min \limits_{w}\sum_{i=1}^{N}(y_i-w^Tx_i)^2\end{align}</script><p>可以得到极大似然估计（noise服从正态分布的条件下）与最小二乘估计等价</p><p>即LSE $\Longleftrightarrow$ MLE​(noise is Gaussian Dist)</p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>过拟合解决方法：</p><ol><li>加数据</li><li>特征选择/特征提取（如PCA）</li><li>正则化（对参数空间的约束）</li></ol><h2 id="正则化框架"><a href="#正则化框架" class="headerlink" title="正则化框架"></a>正则化框架</h2><script type="math/tex; mode=display">\arg\min \limits_{w}J(w)=\arg\min \limits_{w}[L(w)+\lambda P(w)]</script><p>其中$L(w)$代表损失函数，$P(w)$代表penalty</p><ul><li>L1：Lasso，$P(w)=||w||$</li><li>L2：Ridge岭回归（权值衰减），$P(w)=w^Tw$</li></ul><h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>令</p><script type="math/tex; mode=display">\begin{align}J(w)&=L(w)+\lambda P(w)\\&=\sum_{i=1}^{N}||w^Tx_i-y_i||^2+\lambda w^Tw\\&=(w^TX^T-Y^T)(XW-Y)+\lambda w^Tw\\&=w^TX^TXW-w^TX^TY-Y^TXW+Y^TY+\lambda w^Tw\\&=w^TX^TXW-2w^TX^TY+Y^TY+\lambda w^Tw\\&=w^T(X^TX+\lambda I)W-2w^TX^TY+Y^TY\end{align}</script><script type="math/tex; mode=display">\hat{w}=\arg\max \limits_{w}J(w)\\\frac{\partial J(w)}{\partial w}=2(X^TX+\lambda I)W-2X^TY=0\\\hat{w}=(X^TX+\lambda I)^{-1}X^TY</script><p>$X^TX$是半正定矩阵，从而保证$X^TX+\lambda I$一定是一个可逆矩阵，从而一定程度抑制过拟合</p><h2 id="贝叶斯角度"><a href="#贝叶斯角度" class="headerlink" title="贝叶斯角度"></a>贝叶斯角度</h2><script type="math/tex; mode=display">w \sim N(0,\sigma_0^2)\\P(w|y)=\frac{P(y|w)P(w)}{P(y)}</script><p>从MAP（最大后验估计）来估计$w$：</p><script type="math/tex; mode=display">\hat{w}=\arg\max \limits_{w}P(w|y)=\arg\max \limits_{w}P(y|w)P(w)</script><p>其中，由于$y|x;w\sim N(w^Tx,\sigma^2)$（条件同最小二乘法-概率角度，即noise服从$N(0,\sigma^2)$），则</p><script type="math/tex; mode=display">P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\frac{-(y_i-w^Tx_i)^2}{2\sigma^2}}</script><p>又因为$w \sim N(0,\sigma_0^2)$，则</p><script type="math/tex; mode=display">P(w)=\frac{1}{\sqrt{2\pi}\sigma_0}\exp{\frac{-||w||^2}{2\sigma^2_0}}</script><p>从而（为简化过程省略求和符号）</p><script type="math/tex; mode=display">\begin{align}\hat{w}&=\arg\max \limits_{w}P(y|w)P(w)\\&=\arg\max \limits_{w}\log [P(y|w)P(w)]\\&=\arg\max \limits_{w}\log\exp[-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}-\frac{||w||^2}{2\sigma^2_0}]\\&=\arg\min \limits_{w}[\frac{(y_i-w^Tx_i)^2}{2\sigma^2}+\frac{||w||^2}{2\sigma^2_0}]\\&=\arg\min \limits_{w}[(y_i-w^Tx_i)^2+\frac{\sigma^2}{\sigma^2_0}||w||^2]\end{align}</script><p>因此最终的MAP估计为</p><script type="math/tex; mode=display">\hat{w}=\arg\min \limits_{w}[\sum_{i=1}^{N}(y_i-w^Tx_i)^2+\frac{\sigma^2}{\sigma^2_0}||w||^2]</script><p>因此加上正则化的最小二乘估计 $\Longleftrightarrow$ MAP(noise服从高斯分布，先验也服从高斯分布)</p><blockquote><p>总结：</p><p>LSE $\Longleftrightarrow$ MLE(noise is Gaussian Dist)</p><p>Regularized LSE $\Longleftrightarrow$ MAP(noise is Gaussian Dist, prior is Gaussian Dist)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning, Regression, Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列6-支持向量机SVM</title>
      <link href="/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/"/>
      <url>/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>b站up主： <strong>shuhuai008</strong> </p><p><a href="https://www.bilibili.com/video/av70839977" target="_blank" rel="noopener">机器学习-白板推导系列-合集</a> 学习笔记</p></blockquote><p>SVM有三宝：间隔，对偶，核技巧~</p><ul><li>hard-margin SVM</li><li>soft-margin SVM</li><li>kernel SVM</li></ul><h1 id="硬间隔SVM"><a href="#硬间隔SVM" class="headerlink" title="硬间隔SVM"></a>硬间隔SVM</h1><h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><p>找到一个超平面$w^Tx+b=0$，使得两类能够完全分开，且间隔最大。</p><p>Data：$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{+1,-1\}$</p><p>最大间隔分类器</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max margin(w,b)\\s.t.y_i(w^Tx+b)>0,\forall i=1,…,N\end{array} \right.</script><p>其中定义$N$个样本点到直线的最小距离为</p><script type="math/tex; mode=display">margin(w,b)=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}distance=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}\frac{1}{||w||}|w^Tx_i+b|=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}\frac{1}{||w||}y_i(w^Tx_i+b)</script><blockquote><p>上式用到了点到直线的距离公式</p><script type="math/tex; mode=display">distance=\frac{1}{||w||}|w^Tx_i+b|</script></blockquote><p>因此最大间隔分类器可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{w,b} \mathop{\min}\limits_{x_i,i=1,\cdots,N}\frac{1}{||w||}y_i(w^Tx_i+b) \\s.t.y_i(w^Tx+b)>0,\forall i=1,…,N\end{array} \right.</script><p>可以转化为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{w,b} \frac{1}{||w||}\mathop{\min}\limits_{x_i,i=1,\cdots,N}y_i(w^Tx_i+b)\\\exists \gamma>0,s.t.\mathop{\min}\limits_{x_i,y_i,i=1,\cdots,N}y_i(w^Tx_i+b)=\gamma\end{array} \right.</script><p>可以令$\gamma=1$（无论$\gamma$为多少都可以进行缩放至1）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \frac{1}{2}w^Tw\\s.t.y_i(w^Tx_i+b)\ge1,\forall i=1,…,N\end{array} \right.</script><p>（使用$\frac{1}{2}$作为系数仅为了求导方便）</p><p>可以看出该模型是凸二次规划问题，有$N$个约束。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><h3 id="原问题primal-problem"><a href="#原问题primal-problem" class="headerlink" title="原问题primal problem"></a>原问题primal problem</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \frac{1}{2}w^Tw\\s.t. 1-y_i(w^Tx_i+b)\le0,\forall i=1,…,N\end{array} \right.</script><p>使用拉格朗日乘子法，定义拉格朗日函数为</p><script type="math/tex; mode=display">L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b))</script><p>其中$\lambda_i\ge0$，将求解问题转化为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)\\s.t. \lambda_i\ge0\end{array} \right.</script><p>从而将带约束的问题转化成无约束的（对$w,b$无约束）</p><blockquote><p><strong>Q：如何证明带约束的和无约束的是等价的？</strong></p><p>如果$1-y_i(w^Tx_i+b)&gt;0$，则$\max\limits_{\lambda } L(w,b,\lambda)=\frac{1}{2}w^Tw+\infty=\infty$</p><p>如果$1-y_i(w^Tx_i+b)\le0$，则$\max\limits_{\lambda } L(w,b,\lambda)=\frac{1}{2}w^Tw+0=\frac{1}{2}w^Tw$，$\min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)=\min\limits_{w,b}\frac{1}{2}w^Tw$</p><p>因此$\min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)=\min\limits_{w,b}\{\infty,\frac{1}{2}w^Tw\}=\min\limits_{w,b}\frac{1}{2}w^Tw $</p><p>且去除了$1-y_i(w^Tx_i+b)&gt;0$部分，即最优解一定满足$1-y_i(w^Tx_i+b)\le0$</p><p>（奇妙！！）</p></blockquote><h3 id="对偶问题dual-problem"><a href="#对偶问题dual-problem" class="headerlink" title="对偶问题dual problem"></a>对偶问题dual problem</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{\lambda }\min\limits_{w,b}  L(w,b,\lambda)\\s.t. \lambda_i\ge0\end{array} \right.</script><p>经过某种神秘的证明再结合本身成立的“凤尾$\ge$鸡头”弱对偶关系（$\min\max L\ge\max\min L$），可以证明该情况下（凸优化二次问题）满足强对偶关系，即弱对偶关系“$\ge$”等价于强对偶关系“$=$”（$\min\max L=\max\min L$），因此原问题等价于对偶问题。</p><p>对$b$求偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\frac{\partial }{\partial b}[\sum_{i=1}^{N}\lambda_i-\sum_{i=1}^{N}\lambda_iy_i(w^Tx_i+b)]=-\sum_{i=1}^{N}\lambda_iy_i=0\\\Rightarrow\sum_{i=1}^{N}\lambda_iy_i=0</script><p>代入到拉格朗日函数中，从而转化为</p><script type="math/tex; mode=display">L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b))=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)</script><p>再对$w$求偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}=\frac{\partial }{\partial w}[\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)]=w-\sum_{i=1}^{N}\lambda_iy_ix_i=0\\\Rightarrow w=\sum_{i=1}^{N}\lambda_iy_ix_i</script><p>再代入到拉格朗日函数中，从而转化为</p><script type="math/tex; mode=display">\begin{align}L(w,b,\lambda)&=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)\\&=\frac{1}{2}(\sum_{i=1}^{N}\lambda_iy_ix_i)^T(\sum_{j=1}^{N}\lambda_jy_jx_j)+\sum_{i=1}^{N}\lambda_i-\sum_{i=1}^{N}\lambda_iy_i(\sum_{i=1}^{N}\lambda_iy_ix_i)^Tx_i\\&=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum_{i=1}^{N}\lambda_i\end{align}</script><p>最终的优化问题可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{\lambda } \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i\lambda_jy_iy_jx_i^Tx_j-\sum_{i=1}^{N}\lambda_i\\s.t. \lambda_i\ge0,\sum_{i=1}^{N}\lambda_iy_i=0\end{array} \right.</script><h3 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**}  \frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\\\lambda_i(1-y_i(w^Tx_i+b))=0\\\lambda_i \ge 0\\1-y_i(w^Tx_i+b)\le0\end{array} \right.</script><p>原对偶问题具有强对偶关系（默认情况下满足弱对偶关系且是凸二次规划问题且约束是线性的）$\Longleftrightarrow$满足KKT条件</p><blockquote><p>KKT条件（后有详细）</p><p>第一行：梯度条件</p><p>第二行$\lambda_i(1-y_i(w^Tx_i+b))=0$：松弛互补条件</p><p>第三四行：可行条件</p></blockquote><p>已经求出最优的$w$解为</p><script type="math/tex; mode=display">w^*=\sum_{i=1}^{N}\lambda_iy_ix_i</script><p>又因为松弛互补条件，一定存在一个样本点$(x_k,y_k)$使得$1-y_k(w^Tx_k+b)=0$</p><blockquote><p>若不存在，则只能所有$\lambda_i=0$，则目标函数$L(w,b,\lambda)=\frac{1}{2}w^Tw$，没有任何限制</p><p>对于少数$\lambda_i\neq 0$的样本称为支持向量 ，实际上只有这些样本在真正起作用</p></blockquote><p>对其进行化简得到$b^*$</p><script type="math/tex; mode=display">y_k(w^Tx_k+b)=1\\y_k^2(w^Tx_k+b)=y_k\\\Rightarrow b^*=y_k-w^Tx_k=y_k-\sum_{i=1}^{N}\lambda_iy_ix_i^Tx_k</script><p>因此决策超平面为</p><script type="math/tex; mode=display">f(x)=sign({w^*}^Tx+b^*)</script><h1 id="软间隔SVM"><a href="#软间隔SVM" class="headerlink" title="软间隔SVM"></a>软间隔SVM</h1><p>数据是不可分的，或数据是可分的但存在一定噪声，此时应使用软间隔SVM</p><p>soft：允许一点点错误，用loss来表达，加到损失函数上</p><ul><li>定义$loss=\sum_{i=1}^{N}I\{y_i(w^Tx_i+b)&lt;1\}$，但缺点在于该loss function不连续，不能求导，因此不使用该loss function</li><li>定义loss为距离（合页损失hinge loss），$loss=\sum_{i=1}^{N}\max\{0,1-y_i(w^Tx_i+b)\}$，函数是连续的√<ul><li>如果$y_i(w^Tx_i+b)\ge1,loss=0$</li><li>如果$y_i(w^Tx_i+b)&lt;1,loss=1-y_i(w^Tx_i+b)$</li></ul></li></ul><p>引入$\xi_i=1-y_i(w^Tx_i+b),\xi_i\ge0$，得到软间隔SVM的最终形式</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{\lambda } \frac{1}{2}w^Tw+C\sum_{i=1}^{N}\xi_i\\s.t. y_i(w^Tx_i+b)\ge1-\xi_i\end{array} \right.</script><h1 id="约束优化问题"><a href="#约束优化问题" class="headerlink" title="约束优化问题"></a>约束优化问题</h1><blockquote><p>这一节实际和SVM没有直接关系，是最优化的内容</p></blockquote><ul><li>原问题（Primal Problem）</li></ul><p>原问题的一般表达形式（原问题的有约束形式）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x\in R^p }f(x)\\s.t. &m_i(x)\le0, i=1,\cdots,M\\&n_j(x)\le0, j=1,\cdots,N\end{array} \right.</script><p>写成拉格朗日函数的形式</p><script type="math/tex; mode=display">L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_in_i(x)</script><p>可以将原问题转化为（原问题的无约束形式）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x}\max\limits_{\lambda,\eta} L(x,\lambda,\eta)\\s.t. \lambda_i\ge0\end{array} \right.</script><blockquote><p>Q：为何两者是等价的？</p><p>如果$x_i$违反约束$m_i(x)\le0$，即$m_i(x)\gt0$，则$\max\limits_{\lambda} L\rightarrow\infty$</p><p>如果$x_i$符合约束$m_i(x)\le0$，则$\max\limits_{\lambda} L\nrightarrow\infty$</p><p>$\min\limits_{x}\max\limits_{\lambda} L=\min\limits_{x}\{\max\limits_{\lambda} L,\infty\}=\min\limits_{x}\max\limits_{\lambda}  L$</p><p>实际上是进行了过滤，其中蕴含了约束$m_i(x)\le0$，自动去掉了$m_i(x)\gt0$的部分</p></blockquote><ul><li>对偶问题（Dual Problem）</li></ul><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{\lambda,\eta} \min\limits_{x}L(x,\lambda,\eta)\\s.t. \lambda_i\ge0\end{array} \right.</script><blockquote><p>原问题是关于$x$的函数，对偶问题是关于$\lambda,\eta$的函数</p></blockquote><h2 id="弱对偶性"><a href="#弱对偶性" class="headerlink" title="弱对偶性"></a>弱对偶性</h2><p>弱对偶性：对偶问题(d)$\le$原问题(p)</p><p>之前使用了“凤尾$\ge$鸡头”的比喻来说明，此处在理论上进行证明：</p><script type="math/tex; mode=display">\max\limits_{\lambda,\eta} \min\limits_{x} L(x,\lambda,\eta)\le \min\limits_{x}\max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><p>证：</p><p>由于</p><script type="math/tex; mode=display">\min\limits_{x} L(x,\lambda,\eta) \le L(x,\lambda,\eta) \le \max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><blockquote><p>可以理解为下面只是变量而已，好比一个多元函数，在某处取最大值还是最小值时，这时你可以只看成关于不同变量的也是可以的，因为函数最大最小值是确定的</p></blockquote><p>设</p><script type="math/tex; mode=display">A(\lambda,\eta)=\min\limits_{x} L(x,\lambda,\eta) \\B(x)=\max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><p>则上式转化为</p><script type="math/tex; mode=display">A(\lambda,\eta) \le B(x)\\\Rightarrow A(\lambda,\eta) \le \min\limits_{x}  B(x)\\\Rightarrow \max\limits_{\lambda,\eta} A(\lambda,\eta) \le \min\limits_{x}  B(x)\\\Rightarrow \max\limits_{\lambda,\eta} \min\limits_{x} L(x,\lambda,\eta) \le \min\limits_{x}  \max\limits_{\lambda,\eta} L(x,\lambda,\eta)\\</script><p>从而得证。</p><h2 id="对偶性的几何解释"><a href="#对偶性的几何解释" class="headerlink" title="对偶性的几何解释"></a>对偶性的几何解释</h2><p>简化后的优化问题可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x\in R^p }f(x)\\s.t. m_i(x)\le0, i=1,\cdots,M\end{array} \right.</script><p>其中定义域$D=\mathrm{dom}f\cap \mathrm{dom} m_i$</p><p>拉格朗日函数定义为</p><script type="math/tex; mode=display">L(x,\lambda)=f(x)+\lambda m_1(x),\lambda\ge0</script><p>原问题最优解定义为</p><script type="math/tex; mode=display">p^*=\min f(x)</script><p>对偶问题最优解定义为</p><script type="math/tex; mode=display">d^*=\max\limits_{\lambda} \min\limits_{x} L(x,\lambda)</script><p>定义区域$G$为（一般化区域$G$认为其为非凸集）</p><script type="math/tex; mode=display">G=\{(m_1(x),f(x))|x\in D\}=\{(u,t)|x\in D\}</script><p>因此$p^*$可以表达为</p><script type="math/tex; mode=display">p^*=\inf\{t|(u,t)\in G,u\le 0\}</script><p>（集合中的下确界相当于集合中的最小值）</p><p>$d^*$可以表达为</p><script type="math/tex; mode=display">\begin{align}d^*&=\max\limits_{\lambda} \min\limits_{x} L(x,\lambda)\\&= \max\limits_{\lambda} \min\limits_{x} (t+\lambda u)\\&= \max\limits_{\lambda} g(\lambda)\end{align}</script><p>其中$g(\lambda)=\min\limits_{x} (t+\lambda u)=\inf\{t+\lambda u|(u,t)\in G\}$</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200207/dEyLlYLvko39.jpg" alt="mark"></p><h2 id="slater-condition"><a href="#slater-condition" class="headerlink" title="slater condition"></a>slater condition</h2><p>凸优化 + slater条件 $\Rightarrow$ 强对偶条件</p><p>slater condition定义：</p><script type="math/tex; mode=display">\exists \hat{x} \in reliant D\\s.t. \forall i=1,\cdots,M, m_i(\hat{x})<0</script><p>其中reliant为relative interior（相对内部），去除边界的部分，内点的集合</p><ol><li>对于大多数凸优化，slater条件成立</li><li>放松的slater条件：如果$M$中有$K$个仿射函数，只需要校验剩余的$M-K$个函数是否满足上述条件。（仿射函数：一阶的多项式函数）</li></ol><p>凸二次规划：目标函数$f$是凸函数，限制条件$m_i$是仿射函数（线性函数一定是仿射函数），$n_j$是仿射函数。</p><p><strong>SVM是凸二次规划问题，满足强对偶条件，因此可以使用KKT条件直接求解。</strong></p><h2 id="KKT条件-1"><a href="#KKT条件-1" class="headerlink" title="KKT条件"></a>KKT条件</h2><ul><li><p><strong>可行条件</strong></p><script type="math/tex; mode=display">m_i(x^{*})\le0, n_j(x^{*})=0, \lambda^{*} \ge0</script></li><li><p><strong>互补松弛条件</strong>：$\lambda^*_im_i=0, \forall i=1,\cdots,M$</p></li></ul><script type="math/tex; mode=display">\begin{align}d^*&=\max\limits_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^*,\eta^*)\\&=\min\limits_{x}L(x,\lambda^*,\eta^*)\\&\le L(x^*,\lambda^*,\eta^*)\\&=f(x^*)+\sum_{i}\lambda^*_im_i+\sum_{j}\eta^*_jn_j\\&\le f(x^*)=p^*\end{align}</script><p>且又因为强对偶关系</p><script type="math/tex; mode=display">d^*=p^*</script><p>因此不等号只能取等号，且$\sum_{j}\eta^*_jn_j=0$，因此由(10)-(11)得</p><script type="math/tex; mode=display">\sum_{i}\lambda^*_im_i=0\\\Rightarrow \lambda^*_im_i=0, \forall i=1,\cdots,M</script><p>（若存在有一个$\lambda^*_im_i&lt;0$，由于不存在小于0的部分，无法抵消，和无法等于0）</p><ul><li><strong>梯度为0</strong>：<script type="math/tex; mode=display">\frac{\partial L(x,\lambda^{*},\eta^{*})}{\partial x}|_{x=x^{*}}=0</script>由(8)-(9)可得<script type="math/tex; mode=display">\min\limits_{x}L(x,\lambda^*,\eta^*)=L(x^*,\lambda^*,\eta^*)</script>因此<script type="math/tex; mode=display">\frac{\partial L}{\partial x}|_{x=x^*}=0</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning, Classification, Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree 决策树算法及其应用</title>
      <link href="/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/"/>
      <url>/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-qi-ying-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="分类树理论"><a href="#分类树理论" class="headerlink" title="分类树理论"></a>分类树理论</h1><p>递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。</p><ul><li><strong>信息熵</strong>：表示随机变量不确定性的度量</li></ul><script type="math/tex; mode=display">Info(D)=-\sum_{i=1}^{n}{p_ilog_2p_i}</script><p>​        其中$p_i$指样本集合$D$中第$i$类样本所占比例。</p><p>​        信息熵描述样本集合$D$携带的信息量。 信息量越大（值变化越多），则越不确定，越不容易被预测。</p><ul><li><strong>信息熵特点</strong>： <ol><li>不同类别的概率分布越均匀，信息熵越大</li><li>类别个数越多，信息熵越大</li><li>信息熵越大，越不容易被预测（变化个数多，变化之间区分小，则越不容易被预测；对于确定性问题，信息熵为0）</li></ol></li></ul><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p>根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分此时决策树停止生长。</p><p>有三种选择最优特征的标准：信息增益、增益率和基尼指数，分别对应了三种决策树算法：ID3，C4.5，CART。</p><h3 id="ID3算法：信息增益"><a href="#ID3算法：信息增益" class="headerlink" title="ID3算法：信息增益"></a>ID3算法：信息增益</h3><p>计算每个特征的信息增益，并比较它们的大小，每一次都选择使得<strong>信息增益最大</strong>的特征进行分裂，递归地构建决策树。信息增益越大，意味着使用某个特征进行划分所获得的纯度的提升越大。</p><p>信息增益$Gain(A)$：由于特征$A$使数据集$D$的分类不确定性减少的程度</p><script type="math/tex; mode=display">Gain(A)=Info(D)-Info_A(D)</script><p>缺点：</p><ol><li><p>选择取值较多的特征往往会具有较大的信息增益（取值越多意味着确定性更高，条件熵越小，信息增益越大），所以<strong>ID3偏向于选择取值较多的特征</strong>。</p></li><li><p>仅支持分类不支持回归，不支持连续型变量，只有树的生成没有剪枝（容易过拟合），没有缺失值处理方法。</p></li></ol><h3 id="C4-5算法：信息增益率"><a href="#C4-5算法：信息增益率" class="headerlink" title="C4.5算法：信息增益率"></a>C4.5算法：信息增益率</h3><p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征</p><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p><p>信息增益率$GainRate(A)$：特征$A$的信息增益$Gain(A)$与训练数据集$D$关于特征$A$的值的熵$SplitInfo(A)$之比</p><script type="math/tex; mode=display">GainRate(A)=\frac{Gain(A)}{SplitInfo(A)}</script><p>其中$SplitInfo(A)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$，其中$n$是特征$A$取值的个数。</p><p>特征数越多的特征对应的特征熵越大，它作为分母，一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升泛化能力。</p><p>信息增益率准则<strong>对可取值数目较少</strong>的特征有所偏好，因此C4.5算法不是直接选取增益率最大的候选划分特征，<strong>而是先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的</strong>。</p><p>过拟合策略：C4.5引入了正则化系数进行剪枝</p><h3 id="CART算法：基尼指数"><a href="#CART算法：基尼指数" class="headerlink" title="CART算法：基尼指数"></a>CART算法：基尼指数</h3><p>基尼指数也是度量数据集纯度的指标，CART是使用基尼指数来选择最优特征的。基尼指数越小，代表数据集的纯度越高，这与信息增益（率）相反。</p><p>假设有$K$个类，第$k$个类别的概率为$p_k$，</p><script type="math/tex; mode=display">Gini(D)=1-\sum_{k=1}^{K}p_k^2</script><p>代表集合D的不确定性</p><p>特征A的基尼指数为</p><script type="math/tex; mode=display">Gini(D,A)=\sum_{k=1}^{K}\frac{|D_k|}{|D|}Gini(D^k)</script><p>表示经过特征A分割之后集合D的不确定性</p><p>选<strong>基尼指数最小</strong>的特征的作为最优划分特征。</p><p>其实不同的决策树学习算法只是它们<strong>选择特征的依据不同</strong>，决策树的生成过程都是一样的</p><p><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同评估标准，从而衍生出不同的决策树算法。</p><p>特征选择的关键是如何选择最优特征对数据集进行划分，随着划分过程的进行，希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的纯度越来越高。</p><p>由以下两步组成：</p><p>（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</p><p>（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，使用损失函数最小作为剪枝标准</p><p>特征选择</p><p>​    回归树：平方误差最小化准则</p><p>​    分类树：基尼指数最小化准则</p><h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>为缓解决策树过拟合，需要对决策树进行剪枝。分为预剪枝和后剪枝</p><p>往往通过极小化决策树整体的损失函数或代价函数来实现</p><p>决策树生成学习局部的模型，而决策树剪枝学习整体的模型</p><h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p>在生成决策树的过程中提前停止树的增长。核心思想是在树中节点进行扩展之前，先计算当前的划分是否能带来模型泛化性能的提升，如果不能则不再继续生成子树。</p><p>前剪枝停止决策树生长的几种方法：</p><p>(1) 当树达到一定深度时停止生长。</p><p>(2) 当到达当前节点的样本数量小于某个阈值时停止生长。</p><p>(3) 计算每次分类对测试集的准确率提升，当小于某个阈值时停止生长。</p><p>前剪枝的优缺点：</p><p>优点：简单高效，适合解决大规模问题。</p><p>缺点：</p><p>(1) 深度和阈值这些值很难准确估计，针对不同问题会有很大差别。</p><p>(2) 前剪枝存在一定局限性，有<strong>欠拟合的风险</strong>，虽然当前的划分会导致测试集准确率下降，可能在后面会有显著上升。</p><h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>在已生成的过拟合决策树上进行剪枝。核心思想是让算法生成一棵完全生长的决策树，然后从底层向上计算是否剪枝。(剪枝是将子树删除，用一个叶子节点代替，节点类别按多数投票)如果剪枝之后准确率有提升，则剪枝。</p><p>后剪枝的优缺点：</p><p>优点：通常可以得到泛化能力更强的决策树。</p><p>缺点：<strong>时间开销大。</strong></p><h1 id="ID3-C4-5-CART-对比"><a href="#ID3-C4-5-CART-对比" class="headerlink" title="ID3/C4.5/CART 对比"></a>ID3/C4.5/CART 对比</h1><div class="table-container"><table><thead><tr><th>算法</th><th>支持模型</th><th>树结构</th><th>特征选择</th><th>连续值处理</th><th>缺失值</th><th>剪枝</th></tr></thead><tbody><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td><td>不支持</td><td>不支持</td><td>不支持</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>CART</td><td>分类，回归</td><td>二叉树</td><td>基尼指数，均方误差</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table></div><p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。</p><p>回归树：用平方误差最小化作为选择特征的准则</p><p>分类树：采用基尼指数最小化原则进行特征选择，递归地生成二叉树。</p><p>也提供了优化的剪枝策略</p><p><strong>从样本类型的角度：</strong></p><p>ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。</p><p>C4.5会排序找到切分点，将连续变量转换为多个取值区间的离散型变量；</p><p>CART每次会对特征进行二值划分，适用于连续变量。</p><p><strong>从应用角度：</strong></p><p>ID3和C4.5只能用于分类，CART树可以用于分类和回归。</p><p><strong>从细节、优化过程角度：</strong></p><p>ID3对样本特征缺失值比较敏感，而C4.5和CART树都可以对缺失值进行不同方式的处理。</p><p>ID3和C4.5可以产生多叉分支，且每个特征在层级之间不会复用。CART树是二叉树，<strong>每个特征可以被重复利用</strong>。</p><p>ID3和C4.5通过剪枝来权衡树的准确性和泛化性能，CART树枝节利用全部数据发现所有可能的树结构进行对比。</p><h1 id="Sklearn中树模型输出的特征重要程度"><a href="#Sklearn中树模型输出的特征重要程度" class="headerlink" title="Sklearn中树模型输出的特征重要程度"></a>Sklearn中树模型输出的特征重要程度</h1><p>决策树中节点分裂不纯度的改变量的归一化值</p><h1 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h1><p>优点：</p><p>易于理解和解释，可视化分析，容易提取出规则</p><p>可同时处理分类型和数值型变量</p><p>缺点：</p><p>容易过拟合</p><p>通常情况下精确度不如其他算法好</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> DecisionTree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列5——降维</title>
      <link href="/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/"/>
      <url>/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>ML中相比训练误差，更加关注的是泛化误差。过拟合问题就是造成泛化误差大的一个原因。</p><p>解决过拟合的方法：</p><ul><li><p>增加样本数量</p></li><li><p>正则化（Ridge/Lasso）</p></li><li><p>降维</p><ul><li>直接降维——特征选择<em>（不是本节的关注点，本节关注线性和非线性降维）</em></li><li>线性降维——PCA，MDS（多维空间缩放）</li><li>非线性降维——流形（Isomap，LLE）</li></ul><blockquote><p>特征维度高往往会造成<strong>维度灾难( Curse of Dimensionality )</strong></p><ul><li>从数学角度来看，每增加一个特征（属性），为了布满它所有的样本空间，所需要的样本数量呈指数级增长（例如对于一个二值变量，则至少需要$2^n$个样本数）</li><li>从几何角度来看，比如对于一个体积为1的超立方体，内接一个超球体，$V_{超球体}=C(0.5)^d$，其中$d$为维度，当$d\rightarrow\infty $时，超球体的体积无限趋于0，那么会造成<strong>数据的稀疏性</strong>，且大部分集中在一起，很难进行分类。</li></ul></blockquote></li></ul><h1 id="概率相关知识"><a href="#概率相关知识" class="headerlink" title="概率相关知识"></a>概率相关知识</h1><p>Data：$X=(x_1,x_2,…,x_N)^T_{N*P}$，$N$个样本，每个样本是$P$维的</p><p>样本均值和样本方差的矩阵表示：</p><script type="math/tex; mode=display">\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_i=\frac{1}{N}(x_1,x_2,…,x_N)1_N=\frac{1}{N}X^T1_N\\\begin{align}S_{p*p}&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T\\&=\frac{1}{N}(X^T-\bar{x}1_N^T)(X^T-\bar{x}1_N^T)^T\\&=\frac{1}{N}(X^T-\frac{1}{N}X^T1_N1_N^T)(X^T-\frac{1}{N}X^T1_N1_N^T)^T\\&=\frac{1}{N}X^T(I_N-\frac{1}{N}1_N1_N^T)(I_N-\frac{1}{N}1_N1_N^T)^TX\\&=\frac{1}{N}X^THH^TX\\&=\frac{1}{N}X^THX\end{align}</script><p>其中$H=I_N-\frac{1}{N}1_N1_N^T$，且具有性质$H^T=H,H^n=H$，称为中心矩阵( centering matrix )</p><h1 id="主成分分析PCA"><a href="#主成分分析PCA" class="headerlink" title="主成分分析PCA"></a>主成分分析PCA</h1><ul><li><p>一个中心：<strong>原始特征空间的重构</strong>（将一组可能<strong>线性相关</strong>的变量通过正交变换，变换成<strong>线性无关</strong>的变量）</p></li><li><p>两个基本点（两个角度是一个方法）：<strong>最大投影方差，最小重构距离</strong>（在该方向上投影的方差最大，把投影上的点重构回去的代价最小）</p></li></ul><h2 id="最大投影方差"><a href="#最大投影方差" class="headerlink" title="最大投影方差"></a>最大投影方差</h2><ol><li><p>中心化（将中心平移到原点，即减去均值$x_i-\bar{x}$）</p></li><li><p>令被投影的向量的模为1，$||u_1||=1$</p></li><li><p>投影方差为（向量$a$在向量$b$上的投影为$a^Tb$）</p><script type="math/tex; mode=display">J=\frac{1}{N}\sum_{i=1}^{N}((x_i-\bar{x})^Tu_1)^2=u_1^T[\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T]u_1=u_1^TSu_1</script><p>从而求解目标为</p><script type="math/tex; mode=display">\hat{u}_1=\mathop{\arg\max}\limits_{u_1} u_1^TSu_1\\s.t. u_1^Tu_1=1</script><p>使用拉格朗日乘子法</p><script type="math/tex; mode=display">L(u_1,\lambda)=u_1^TSu_1+\lambda(1-u_1^Tu_1)\\\frac{\partial L}{\partial u_1}=2Su_1-\lambda2u_1=0\\Su_1=\lambda u_1</script><p>从而$\lambda$是特征值，$u_1$是特征向量（主成分）</p></li></ol><h2 id="最小重构距离"><a href="#最小重构距离" class="headerlink" title="最小重构距离"></a>最小重构距离</h2><p>PCA可以视为以下两个部分：</p><ul><li>先进行特征空间的重构，得到$\{u_1,u_2,…,u_p\}$共$p$个特征向量</li><li>再对这$p$个特征向量进行筛选，选出前$q$个特征向量，从而实现降维</li></ul><p>原先样本为（用新的坐标轴去重构，并认为$x_i$已经是中心化后的）</p><script type="math/tex; mode=display">x_i=\sum_{k=1}^{p}(x_i^Tu_k)u_k</script><p>重构回来的样本为</p><script type="math/tex; mode=display">\hat{x}_i=\sum_{k=1}^{q}(x_i^Tu_k)u_k</script><p>重构代价为（目标是希望重构代价最小）</p><script type="math/tex; mode=display">\begin{align}J&=\frac{1}{N}\sum_{i=1}^{N}||x_i-\hat{x}_i||^2\\&=\frac{1}{N}\sum_{i=1}^{N}||\sum_{k=q+1}^{p}(x_i^Tu_k)u_k||^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}(x_i^Tu_k)^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}((x_i-\bar{x})^Tu_k)^2\\&=\sum_{k=q+1}^{p}[\sum_{i=1}^{N}\frac{1}{N}((x_i-\bar{x})^Tu_k)^2]\\&=\sum_{k=q+1}^{p}u_k^T·S·u_k\end{align}</script><p>其中坐标轴为$u_{q+1},…,u_{p}$，坐标为$x_i^Tu_{q+1},…,x_i^Tu_{p}$，对向量求模的平方即对各个坐标轴下的坐标求平方和</p><p>因此目标函数为</p><script type="math/tex; mode=display">\hat{u}_k=\mathop{\arg\max}\limits_{u_k} u_k^TSu_k\\s.t. u_k^Tu_k=1</script><h2 id="从SVD角度看PCA"><a href="#从SVD角度看PCA" class="headerlink" title="从SVD角度看PCA"></a>从SVD角度看PCA</h2><p>奇异值分解SVD：</p><script type="math/tex; mode=display">S=GKG^T\\G^TG=I</script><p>$K$为由特征值从大到小排列构成的对角矩阵</p><p>对中心化后的数据矩阵（原数据矩阵$X_{N·p}$）进行奇异值分解（任何实数矩阵可以进行奇异值分解）</p><script type="math/tex; mode=display">HX=U\Sigma V^T</script><p>样本方差矩阵为（忽略$\frac{1}{N}$）</p><script type="math/tex; mode=display">S=X^THX=X^TH^THX=V\Sigma U ^TU\Sigma V^T</script><p>由于SVD的性质</p><script type="math/tex; mode=display">U^TU=I\\V^TV=VV^T=I</script><p>且$\Sigma$为对角矩阵</p><p>则可以转换为</p><script type="math/tex; mode=display">S=V\Sigma U ^TU\Sigma V^T=V\Sigma ^2 V^T</script><p>因此不需要求样本方差矩阵$S$，可以对中心化后的数据矩阵$HX$进行奇异值分解，同样可以求得$V$和$\Sigma$，从而得到特征向量和特征值。</p><p>定义矩阵</p><script type="math/tex; mode=display">T=HXX^TH=U\Sigma V^TV\Sigma U^T=U\Sigma^2 U^T</script><p>因此$T$和$S$有相同的特征值（$\Sigma^2$）</p><ul><li><p>对$S$做特征分解，得到方向（主成分）$V$，然后通过将数据矩阵乘以方向$V$</p><script type="math/tex; mode=display">HX·V=U\Sigma V^TV=U\Sigma</script><p>从而得到在新的方向下的坐标矩阵$U\Sigma$</p></li><li><p>对$T$做特征分解（<strong>主坐标分析</strong>，Principle Coordinate Analysis，PCoA），可以直接得到坐标</p><script type="math/tex; mode=display">T=U\Sigma^2 U^T\\TU\Sigma=U\Sigma^2 U^TU\Sigma=U\Sigma^3=U\Sigma\Sigma^2</script><p>得到特征向量组成的矩阵$U\Sigma$和特征值组成的矩阵$\Sigma^2$</p></li></ul><blockquote><p>Q：为什么$U\Sigma$是$T$的特征向量组成的矩阵？$T$的特征向量组成的矩阵不应该直接是$U$吗？ </p><p>$U\Sigma$是将$T$的每个特征向量依据$HX$的相应特征值大小做缩放之后的矩阵 </p><p><strong>？？？……没懂</strong></p><p>PCA的目的是求出在新的投影方向上的坐标。PCA先通过SVD找到主成分（方向）$u_1$ ，然后对于样本点$x_i$来说，先进行中心化再乘上主成分$(x_i-\bar{x})u_i=z_i$，从而得到该样本点在新坐标轴$u_1$上的坐标$z_i$。即先求的是方向，再对样本进行投影才能得到坐标。</p><p>PCoA没有通过求方向再进行投影得到坐标的方式，而是通过对矩阵$T$进行分解，直接求出坐标</p></blockquote><p>PCoA好处：</p><p>维度方面：$S_{p<em>p},T_{N</em>N}$，当维度高时可以简化运算</p><h1 id="概率角度P-PCA"><a href="#概率角度P-PCA" class="headerlink" title="概率角度P-PCA"></a>概率角度P-PCA</h1><p>原始样本$x\in R^p$(observed data)，降维后的样本$z\in R^q$(latent data)，且$q&lt;p$</p><p>令</p><script type="math/tex; mode=display">z \sim N(0_q,I_q)\\x=w_{p*q}z+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I_p)</script><p>是线性高斯模型(Linear Gaussian Model)，$\sigma^2I_p$矩阵各向同性。</p><p>P-PCA关注两个问题：</p><ul><li>Inference：$P(z|x)$</li><li>Learning：$w,\mu,\sigma^2$——可使用EM算法求解，较复杂，此处省略</li></ul><p>条件：</p><script type="math/tex; mode=display">z \sim N(0,I)\\x=wz+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I)，\varepsilon \perp x\\E[x|z]=E[wz+\mu+\varepsilon|z]=wz+\mu\\Var[x|z]=Var[wz+\mu+\varepsilon|z]=\sigma^2I \\\Rightarrow x|z \sim N(wz+\mu,\sigma^2I)</script><p>则</p><script type="math/tex; mode=display">E[x]=E[wz+\mu+\varepsilon]=E[wz+\mu]+E[\varepsilon]=\mu\\Var[x]=Var[wz+\mu+\varepsilon]=Var[wz]+Var[\varepsilon]=ww^T+\sigma^2I\\\Rightarrow x \sim N(\mu,ww^T+\sigma^2I)</script><p>构造$x$和$z$的联合概率：</p><script type="math/tex; mode=display">Cov(x,z)=E[(x-\mu)z^T]=E[(wz+\varepsilon)z^T]=wE[zz^T]+E[\varepsilon]E[z^T]=w</script><p>则</p><script type="math/tex; mode=display">\begin{pmatrix} x  \\ z  \end{pmatrix}\sim N\begin{pmatrix} ww^T+\sigma^2I & w \\ w & I \end{pmatrix}</script><p>再由公式$x_b|x_a \sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\Sigma_{bb·a})$，从而得到$z|x$的条件概率分布</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning, Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列4——线性分类</title>
      <link href="/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/"/>
      <url>/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>b站up主： <strong>shuhuai008</strong> </p><p><a href="https://www.bilibili.com/video/av70839977" target="_blank" rel="noopener">机器学习-白板推导系列-合集</a> 学习笔记</p></blockquote><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>对线性回归的条件逐一打破，从而引申到其它模型：</p><ul><li><p>线性：</p><ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性（激活函数是非线性，如逻辑回归）</li><li>系数非线性：神经网络，感知机</li></ul></li><li><p>全局性（全部是由一条线拟合）：线性样条回归、决策树</p></li><li>数据未加工：PCA，流形</li></ul><p>线性回归通过激活函数进行降维，达到线性分类。</p><script type="math/tex; mode=display">y=f(w^Tx+b),x\in R^p</script><p>$f$是激活函数（activation function），$f^{-1}$是链接函数（link function）。</p><p>激活函数$f$将数据的线性组合作为输入，映射到{0,1}或[0,1]区间上；</p><p>链接函数$f^{-1}$将{0,1}或[0,1]区间映射到线性组合上。</p><p>线性分类分为两大类：</p><ol><li>硬分类：$y\in \{0,1\}$，代表模型有线性判别分析(Fisher判别分析)、感知机</li><li>软分类：$y\in [0,1]$，分为生成式模型和判别式模型。判别式直接对$P(Y|X)$进行求解，如逻辑回归；生成式不直接求解$P(Y|X)$，而是通过贝叶斯定理，即通过式$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$进行求解，如高斯判别分析(GDA)（假设数据本身是连续的），朴素贝叶斯（假设数据本身是离散的）。</li></ol><h1 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机(Perceptron)"></a>感知机(Perceptron)</h1><p>思想：错误驱动——不断向正确分类的方向移动</p><p>模型</p><script type="math/tex; mode=display">f(x)=sign(w^Tx),x\in R^p,w\in R^p\\</script><p>前提：假定模型是线性可分的（若不满足可使用pocket algorithm）</p><p>策略：假设样本集$\{(x_i,y_i)\}_{i=1}^{N}$，将损失函数定义为</p><script type="math/tex; mode=display">L(w)=\sum_{i=1}^{N}I\{y_iw^Tx_i\lt 0\}</script><p>但该损失函数不可导不连续，难以求解</p><p>因此采用新的损失函数</p><script type="math/tex; mode=display">L(w)=\sum_{x_i\in D}-y_iw^Tx_i</script><p>是连续函数且可导，对其进行求导$\triangledown_wL=\sum_{x_i\in D}-y_ix_i$，可采用算法SGD（随机梯度下降）求解</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\lambda\triangledown_wL=w^t+\lambda y_ix_i</script><h1 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h1><p><strong>思想：类内小，类间大。</strong>（高类聚，低耦合）</p><p>将点投影到一维的坐标轴上，每个点都对应该坐标轴上的一个值，并设定一个阈值，根据值与阈值大小进行分类。</p><p>重点：找到一个合适的投影方向，使得类内方差小，类间方差大</p><h2 id="目标函数推导"><a href="#目标函数推导" class="headerlink" title="目标函数推导"></a>目标函数推导</h2><p>给定样本$X=(x_1,x_2,…,x_N)^T,Y=(y_1,y_2,…,y_N)^T,\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{+1,-1\}$</p><p>定义样本集合$x_{c_1}=\{x_i|y_i=+1\},x_{c_2}=\{x_i|y_i=-1\}$</p><p>令$x_i$在$w$方向上的投影为$z_i=w^Tx_i$(假设$||w||=1)$</p><script type="math/tex; mode=display">\begin{align} \bar{z}&=\frac{1}{N}\sum_{i=1}^{N}z_i=\frac{1}{N}\sum_{i=1}^{N}w^Tx_i\\S_z  &=\frac{1}{N}\sum_{i=1}^{N}(z_i-\bar{z})(z_i-\bar{z})^T     =\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar{z})(w^Tx_i-\bar{z})^T\\c_1: \bar{z}_1&=\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i\\S_1  &=\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T     =\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\     c_2: \bar{z}_2&=\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i\\S_2  &=\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T     =\frac{1}{N_2}\sum_{i=1}^{N_2}(w^Tx_i-\bar{z}_2)(w^Tx_i-\bar{z}_2)^T\end{align}</script><p>则类间用$(\bar{z}_1-\bar{z}_2)^2$来表达，类内用$S_1+S_2$来表达</p><p>目标函数为</p><script type="math/tex; mode=display">J(w)=\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\\\hat{w}=\mathop{\arg\max}\limits_{w} J(w)</script><p>则分子部分为</p><script type="math/tex; mode=display">(\bar{z_1}-\bar{z_2})^2=(\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i)^2=(w^T(\bar{x}_{c_1}-\bar{x}_{c_2}))^2=w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw</script><p>由于$S_1$可表达为</p><script type="math/tex; mode=display">\begin{align}S_1 &=\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\&=w^T[\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{c_1})(x_i-\bar{x}_{c_2})^T]w\\&=w^T*S_{c_1}*w\end{align}</script><p>则分母部分为</p><script type="math/tex; mode=display">S_1+S_2=w^TS_{c_1}w+w^TS_{c_2}w=w^T(S_{c_1}+S_{c_2})w</script><p>因此</p><script type="math/tex; mode=display">\begin{align}J(w) &=\frac{w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw}{w^T(S_{c_1}+S_{c_2})w}\\     &=\frac{w^TS_bw}{w^TS_ww}=w^TS_bw(w^TS_ww)^{-1}\end{align}</script><p>其中$S_b=(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^T$为类间方差（between-class），$S_w=S_{c_1}+S_{c_2}$为类内方差（within-class），再对$w$求偏导</p><script type="math/tex; mode=display">\frac{\partial J(w)}{\partial w}=2S_bw(w^TS_ww)^{-1}+w^TS_bw(-1)(w^TS_ww)^{-2}2S_ww=0\\S_bw(w^TS_ww)=(w^TS_bw)S_ww\\S_ww=\frac{w^TS_ww}{w^TS_bw}S_bw</script><p>只需要求$w$的方向不需要求大小，因此常数不影响，因此</p><script type="math/tex; mode=display">\begin{align}w=\frac{w^TS_ww}{w^TS_bw}{S_w}^{-1}S_bw & \propto{S_w}^{-1}S_bw={S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})[(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw]\\&\propto{S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})\end{align}</script><p>若$S_w^{-1}$是对角矩阵，且各向同性，则$S_w^{-1}\propto I$，那么</p><script type="math/tex; mode=display">w\propto(\bar{x}_{c_1}-\bar{x}_{c_2})</script><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>是判别式模型，直接对$P(Y|X)$进行建模，采用极大似然估计求解参数。</p><p>sigmoid函数：</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}\\\sigma:R \longmapsto (0,1)\\w^Tx \longmapsto Probability</script><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><script type="math/tex; mode=display">\begin{align}p_1&:P(y=1|x)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},y=1\\p_0&:P(y=0|x)=1-P(y=1|x)=\frac{e^{-w^Tx}}{1+e^{-w^Tx}},y=0\\\end{align}</script><script type="math/tex; mode=display">\Rightarrow P(y|x)=p_1^yp_0^{1-y}</script><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>MLE:</p><script type="math/tex; mode=display">\begin{align} \hat{w}&=\mathop{\arg\max}\limits_{w} \log P(Y|X)\\&=\mathop{\arg\max}\limits_{w} \log \prod_{i=1}^{N}P(y_i|x_i) \\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}\log P(y_i|x_i)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log p_i+(1-y_i)\log p_0)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log \psi(x_i;w)+(1-y_i)\log (1-\psi(x_i;w)))\end{align}</script><p>MLE是最大化问题，可以导出一个Loss function，转化为最小化问题，等价于最小化cross entropy</p><h1 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>高斯判别分析是生成式模型，<strong>生成式模型并不是求取$P(Y|X)$，而是利用贝叶斯定理，比较$P(y=0|x)$与$P(y=1|x)$的大小，选择较大者作为预测的类</strong>。</p><p>由贝叶斯定理</p><script type="math/tex; mode=display">P(y|x)=\frac{P(x|y)P(y)}{P(x)}\propto P(x|y)P(y)</script><p>由于$P(x|y)P(y)=P(x,y)$，因此我们主要是对于联合概率进行建模。</p><p>其中$P(y)$是先验（prior），$p(x|y)$是似然（likelihood），$P(y|x)$是后验（posterior），因此求解可以表达为</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p><strong>假设$y$服从伯努利分布$Bernoulli(\phi)$，$x|y$服从高斯分布</strong></p><script type="math/tex; mode=display">\begin{align}&x|y=1 \sim N(\mu_1,\Sigma)\\&x|y=0 \sim N(\mu_2,\Sigma)\\\Rightarrow &x|y \sim N(\mu_1,\Sigma)^y*N(\mu_2,\Sigma)^{1-y}\end{align}</script><p>令参数部分</p><script type="math/tex; mode=display">\theta = (\mu_1,\mu_2,\Sigma,\phi)\\\hat{\theta}=\mathop{\arg\max}\limits_{\theta} l(\theta)</script><p>定义对数似然函数为</p><script type="math/tex; mode=display">\begin{align}l(\theta)&=\log \prod_{i=1}^{N}p(x_i,y_i)\\&=\sum_{i=1}^{N}\log(P(x_i|y_i)P(y_i))\\&=\sum_{i=1}^{N}[\log P(x_i|y_i)+\log P(y_i)]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}*N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}+\log N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\\end{align}</script><h2 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h2><p>将上式分为三个部分①+②+③，且有$N_1$个样本标签为1，有$N_2$个样本标签为0（$N_1+N_2=N$）</p><p>求$\phi$：</p><script type="math/tex; mode=display">③=\sum_{i=1}^{N}[y_i\log \phi+(1-y_i)\log(1-\phi)]\\\frac{\partial ③}{\partial \phi}=\sum_{i=1}^{N}[\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}]=0\\\phi=\frac{1}{N}\sum_{i=1}^{N}y_i=\frac{N_1}{N}</script><p>求$\mu_1$：</p><script type="math/tex; mode=display">\begin{align}①&=\sum_{i=1}^{N}\log N(\mu_1,\Sigma)^{y_i}\\&=\sum_{i=1}^{N}y_i\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\end{align}</script><p>去除与$\mu_1$无关的常数</p><script type="math/tex; mode=display">\hat{\mu}_1=\mathop{\arg\max}\limits_{\mu_1} ①=\mathop{\arg\max}\limits_{\mu_1}\sum_{i=1}^{N}y_i[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\\-\frac{1}{2}\sum_{i=1}^{N}y_i[(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]=-\frac{1}{2}\sum_{i=1}^{N}y_i[x_i^T\Sigma^{-1}x_i-2\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1=\bigtriangleup\\\frac{\partial \bigtriangleup}{\partial \mu_1}=\sum_{i=1}^{N}y_i(\Sigma^{-1}x_i-\Sigma^{-1}\mu_1)=0\\\hat{\mu}_1=\frac{\sum_{i=1}^{N}y_ix_i}{\sum_{i=1}^{N}y_i}=\frac{\sum_{i=1}^{N}y_ix_i}{N_1}</script><p>求$\Sigma$：</p><p>首先令</p><script type="math/tex; mode=display">C_1=\{x_i|y_i=1,i=1,…,N\}\\C_2=\{x_i|y_i=0,i=1,…,N\}\\|C_1|=N_1,|C_2|=N_2,N_1+N_2=N</script><p>则</p><script type="math/tex; mode=display">\hat{\Sigma}_1=\mathop{\arg\max}\limits_{\Sigma}(①+②)\\①+②=\sum_{x_i\in C_1}\log N(\mu_1,\Sigma)+\sum_{x_i\in C_2}\log N(\mu_2,\Sigma)\\\begin{align}\log N(\mu,\Sigma)&=\sum_{i=1}^{N}\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[\log\frac{1}{(2\pi)^\frac{p}{2}}+\log{|\Sigma|^\frac{1}{2}}-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[C-\frac{1}{2}\log|\Sigma|-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}tr[(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}tr[\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T\Sigma^{-1}]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}Ntr(S\Sigma^{-1})\\\end{align}</script><p>其中$S=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T$为样本方差</p><p>则</p><script type="math/tex; mode=display">\begin{align}①+②&=-\frac{N_1}{2}\log|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})-\frac{N_2}{2}\log|\Sigma|-\frac{1}{2}N_2tr(S_2\Sigma^{-1})+C\\&=-\frac{1}{2}[N\log|\Sigma|+N_1tr(S_1\Sigma^{-1})+N_2tr(S_2\Sigma^{-1})]+C\\\frac{\partial (①+②)}{\partial \Sigma}&=-\frac{1}{2}(N\Sigma^{-1}-N_1S_1\Sigma^{-2}-N_2S_2\Sigma^{-2})=0\\\end{align}</script><script type="math/tex; mode=display">N\Sigma-N_1S_1-N_2S_2=0\\ \Sigma=\frac{1}{N}(N_1S_1+N_2S_2)</script><p>其中用到求导$\frac{tr(\Sigma^{-1}S)}{\partial \Sigma}=S^T(-1)\Sigma^{-2}$</p><h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>朴素贝叶斯假设：条件独立性假设</p><script type="math/tex; mode=display">x_i \perp x_j | y(i\neq j)\\P(x|y)=\prod_{j=1}^{p}P(x_j|y)</script><p>最简单的概率图（有向图）模型</p><p>动机：简化运算</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p>若$x$离散，则认为$x_j$服从伯努利分布/多项分布</p><p>若$x$连续，则认为$x_j$服从正态分布$x_j\sim N(\mu_j,\sigma^2_j)$</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning, Classification, Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression 逻辑回归算法及实现</title>
      <link href="/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/"/>
      <url>/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><p>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到二分类的目的。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>逻辑回归是在线性回归的基础上，利用sigmoid函数（或称为logistic函数）</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>进行映射，代入线性回归部分</p><script type="math/tex; mode=display">z=\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n</script><p>得到二元逻辑回归模型的一般形式：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的$h_\theta(x)$就是逻辑回归返回的值，介于0到1之间，可以将其当做样本取正类的“概率”。因此对于样本$x$分类结果为正类1和负类0的概率分别为</p><script type="math/tex; mode=display">\begin{cases}P(y=1|x;\theta)=h_\theta(x)\\P(y=0|x;\theta)=1-h_\theta(x)\end{cases}</script><p>对$h_\theta(x)$进行变换可以得到对数几率的表达式</p><script type="math/tex; mode=display">ln\frac{h_\theta(x)}{1-h_\theta(x)}=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})=ln(\frac{1}{e^{-\theta^Tx}})=ln(e^{\theta^Tx})=\theta^Tx</script><p>从上式可以看出，<strong>逻辑回归的本质是在对线性回归模型的预测去逼近真实标记的对数几率</strong>。求解的关注点在于求解参数$\theta$上，通常使用极大似然估计的方法对$\theta$进行估计。</p><p>令</p><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的似然函数为</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^{m}P(y^i|x^i;\theta)=\prod_{i=1}^{m}h_\theta(x^i)^{y^i}*(1-h_\theta(x^i))^{1-y^i}</script><p>其中，$x^i$为第$i$个样本的特征做构成的向量（每个向量$n+1$维，共$m$个向量），$y^i$为第$i$个样本的标签，$m$为样本量。实际中为了简化计算，同时防止连乘所造成的浮点数下溢，通常会转化为对数似然函数</p><script type="math/tex; mode=display">l(\theta)=\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>逻辑回归所要解决的问题即为找到参数$\theta$，使得对数似然函数达到最大。</p><p>令损失函数为（忽略正则化项）</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}l(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>利用梯度下降求解参数</p><script type="math/tex; mode=display">\begin{align}\frac{∂J(\theta)}{∂\theta_j}& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{h_\theta(x^i)}-\frac{1-y^i}{1-h_\theta(x^i)}]\frac{∂h_\theta(x^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]\frac{∂g(\theta^Tx^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))\frac{∂\theta^Tx^i}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i(1-g(\theta^Tx^i))-(1-y^i)g(\theta^Tx^i)]x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i-g(\theta^Tx^i)]x^i_j\\& =\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j\\\end{align}</script><p>因此最终得到参数迭代式</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>（参考：<a href="https://www.cnblogs.com/Luv-GEM/p/10674719.html" target="_blank" rel="noopener">Logistic回归（逻辑回归）和softmax回归</a>）</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点：</strong></p><ol><li><p>速度快，在时间和内存需求上相当高效，它可以应用于分布式数据和在线算法实现，用较少的资源处理大型数据</p></li><li><p>对线性分类问题拟合很好</p></li><li>简单易于理解，直接看到各个特征的权重</li></ol><p><strong>缺点：</strong></p><ol><li>分类精度可能不高，在非线性分类问题上表现不好</li><li>数据特征有缺失或者特征空间很大时表现效果并不好，受异常值影响大</li></ol><h2 id="与线性回归的异同"><a href="#与线性回归的异同" class="headerlink" title="与线性回归的异同"></a>与线性回归的异同</h2><p>本质是线性的，只是特征到结果映射用的是sigmoid函数，属于广义线性模型（GLM）</p><ul><li><p>相同</p><ol><li><p>都使用极大似然估计对训练样本进行建模；求解超参数时都可以使用梯度下降。</p></li><li><p>都是广义线性模型，逻辑回归本质上是一个线性回归模型，LR是以线性回归为理论支持的。</p></li></ol></li><li><p>不同</p><ol><li>本质：逻辑回归是分类，线性回归是回归，逻辑回归中$y$是因变量而非$\frac{p}{1-p}$，因变量是离散而非连续</li><li>LR形式上是线性回归，实质上是在求取输入空间到输出空间的非线性函数映射（对率函数起到将线性回归模型的预测值与真实标记联系起来的作用）</li><li>LR是直接对分类可能性进行建模，无需事先假设数据分布，而线性回归需要假设数据分布</li></ol></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数是它的极大似然函数取对数再除以样本量的相反数</strong></p><p>极大似然函数：</p><script type="math/tex; mode=display">L_\theta(x)=\prod_{i=1}^{m}h_\theta(x^i;\theta)^{y^i}*(1-h_\theta(x^i;\theta))^{1-y^i}</script><p>损失函数：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]</script><p>除以样本量$m$并不改变最终求导极值结果，通过除以$m$可以得到<strong>平均损失值</strong>，避免<strong>样本数量对于损失值的影响</strong></p><p>（但是也有不除以样本量的，比如sklearn中的损失函数就不除以样本量）</p><p>（乘上样本量的倒数也并不影响梯度下降的过程┓( ´∀` )┏ ）</p><blockquote><p><strong>Q：为什么要用极大似然函数作为损失函数？</strong></p><p>损失函数一般有四种：平方损失函数，对数损失函数，HingeLoss损失函数，绝对值损失函数。</p><p>将极大似然函数取对数以后等同于对数损失函数。</p><p><strong>在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</strong></p><p>梯度更新公式：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>这个式子的更新速度只和$x^i_j$和$y^i$相关，和sigmoid函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 </p><p><strong>Q：为什么不选平方损失函数？</strong></p><p>其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmoid函数本身的梯度是很相关的。</p><script type="math/tex; mode=display">θ_j=θ_j-2(sigmoid(x)*(1-sigmoid(x)))x^i_j</script><p>sigmoid函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p><p>实际上也可以用最小二乘，但是最小二乘得到的权重效果比较差。</p><p>如果用最小二乘法，目标函数就是差值的平方和，<strong>是非凸的，不容易求解，很容易陷入到局部最优</strong>。</p><p>如果用极大似然估计，目标函数就是对数似然函数，是关于$(w,b)$的高阶<strong>连续可导凸函数</strong>，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。</p></blockquote><h2 id="参数求解方法"><a href="#参数求解方法" class="headerlink" title="参数求解方法"></a>参数求解方法</h2><ul><li>梯度下降法</li></ul><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼近最优解。</p><blockquote><p>梯度下降：随机梯度下降，批梯度下降，small-batch梯度下降</p><p>Q：三种方式的优劣以及如何选择最合适的梯度下降方式</p><ol><li><p>批梯度下降(BGD)：每次迭代使用所有样本来进行梯度的更新，能得到全局最优解。缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降(SGD)：每次迭代随机使用一个样本来对参数进行更新，优点是每一轮参数的更新速度大大加快，缺点是准确度下降，可能会收敛到局部最优（单个样本不能代表全体样本的趋势）。</p></li><li><p>小批量梯度下降：结合了BGD和SGD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p></li></ol></blockquote><ul><li><p>牛顿法</p><p>(待更新)</p></li><li><p>拟牛顿法</p><p>(待更新)</p></li></ul><blockquote><p>牛顿法与梯度下降法求解参数的区别：</p><p>两种方法不同在于牛顿法中<strong>多了一项二阶导数</strong>，这项二阶导数对参数更新的影响主要体现在<strong>改变参数更新方向上</strong>。如下图所示，红色是牛顿法参数更新的方向，绿色为梯度下降法参数更新方向，因为牛顿法考虑了二阶导数，因而可以<strong>找到更优的参数更新方向</strong>，在每次更新的步幅相同的情况下，可以<strong>比梯度下降法节省很多的迭代次数</strong>。</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/zdow4EB72J3r.png" alt="mark"></p></blockquote><h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>作用：需要一个单调可微的函数，把分类任务的真实标记与线性回归模型的预测值联系起来。</p><p>对于二分类问题，由线性回归得来的启发是根据特征的加权平均进行预测。很自然地想到设定一个阈值，如果加权平均大于该阈值就判为正类，反之判为负类。但<strong>阶跃函数不可导</strong>，所以<strong>引入Sigmoid函数，将样本的加权平均代入函数得到的值就是样本属于正类的概率，即将输入空间到输出空间作非线性函数映射</strong>。</p><p>Sigmoid函数形式：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid函数是一个S型的函数，函数图像：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><p>当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0。</p><p>它能够<strong>将任何实数映射到(0,1)区间</strong>（开区间，不可等于0或1），使其可用于将任意值函数转换为更适合二分类的函数。 </p><p>因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。</p><p>区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p><h2 id="共线性问题"><a href="#共线性问题" class="headerlink" title="共线性问题"></a>共线性问题</h2><p>对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。</p><p>通常做法为：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF， VIF越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><ol><li><p>多项逻辑回归(Softmax Regression)</p><p>（二分类逻辑回归在多标签分类下的一种拓展）</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/4HnDRP6ijvH6.png" alt="mark"></p></li><li><p>one v.s. rest</p><p>k个二分类LR分类器，把标签重新整理为“第i类标签”与“非第i类标签”</p></li></ol><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a> </p><blockquote><p><em>class</em> <code>sklearn.linear_model.LogisticRegression</code>(<em>penalty=’l2’</em>, <em>dual=False</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>random_state=None</em>, <em>solver=’lbfgs’</em>, <em>max_iter=100</em>, <em>multi_class=’auto’</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>n_jobs=None</em>, <em>l1_ratio=None</em>) </p></blockquote><h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数"><a href="#1-基本模型参数" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fit_intercept</strong>：bool，指定是否将截距项添加到线性回归部分中。默认True。</p></li><li><p><strong>intercept_scaling</strong>：float，默认1。仅在solver=’liblinear’且fit_intercept=True时有用。 在这种情况下原本的向量是[x]就变成[x,intercept_scaling]，即具有等于设定的intercept_scaling值的“合成”特征会被添加到实例矢量。截距会变为intercept_scaling * synthetic_feature_weight(合成特征权重)。synthetic_feature_weight会与其他特征经历l1和l2正则化，为减小正则化对synthetic_feature_weight（并因此对截距）的影响，必须增加intercept_scaling。</p><blockquote><p>因为本身截距项是不需要进行正则化的，当采用fit_intcept时相当于人造一个特征出来，特征恒为1，权重为b。在计算正则化项的时候，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 (O_o)??</p></blockquote></li><li><p><strong>multi_class</strong>：str，’auto’（默认）/‘ovr’/‘multinomial’，表示要预测的分类是二分类或一对多形式的多分类问题，还是多对多形式的多分类问题。</p><ul><li><p><strong>‘auto’</strong>：表示自动选择，会根据数据的分类情况和其他参数确定模型要处理的分类问题的类型。</p><blockquote><p>根据源码得到判定方法如下：</p><p>step 1：if solver = ‘liblinear’: multi_class = ‘ovr’</p><p>step 2：elif n_classes &gt; 2: multi_class = ‘multinomial’</p><p>step 3：else: multi_class = ‘ovr’</p></blockquote></li><li><p><strong>‘ovr’</strong>：表示当前处理的是二分类或一对多形式的多分类问题</p></li><li><p><strong>‘multinomial’</strong>：表示当前处理的是多对多形式的多分类问题</p></li></ul></li><li><p><strong>class_weight</strong>：None（默认）/‘balanced’/dict，标签(label)的权重。</p><ul><li><p><strong>None</strong>：所有的label持有相同的权重， 所有类别的权值为1 </p></li><li><p><strong>‘balanced’</strong>：自动调整与样本中类频率成反比的权重，即<code>n_samples/(n_classes*np.bincount(y))</code></p><blockquote><p><strong>‘balanced’如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line"></span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment"># 标签值，一共16个样本</span></span><br><span class="line">np.bincount(y)</span><br><span class="line"><span class="comment"># array([8, 6, 2], dtype=int64) 计算每个类别的样本数量，顺序按类别的出现次序</span></span><br><span class="line">class_weight = <span class="string">'balanced'</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight) <span class="comment"># [0.66666667 0.88888889 2.66666667]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">8</span>))  <span class="comment">#输出 0.6666666666666666</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">6</span>))  <span class="comment">#输出 0.8888888888888888</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">2</span>))  <span class="comment">#输出 2.6666666666666665</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>dict类型</strong></p><p>对于二分类问题，可定义class_weight = {0:0.9, 1:0.1}，这样类别0的权重为0.9，类别1的权重为0.1。</p><p>对于多分类问题，定义的权重必须具体到每个标签下的每个类，其中类是key-value中的key，权重是value。</p><blockquote><p><strong>dict类型如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment">#标签值，一共16个样本</span></span><br><span class="line"></span><br><span class="line">class_weight = &#123;<span class="number">0</span>:<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>,<span class="number">2</span>:<span class="number">5</span>&#125;   <span class="comment"># 设置</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight)   <span class="comment"># 输出：[1. 3. 5.]，也就是字典中设置的值</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><p><strong>class_weight如何体现在逻辑回归的损失函数上？</strong></p></blockquote><p>  class_weight给每个类别分别设置不同的<strong>惩罚参数C</strong>。</p><p>  惩罚项C会相应的放大或者缩小某一类的损失，如果某一类C越大，这一类的损失也被（相对于其他类来说）放大，那么系统会把本次学习重点放在这一类上，使得系统尽可能的预测对这一类的输入，所以惩罚项C不会影响计算的损失，但反向学习时会相应的放大或缩小损失，间接影响学习的方向。</p><p>  (参考：<a href="https://www.zhihu.com/question/265420166/answer/293896934" target="_blank" rel="noopener">https://www.zhihu.com/question/265420166/answer/293896934</a>)</p><blockquote><p><strong>源码关于class_weight与sample_weight在LR损失函数上的具体计算方式？</strong></p></blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sample_weight *= class_weight_[le.fit_transform(y_bin)] </span><br><span class="line"><span class="comment"># 将class_weight乘到每个样本的sample_weight上</span></span><br><span class="line"><span class="comment"># sample_weight : shape (n_samples,)</span></span><br><span class="line"><span class="comment"># le即LabelEncoder，将标签标准化为0/1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">out = -np.sum(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure><h3 id="2-求解算法参数"><a href="#2-求解算法参数" class="headerlink" title="2. 求解算法参数"></a>2. 求解算法参数</h3><ul><li><p><strong>solver</strong>：str，用于求解模型最优化问题的算法，可选{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认’lbfgs’。</p><ul><li><strong>‘liblinear’</strong>：使用坐标轴下降法来迭代优化损失函数。</li><li><strong>‘lbfgs’</strong>：拟牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。</li><li><strong>‘newton-cg’</strong>：牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。？</li><li><strong>‘sag’</strong>：随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅用一部分的样本来计算梯度，适用于样本量大的情况。</li><li><strong>‘saga’</strong>：线性收敛的随机优化算法的变种。</li></ul><blockquote><ul><li><p>对于数据量大小方面，’liblinear’仅限于处理二分类和一对多（OvR）问题，适用于小型数据集。’sag’和’saga’对于大型数据集来说更快（快速收敛仅在量纲大致相同的数据上得到保证），’sag’每次仅使用了部分样本进行梯度迭代，样本量少时不适合。</p></li><li><p>对于多分类问题来说，’liblinear’只能用于一对多（OvR），其它算法还可处理多对多（MvM），而多对多一般比一对多分类相对更准确一些。</p></li><li><p>对于正则化方法来说，’newton-cg’,’sag’,’lbfgs’这三种算法计算时都需要涉及到损失函数的一阶导或二阶导，因此不能用于没有连续导数的l1正则化，只能用于l2正则化。其他两种算法均可使用l1和l2正则化。</p></li></ul></blockquote><p>（这部分还不是很了解……待补充）</p></li><li><p><strong>dual</strong>：bool，是否使用对偶或原始计算方式。对偶方式仅在solver=’liblinear’与penalty=’l2’连用的情况下有小。如果样本量大于特征的数目，这个参数设置为False会更好。（逻辑回归的对偶形式是什么？……待补充）</p></li></ul><h3 id="3-正则化参数"><a href="#3-正则化参数" class="headerlink" title="3. 正则化参数"></a>3. 正则化参数</h3><p>损失函数</p><script type="math/tex; mode=display">\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>其中$C$为正则化参数（$\lambda\ge0$)，$\alpha$为l1正则化的占比（$\alpha\in[0,1]$）。</p><blockquote><p>这里用的是<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" rel="noopener">sklearn官网</a>给出的损失函数形式， sklearn中假设y正负label定义为1和-1，与之前1/0不一样</p><p>损失函数除去正则化和求和部分剩余部分为：$\log(\exp(- y_i (X_i^T w + c)) + 1)$</p><p>当$y_i=+1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp(- (X_i^T w + c)) + 1)$</p><p>当$y_i=-1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp((X_i^T w + c)) + 1)$</p><p>上式难以继续化简，因此对比之前的损失函数形式，观测结果是否一致</p><p>之前的表达式对应的部分为$-[y^i\log(h_\theta(x^i;\theta))+(1-y^i)\log(1-h_\theta(x^i;\theta))]$</p><p>当$y_i=1$时，$-y^i\log(h_\theta(x^i;\theta))=-\log(\frac{1}{1+\exp(-(X_i^T w + c))})=\log(\exp(-(X_i^T w + c))+1)$</p><p>当$y_i=0$时，$-(1-y^i)\log(1-h_\theta(x^i;\theta))=-\log(1-\frac{1}{\exp(-(X_i^T w + c))+1})=\log(\exp((X_i^T w + c)) + 1)$</p><p>从而证明这两种表达形式是等价的。</p><p>（注意sklearn损失函数里没有除以样本量）</p></blockquote><ul><li><strong>penalty</strong>：str，指定正则化策略 ， {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}，默认’l2’。’elasticnet’同时包含‘l1’和‘l2’正则化。</li><li><strong>C</strong>：float，正则化系数$\lambda$的倒数（乘在损失函数的前面，与乘在正则化部分前效果相同，均用来平衡两个部分的比重），必须是一个大于0的浮点数，默认值1.0，即默认正则项与损失函数的比值是1:1。</li><li><strong>l1_ratio</strong>：float，l1正则化的占比$\rho$，取值范围[0,1]，默认为None。仅当penalty=’elasticnet’时使用，对于0&lt; l1_ratio &lt;1，惩罚是l1和l2正则化的组合。</li></ul><h3 id="4-控制迭代次数参数"><a href="#4-控制迭代次数参数" class="headerlink" title="4. 控制迭代次数参数"></a>4. 控制迭代次数参数</h3><ul><li><strong>max_iter</strong>：int，控制梯度下降的迭代次数（仅适用于solver=’newton-cg’, ‘lbfgs’, ‘sag’）。默认值为100。值过小损失函数可能会没有收敛到最小值，值过大会使得梯度下降迭代次数过多，模型运行时间缓慢。</li><li><strong>tol</strong>：float，让迭代停下的最小值。默认1e-4。数字越大迭代越早停下。</li></ul><h3 id="5-其他参数"><a href="#5-其他参数" class="headerlink" title="5. 其他参数"></a>5. 其他参数</h3><ul><li><strong>random_state</strong>：int(可选)，随机数种子，可选参数（仅适用于solver=’liblinear’, ‘sag’）。默认为无。</li><li><strong>verbose</strong>：int，日志冗长度。对于solver=’liblinear’, ‘lbfgs’，当设置为大于等于1的任何整数时，输出训练的详细过程 。默认为0，不输出训练过程。</li><li><strong>warm_start</strong>：bool，是否进行热启动，默认为False。若设置为True，则以上一次fit的结果作为此次的初始化，如果”solver”参数为”liblinear”时无效。 </li><li><strong>n_jobs</strong>：int，并行数。int类型，默认为1。等于1时用CPU的一个内核运行程序，等于-1时用所有CPU的内核运行程序。 </li></ul><h2 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h2><ul><li><strong>coef_</strong>：预测函数中特征对应的系数$w$。</li><li><strong>intercept_</strong>：预测函数中的截距$c$。</li><li><strong>n_iter_</strong>：实际迭代次数。</li></ul><h2 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h2><ul><li><strong>fit(x,y[,sample_weight])</strong>：训练模型。</li><li><strong>predict(x)</strong>：用模型进行训练，返回预测值。</li><li><strong>predict_log_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率的对数值。</li><li><strong>predict_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率值。</li><li><strong>score(x,y[,sample_weight])</strong>：返回在(x,y)上预测的准确率。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line">clf.predict(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本的类别</span></span><br><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本属于各个类的概率</span></span><br><span class="line">clf.score(X, y) <span class="comment"># 返回准确率</span></span><br><span class="line"></span><br><span class="line">clf.coef_ <span class="comment"># 系数</span></span><br><span class="line">clf.intercept_ <span class="comment">#截距</span></span><br><span class="line">clf.n_iter_ <span class="comment"># 迭代次数</span></span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression</a> </p><p><em>class</em> <code>pyspark.ml.classification.LogisticRegression</code>(<em>featuresCol=’features’</em>, <em>labelCol=’label’</em>, <em>predictionCol=’prediction’</em>, <em>maxIter=100</em>, <em>regParam=0.0</em>, <em>elasticNetParam=0.0</em>, <em>tol=1e-06</em>, <em>fitIntercept=True</em>, <em>threshold=0.5</em>, <em>thresholds=None</em>, <em>probabilityCol=’probability’</em>, <em>rawPredictionCol=’rawPrediction’</em>, <em>standardization=True</em>, <em>weightCol=None</em>, <em>aggregationDepth=2</em>, <em>family=’auto’</em>, <em>lowerBoundsOnCoefficients=None</em>, <em>upperBoundsOnCoefficients=None</em>, <em>lowerBoundsOnIntercepts=None</em>, <em>upperBoundsOnIntercepts=None</em>) </p><h2 id="参数列表-1"><a href="#参数列表-1" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数-1"><a href="#1-基本模型参数-1" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fitIntercept</strong>：bool，是否包含截距项，默认True。</p></li><li><p><strong>family</strong>：str，表示分类是二分类还是多分类，默认’auto’，还可选’binomial’和’multinomial’。</p><blockquote><p>spark中处理多分类问题的’multinomial’使用的是<strong>softmax回归</strong></p><p>(参考：<a href="https://blog.csdn.net/u013855234/article/details/84343963" target="_blank" rel="noopener">spark 2.x 源码分析 之 Logistic Regression 逻辑回归</a>)</p><p>在多分类问题中，假设有$C$个类，即类别标签$y\in\{1,2,…,C\}$，则给定一个样本$x$，softmax回归预测样本$x$属于类别$c$的后验概率为</p><script type="math/tex; mode=display">P(y=c|x;\theta)=\frac{\exp(\theta^T_cx)}{\sum_{c=1}^{C}\exp(\theta^T_cx)}</script><p>其中$\theta^T_c$是第$c$类的权重向量，则样本$x$属于每个类别的概率可以由向量表示，向量的第$c$个元素就是样本被预测为第$c$类的概率。</p></blockquote></li><li><p><strong>threshold</strong>：float，分类中的阈值，默认为0.5。</p></li><li><p><strong>thresholds</strong>：list，分类中的阈值，默认为0.5。多元分类中的thresholds是为了调整预测每个类别时的概率。数组长度必须和类别数目相等，且值都大于0。若thresholds长度为2（即对于二分类问题），要满足<code>threshold = 1/(1+threholds[0]/threholds[1])</code></p><blockquote><p>这两个阈值看上去很迷惑，因此直接进行测试……</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类的情况</span></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThreshold(<span class="number">0.8</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment"># 会认为Thresholds = [1,6], Threshold = 0.8</span></span><br><span class="line"><span class="comment"># 报错，因为不满足threshold = 1/(1+threholds[0]/threholds[1])</span></span><br><span class="line"></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>]).setThreshold(<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># 会认为Threshold = 0.8, 忽略Thresholds(被覆盖，之前定义无效)</span></span><br><span class="line"><span class="comment"># 不报错，将超过0.8的类概率预测为该类</span></span><br></pre></td></tr></table></figure><p>对于多分类的情况比较简单，无论thresholds和threshold如何设定，仍会按照概率最高类进行预测（迷惑行为）</p><p>（参考：<a href="https://stackoverflow.com/questions/47325607/set-thresholds-in-pyspark-multinomial-logistic-regression" target="_blank" rel="noopener">Set thresholds in PySpark multinomial logistic regression</a>）</p></li><li><p><strong>standardization</strong>：bool，是否在训练模型之前对特征进行标准化，默认True。</p><blockquote><p>这里体现出spark与python的不同，spark会默认对特征进行标准化。</p><p>但如果设置<code>standardization = False</code>，仍会将数据进行标准化以提高收敛速度（又一迷惑行为），从而获得相同效果的目标函数。</p><p>这里的标准化是直接除以变量的标准差，没有减去均值的部分。</p></blockquote><p><a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L683" target="_blank" rel="noopener">源码标准化部分</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> standardizationParam = $(standardization)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regParamL1Fun</span> </span>= (index: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">// Remove the L1 penalization on the intercept</span></span><br><span class="line">  <span class="keyword">val</span> isIntercept = $(fitIntercept) &amp;&amp; index &gt;= numFeatures * numCoefficientSets</span><br><span class="line">  <span class="keyword">if</span> (isIntercept) &#123;</span><br><span class="line">    <span class="number">0.0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (standardizationParam) &#123;</span><br><span class="line">      regParamL1</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> featureIndex = index / numCoefficientSets</span><br><span class="line">      <span class="comment">// If `standardization` is false, we still standardize the data</span></span><br><span class="line">      <span class="comment">// to improve the rate of convergence; as a result, we have to</span></span><br><span class="line">      <span class="comment">// perform this reverse standardization by penalizing each component</span></span><br><span class="line">      <span class="comment">// differently to get effectively the same objective function when</span></span><br><span class="line">      <span class="comment">// the training dataset is not standardized.</span></span><br><span class="line">      <span class="keyword">if</span> (featuresStd(featureIndex) != <span class="number">0.0</span>) &#123;</span><br><span class="line">        regParamL1 / featuresStd(featureIndex)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="number">0.0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>aggregationDepth</strong>：int，树聚合所建议的深度，默认为2</p></li><li><p><strong>lowerBoundsOnCoefficients(upperBoundsOnCoefficients)</strong>：？</p></li><li><p><strong>lowerBoundsOnIntercepts(upperBoundsOnIntercepts)</strong>：？</p></li></ul><h3 id="2-正则化参数"><a href="#2-正则化参数" class="headerlink" title="2. 正则化参数"></a>2. 正则化参数</h3><p>正则化部分</p><script type="math/tex; mode=display">\lambda[\frac{1 - \rho}{2}w^T w + \rho \|w\|_1], \rho\in[0,1], \lambda\ge0</script><p>其中$\lambda$为regParam，$\rho$为elasticNetParam。</p><ul><li><p><strong>regParam</strong>：float，正则化参数，默认0.0，即不进行正则化。</p></li><li><p><strong>elasticNetParam</strong>：float，正则化范式比，即l1正则化的占比。默认0.0，即只使用l2正则化。</p><blockquote><p>与sklearn的正则化参数$C$不同，这里的$\lambda$是乘在正则化部分，而$C$乘在损失部分</p><p>仅表达方式不同，改变了参数的位置</p><p>以l2正则为例</p><script type="math/tex; mode=display">J(\theta)+\lambda L_2 \Longleftrightarrow CJ(\theta)+L_2</script><p>$\lambda$越大$C$越小，正则项的地位越高，优化时集中优化$L_2$，从而使参数$\theta$中的元素尽量小</p></blockquote></li></ul><h3 id="3-控制迭代次数参数"><a href="#3-控制迭代次数参数" class="headerlink" title="3. 控制迭代次数参数"></a>3. 控制迭代次数参数</h3><ul><li><strong>maxIter</strong>：int，最大迭代次数，默认100。（与sklearn完全一致）</li><li><strong>tol</strong>：float，让迭代停下的最小值，数字越大迭代越早停下，默认1e-6。（sklearn默认1e-4)</li></ul><h3 id="4-设定列名参数"><a href="#4-设定列名参数" class="headerlink" title="4. 设定列名参数"></a>4. 设定列名参数</h3><p>这部分参数仅用于指定列名和设置输出的列名</p><ul><li><p><strong>featuresCol</strong>：str，输入的数据集中的特征列名（一个合并后的列名），默认’features’</p><blockquote><p>spark输入到模型训练的数据需要把多列的特征合并成一列（测试集也一样）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">model_train_df = model_train.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line">model_test_df = model_test.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line"><span class="comment"># 这里标签列均在最后一列</span></span><br><span class="line"><span class="comment"># 然后就可以在训练模型时令参数featuresCol = 'features'（也可以不用设定，默认值即为'features'）</span></span><br></pre></td></tr></table></figure></li><li><p><strong>labelCol</strong>：str，输入的数据集中的标签列名（同featuresCol），默认’label’</p></li><li><p><strong>predictionCol</strong>：str，输出的模型预测结果中样本预测类的列名，默认’prediction’</p></li><li><p><strong>rawPredictionCol</strong>：str，输出的模型预测结果中原始概率的列名，默认’rawPrediction’</p></li><li><p><strong>probabilityCol</strong>：str，输出的模型预测结果中最终预测概率的列名，默认’probability’</p></li><li><p><strong>weightCol</strong>：str，样本权重列名，默认None，即所有样本的权重均视为等权重。</p></li></ul><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression, LogisticRegressionModel</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">df = sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line"><span class="comment"># 转换为features和label两列的形式</span></span><br><span class="line">model_df = df.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>])</span><br><span class="line"><span class="comment"># 划分训练集、测试集</span></span><br><span class="line">train, test = model_df.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>], seed = <span class="number">123</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lrModel = lr.fit(train)</span><br><span class="line"><span class="comment"># 查看模型系数和截距</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'coefficient'</span>:list(lrModel.coefficientMatrix.toArray()[<span class="number">0</span>])&#125;, index = df.columns[:<span class="number">-1</span>]).sort_values(by = <span class="string">'coefficient'</span>, ascending = <span class="literal">False</span>)</span><br><span class="line">lrModel.interceptVector</span><br><span class="line"><span class="comment"># 在测试集上预测</span></span><br><span class="line">lr_test = lrModel.transform(test)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator()</span><br><span class="line">print(evaluator.evaluate(lr_test, &#123;evaluator.matricName: <span class="string">'areaUnderROC'</span>&#125;))</span><br></pre></td></tr></table></figure><p>模型输出结果（仅取前两行为例）</p><div class="table-container"><table><thead><tr><th>features</th><th>label</th><th>rawPrediction</th><th>probability</th><th>prediction</th></tr></thead><tbody><tr><td>[4.9,3.1,1.5,0.1]</td><td>0</td><td>[60.297,-7.393,-52.905]</td><td>[1,0,0]</td><td>0</td></tr><tr><td>[5.0,3.2,1.2,0.2]</td><td>0</td><td>[64.815,-8.896,-55.919]</td><td>[1,0,0]</td><td>0</td></tr></tbody></table></div><p>该模型为多分类情况，其中rawPrediction为线性回归模型输出结果，probability为经过softmax过后得到的逻辑回归结果。具体来说，以第一行为例，根据rawPrediction输出probability，并以最大值对应的类作为最终预测的类</p><script type="math/tex; mode=display">\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 1\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0</script><h1 id="集群scala实现"><a href="#集群scala实现" class="headerlink" title="集群scala实现"></a>集群scala实现</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.param.<span class="type">ParamMap</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.&#123;<span class="type">Vector</span>, <span class="type">Vectors</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// basic variable settings</span></span><br><span class="line"><span class="keyword">val</span> train_tbl = <span class="string">"test.sklearn_dataset_iris_train"</span></span><br><span class="line"><span class="keyword">val</span> test_tbl = <span class="string">"test.sklearn_dataset_iris_test"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hc = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">//data processing</span></span><br><span class="line"><span class="keyword">val</span> train_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$train_tbl</span>"</span>).cache())</span><br><span class="line"><span class="keyword">val</span> test_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$test_tbl</span>"</span>).cache())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> featureCols = train_dataset.columns.filter(x=&gt;x.split(<span class="string">"_"</span>)(x.split(<span class="string">"_"</span>).length<span class="number">-1</span>)!=<span class="string">"label"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> assembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>().setInputCols(featureCols).setOutputCol(<span class="string">"features"</span>))</span><br><span class="line"><span class="keyword">val</span> train_data = assembler.transform(train_dataset)</span><br><span class="line"><span class="keyword">val</span> test_data = assembler.transform(test_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line">lr.setMaxIter(<span class="number">10</span>).setRegParam(<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">val</span> lr_model = lr.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model evaluation</span></span><br><span class="line"><span class="keyword">val</span> auc_calculator = lr_model.transform(test_data)</span><br><span class="line"><span class="keyword">val</span> evaluator = (<span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>())</span><br><span class="line"><span class="keyword">val</span> auc = evaluator.evaluate(auc_calculator)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> LogisticRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WOE与IV理论介绍及实现</title>
      <link href="/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="WOE定义"><a href="#WOE定义" class="headerlink" title="WOE定义"></a>WOE定义</h2><p>全称：Weight of Evidence</p><p>前提：计算之前需要是离散化后的连续变量or离散变量。</p><p>对于变量第$i$个取值的$WOE_i$的计算公式为</p><script type="math/tex; mode=display">WOE_i = ln(\frac{py_i} {pn_i})=ln(\frac{ \frac{ \#y_i }{ \#y_T } } { \frac{ \#n_i } { \#n_T }})=ln(\frac{\frac{ \#y_i }{ \#n_i } } {\frac{ \#y_T } { \#n_T } })</script><p>其中 #$y_i $表示在第$i$个取值的样本中（或第$i$个箱内）的正样本个数，#$ y_T $表示所有正样本的个数，$py_i$表示在第$i$个取值的样本中（或第$i$个箱内）正样本占所有正样本的比例。</p><p>从最后一个等号后的表达式可以看出，$WOE_i$表示的是某个取值下正样本和负样本的比值，与所有样本中这个比值的差异。这个差异是用比值再取对数来表示的。$WOE_i$越大，表示差异越大，则这个取值下的样本为正的可能性越大。</p><h2 id="WOE编码作用"><a href="#WOE编码作用" class="headerlink" title="WOE编码作用"></a>WOE编码作用</h2><ol><li>标准化功能。编码过后的自变量其实具备了某种标准化的性质，自变量内部的各个取值之间都可以直接进行比较，且不同自变量之间的各个取值也可以通过$WOE_i$进行直接的比较。</li><li>可以反映出自变量的贡献情况（？）。自变量内部$WOE_i$值的变异（波动）情况，结合模型拟合出的系数，构造出各个自变量的贡献率和相对重要性。一般地，系数越大，$WOE_i$的方差越大，则自变量的贡献率越大。</li></ol><h2 id="IV​"><a href="#IV​" class="headerlink" title="IV​"></a>IV​</h2><p>全程：Information Value</p><p>在$WOE_i$的基础上，$IV$在$WOE_i$的前面乘以一个系数$(py_i-pn_i)$作为各箱$WOE_i$的权重，并进行加权求和，其计算公式如下</p><script type="math/tex; mode=display">IV=\sum_{i=1}^{N}(py_i-pn_i)*WOE_i=\sum_{i=1}^{N}(py_i-pn_i)*ln(\frac{py_i}{pn_i})</script><p>其中$N$为分箱的个数（变量的取值个数），系数$(py_i-pn_i)$为箱内正样本占比与负样本占比的差。该系数不仅保证了每个箱乘积的值为非负数，更重要的是考虑了变量当前箱中样本的数量占整体样本数量的比例，比例越高，该箱对变量整体预测能力的贡献越高。</p><p>$IV$衡量某个特征对目标的影响程度，通过该特征中正负样本的比例与总体正负样本的比例，来对比和计算其关联程度，因此可以代表<strong>该特征上的信息量</strong>以及<strong>该特征对模型的贡献</strong>。</p><p>$IV$是对于整个特征来说的，代表的意义由下表来控制：</p><div class="table-container"><table><thead><tr><th>IV</th><th>特征对预测函数的贡献</th></tr></thead><tbody><tr><td>&lt;0.03</td><td>特征几乎不含有效信息，对模型没有贡献，可以删除</td></tr><tr><td>[0.03, 0.10)</td><td>有效信息很少，对模型的贡献度低</td></tr><tr><td>[0.10, 0.30)</td><td>有效信息一般，对模型的贡献度中等</td></tr><tr><td>[0.30, 0.50)</td><td>有效信息较多，对模型的贡献度较高</td></tr><tr><td>&gt;=0.50</td><td>有效信息非常多，对模型的贡献极高且可疑</td></tr></tbody></table></div><p>因此通常会选择$IV$值在0.1~0.5范围内的特征。</p><h1 id="单机python"><a href="#单机python" class="headerlink" title="单机python"></a>单机python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcWOE</span><span class="params">(df, col, target)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得某个变量的分箱操作后每个箱体对应的WOE值，并且基于WOE值计算该变量的IV值</span></span><br><span class="line"><span class="string">    :param df: 包含需要计算WOE的变量和目标变量</span></span><br><span class="line"><span class="string">    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的离散型变量</span></span><br><span class="line"><span class="string">    :param target: 目标变量，0、1表示好、坏</span></span><br><span class="line"><span class="string">    :return: 返回WOE和IV</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    total = df.groupby([col])[target].count()  <span class="comment"># 每个箱体的总数</span></span><br><span class="line">    total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)    </span><br><span class="line">    bad = df.groupby([col])[target].sum()      <span class="comment"># 每个箱体的坏样本数</span></span><br><span class="line">    bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line">    regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>)</span><br><span class="line">    regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    N = sum(regroup[<span class="string">'total'</span>]) <span class="comment"># 总体样本数</span></span><br><span class="line">    B = sum(regroup[<span class="string">'bad'</span>])   <span class="comment"># 总体坏样本数</span></span><br><span class="line">    regroup[<span class="string">'good'</span>] = regroup[<span class="string">'total'</span>] - regroup[<span class="string">'bad'</span>]  <span class="comment"># 每个箱体的好样本数</span></span><br><span class="line">    G = N - B                 <span class="comment"># 总体好样本数</span></span><br><span class="line">    regroup[<span class="string">'bad_pcnt'</span>] = regroup[<span class="string">'bad'</span>].map(<span class="keyword">lambda</span> x: x/B)    <span class="comment"># 每个箱体的坏样本数占总体的坏样本数的比例</span></span><br><span class="line">    regroup[<span class="string">'good_pcnt'</span>] = regroup[<span class="string">'good'</span>].map(<span class="keyword">lambda</span> x: x/G)  <span class="comment"># 每个箱体的好样本数占总体的好样本数的比例</span></span><br><span class="line">    <span class="comment"># WOEi计算公式(不含系数)： WOE(每个箱体) = ln(该箱体的好样本数占总体的好样本数的比例/该箱体的好样本数占总体的好样本数的比例）</span></span><br><span class="line">    regroup[<span class="string">'WOE'</span>] = regroup.apply(<span class="keyword">lambda</span> x: np.log(x.good_pcnt/x.bad_pcnt),axis=<span class="number">1</span>)   </span><br><span class="line">    WOE_dict = regroup[[col,<span class="string">'WOE'</span>]].set_index(col).to_dict(orient=<span class="string">'index'</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> WOE_dict.items(): <span class="comment"># k代表箱体名，v代表了以WOE为键，箱体的实际WOE值为value的字典</span></span><br><span class="line">        WOE_dict[k] = v[<span class="string">'WOE'</span>]</span><br><span class="line">    <span class="comment"># 计算该变量的IV值：sum((某箱体的好样本数占总体的好样本数的比例 - 该箱体的坏样本数占总体的坏样本数的比例)*WOE(某个箱体))</span></span><br><span class="line">    IV = regroup.apply(<span class="keyword">lambda</span> x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt/x.bad_pcnt),axis = <span class="number">1</span>)</span><br><span class="line">    IV = sum(IV)</span><br><span class="line">    <span class="keyword">return</span> &#123;col: &#123;<span class="string">"WOE"</span>: WOE_dict, <span class="string">'IV'</span>:IV&#125;&#125;</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先进行等频分箱和编码（用到的函数见'分箱'）</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">     df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后循环计算每个变量的各箱WOE和IV，输出result</span></span><br><span class="line">result = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     woe_iv_dict = CalcWOE(df_cut, col, target = <span class="string">'label'</span>)</span><br><span class="line">     result = pd.concat([result, pd.DataFrame(woe_iv_dict).T])</span><br></pre></td></tr></table></figure><h1 id="集群pyspark"><a href="#集群pyspark" class="headerlink" title="集群pyspark"></a>集群pyspark</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_woe_iv</span><span class="params">(tableName, featureCol, labelCol)</span>:</span></span><br><span class="line">     <span class="string">'''</span></span><br><span class="line"><span class="string">     计算spark.sql.DataFrame的WOE和IV</span></span><br><span class="line"><span class="string">     :param tableName: 储存在HDFS上的表名</span></span><br><span class="line"><span class="string">     :param featureCol: 所需要计算的字段名</span></span><br><span class="line"><span class="string">     :param labelCol: 标签列名</span></span><br><span class="line"><span class="string">     :return WOE字典和IV值</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">     df = spark.sql(<span class="string">"select &#123;featureCol&#125;, &#123;labelCol&#125; from &#123;tableName&#125;"</span>.format(featureCol=featureCol, labelCol=labelCol, tableName=tableName))</span><br><span class="line">     <span class="comment"># 使用crosstab函数生成列联表，并对列进行命名</span></span><br><span class="line">     binCount = df.crosstab(featureCol,labelCol).toDF(<span class="string">'feature'</span>,<span class="string">'neg_cnt'</span>,<span class="string">'pos_cnt'</span>)</span><br><span class="line">     <span class="comment"># 注册临时表便于后续调用</span></span><br><span class="line">     binCount.registerTempTable(<span class="string">"binCount"</span>)</span><br><span class="line">     <span class="comment"># 计算主要步骤</span></span><br><span class="line">     woe_df = spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">        select</span></span><br><span class="line"><span class="string">          feature,</span></span><br><span class="line"><span class="string">          nvl(log(pos_per/neg_per), 0) as woe_i,</span></span><br><span class="line"><span class="string">          nvl((pos_per-neg_per)*log(pos_per/neg_per), 0) as iv_i</span></span><br><span class="line"><span class="string">        from</span></span><br><span class="line"><span class="string">        (</span></span><br><span class="line"><span class="string">          select</span></span><br><span class="line"><span class="string">            feature,</span></span><br><span class="line"><span class="string">            neg_cnt/(select sum(neg_cnt) from binCount) as neg_per,</span></span><br><span class="line"><span class="string">            pos_cnt/(select sum(pos_cnt) from binCount) as pos_per</span></span><br><span class="line"><span class="string">          from</span></span><br><span class="line"><span class="string">            binCount</span></span><br><span class="line"><span class="string">        ) t</span></span><br><span class="line"><span class="string">     """</span>)</span><br><span class="line">     <span class="comment"># 将计算结果转化为pandas.DataFrame</span></span><br><span class="line">     woe_df_pandas = woe_df.toPandas()</span><br><span class="line">     woe = dict(zip(woe_df_pandas[<span class="string">"feature"</span>].values.reshape(<span class="number">-1</span>,), woe_df_pandas[<span class="string">"woe_i"</span>].values.reshape(<span class="number">-1</span>,),))</span><br><span class="line">     iv = sum(woe_df_pandas[<span class="string">"iv_i"</span>].values)</span><br><span class="line">     <span class="keyword">return</span> woe, iv</span><br></pre></td></tr></table></figure><p>调用并测试运算时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    spark = SparkSession.builder.enableHiveSupport().getOrCreate()</span><br><span class="line">    tableName = <span class="string">"test.calc_woe_iv_sample"</span></span><br><span class="line">    featureCol = <span class="string">"age_level"</span></span><br><span class="line">    labelCol = <span class="string">"label"</span></span><br><span class="line">    time_cost = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        woe, iv = cal_woe_iv(tableName, featureCol, labelCol)</span><br><span class="line">        end = time.time()</span><br><span class="line">        time_cost.append(end-start)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'time cost: %.5f sec'</span> % (np.mean(time_cost)))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'woe:'</span>, woe)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'iv:'</span>, iv)</span><br></pre></td></tr></table></figure><p>集群资源配置：</p><blockquote><p>spark.driver.memory=5G<br>spark.driver.maxResultSize=5G<br>num-executors=20<br>executor-cores=6<br>executor-memory=4G</p></blockquote><p>根据样本量的不同，测试计算时间结果（循环30次取平均）</p><div class="table-container"><table><thead><tr><th></th><th>10w</th><th>100w</th><th>1000w</th><th>1e</th></tr></thead><tbody><tr><td>计算时间(s)</td><td>0.872</td><td>1.819</td><td>3.172</td><td>25.793</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> FeatureEngineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分箱基本方法介绍及实现</title>
      <link href="/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将连续型的数据分为几个数据段，即特征离散化。</p><p>把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率。（百度百科） </p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li><p><strong>离散化后的特征对异常数据有很强的鲁棒性，模型更加稳定。</strong>比如年龄&gt;70岁为一个分箱，异常数据如年龄为300岁同样会划分到该箱，不会给模型造成很大的干扰；比如将20岁-30岁划分为一个区间，不会因为年龄增长一岁就变成一个完全不同的人。而对于处于区间相邻处的样本会刚好相反，因此如何划分区间也很重要。</p></li><li><p>对于如逻辑回归的广义线性模型，表达能力受限，单变量离散化为$N$个变量后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。（？）</p></li><li><p>离散化后可以进行特征交叉，进一步<strong>引入非线性，提升模型表达能力</strong>。</p></li><li><p>可以将缺失值作为独立的箱代入变量。</p></li><li><p>可以将所有变量转换到相似的尺度上（不需要进行归一化）。</p></li></ol><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="无监督分箱"><a href="#无监督分箱" class="headerlink" title="无监督分箱"></a>无监督分箱</h3><h4 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h4><p>每个箱包含大致相等的样本数量，可以根据分位数进行划分，如四分位数等。</p><h4 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h4><p>从变量的最小值到最大值之间均等分为$N$等份。若$m$和$M$分别代表最小值和最大值，则每个区间的长度为$w=\frac{M-m}{N}$，区间的边界值为$m+w,m+2w,…,m+(N-1)w$。这里只考虑边界，每个等份的样本数量一般不等。</p><h3 id="有监督分箱"><a href="#有监督分箱" class="headerlink" title="有监督分箱"></a>有监督分箱</h3><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p><strong>初始化</strong></p><ul><li><p>根据连续变量值的大小进行排序。</p></li><li><p>把每一个单独的值视为一个箱体，构建最初的离散化。目的是想从每个单独的箱体开始逐渐合并。</p></li></ul></li><li><p><strong>合并</strong>：在初始化构建完毕后，该步就是不断地进行自底向上的合并，直到满足停止条件。</p><ul><li><p><strong>计算所有相邻分箱的卡方值</strong>。比如有1,2,3,4这4个分箱，绑定相邻的两个分箱，一共有3组：12,23,34，然后分别计算三个绑定组的卡方值。卡方值的计算公式为</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}</script><p>其中，$m$表示要合并相邻分箱的数目（$m=2$表示对两个箱进行合并），$k$表示目标变量的类别数（两分类或多分类），$A_{ij}$表示实际频数（即第$i$个分箱第$j$类别的频数），$E_{ij}$表示期望频数，计算公式如下（可根据公式$P(AB)=P(A)P(B)$推导出来）</p><script type="math/tex; mode=display">E_{ij}=\frac{R_i*C_j}{N}</script><p>其中$R_i$和$C_j$分别是实际频数整行和整列的加和，$N$为总样本量。</p></li><li><p><strong>从计算的卡方值中找出最小的一个，并把这两个分箱合并</strong>。比如23是卡方值最小的一个，那么就将2和3合并，经过本轮的计算后分箱就变为1,23,4。</p><p>从合并的方法可以看出，卡方分箱的基本思想是，<strong>如果两个相邻的区间具有非常类似的类分布（即低卡方值，低卡方值表明它们具有类似的类分布），那么这两个区间可以合并，否则应该分开</strong>。</p></li></ul></li><li><p><strong>停止条件</strong>：以上仅是每一轮需要计算的内容，若不设置停止条件，算法会一直运行。一般从以下两个方面设置停止条件：</p><ul><li>卡方停止的阈值</li><li>分箱数目的限制</li></ul><p>即当所有分箱对的卡方值均大于阈值，且分箱数大于分箱数时，计算就会继续，直到不满足。</p><p>以上两个阈值一般根据经验来定义，来作为分箱函数的参数进行设置，一般使用0.90,0.95,0.99的置信度，分箱数一般可以设置为5。</p></li></ul><p><em>例如：</em></p><p>对于某两个箱（分箱1和分箱2），实际频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th><th>行频数和</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$A_{11}$</td><td>$A_{12}$</td><td>$R_1$</td></tr><tr><td><strong>分箱2</strong></td><td>$A_{21}$</td><td>$A_{22}$</td><td>$R_2$</td></tr><tr><td><strong>列频数和</strong></td><td>$C_1$</td><td>$C_2$</td><td>$N$</td></tr></tbody></table></div><p>期望频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$E_{11}=\frac{R_1*C_1}{N}$</td><td>$E_{12}=\frac{R_1*C_2}{N}$</td></tr><tr><td><strong>分箱2</strong></td><td>$E_{21}=\frac{R_2*C_1}{N}$</td><td>$E_{22}=\frac{R_2*C_2}{N}$</td></tr></tbody></table></div><p>代入卡方公式求解，过程如下：</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}=[\frac{(A_{11}-E_{11})^2}{E_{11}}+\frac{(A_{12}-E_{12})^2}{E_{12}}]+[\frac{(A_{21}-E_{21})^2}{E_{21}}+\frac{(A_{22}-E_{22})^2}{E_{22}}]</script><p>如果计算结果是所有卡方值中最小的，说明这两个分箱具有最相似的类分布，因此把它们合并。</p><p>（参考 <a href="https://cloud.tencent.com/developer/article/1418720" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1418720</a> ）</p><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><h2 id="无监督分箱-1"><a href="#无监督分箱-1" class="headerlink" title="无监督分箱"></a>无监督分箱</h2><ul><li>UnsupervisedSplitBin函数：对数值型变量进行分组，分组的依据有等频和等距，最后返回划分点列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UnsupervisedSplitBin</span><span class="params">(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对数值型变量进行划分，最后返回划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: 数据集</span></span><br><span class="line"><span class="string">:param col: 需要分箱的变量。仅限数值型变量</span></span><br><span class="line"><span class="string">:param numOfSplit: 需要分箱个数，默认是5</span></span><br><span class="line"><span class="string">:param method: 分箱方法，'equalFreq'：默认是等频，否则是等距</span></span><br><span class="line"><span class="string">:return: 返回划分点组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'equalFreq'</span>:</span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = [np.percentile(df[col], <span class="number">100</span>/numOfSplit*i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line">splitPoint = sorted(list(set(frequency_cutoff)))</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">'equalDis'</span>:</span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">var_max, var_min = max(df[col]), min(df[col])</span><br><span class="line">interval_len = (var_max-var_min)/numOfSplit</span><br><span class="line">splitPoint = [var_min+i*interval_len <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="string">'Sorry, no such method'</span></span><br></pre></td></tr></table></figure><h2 id="有监督分箱：卡方分箱"><a href="#有监督分箱：卡方分箱" class="headerlink" title="有监督分箱：卡方分箱"></a>有监督分箱：卡方分箱</h2><ul><li><p>SplitData函数：用于对某变量进行划分，根据划分的组数，返回划分点数值组成的列表</p></li><li><p>Chi2函数：用于计算卡方值，返回卡方值（按照卡方检验的原理进行计算）</p></li><li><p>BinBadRate函数：按某变量进行分组，计算分组后每组的坏样本率，返回的有，字典形式，数据框，总体坏样本率（可选）</p></li><li><p>AssignGroup函数：根据分组后的划分点列表，给某个需分箱的变量的每个取值进行分箱前的匹配，形成对应箱的映射</p></li><li><p>AssignBin函数：将某列的每个取值进行分箱编号</p></li><li><p>ChiMerge函数：卡方分箱的主体函数，其中调用了前面五个基础函数，返回的是最终的满足所有限制条件的划分点列表</p><ul><li><p>其中需要满足：</p><p>（1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</p><p>（2）每个箱体必须同时包含好坏样本</p><p>（3）每个箱体的占比不低于预设值（可选）</p><p>（4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(df,col,numOfSplit,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得数据切分时对应位置的数值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame</span></span><br><span class="line"><span class="string">:param col: str 选择数据集中的某列进行操作</span></span><br><span class="line"><span class="string">:param numOfSplit: int 划分的组数</span></span><br><span class="line"><span class="string">:param special_attribute: 用于过滤掉一些特殊的值，不参与数据划分（可选，默认不过滤）</span></span><br><span class="line"><span class="string">:return: 返回划分点位置对应的数值组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"><span class="keyword">if</span> special_attribute != []:</span><br><span class="line">df2 = df2.loc[~df2[col].isin(special_attribute)]  <span class="comment"># 排除有特殊值的样本行</span></span><br><span class="line">N = len(df2) <span class="comment"># 样本总数</span></span><br><span class="line">n = int(N/numOfSplit) <span class="comment"># 每组包含的样本量</span></span><br><span class="line">SplitPointIndex = [i*n <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)] <span class="comment"># 不包含numOfSplit</span></span><br><span class="line"><span class="comment"># 如总样本N=20，numofsplit为5组，则每组包含4个元素(n=4)</span></span><br><span class="line"><span class="comment"># 则最后得到的SplitPointIndex=[4,8,12,16]，即切分点位置，各组样本为0-3/4-7/8-11/12-15</span></span><br><span class="line">rawValues = sorted(list(df[col])) <span class="comment"># sorted返回一个新的排列后的列表</span></span><br><span class="line">SplitPoint = [rawValues[i] <span class="keyword">for</span> i <span class="keyword">in</span> SplitPointIndex] <span class="comment"># 返回位置索引上对应的数值</span></span><br><span class="line"><span class="comment"># 为了以防万一，去重再排序</span></span><br><span class="line">SplitPoint = sorted(list(set(SplitPoint)))</span><br><span class="line"><span class="keyword">return</span> SplitPoint </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignGroup</span><span class="params">(x,bin)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于：根据分组后的划分点列表（bin)，给某个需要分箱的变量进行分箱前的匹配，形成对应箱的映射，以便后期分箱操作，将值划分到不同箱体</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param bin: 上述变量分组后（通过SplitData函数）对应的划分位置的数值组成的列表</span></span><br><span class="line"><span class="string">:return: x在分箱结果下的映射 </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">N = len(bin)            <span class="comment"># 划分点的长度</span></span><br><span class="line"><span class="keyword">if</span> x&lt;=min(bin):         <span class="comment"># 如果某个取值小于等于最小划分点，则返回最小划分点</span></span><br><span class="line"><span class="keyword">return</span> min(bin)</span><br><span class="line"><span class="keyword">elif</span> x&gt;max(bin):        <span class="comment"># 如果某个取值大于最小划分点，则返回10e10</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">10e10</span></span><br><span class="line"><span class="keyword">else</span>:                   <span class="comment"># 除此之外，返回其他对应的划分点</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> bin[i] &lt; x &lt;= bin[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> bin[i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BinBadRate</span><span class="params">(df,col,target,grantRateIndicator=False)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对变量按照取值（每个取值都是唯一的）进行分组，获得每箱的坏样本率，后期基于该值判断是否需要合并操作</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 需要计算好坏比率的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要计算好坏比率的特征</span></span><br><span class="line"><span class="string">:param target: str 好坏标签</span></span><br><span class="line"><span class="string">:param grantRateIndicator: bool True返回总体的坏样本率，False不返回（可选，默认不返回）</span></span><br><span class="line"><span class="string">:return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator==True时）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">total = df.groupby([col])[target].count()</span><br><span class="line">total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)</span><br><span class="line">bad = df.groupby([col])[target].sum()</span><br><span class="line">bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line"></span><br><span class="line">regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>) <span class="comment"># 每箱的坏样本数，总样本数</span></span><br><span class="line">regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regroup[<span class="string">'bad_rate'</span>] = regroup.apply(<span class="keyword">lambda</span> x: x.bad / x.total, axis=<span class="number">1</span>) <span class="comment"># 加上一列坏样本率</span></span><br><span class="line">dicts = dict(zip(regroup[col],regroup[<span class="string">'bad_rate'</span>])) <span class="comment"># 每箱对应的坏样本率组成的字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> grantRateIndicator:</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">N = sum(regroup[<span class="string">'total'</span>])</span><br><span class="line">B = sum(regroup[<span class="string">'bad'</span>])</span><br><span class="line">overallRate = B / N</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup, overallRate)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Chi2</span><span class="params">(df,totalCol,badCol)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得卡方值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含了每个分组下的样本总数和坏样本数</span></span><br><span class="line"><span class="string">:param totalCol: str 列名，元素由各个分组下的样本总数构成</span></span><br><span class="line"><span class="string">:param badCol: str 列名，元素由各个分组下的坏样本数构成</span></span><br><span class="line"><span class="string">:return: 卡方值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算该特征各个分组下的好样本数</span></span><br><span class="line">df2[<span class="string">'good'</span>] = df2.apply(<span class="keyword">lambda</span> x: x[totalCol]-x[badCol], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求出期望值E</span></span><br><span class="line"><span class="comment"># 1.求出总体的坏样本率和好样本率</span></span><br><span class="line">badRate = sum(df2[badCol]) / sum(df2[totalCol])</span><br><span class="line">goodRate = sum(df2[<span class="string">'good'</span>]) / sum(df2[totalCol])</span><br><span class="line"><span class="comment"># 特殊情况：当全部样本只有好或者坏样本时，卡方值为0</span></span><br><span class="line"><span class="keyword">if</span> badRate <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.根据总体的好坏样本率算出期望的好坏样本数，计算公式为：</span></span><br><span class="line"><span class="comment"># 期望坏（好）样本个数 ＝ 全部样本个数 * 总体的坏（好）样本率</span></span><br><span class="line">df2[<span class="string">'badExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*badRate)</span><br><span class="line">df2[<span class="string">'goodExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*goodRate)</span><br><span class="line"></span><br><span class="line">badCombined = zip(df2[<span class="string">'badExpected'</span>], df2[badCol])</span><br><span class="line">goodCombined = zip(df2[<span class="string">'goodExpected'</span>], df2[<span class="string">'good'</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 按照卡方计算的公式计算卡方值：  ∑ (O - E)^2 / E, O代表实际值，E代表期望值     </span></span><br><span class="line">badChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> badCombined]</span><br><span class="line">goodChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> goodCombined]</span><br><span class="line">chi2 = sum(badChi) + sum(goodChi)</span><br><span class="line"><span class="keyword">return</span> chi2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignBin</span><span class="params">(x,cutOffPoints,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对某个列的某个取值进行分箱编号（编码）</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param cutOffPoints: 上述变量的分组结果，用切分点表示，列表形式</span></span><br><span class="line"><span class="string">:param special_attribute: 不参与分箱的特殊取值（可选）</span></span><br><span class="line"><span class="string">:return: 分箱后的对应的第几个箱，从0开始</span></span><br><span class="line"><span class="string">比如，若cutOffPoints=[10,20,30]，当x=7，返回0；当x=35，返回3</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">numBin = len(cutOffPoints) + <span class="number">1</span> + len(special_attribute)</span><br><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> special_attribute:</span><br><span class="line">i = special_attribute.index(x)+<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> (<span class="number">0</span>-i)</span><br><span class="line"><span class="keyword">if</span> x &lt;= cutOffPoints[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">elif</span> x &gt; cutOffPoints[<span class="number">-1</span>]:</span><br><span class="line"><span class="keyword">return</span> (numBin<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,numBin<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> cutOffPoints[i] &lt; x &lt;= cutOffPoints[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> (i+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ChiMerge</span><span class="params">(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于实际的卡方分箱算法操作，最后返回有实际的划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含目标变量与需要分箱变量的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要分箱的属性</span></span><br><span class="line"><span class="string">:param target: str 目标变量，取值0或1</span></span><br><span class="line"><span class="string">:param max_interval: int 最大分箱数（默认5）。如果原始属性的取值个数低于该参数，不执行这段函数</span></span><br><span class="line"><span class="string">:param special_attribute: list 不参与分箱的属性取值（可选）</span></span><br><span class="line"><span class="string">:param minBinPcnt：最小箱的占比（默认0），如果不满足最小分箱占比继续进行组别合并</span></span><br><span class="line"><span class="string">:return: 分箱结果</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">colLevels = sorted(list(set(df[col])))  <span class="comment"># 某列的不重复值</span></span><br><span class="line">N_distinct = len(colLevels)             <span class="comment"># 某列的不重复值计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> N_distinct &lt;= max_interval:  <span class="comment">#如果原始属性的取值个数低于max_interval，不执行这段函数（不参与分箱）</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The number of original levels for '&#123;col&#125;' is &#123;dis_cnt&#125;, which is less than or equal to max intervals"</span>.format(col=col, dis_cnt=N_distinct))</span><br><span class="line"><span class="keyword">return</span> colLevels[:<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">if</span> len(special_attribute)&gt;=<span class="number">1</span>:</span><br><span class="line">df2 = df.loc[~df[col].isin(special_attribute)] <span class="comment"># 去掉special_attribute后的df</span></span><br><span class="line">N_distinct = len(list(set(df2[col])))  <span class="comment"># 去掉special_attribute后的非重复值计数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤一: 通过col对数据进行分组，求出每组的总样本数和坏样本数</span></span><br><span class="line"><span class="keyword">if</span> N_distinct &gt; <span class="number">100</span>:</span><br><span class="line">split_x = SplitData(df2,col,<span class="number">100</span>) <span class="comment"># 若非重复值计数超过100组，均将其转化成100组，将多余样本都划分到最后一个箱中</span></span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col].map(<span class="keyword">lambda</span> x: AssignGroup(x,split_x))</span><br><span class="line"><span class="comment"># Assgingroup函数：每一行的数值和切分点做对比，返回原值在切分后的映射，</span></span><br><span class="line"><span class="comment"># 经过map以后，生成该特征的值对象的"分箱"后的值        </span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col] <span class="comment"># 不重复值计数不超过100时，不需要进行上述步骤</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过上述过程，我们现在可以将该列进行BadRate计算，用来计算每个箱体的坏样本率 以及总体的坏样本率      </span></span><br><span class="line">(binBadRate, regroup, overallRate) = BinBadRate(df2, <span class="string">'temp_cutoff'</span>, target, grantRateIndicator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在此，我们将每个单独的属性值分成单独一组</span></span><br><span class="line"><span class="comment"># 对属性值进行去重排序，然后两两组别进行合并，用于后续的卡方值计算</span></span><br><span class="line">colLevels = sorted(list(set(df2[<span class="string">'temp_cutoff'</span>])))</span><br><span class="line">groupIntervals = [[i] <span class="keyword">for</span> i <span class="keyword">in</span> colLevels] <span class="comment"># 把每个箱的值打包成[[],[]]的形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤二：通过循环的方式，不断的合并相邻的两个组别，直到：</span></span><br><span class="line"><span class="comment"># （1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</span></span><br><span class="line"><span class="comment"># （2）每个箱体必须同时包含好坏样本</span></span><br><span class="line"><span class="comment"># （3）每个箱体的占比不低于预设值（可选）</span></span><br><span class="line"><span class="comment"># （4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</span></span><br><span class="line">split_intervals = max_interval - len(special_attribute)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次循环时, 计算合并相邻组别后的卡方值。当组别数大于预设的分箱数时，持续选择最小卡方值的组合并</span></span><br><span class="line"><span class="keyword">while</span> len(groupIntervals)&gt;split_intervals:</span><br><span class="line">chisqList = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(len(groupIntervals)<span class="number">-1</span>):</span><br><span class="line">temp_group = groupIntervals[k] + groupIntervals[k+<span class="number">1</span>]  <span class="comment"># 返回的是，这两个值组成的列表</span></span><br><span class="line"><span class="comment"># 因此，可以通过temp_group，每次只选相邻的两组进行相关操作</span></span><br><span class="line">df2b = regroup.loc[regroup[<span class="string">'temp_cutoff'</span>].isin(temp_group)]</span><br><span class="line"><span class="comment"># 计算相邻两组的卡方值(通过调用Chi2函数)</span></span><br><span class="line">chisq = Chi2(df2b,<span class="string">'total'</span>,<span class="string">'bad'</span>)</span><br><span class="line">chisqList.append(chisq)</span><br><span class="line">best_comnbined = chisqList.index(min(chisqList)) <span class="comment"># 检索最小卡方值所在的索引    </span></span><br><span class="line"><span class="comment"># 把groupIntervals的值改成类似的值改成类似从[[1],[2],[3]]到[[1,2],[3]]</span></span><br><span class="line">groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+<span class="number">1</span>]</span><br><span class="line">groupIntervals.remove(groupIntervals[best_comnbined+<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述循环结束后，即可获得预设的分箱数，并且可以得到各分箱的划分点</span></span><br><span class="line">groupIntervals = [sorted(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals]</span><br><span class="line">cutOffPoints = [max(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals[:<span class="number">-1</span>]] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后，我们将某列进行分箱编号</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp_cutoff'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line"><span class="comment"># AssignBin函数：每一行的数值和切分点做对比，返回原值所在的分箱编号（形成新列）    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步，我们想要保证每个箱体都有好坏样本数</span></span><br><span class="line"><span class="comment"># 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本</span></span><br><span class="line">(binBadRate,regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)    <span class="comment"># 返回每箱坏样本率字典，和包含'分箱号、总样本数、坏样本数、坏样本率的数据框'）</span></span><br><span class="line">minBadRate, maxBadRate = min(binBadRate.values()), max(binBadRate.values())</span><br><span class="line"><span class="keyword">while</span> minBadRate ==<span class="number">0</span> <span class="keyword">or</span> maxBadRate == <span class="number">1</span>:</span><br><span class="line"><span class="comment"># 找出全部为好／坏样本的箱</span></span><br><span class="line">indexForBad01 = regroup[regroup[<span class="string">'bad_rate'</span>].isin([<span class="number">0</span>,<span class="number">1</span>])].temp_Bin.tolist()    </span><br><span class="line">bin = indexForBad01[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个划分点需要移除</span></span><br><span class="line"><span class="keyword">if</span> bin == max(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> bin == min(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果是中间的某一箱，则需要和前后中的一个箱体进行合并，具体选择哪个箱体，要依据前后箱体哪个卡方值较小</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(regroup.temp_Bin).index(bin)</span><br><span class="line">prevIndex = list(regroup.temp_Bin)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(regroup.temp_Bin)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成此项合并后，需要再次计算，在新的分箱准则下，每箱是否同时包含好坏样本，</span></span><br><span class="line"><span class="comment"># 如何仍然出现不能同时包含好坏样本的箱体，继续循坏，直到好坏样本同时出现在每个箱体后，跳出</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">(binBadRate, regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">[minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们来检查分箱后的最小占比</span></span><br><span class="line"><span class="keyword">if</span> minBinPcnt &gt; <span class="number">0</span>: <span class="comment"># 如果函数调用初期给的minBinPct不是零，则进一步对箱体进行合并</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>]) <span class="comment"># 得到箱体的最小占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果箱体最小占比小于给定的分箱占比阈值 且 划分点大于2，进入合并循环中</span></span><br><span class="line"><span class="keyword">while</span> minPcnt &lt; minBinPcnt <span class="keyword">and</span> len(cutOffPoints) &gt; <span class="number">2</span>: </span><br><span class="line"><span class="comment"># 找出占比最小的箱</span></span><br><span class="line">indexForMinPcnt = valueCounts[valueCounts[<span class="string">'pcnt'</span>] == minPcnt].index.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除</span></span><br><span class="line"><span class="keyword">if</span> indexForMinPcnt == max(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> indexForMinPcnt == min(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，合并依据是较小的卡方值</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(valueCounts.index).index(indexForMinPcnt)</span><br><span class="line">prevIndex = list(valueCounts.index)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(valueCounts.index)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])        </span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x * <span class="number">1.0</span> / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>])</span><br><span class="line"></span><br><span class="line">cutOffPoints = special_attribute + cutOffPoints</span><br><span class="line"><span class="keyword">return</span> cutOffPoints</span><br></pre></td></tr></table></figure><p>(参考 <a href="https://blog.csdn.net/LuLuYao9494/article/details/92083755" target="_blank" rel="noopener">https://blog.csdn.net/LuLuYao9494/article/details/92083755</a> )</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置分箱参数</span></span><br><span class="line">bins = <span class="number">5</span> <span class="comment"># 分箱数</span></span><br><span class="line">col = <span class="string">'mean radius'</span> <span class="comment"># 分箱字段名</span></span><br><span class="line">target = <span class="string">'label'</span> <span class="comment"># 标签字段名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line"><span class="comment"># [11.366, 12.726, 14.058000000000002, 17.067999999999998]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">distance_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalDis'</span>)</span><br><span class="line"><span class="comment"># [11.2068, 15.432599999999999, 19.6584, 23.8842]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卡方分箱</span></span><br><span class="line">chiMerge_cutoff = ChiMerge(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># [12.46, 13.38, 15.0, 16.84]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以等频分箱为例在数据集上进行编码</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">    <span class="comment"># 得到切分点（获取切分点的方法可替换）</span></span><br><span class="line">    cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">    <span class="comment"># 进行编码</span></span><br><span class="line">    df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p>pyspark.ml.feature提供等频分箱的API可以直接调用</p><p><em>class</em> <code>pyspark.ml.feature.QuantileDiscretizer</code>(<em>numBuckets=2</em>, <em>inputCol=None</em>, <em>outputCol=None</em>, <em>relativeError=0.001</em>, <em>handleInvalid=’error’</em>)<a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer" target="_blank" rel="noopener"><a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/feature.html#QuantileDiscretizer" target="_blank" rel="noopener">source\</a></a></p><p>参数：</p><ol><li>numBuckets：分箱数，但若样本数据只有3个取值，但numBuckets=4，则仍只划分为3个箱</li><li>relativeError：用于控制近似的精度，取值范围为[0,1]，当设置为0时会计算精确的分位数（计算代价较高）</li><li>handleInvalid：选择处理空值的方式，有三种选项：’keep’将空值放入专门的箱中，例如如果使用4个箱，则将非空数据放入箱0-3中，将空值放入特殊的箱4中；’skip’：过滤掉含有空值的行；’error’报错。（？实验了一下这三种选项并没有区别？？？结果没有处理缺失值仍输出空值，也没有报错……？？）</li><li>inputCol：输入要分箱的列名，outputCol：输出分箱后新特征的列名</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line">df = spark.sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line">col = <span class="string">'sepal length (cm)'</span> <span class="comment"># 对该列进行等频分箱</span></span><br><span class="line"></span><br><span class="line">qd = QuantileDiscretizer(numBuckets=<span class="number">5</span>, inputCol=col, outputCol=col+<span class="string">'_bin'</span>)</span><br><span class="line">qd_model = qd.fit(df)</span><br><span class="line"><span class="keyword">print</span> (qd_model.getSplits()) <span class="comment"># 打印分箱的节点</span></span><br><span class="line">df_new = qd_model.transform(df)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> FeatureEngineering </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
