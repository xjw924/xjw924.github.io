<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>WOE与IV</title>
      <link href="/2020/01/27/woe-yu-iv/"/>
      <url>/2020/01/27/woe-yu-iv/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="WOE定义"><a href="#WOE定义" class="headerlink" title="WOE定义"></a>WOE定义</h2><p>全称：Weight of Evidence</p><p>前提：计算之前需要是离散化后的连续变量or离散变量。</p><p>对于变量第$i$个取值的$WOE_i$的计算公式为</p><script type="math/tex; mode=display">WOE_i = ln(\frac{py_i} {pn_i})=ln(\frac{ \frac{ \#y_i }{ \#y_T } } { \frac{ \#n_i } { \#n_T }})=ln(\frac{\frac{ \#y_i }{ \#n_i } } {\frac{ \#y_T } { \#n_T } })</script><p>其中 #$y_i $表示在第$i$个取值的样本中（或第$i$个箱内）的正样本个数，#$ y_T $表示所有正样本的个数，$py_i$表示在第$i$个取值的样本中（或第$i$个箱内）正样本占所有正样本的比例。</p><p>从最后一个等号后的表达式可以看出，$WOE_i$表示的是某个取值下正样本和负样本的比值，与所有样本中这个比值的差异。这个差异是用比值再取对数来表示的。$WOE_i$越大，表示差异越大，则这个取值下的样本为正的可能性越大。</p><h2 id="WOE编码作用"><a href="#WOE编码作用" class="headerlink" title="WOE编码作用"></a>WOE编码作用</h2><ol><li>标准化功能。编码过后的自变量其实具备了某种标准化的性质，自变量内部的各个取值之间都可以直接进行比较，且不同自变量之间的各个取值也可以通过$WOE_i$进行直接的比较。</li><li>可以反映出自变量的贡献情况（？）。自变量内部$WOE_i$值的变异（波动）情况，结合模型拟合出的系数，构造出各个自变量的贡献率和相对重要性。一般地，系数越大，$WOE_i$的方差越大，则自变量的贡献率越大。</li></ol><h2 id="IV​"><a href="#IV​" class="headerlink" title="IV​"></a>IV​</h2><p>全程：Information Value</p><p>在$WOE_i$的基础上，$IV$在$WOE_i$的前面乘以一个系数$(py_i-pn_i)$作为各箱$WOE_i$的权重，并进行加权求和，其计算公式如下</p><script type="math/tex; mode=display">IV=\sum_{i=1}^{N}(py_i-pn_i)*WOE_i=\sum_{i=1}^{N}(py_i-pn_i)*ln(\frac{py_i}{pn_i})</script><p>其中$N$为分箱的个数（变量的取值个数），系数$(py_i-pn_i)$为箱内正样本占比与负样本占比的差。该系数不仅保证了每个箱乘积的值为非负数，更重要的是考虑了变量当前箱中样本的数量占整体样本数量的比例，比例越高，该箱对变量整体预测能力的贡献越高。</p><p>$IV$衡量某个特征对目标的影响程度，通过该特征中正负样本的比例与总体正负样本的比例，来对比和计算其关联程度，因此可以代表<strong>该特征上的信息量</strong>以及<strong>该特征对模型的贡献</strong>。</p><p>$IV$是对于整个特征来说的，代表的意义由下表来控制：</p><div class="table-container"><table><thead><tr><th>IV</th><th>特征对预测函数的贡献</th></tr></thead><tbody><tr><td>&lt;0.03</td><td>特征几乎不含有效信息，对模型没有贡献，可以删除</td></tr><tr><td>[0.03, 0.10)</td><td>有效信息很少，对模型的贡献度低</td></tr><tr><td>[0.10, 0.30)</td><td>有效信息一般，对模型的贡献度中等</td></tr><tr><td>[0.30, 0.50)</td><td>有效信息较多，对模型的贡献度较高</td></tr><tr><td>&gt;=0.50</td><td>有效信息非常多，对模型的贡献极高且可疑</td></tr></tbody></table></div><p>因此通常会选择$IV$值在0.1~0.5范围内的特征。</p><h1 id="单机python"><a href="#单机python" class="headerlink" title="单机python"></a>单机python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcWOE</span><span class="params">(df, col, target)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得某个变量的分箱操作后每个箱体对应的WOE值，并且基于WOE值计算该变量的IV值</span></span><br><span class="line"><span class="string">    :param df: 包含需要计算WOE的变量和目标变量</span></span><br><span class="line"><span class="string">    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的离散型变量</span></span><br><span class="line"><span class="string">    :param target: 目标变量，0、1表示好、坏</span></span><br><span class="line"><span class="string">    :return: 返回WOE和IV</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    total = df.groupby([col])[target].count()  <span class="comment"># 每个箱体的总数</span></span><br><span class="line">    total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)    </span><br><span class="line">    bad = df.groupby([col])[target].sum()      <span class="comment"># 每个箱体的坏样本数</span></span><br><span class="line">    bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line">    regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>)</span><br><span class="line">    regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    N = sum(regroup[<span class="string">'total'</span>]) <span class="comment"># 总体样本数</span></span><br><span class="line">    B = sum(regroup[<span class="string">'bad'</span>])   <span class="comment"># 总体坏样本数</span></span><br><span class="line">    regroup[<span class="string">'good'</span>] = regroup[<span class="string">'total'</span>] - regroup[<span class="string">'bad'</span>]  <span class="comment"># 每个箱体的好样本数</span></span><br><span class="line">    G = N - B                 <span class="comment"># 总体好样本数</span></span><br><span class="line">    regroup[<span class="string">'bad_pcnt'</span>] = regroup[<span class="string">'bad'</span>].map(<span class="keyword">lambda</span> x: x/B)    <span class="comment"># 每个箱体的坏样本数占总体的坏样本数的比例</span></span><br><span class="line">    regroup[<span class="string">'good_pcnt'</span>] = regroup[<span class="string">'good'</span>].map(<span class="keyword">lambda</span> x: x/G)  <span class="comment"># 每个箱体的好样本数占总体的好样本数的比例</span></span><br><span class="line">    <span class="comment"># WOEi计算公式(不含系数)： WOE(每个箱体) = ln(该箱体的好样本数占总体的好样本数的比例/该箱体的好样本数占总体的好样本数的比例）</span></span><br><span class="line">    regroup[<span class="string">'WOE'</span>] = regroup.apply(<span class="keyword">lambda</span> x: np.log(x.good_pcnt/x.bad_pcnt),axis=<span class="number">1</span>)   </span><br><span class="line">    WOE_dict = regroup[[col,<span class="string">'WOE'</span>]].set_index(col).to_dict(orient=<span class="string">'index'</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> WOE_dict.items(): <span class="comment"># k代表箱体名，v代表了以WOE为键，箱体的实际WOE值为value的字典</span></span><br><span class="line">        WOE_dict[k] = v[<span class="string">'WOE'</span>]</span><br><span class="line">    <span class="comment"># 计算该变量的IV值：sum((某箱体的好样本数占总体的好样本数的比例 - 该箱体的坏样本数占总体的坏样本数的比例)*WOE(某个箱体))</span></span><br><span class="line">    IV = regroup.apply(<span class="keyword">lambda</span> x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt/x.bad_pcnt),axis = <span class="number">1</span>)</span><br><span class="line">    IV = sum(IV)</span><br><span class="line">    <span class="keyword">return</span> &#123;col: &#123;<span class="string">"WOE"</span>: WOE_dict, <span class="string">'IV'</span>:IV&#125;&#125;</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先进行等频分箱和编码（用到的函数见'分箱'）</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">     df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后循环计算每个变量的各箱WOE和IV，输出result</span></span><br><span class="line">result = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     woe_iv_dict = CalcWOE(df_cut, col, target = <span class="string">'label'</span>)</span><br><span class="line">     result = pd.concat([result, pd.DataFrame(woe_iv_dict).T])</span><br></pre></td></tr></table></figure><h1 id="集群pyspark"><a href="#集群pyspark" class="headerlink" title="集群pyspark"></a>集群pyspark</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_woe_iv</span><span class="params">(tableName, featureCol, labelCol)</span>:</span></span><br><span class="line">     <span class="string">'''</span></span><br><span class="line"><span class="string">     计算spark.sql.DataFrame的WOE和IV</span></span><br><span class="line"><span class="string">     :param tableName: 储存在HDFS上的表名</span></span><br><span class="line"><span class="string">     :param featureCol: 所需要计算的字段名</span></span><br><span class="line"><span class="string">     :param labelCol: 标签列名</span></span><br><span class="line"><span class="string">     :return WOE字典和IV值</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">     df = spark.sql(<span class="string">"select &#123;featureCol&#125;, &#123;labelCol&#125; from &#123;tableName&#125;"</span>.format(featureCol=featureCol, labelCol=labelCol, tableName=tableName))</span><br><span class="line">     <span class="comment"># 使用crosstab函数生成列联表，并对列进行命名</span></span><br><span class="line">     binCount = df.crosstab(featureCol,labelCol).toDF(<span class="string">'feature'</span>,<span class="string">'neg_cnt'</span>,<span class="string">'pos_cnt'</span>)</span><br><span class="line">     <span class="comment"># 注册临时表便于后续调用</span></span><br><span class="line">     binCount.registerTempTable(<span class="string">"binCount"</span>)</span><br><span class="line">     <span class="comment"># 计算主要步骤</span></span><br><span class="line">     woe_df = spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">        select</span></span><br><span class="line"><span class="string">          feature,</span></span><br><span class="line"><span class="string">          nvl(log(pos_per/neg_per), 0) as woe_i,</span></span><br><span class="line"><span class="string">          nvl((pos_per-neg_per)*log(pos_per/neg_per), 0) as iv_i</span></span><br><span class="line"><span class="string">        from</span></span><br><span class="line"><span class="string">        (</span></span><br><span class="line"><span class="string">          select</span></span><br><span class="line"><span class="string">            feature,</span></span><br><span class="line"><span class="string">            neg_cnt/(select sum(neg_cnt) from binCount) as neg_per,</span></span><br><span class="line"><span class="string">            pos_cnt/(select sum(pos_cnt) from binCount) as pos_per</span></span><br><span class="line"><span class="string">          from</span></span><br><span class="line"><span class="string">            binCount</span></span><br><span class="line"><span class="string">        ) t</span></span><br><span class="line"><span class="string">     """</span>)</span><br><span class="line">     <span class="comment"># 将计算结果转化为pandas.DataFrame</span></span><br><span class="line">     woe_df_pandas = woe_df.toPandas()</span><br><span class="line">     woe = dict(zip(woe_df_pandas[<span class="string">"feature"</span>].values.reshape(<span class="number">-1</span>,), woe_df_pandas[<span class="string">"woe_i"</span>].values.reshape(<span class="number">-1</span>,),))</span><br><span class="line">     iv = sum(woe_df_pandas[<span class="string">"iv_i"</span>].values)</span><br><span class="line">     <span class="keyword">return</span> woe, iv</span><br></pre></td></tr></table></figure><p>调用并测试运算时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    spark = SparkSession.builder.enableHiveSupport().getOrCreate()</span><br><span class="line">    tableName = <span class="string">"test.calc_woe_iv_sample"</span></span><br><span class="line">    featureCol = <span class="string">"age_level"</span></span><br><span class="line">    labelCol = <span class="string">"label"</span></span><br><span class="line">    time_cost = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        woe, iv = cal_woe_iv(tableName, featureCol, labelCol)</span><br><span class="line">        end = time.time()</span><br><span class="line">        time_cost.append(end-start)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'time cost: %.5f sec'</span> % (np.mean(time_cost)))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'woe:'</span>, woe)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'iv:'</span>, iv)</span><br></pre></td></tr></table></figure><p>集群资源配置：</p><blockquote><p>spark.driver.memory=5G<br>spark.driver.maxResultSize=5G<br>num-executors=20<br>executor-cores=6<br>executor-memory=4G</p></blockquote><p>根据样本量的不同，测试计算时间结果（循环30次取平均）</p><div class="table-container"><table><thead><tr><th></th><th>10w</th><th>100w</th><th>1000w</th><th>1e</th></tr></thead><tbody><tr><td>计算时间(s)</td><td>0.872</td><td>1.819</td><td>3.172</td><td>25.793</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> FeatureEngineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分箱</title>
      <link href="/2020/01/20/fen-xiang/"/>
      <url>/2020/01/20/fen-xiang/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将连续型的数据分为几个数据段，即特征离散化。</p><p>把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率。（百度百科） </p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li><p><strong>离散化后的特征对异常数据有很强的鲁棒性，模型更加稳定。</strong>比如年龄&gt;70岁为一个分箱，异常数据如年龄为300岁同样会划分到该箱，不会给模型造成很大的干扰；比如将20岁-30岁划分为一个区间，不会因为年龄增长一岁就变成一个完全不同的人。而对于处于区间相邻处的样本会刚好相反，因此如何划分区间也很重要。</p></li><li><p>对于如逻辑回归的广义线性模型，表达能力受限，单变量离散化为$N$个变量后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。（？）</p></li><li><p>离散化后可以进行特征交叉，进一步<strong>引入非线性，提升模型表达能力</strong>。</p></li><li><p>可以将缺失值作为独立的箱代入变量。</p></li><li><p>可以将所有变量转换到相似的尺度上（不需要进行归一化）。</p></li></ol><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="无监督分箱"><a href="#无监督分箱" class="headerlink" title="无监督分箱"></a>无监督分箱</h3><h4 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h4><p>每个箱包含大致相等的样本数量，可以根据分位数进行划分，如四分位数等。</p><h4 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h4><p>从变量的最小值到最大值之间均等分为$N$等份。若$m$和$M$分别代表最小值和最大值，则每个区间的长度为$w=\frac{M-m}{N}$，区间的边界值为$m+w,m+2w,…,m+(N-1)w$。这里只考虑边界，每个等份的样本数量一般不等。</p><h3 id="有监督分箱"><a href="#有监督分箱" class="headerlink" title="有监督分箱"></a>有监督分箱</h3><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p><strong>初始化</strong></p><ul><li><p>根据连续变量值的大小进行排序。</p></li><li><p>把每一个单独的值视为一个箱体，构建最初的离散化。目的是想从每个单独的箱体开始逐渐合并。</p></li></ul></li><li><p><strong>合并</strong>：在初始化构建完毕后，该步就是不断地进行自底向上的合并，直到满足停止条件。</p><ul><li><p><strong>计算所有相邻分箱的卡方值</strong>。比如有1,2,3,4这4个分箱，绑定相邻的两个分箱，一共有3组：12,23,34，然后分别计算三个绑定组的卡方值。卡方值的计算公式为</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}</script><p>其中，$m$表示要合并相邻分箱的数目（$m=2$表示对两个箱进行合并），$k$表示目标变量的类别数（两分类或多分类），$A_{ij}$表示实际频数（即第$i$个分箱第$j$类别的频数），$E_{ij}$表示期望频数，计算公式如下（可根据公式$P(AB)=P(A)P(B)$推导出来）</p><script type="math/tex; mode=display">E_{ij}=\frac{R_i*C_j}{N}</script><p>其中$R_i$和$C_j$分别是实际频数整行和整列的加和，$N$为总样本量。</p></li><li><p><strong>从计算的卡方值中找出最小的一个，并把这两个分箱合并</strong>。比如23是卡方值最小的一个，那么就将2和3合并，经过本轮的计算后分箱就变为1,23,4。</p><p>从合并的方法可以看出，卡方分箱的基本思想是，<strong>如果两个相邻的区间具有非常类似的类分布（即低卡方值，低卡方值表明它们具有类似的类分布），那么这两个区间可以合并，否则应该分开</strong>。</p></li></ul></li><li><p><strong>停止条件</strong>：以上仅是每一轮需要计算的内容，若不设置停止条件，算法会一直运行。一般从以下两个方面设置停止条件：</p><ul><li>卡方停止的阈值</li><li>分箱数目的限制</li></ul><p>即当所有分箱对的卡方值均大于阈值，且分箱数大于分箱数时，计算就会继续，直到不满足。</p><p>以上两个阈值一般根据经验来定义，来作为分箱函数的参数进行设置，一般使用0.90,0.95,0.99的置信度，分箱数一般可以设置为5。</p></li></ul><p><em>例如：</em></p><p>对于某两个箱（分箱1和分箱2），实际频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th><th>行频数和</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$A_{11}$</td><td>$A_{12}$</td><td>$R_1$</td></tr><tr><td><strong>分箱2</strong></td><td>$A_{21}$</td><td>$A_{22}$</td><td>$R_2$</td></tr><tr><td><strong>列频数和</strong></td><td>$C_1$</td><td>$C_2$</td><td>$N$</td></tr></tbody></table></div><p>期望频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$E_{11}=\frac{R_1*C_1}{N}$</td><td>$E_{12}=\frac{R_1*C_2}{N}$</td></tr><tr><td><strong>分箱2</strong></td><td>$E_{21}=\frac{R_2*C_1}{N}$</td><td>$E_{22}=\frac{R_2*C_2}{N}$</td></tr></tbody></table></div><p>代入卡方公式求解，过程如下：</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}=[\frac{(A_{11}-E_{11})^2}{E_{11}}+\frac{(A_{12}-E_{12})^2}{E_{12}}]+[\frac{(A_{21}-E_{21})^2}{E_{21}}+\frac{(A_{22}-E_{22})^2}{E_{22}}]</script><p>如果计算结果是所有卡方值中最小的，说明这两个分箱具有最相似的类分布，因此把它们合并。</p><p>（参考 <a href="https://cloud.tencent.com/developer/article/1418720" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1418720</a> ）</p><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><h2 id="无监督分箱-1"><a href="#无监督分箱-1" class="headerlink" title="无监督分箱"></a>无监督分箱</h2><ul><li>UnsupervisedSplitBin函数：对数值型变量进行分组，分组的依据有等频和等距，最后返回划分点列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UnsupervisedSplitBin</span><span class="params">(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对数值型变量进行划分，最后返回划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: 数据集</span></span><br><span class="line"><span class="string">:param col: 需要分箱的变量。仅限数值型变量</span></span><br><span class="line"><span class="string">:param numOfSplit: 需要分箱个数，默认是5</span></span><br><span class="line"><span class="string">:param method: 分箱方法，'equalFreq'：默认是等频，否则是等距</span></span><br><span class="line"><span class="string">:return: 返回划分点组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'equalFreq'</span>:</span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = [np.percentile(df[col], <span class="number">100</span>/numOfSplit*i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line">splitPoint = sorted(list(set(frequency_cutoff)))</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">'equalDis'</span>:</span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">var_max, var_min = max(df[col]), min(df[col])</span><br><span class="line">interval_len = (var_max-var_min)/numOfSplit</span><br><span class="line">splitPoint = [var_min+i*interval_len <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="string">'Sorry, no such method'</span></span><br></pre></td></tr></table></figure><h2 id="有监督分箱：卡方分箱"><a href="#有监督分箱：卡方分箱" class="headerlink" title="有监督分箱：卡方分箱"></a>有监督分箱：卡方分箱</h2><ul><li><p>SplitData函数：用于对某变量进行划分，根据划分的组数，返回划分点数值组成的列表</p></li><li><p>Chi2函数：用于计算卡方值，返回卡方值（按照卡方检验的原理进行计算）</p></li><li><p>BinBadRate函数：按某变量进行分组，计算分组后每组的坏样本率，返回的有，字典形式，数据框，总体坏样本率（可选）</p></li><li><p>AssignGroup函数：根据分组后的划分点列表，给某个需分箱的变量的每个取值进行分箱前的匹配，形成对应箱的映射</p></li><li><p>AssignBin函数：将某列的每个取值进行分箱编号</p></li><li><p>ChiMerge函数：卡方分箱的主体函数，其中调用了前面五个基础函数，返回的是最终的满足所有限制条件的划分点列表</p><ul><li><p>其中需要满足：</p><p>（1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</p><p>（2）每个箱体必须同时包含好坏样本</p><p>（3）每个箱体的占比不低于预设值（可选）</p><p>（4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(df,col,numOfSplit,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得数据切分时对应位置的数值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame</span></span><br><span class="line"><span class="string">:param col: str 选择数据集中的某列进行操作</span></span><br><span class="line"><span class="string">:param numOfSplit: int 划分的组数</span></span><br><span class="line"><span class="string">:param special_attribute: 用于过滤掉一些特殊的值，不参与数据划分（可选，默认不过滤）</span></span><br><span class="line"><span class="string">:return: 返回划分点位置对应的数值组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"><span class="keyword">if</span> special_attribute != []:</span><br><span class="line">df2 = df2.loc[~df2[col].isin(special_attribute)]  <span class="comment"># 排除有特殊值的样本行</span></span><br><span class="line">N = len(df2) <span class="comment"># 样本总数</span></span><br><span class="line">n = int(N/numOfSplit) <span class="comment"># 每组包含的样本量</span></span><br><span class="line">SplitPointIndex = [i*n <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)] <span class="comment"># 不包含numOfSplit</span></span><br><span class="line"><span class="comment"># 如总样本N=20，numofsplit为5组，则每组包含4个元素(n=4)</span></span><br><span class="line"><span class="comment"># 则最后得到的SplitPointIndex=[4,8,12,16]，即切分点位置，各组样本为0-3/4-7/8-11/12-15</span></span><br><span class="line">rawValues = sorted(list(df[col])) <span class="comment"># sorted返回一个新的排列后的列表</span></span><br><span class="line">SplitPoint = [rawValues[i] <span class="keyword">for</span> i <span class="keyword">in</span> SplitPointIndex] <span class="comment"># 返回位置索引上对应的数值</span></span><br><span class="line"><span class="comment"># 为了以防万一，去重再排序</span></span><br><span class="line">SplitPoint = sorted(list(set(SplitPoint)))</span><br><span class="line"><span class="keyword">return</span> SplitPoint </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignGroup</span><span class="params">(x,bin)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于：根据分组后的划分点列表（bin)，给某个需要分箱的变量进行分箱前的匹配，形成对应箱的映射，以便后期分箱操作，将值划分到不同箱体</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param bin: 上述变量分组后（通过SplitData函数）对应的划分位置的数值组成的列表</span></span><br><span class="line"><span class="string">:return: x在分箱结果下的映射 </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">N = len(bin)            <span class="comment"># 划分点的长度</span></span><br><span class="line"><span class="keyword">if</span> x&lt;=min(bin):         <span class="comment"># 如果某个取值小于等于最小划分点，则返回最小划分点</span></span><br><span class="line"><span class="keyword">return</span> min(bin)</span><br><span class="line"><span class="keyword">elif</span> x&gt;max(bin):        <span class="comment"># 如果某个取值大于最小划分点，则返回10e10</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">10e10</span></span><br><span class="line"><span class="keyword">else</span>:                   <span class="comment"># 除此之外，返回其他对应的划分点</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> bin[i] &lt; x &lt;= bin[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> bin[i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BinBadRate</span><span class="params">(df,col,target,grantRateIndicator=False)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对变量按照取值（每个取值都是唯一的）进行分组，获得每箱的坏样本率，后期基于该值判断是否需要合并操作</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 需要计算好坏比率的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要计算好坏比率的特征</span></span><br><span class="line"><span class="string">:param target: str 好坏标签</span></span><br><span class="line"><span class="string">:param grantRateIndicator: bool True返回总体的坏样本率，False不返回（可选，默认不返回）</span></span><br><span class="line"><span class="string">:return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator==True时）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">total = df.groupby([col])[target].count()</span><br><span class="line">total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)</span><br><span class="line">bad = df.groupby([col])[target].sum()</span><br><span class="line">bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line"></span><br><span class="line">regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>) <span class="comment"># 每箱的坏样本数，总样本数</span></span><br><span class="line">regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regroup[<span class="string">'bad_rate'</span>] = regroup.apply(<span class="keyword">lambda</span> x: x.bad / x.total, axis=<span class="number">1</span>) <span class="comment"># 加上一列坏样本率</span></span><br><span class="line">dicts = dict(zip(regroup[col],regroup[<span class="string">'bad_rate'</span>])) <span class="comment"># 每箱对应的坏样本率组成的字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> grantRateIndicator:</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">N = sum(regroup[<span class="string">'total'</span>])</span><br><span class="line">B = sum(regroup[<span class="string">'bad'</span>])</span><br><span class="line">overallRate = B / N</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup, overallRate)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Chi2</span><span class="params">(df,totalCol,badCol)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得卡方值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含了每个分组下的样本总数和坏样本数</span></span><br><span class="line"><span class="string">:param totalCol: str 列名，元素由各个分组下的样本总数构成</span></span><br><span class="line"><span class="string">:param badCol: str 列名，元素由各个分组下的坏样本数构成</span></span><br><span class="line"><span class="string">:return: 卡方值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算该特征各个分组下的好样本数</span></span><br><span class="line">df2[<span class="string">'good'</span>] = df2.apply(<span class="keyword">lambda</span> x: x[totalCol]-x[badCol], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求出期望值E</span></span><br><span class="line"><span class="comment"># 1.求出总体的坏样本率和好样本率</span></span><br><span class="line">badRate = sum(df2[badCol]) / sum(df2[totalCol])</span><br><span class="line">goodRate = sum(df2[<span class="string">'good'</span>]) / sum(df2[totalCol])</span><br><span class="line"><span class="comment"># 特殊情况：当全部样本只有好或者坏样本时，卡方值为0</span></span><br><span class="line"><span class="keyword">if</span> badRate <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.根据总体的好坏样本率算出期望的好坏样本数，计算公式为：</span></span><br><span class="line"><span class="comment"># 期望坏（好）样本个数 ＝ 全部样本个数 * 总体的坏（好）样本率</span></span><br><span class="line">df2[<span class="string">'badExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*badRate)</span><br><span class="line">df2[<span class="string">'goodExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*goodRate)</span><br><span class="line"></span><br><span class="line">badCombined = zip(df2[<span class="string">'badExpected'</span>], df2[badCol])</span><br><span class="line">goodCombined = zip(df2[<span class="string">'goodExpected'</span>], df2[<span class="string">'good'</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 按照卡方计算的公式计算卡方值：  ∑ (O - E)^2 / E, O代表实际值，E代表期望值     </span></span><br><span class="line">badChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> badCombined]</span><br><span class="line">goodChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> goodCombined]</span><br><span class="line">chi2 = sum(badChi) + sum(goodChi)</span><br><span class="line"><span class="keyword">return</span> chi2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignBin</span><span class="params">(x,cutOffPoints,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对某个列的某个取值进行分箱编号（编码）</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param cutOffPoints: 上述变量的分组结果，用切分点表示，列表形式</span></span><br><span class="line"><span class="string">:param special_attribute: 不参与分箱的特殊取值（可选）</span></span><br><span class="line"><span class="string">:return: 分箱后的对应的第几个箱，从0开始</span></span><br><span class="line"><span class="string">比如，若cutOffPoints=[10,20,30]，当x=7，返回0；当x=35，返回3</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">numBin = len(cutOffPoints) + <span class="number">1</span> + len(special_attribute)</span><br><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> special_attribute:</span><br><span class="line">i = special_attribute.index(x)+<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> (<span class="number">0</span>-i)</span><br><span class="line"><span class="keyword">if</span> x &lt;= cutOffPoints[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">elif</span> x &gt; cutOffPoints[<span class="number">-1</span>]:</span><br><span class="line"><span class="keyword">return</span> (numBin<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,numBin<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> cutOffPoints[i] &lt; x &lt;= cutOffPoints[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> (i+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ChiMerge</span><span class="params">(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于实际的卡方分箱算法操作，最后返回有实际的划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含目标变量与需要分箱变量的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要分箱的属性</span></span><br><span class="line"><span class="string">:param target: str 目标变量，取值0或1</span></span><br><span class="line"><span class="string">:param max_interval: int 最大分箱数（默认5）。如果原始属性的取值个数低于该参数，不执行这段函数</span></span><br><span class="line"><span class="string">:param special_attribute: list 不参与分箱的属性取值（可选）</span></span><br><span class="line"><span class="string">:param minBinPcnt：最小箱的占比（默认0），如果不满足最小分箱占比继续进行组别合并</span></span><br><span class="line"><span class="string">:return: 分箱结果</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">colLevels = sorted(list(set(df[col])))  <span class="comment"># 某列的不重复值</span></span><br><span class="line">N_distinct = len(colLevels)             <span class="comment"># 某列的不重复值计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> N_distinct &lt;= max_interval:  <span class="comment">#如果原始属性的取值个数低于max_interval，不执行这段函数（不参与分箱）</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The number of original levels for '&#123;col&#125;' is &#123;dis_cnt&#125;, which is less than or equal to max intervals"</span>.format(col=col, dis_cnt=N_distinct))</span><br><span class="line"><span class="keyword">return</span> colLevels[:<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">if</span> len(special_attribute)&gt;=<span class="number">1</span>:</span><br><span class="line">df2 = df.loc[~df[col].isin(special_attribute)] <span class="comment"># 去掉special_attribute后的df</span></span><br><span class="line">N_distinct = len(list(set(df2[col])))  <span class="comment"># 去掉special_attribute后的非重复值计数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤一: 通过col对数据进行分组，求出每组的总样本数和坏样本数</span></span><br><span class="line"><span class="keyword">if</span> N_distinct &gt; <span class="number">100</span>:</span><br><span class="line">split_x = SplitData(df2,col,<span class="number">100</span>) <span class="comment"># 若非重复值计数超过100组，均将其转化成100组，将多余样本都划分到最后一个箱中</span></span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col].map(<span class="keyword">lambda</span> x: AssignGroup(x,split_x))</span><br><span class="line"><span class="comment"># Assgingroup函数：每一行的数值和切分点做对比，返回原值在切分后的映射，</span></span><br><span class="line"><span class="comment"># 经过map以后，生成该特征的值对象的"分箱"后的值        </span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col] <span class="comment"># 不重复值计数不超过100时，不需要进行上述步骤</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过上述过程，我们现在可以将该列进行BadRate计算，用来计算每个箱体的坏样本率 以及总体的坏样本率      </span></span><br><span class="line">(binBadRate, regroup, overallRate) = BinBadRate(df2, <span class="string">'temp_cutoff'</span>, target, grantRateIndicator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在此，我们将每个单独的属性值分成单独一组</span></span><br><span class="line"><span class="comment"># 对属性值进行去重排序，然后两两组别进行合并，用于后续的卡方值计算</span></span><br><span class="line">colLevels = sorted(list(set(df2[<span class="string">'temp_cutoff'</span>])))</span><br><span class="line">groupIntervals = [[i] <span class="keyword">for</span> i <span class="keyword">in</span> colLevels] <span class="comment"># 把每个箱的值打包成[[],[]]的形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤二：通过循环的方式，不断的合并相邻的两个组别，直到：</span></span><br><span class="line"><span class="comment"># （1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</span></span><br><span class="line"><span class="comment"># （2）每个箱体必须同时包含好坏样本</span></span><br><span class="line"><span class="comment"># （3）每个箱体的占比不低于预设值（可选）</span></span><br><span class="line"><span class="comment"># （4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</span></span><br><span class="line">split_intervals = max_interval - len(special_attribute)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次循环时, 计算合并相邻组别后的卡方值。当组别数大于预设的分箱数时，持续选择最小卡方值的组合并</span></span><br><span class="line"><span class="keyword">while</span> len(groupIntervals)&gt;split_intervals:</span><br><span class="line">chisqList = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(len(groupIntervals)<span class="number">-1</span>):</span><br><span class="line">temp_group = groupIntervals[k] + groupIntervals[k+<span class="number">1</span>]  <span class="comment"># 返回的是，这两个值组成的列表</span></span><br><span class="line"><span class="comment"># 因此，可以通过temp_group，每次只选相邻的两组进行相关操作</span></span><br><span class="line">df2b = regroup.loc[regroup[<span class="string">'temp_cutoff'</span>].isin(temp_group)]</span><br><span class="line"><span class="comment"># 计算相邻两组的卡方值(通过调用Chi2函数)</span></span><br><span class="line">chisq = Chi2(df2b,<span class="string">'total'</span>,<span class="string">'bad'</span>)</span><br><span class="line">chisqList.append(chisq)</span><br><span class="line">best_comnbined = chisqList.index(min(chisqList)) <span class="comment"># 检索最小卡方值所在的索引    </span></span><br><span class="line"><span class="comment"># 把groupIntervals的值改成类似的值改成类似从[[1],[2],[3]]到[[1,2],[3]]</span></span><br><span class="line">groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+<span class="number">1</span>]</span><br><span class="line">groupIntervals.remove(groupIntervals[best_comnbined+<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述循环结束后，即可获得预设的分箱数，并且可以得到各分箱的划分点</span></span><br><span class="line">groupIntervals = [sorted(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals]</span><br><span class="line">cutOffPoints = [max(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals[:<span class="number">-1</span>]] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后，我们将某列进行分箱编号</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp_cutoff'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line"><span class="comment"># AssignBin函数：每一行的数值和切分点做对比，返回原值所在的分箱编号（形成新列）    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步，我们想要保证每个箱体都有好坏样本数</span></span><br><span class="line"><span class="comment"># 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本</span></span><br><span class="line">(binBadRate,regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)    <span class="comment"># 返回每箱坏样本率字典，和包含'分箱号、总样本数、坏样本数、坏样本率的数据框'）</span></span><br><span class="line">minBadRate, maxBadRate = min(binBadRate.values()), max(binBadRate.values())</span><br><span class="line"><span class="keyword">while</span> minBadRate ==<span class="number">0</span> <span class="keyword">or</span> maxBadRate == <span class="number">1</span>:</span><br><span class="line"><span class="comment"># 找出全部为好／坏样本的箱</span></span><br><span class="line">indexForBad01 = regroup[regroup[<span class="string">'bad_rate'</span>].isin([<span class="number">0</span>,<span class="number">1</span>])].temp_Bin.tolist()    </span><br><span class="line">bin = indexForBad01[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个划分点需要移除</span></span><br><span class="line"><span class="keyword">if</span> bin == max(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> bin == min(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果是中间的某一箱，则需要和前后中的一个箱体进行合并，具体选择哪个箱体，要依据前后箱体哪个卡方值较小</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(regroup.temp_Bin).index(bin)</span><br><span class="line">prevIndex = list(regroup.temp_Bin)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(regroup.temp_Bin)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成此项合并后，需要再次计算，在新的分箱准则下，每箱是否同时包含好坏样本，</span></span><br><span class="line"><span class="comment"># 如何仍然出现不能同时包含好坏样本的箱体，继续循坏，直到好坏样本同时出现在每个箱体后，跳出</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">(binBadRate, regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">[minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们来检查分箱后的最小占比</span></span><br><span class="line"><span class="keyword">if</span> minBinPcnt &gt; <span class="number">0</span>: <span class="comment"># 如果函数调用初期给的minBinPct不是零，则进一步对箱体进行合并</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>]) <span class="comment"># 得到箱体的最小占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果箱体最小占比小于给定的分箱占比阈值 且 划分点大于2，进入合并循环中</span></span><br><span class="line"><span class="keyword">while</span> minPcnt &lt; minBinPcnt <span class="keyword">and</span> len(cutOffPoints) &gt; <span class="number">2</span>: </span><br><span class="line"><span class="comment"># 找出占比最小的箱</span></span><br><span class="line">indexForMinPcnt = valueCounts[valueCounts[<span class="string">'pcnt'</span>] == minPcnt].index.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除</span></span><br><span class="line"><span class="keyword">if</span> indexForMinPcnt == max(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> indexForMinPcnt == min(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，合并依据是较小的卡方值</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(valueCounts.index).index(indexForMinPcnt)</span><br><span class="line">prevIndex = list(valueCounts.index)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(valueCounts.index)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])        </span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x * <span class="number">1.0</span> / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>])</span><br><span class="line"></span><br><span class="line">cutOffPoints = special_attribute + cutOffPoints</span><br><span class="line"><span class="keyword">return</span> cutOffPoints</span><br></pre></td></tr></table></figure><p>(参考 <a href="https://blog.csdn.net/LuLuYao9494/article/details/92083755" target="_blank" rel="noopener">https://blog.csdn.net/LuLuYao9494/article/details/92083755</a> )</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置分箱参数</span></span><br><span class="line">bins = <span class="number">5</span> <span class="comment"># 分箱数</span></span><br><span class="line">col = <span class="string">'mean radius'</span> <span class="comment"># 分箱字段名</span></span><br><span class="line">target = <span class="string">'label'</span> <span class="comment"># 标签字段名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line"><span class="comment"># [11.366, 12.726, 14.058000000000002, 17.067999999999998]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">distance_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalDis'</span>)</span><br><span class="line"><span class="comment"># [11.2068, 15.432599999999999, 19.6584, 23.8842]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卡方分箱</span></span><br><span class="line">chiMerge_cutoff = ChiMerge(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># [12.46, 13.38, 15.0, 16.84]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以等频分箱为例在数据集上进行编码</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">    <span class="comment"># 得到切分点（获取切分点的方法可替换）</span></span><br><span class="line">    cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">    <span class="comment"># 进行编码</span></span><br><span class="line">    df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p>pyspark.ml.feature提供等频分箱的API可以直接调用</p><p><em>class</em> <code>pyspark.ml.feature.QuantileDiscretizer</code>(<em>numBuckets=2</em>, <em>inputCol=None</em>, <em>outputCol=None</em>, <em>relativeError=0.001</em>, <em>handleInvalid=’error’</em>)<a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer" target="_blank" rel="noopener"><a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/feature.html#QuantileDiscretizer" target="_blank" rel="noopener">source\</a></a></p><p>参数：</p><ol><li>numBuckets：分箱数，但若样本数据只有3个取值，但numBuckets=4，则仍只划分为3个箱</li><li>relativeError：用于控制近似的精度，取值范围为[0,1]，当设置为0时会计算精确的分位数（计算代价较高）</li><li>handleInvalid：选择处理空值的方式，有三种选项：’keep’将空值放入专门的箱中，例如如果使用4个箱，则将非空数据放入箱0-3中，将空值放入特殊的箱4中；’skip’：过滤掉含有空值的行；’error’报错。（？实验了一下这三种选项并没有区别？？？结果没有处理缺失值仍输出空值，也没有报错……？？）</li><li>inputCol：输入要分箱的列名，outputCol：输出分箱后新特征的列名</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line">df = spark.sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line">col = <span class="string">'sepal length (cm)'</span> <span class="comment"># 对该列进行等频分箱</span></span><br><span class="line"></span><br><span class="line">qd = QuantileDiscretizer(numBuckets=<span class="number">5</span>, inputCol=col, outputCol=col+<span class="string">'_bin'</span>)</span><br><span class="line">qd_model = qd.fit(df)</span><br><span class="line"><span class="keyword">print</span> (qd_model.getSplits()) <span class="comment"># 打印分箱的节点</span></span><br><span class="line">df_new = qd_model.transform(df)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> FeatureEngineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/2019/11/14/svm/"/>
      <url>/2019/11/14/svm/</url>
      
        <content type="html"><![CDATA[<p>SVM是一种二类分类模型。它的基本模型是<strong>在特征空间中寻找间隔最大化的分离超平面的线性分类器</strong>。（间隔最大是它有别于感知机）</p><p>训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；</p><p>训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；</p><p>训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。</p><h2 id="为什么要转化为对偶问题？"><a href="#为什么要转化为对偶问题？" class="headerlink" title="为什么要转化为对偶问题？"></a>为什么要转化为对偶问题？</h2><p>对偶问题通常更易求解</p><p>便于引入核函数，推广到非线性分类问题</p><h2 id="SVM的核函数，引入核函数本质"><a href="#SVM的核函数，引入核函数本质" class="headerlink" title="SVM的核函数，引入核函数本质"></a>SVM的核函数，引入核函数本质</h2><p>对于非线性的情况，SVM 的处理方法是选择一个核函数 κ(⋅,⋅) ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。</p><p>此外，因为训练样例一般是不会独立出现的，它们总是以成对样例的内积形式出现，而用对偶形式表示学习器的优势在为在该表示中可调参数的个数不依赖输入属性的个数，通过使用恰当的核函数来替代内积，可以隐式得将非线性的训练数据映射到高维空间</p><p><img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image002.jpg" alt="img"></p><p><strong>核方法有什么优点：</strong></p><p>\1.    使我们能够使用保证有效收敛的凸优化技术来学习非线性模型（关于x的函数），因为可认为φ是固定的，仅优化α，即优化算法可以将决策函数视为不同空间中的线性函数</p><p>\2.    核函数的实现方法通常比直接构建φ(x)再计算内积高效很多，而且某些时候φ(x)是无限维的，普通地计算内积很困难。</p><h2 id="²-SVM的优缺点"><a href="#²-SVM的优缺点" class="headerlink" title="² SVM的优缺点"></a>² SVM的优缺点</h2><p>优点：</p><p>可以很好地处理小样本数据</p><p>可解决非线性问题</p><p>缺点：</p><p>对缺失数据敏感</p><p>SVC函数的训练时间是随训练样本平方级增长</p><p>对于多分类问题，SVC采用的是OVO投票机制，需要两两类别建立分类器，训练时间可能比较长。</p><h2 id="²-SVM核技巧原理，如何选择核函数"><a href="#²-SVM核技巧原理，如何选择核函数" class="headerlink" title="² SVM核技巧原理，如何选择核函数"></a>² SVM核技巧原理，如何选择核函数</h2><h2 id="²-SVM的损失函数"><a href="#²-SVM的损失函数" class="headerlink" title="² SVM的损失函数"></a>² SVM的损失函数</h2><h2 id="²-SVM的推导过程"><a href="#²-SVM的推导过程" class="headerlink" title="² SVM的推导过程"></a>² SVM的推导过程</h2><h2 id="²-SVM怎么扩展到多分类问题"><a href="#²-SVM怎么扩展到多分类问题" class="headerlink" title="² SVM怎么扩展到多分类问题"></a>² SVM怎么扩展到多分类问题</h2><p>1）直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类，但计算复杂，实现起来较困难</p><p>2）间接法：通过组合多个二分类器来实现多分类器的构造，有一对一，一对多</p><h2 id="²-SVM需要解决的重要数学问题是什么"><a href="#²-SVM需要解决的重要数学问题是什么" class="headerlink" title="² SVM需要解决的重要数学问题是什么"></a>² SVM需要解决的重要数学问题是什么</h2><h2 id="²-LR与SVM的区别"><a href="#²-LR与SVM的区别" class="headerlink" title="² LR与SVM的区别"></a>² LR与SVM的区别</h2><p>联系：</p><p>1）LR和SVM都可以处理分类问题</p><p>2）都基于线性函数wTx+b</p><p>区别：</p><p>1） LR是参数模型，SVM是非参数模型。 </p><p>2） LR可输出概率，SVM不输出概率，只输出类别</p><p>3） 从损失函数来看，LR是logistic loss，SVM是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重</p><p>4） 在训练集较小时，SVM较适用，而LR需要较多的样本。LR相对来说模型更简单更快，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些</p><p>5） LR模型找到的超平面是尽量让所有点都远离他，而SVM寻找的那个超平面，是只让最靠近中间分割线的那些点尽量远离，即只用到那些支持向量的样本。</p><p>6） 对非线性问题的处理方式不同，LR主要靠特征变换或构造（组合交叉特征）；SVM也可以这样，还可以通过核方法</p><h2 id="²-什么时候用LR或SVM？"><a href="#²-什么时候用LR或SVM？" class="headerlink" title="² 什么时候用LR或SVM？"></a>² 什么时候用LR或SVM？</h2><p>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM<br> 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel<br> 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。(LR和不带核函数的SVM比较类似。)</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GBDT</title>
      <link href="/2019/11/13/gbdt/"/>
      <url>/2019/11/13/gbdt/</url>
      
        <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>GBDT是集成学习Boosting家族的成员，是对提升树的改进。</p><p>使用的决策树通常为CART回归树（使用回归树的原因是因为：GBDT每次迭代要拟合的是梯度值，是连续值所以要用回归树）</p><p>在每一轮的迭代中，首先<strong>计算当前模型在所有样本上的负梯度</strong>，然后以该值为目标，训练一个新的弱分类器，并计算该弱分类器的权重，以累加的形式结合到现有模型中，实现对模型的更新</p><p>利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树。</p><h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>优点：它能灵活的处理各种类型的数据；在相对较少的调参时间下，预测的准确度较高，这个是相对SVM来说的。 </p><p>缺点：基学习器之前存在串行关系，难以并行训练数据。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>1.对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种: </p><p>(1)如果是指数损失函数，此时GBDT退化为Adaboost算法，损失函数表达式为</p><p><img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image002.jpg" alt="img"></p><p><img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image004.jpg" alt="img"></p><h1 id="GBDT与Boosting的区别"><a href="#GBDT与Boosting的区别" class="headerlink" title="GBDT与Boosting的区别"></a>GBDT与Boosting的区别</h1><p>GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型，所以说，在GBDT每个新的模型的建立是为了使得之前的模型的<strong>残差往梯度下降的方向</strong>，与传统的Boosting中关注正确错误的样本加权有着很大的区别。</p><h1 id="GBDT与Boosting-Tree的区别"><a href="#GBDT与Boosting-Tree的区别" class="headerlink" title="GBDT与Boosting Tree的区别"></a>GBDT与Boosting Tree的区别</h1><p>Boosting Tree的适合于损失函数为<strong>平方损失</strong>或者<strong>指数损失</strong>。而Gradient Boosting适合<strong>各类损失函数</strong>（损失函数为平方损失则相当于Boosting Tree拟合残差；损失函数为指数损失则可以近似于Adaboost，但树是回归树）</p><h1 id="RF与GBDT之间的区别与联系"><a href="#RF与GBDT之间的区别与联系" class="headerlink" title="RF与GBDT之间的区别与联系"></a>RF与GBDT之间的区别与联系</h1><p>1）相同点：都是<strong>由多棵树组成</strong>，最终的结果都是由多棵树共同决定。 </p><p>2）不同点：</p><p>典型的Bagging与Boosting的区别：</p><p>并行/串行：组成RF的树可以<strong>并行</strong>生成，而GBDT是<strong>串行</strong>生成</p><p>方差/偏差：RF是减少模型的<strong>方差</strong>，而GBDT是减少模型的<strong>偏差</strong></p><p>RF的结果是<strong>多数表决</strong>的，而GBDT则是<strong>多棵树累加</strong>的结果</p><p>分类树/回归树：组成RF的树可以分类树也可以是回归树，而GBDT只由<strong>回归树</strong>组成</p><p>异常值：RF对异常值不敏感，因为是多棵树表决，而GBDT对<strong>异常值比较敏感</strong>，因为当前的错误会延续给下一棵树</p><h1 id="GBDT常用调参参数"><a href="#GBDT常用调参参数" class="headerlink" title="GBDT常用调参参数"></a>GBDT常用调参参数</h1><p>在scikit-learn中，GradientBoostingClassifier为GBDT的分类，而GradientBoostingRegressor为GBDT的回归类。两者的参数类型完全相同，当然有些参数比如损失函数loss的可选择项并不相同。把重要参数分为两类，第一类是Boosting参数，第二类是树参数。</p><h2 id="boosting参数"><a href="#boosting参数" class="headerlink" title="boosting参数"></a>boosting参数</h2><p>n_estimators: </p><p>弱学习器的最大迭代次数，或者说弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，容易过拟合，默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</p><p>learning_rate: </p><p>每个弱学习器的权重缩减系数ν，也称作步长，ν的取值范围为0&lt;ν≤1。对于同样的训练集拟合效果，较小的ν意味着需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。<strong>所以这两个参数n_estimators和learning_rate要一起调参</strong>，默认是1。</p><p>subsample: </p><p>子采样比率，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是<strong>不放回抽样</strong>。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1。</p><p>loss: </p><p>损失函数。对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等价于Adaboost。</p><p>对于回归模型，有均方差”ls”, 绝对损失”lad”, Huber损失”huber”和分位数损失“quantile”。默认是均方差”ls”。一般来说，如果数据的噪音点不多，用默认的均方差”ls”比较好。如果是噪音点较多，则推荐用抗噪音的损失函数”huber”。而如果需要对训练集进行分段预测的时候，则采用“quantile”。</p><p>alpha：</p><p>这个参数只有GradientBoostingRegressor有，当我们使用Huber损失”huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。</p><h2 id="树参数"><a href="#树参数" class="headerlink" title="树参数"></a>树参数</h2><p>由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。</p><p>max_features: </p><p>划分时考虑的最大特征数，可以使用很多种类型的值，</p><p>默认是”None”：意味着考虑所有的特征数；</p><p>“log2”：意味着划分时最多考虑<img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image006.png" alt="img">个特征；</p><p>“sqrt”或者”auto”：意味着划分时最多考虑<img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image008.png" alt="img">个特征；</p><p>整数：代表考虑的特征绝对数；</p><p>浮点数：代表考虑特征百分比，即考虑（百分比xN）取整后的特征数，其中N为样本总特征数。</p><p>一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</p><p>max_depth: </p><p>决策树最大深度，如果不输入的话，默认值是3。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。</p><p>min_samples_split: </p><p>内部节点再划分所需最小样本数，这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认是2。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><p>min_samples_leaf: </p><p>叶子节点最少样本数，这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1。可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</p><p>min_weight_fraction_leaf</p><p>和上面min_ samples_ leaf很像，不同的是这里需要的是一个比例而不是绝对数值：终点节点所需的样本数占总样本数的比值。（两者只需定义一个就行）</p><p>min_weight_fraction_leaf：</p><p>叶子节点最小的样本权重和，这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题。一般来说，如果有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意这个值了。</p><p>max_leaf_nodes: </p><p>最大叶子节点数，通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征很多的话，可以加以限制，具体的值可以通过交叉验证得到。</p><p>min_impurity_split: </p><p>节点划分的最小不纯度，这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点，即为叶子节点 。一般不推荐改动，默认值1e-7。</p><p>参数调节的一般方法：</p><p>我们要调节的参数有两种：树参数和boosting参数。learning rate没有什么特别的调节方法，因为只要我们训练的树足够多learning rate总是小值来得好。虽然随着决定树的增多GBM并不会明显得过度拟合，高learing rate还是会导致这个问题，但如果一味地减小learning rate、增多树,计算就会非常昂贵而且需要运行很长时间。</p><p>可以采取以下方法调参：</p><ol><li>选择一个相对来说稍微高一点的learning rate。一般默认的值是0.1，不过针对不同的问题，0.05到0.2之间都可以</li><li>决定当前learning rate下最优的决定树数量。它的值应该在40-70之间。记得选择一个你的电脑还能快速运行的值，因为之后这些树会用来做很多测试和调参。</li><li>接着调节树参数来调整learning rate和树的数量。我们可以选择不同的参数来定义一个决定树， </li><li>降低learning rate，同时会增加相应的决定树数量使得模型更加稳健</li></ol><p>树参数可以按照这些步骤调节：</p><p>调节max_depth和 num_samples_split<br>调节min_samples_leaf<br>调节max_features</p><p>需要注意下调参顺序，对结果影响最大的参数应该优先调节，就像max_depth和num_samples_split。</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Ensemble </tag>
            
            <tag> Classification </tag>
            
            <tag> GBDT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DecisionTree</title>
      <link href="/2019/11/13/decisiontree/"/>
      <url>/2019/11/13/decisiontree/</url>
      
        <content type="html"><![CDATA[<p>递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。</p><h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>表示随机变量不确定性的度量</p><script type="math/tex; mode=display">H(D)=-\sum_{i=1}^{n}{p_ilog_2p_i}</script><p>其中$p_i$指样本集合$D$中第i类样本所占比例</p><p>描述样本集合$D$携带的信息量。 信息量越大（值变化越多），则越不确定，越不容易被预测。</p><p><strong>信息熵特点</strong>： </p><p>　　a) 不同类别的概率分布越均匀，信息熵越大；</p><p>　　b) 类别个数越多，信息熵越大；</p><p>　　c) 信息熵越大，越不容易被预测；（变化个数多，变化之间区分小，则越不容易被预测）（对于确定性问题，信息熵为0；p=1; E=p*logp=0）</p><h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><p>其实不同的决策树学习算法只是它们<strong>选择特征的依据不同</strong>，决策树的生成过程都是一样的</p><p><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同评估标准，从而衍生出不同的决策树算法。</p><p>特征选择的关键是如何选择最优特征对数据集进行划分，随着划分过程的进行，希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的纯度越来越高。</p><p><strong>决策树生成</strong>：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分此时决策树停止生长。</p><p>有三种选择最优特征的标准：信息增益、增益率和基尼指数，分别对应了三种决策树算法：ID3，C4.5，CART</p><p>信息增益越大，意味着使用某个特征进行划分所获得的纯度提升越大，所以每次选择信息增益最大的特征进行划分。ID3就是使用信息增益作为标准进行划分的。但信息增益准则对可取值数目较多的特征有偏好（因为信息增益反映的是给定条件后不确定性减少的程度，特征取值越多意味着确定性更高，即条件熵越小，信息增益越大）</p><p>C4.5算法在信息增益的基础上做了一些改进，使用增益率来选择最优划分特征。不过增益率对可取值数目较少的特征有偏好，所以<strong>实际中C4.5算法并不是直接选择增益率最大的特征，而是先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的</strong>。</p><p>基尼指数也是度量数据集纯度的指标，基尼指数越小代表数据集的纯度越高，CART是使用基尼指数来选择最优特征的。</p><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><p>核心是在决策树各个节点上应用信息增益准则选择特征，每一次都选择使得<strong>信息增益最大</strong>的特征进行分裂，递归地构建决策树。</p><h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><p>由于特征A使D的分类不确定性减少的程度</p><script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script><p>根据信息增益准则的特征选择方法：计算每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p><p>缺点：选择取值较多的特征往往会具有较大的信息增益（取值越多意味着确定性更高，条件熵越小，信息增益越大），所以<strong>ID3偏向于选择取值较多的特征</strong>。</p><p> 仅支持分类不支持回归，不支持连续型变量，只有树的生成没有剪枝（容易过拟合），没有缺失值处理方法</p><h2 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h2><p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征</p><h3 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h3><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p><p>定义为信息增益<strong>g(D,A)</strong>与训练数据集D关于特征A的值的熵$H_A(D)$之比</p><script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)}{H_A(D)}</script><p> 其中$H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}$，其中$n$是特征A取值的个数</p><p>特征数越多的特征对应的特征熵越大，它作为分母，一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升泛化能力 </p><p>但增益率准则<strong>对可取值数目较少</strong>的特征有所偏好，因此C4.5算法不是直接选取增益率最大的候选划分特征，而是使用了一个启发式算法：先从候选特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的。</p><p><strong>过拟合策略：C4.5引入了正则化系数进行剪枝</strong></p><h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>由以下两步组成：</p><p>（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大</p><p>（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，使用损失函数最小作为剪枝标准</p><p>特征选择</p><p>​    回归树：平方误差最小化准则</p><p>​    分类树：基尼指数最小化准则</p><h3 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h3><p>假设有K个类</p><script type="math/tex; mode=display">Gini(D)=1-\sum_{k=1}^{K}p_k^2</script><p>代表集合D的不确定性</p><p>特征A的基尼指数为</p><script type="math/tex; mode=display">Gini(D,A)=\sum_{k=1}^{K}\frac{|D_k|}{|D|}Gini(D^k)</script><p>表示经过特征A分割之后集合D的不确定性</p><p>选<strong>基尼指数最小</strong>的特征的作为最优划分特征。</p><h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>为缓解决策树过拟合，需要对决策树进行剪枝。分为预剪枝和后剪枝</p><p>往往通过极小化决策树整体的损失函数或代价函数来实现</p><p>决策树生成学习局部的模型，而决策树剪枝学习整体的模型</p><h2 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h2><p>在生成决策树的过程中提前停止树的增长。核心思想是在树中节点进行扩展之前，先计算当前的划分是否能带来模型泛化性能的提升，如果不能则不再继续生成子树。</p><p>前剪枝停止决策树生长的几种方法：</p><p>(1) 当树达到一定深度时停止生长。</p><p>(2) 当到达当前节点的样本数量小于某个阈值时停止生长。</p><p>(3) 计算每次分类对测试集的准确率提升，当小于某个阈值时停止生长。</p><p>前剪枝的优缺点：</p><p>优点：简单高效，适合解决大规模问题。</p><p>缺点：</p><p>(1) 深度和阈值这些值很难准确估计，针对不同问题会有很大差别。</p><p>(2) 前剪枝存在一定局限性，有<strong>欠拟合的风险</strong>，虽然当前的划分会导致测试集准确率下降，可能在后面会有显著上升。</p><h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>在已生成的过拟合决策树上进行剪枝。核心思想是让算法生成一棵完全生长的决策树，然后从底层向上计算是否剪枝。(剪枝是将子树删除，用一个叶子节点代替，节点类别按多数投票)如果剪枝之后准确率有提升，则剪枝。</p><p>后剪枝的优缺点：</p><p>优点：通常可以得到泛化能力更强的决策树。</p><p>缺点：<strong>时间开销大。</strong></p><h1 id="ID3-C4-5-CART-对比"><a href="#ID3-C4-5-CART-对比" class="headerlink" title="ID3/C4.5/CART 对比"></a>ID3/C4.5/CART 对比</h1><div class="table-container"><table><thead><tr><th>算法</th><th>支持模型</th><th>树结构</th><th>特征选择</th><th>连续值处理</th><th>缺失值</th><th>剪枝</th></tr></thead><tbody><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td><td>不支持</td><td>不支持</td><td>不支持</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>CART</td><td>分类，回归</td><td>二叉树</td><td>基尼指数，均方误差</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table></div><p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。</p><p>回归树：用平方误差最小化作为选择特征的准则</p><p>分类树：采用基尼指数最小化原则进行特征选择，递归地生成二叉树。</p><p>也提供了优化的剪枝策略</p><p><strong>从样本类型的角度：</strong></p><p>ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。</p><p>C4.5会排序找到切分点，将连续变量转换为多个取值区间的离散型变量；</p><p>CART每次会对特征进行二值划分，适用于连续变量。</p><p><strong>从应用角度：</strong></p><p>ID3和C4.5只能用于分类，CART树可以用于分类和回归。</p><p><strong>从细节、优化过程角度：</strong></p><p>ID3对样本特征缺失值比较敏感，而C4.5和CART树都可以对缺失值进行不同方式的处理。</p><p>ID3和C4.5可以产生多叉分支，且每个特征在层级之间不会复用。CART树是二叉树，<strong>每个特征可以被重复利用</strong>。</p><p>ID3和C4.5通过剪枝来权衡树的准确性和泛化性能，CART树枝节利用全部数据发现所有可能的树结构进行对比。</p><h1 id="Sklearn中树模型输出的特征重要程度"><a href="#Sklearn中树模型输出的特征重要程度" class="headerlink" title="Sklearn中树模型输出的特征重要程度"></a>Sklearn中树模型输出的特征重要程度</h1><p>决策树中节点分裂不纯度的改变量的归一化值</p><h1 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h1><p>优点：</p><p>易于理解和解释，可视化分析，容易提取出规则</p><p>可同时处理分类型和数值型变量</p><p>缺点：</p><p>容易过拟合</p><p>通常情况下精确度不如其他算法好</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> DecisionTree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensemble</title>
      <link href="/2019/10/18/ensemble/"/>
      <url>/2019/10/18/ensemble/</url>
      
        <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>集成学习的目的是通过结合多个基学习器的预测结果，来改善基学习器的泛化能力和鲁棒性。 </p><p>RF, GBDT, XGBoost, LightGBM都属于集成学习。</p><p>根据基学习器的生成方式，目前的集成学习方法大致分为两大类：</p><ol><li><p><strong>基学习器之间存在强依赖关系、必须串行生成的序列化方法</strong>（代表是Boosting）</p><p>从偏差-方差分解的角度看，Boosting主要关注降低偏差（即降低错误率，偏差反映学习算法的拟合能力），因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。</p></li><li><p><strong>基学习器间不存在强依赖关系、可同时生成的并行化方法</strong>（代表是Bagging和RF）</p><p>从偏差-方差分解的角度看，Bagging主要关注降低方差（方差度量同样大小的数据集的变动所导致的学习性能的变化）。当基学习器不稳定（large variance）时，Bagging带来的性能提升尤为明显。因此它在不剪枝决策树、神经网络等<strong>易受样本扰动的学习器</strong>上效用更为明显。</p></li></ol><h2 id="Boosting与Bagging的不同"><a href="#Boosting与Bagging的不同" class="headerlink" title="Boosting与Bagging的不同"></a>Boosting与Bagging的不同</h2><ol><li><strong>方差与偏差</strong>：Bagging是减少方差（因此在易受样本扰动的学习器上效用很明显），而Boosting是减少偏差（因此能基于相当弱的基学习器构建出强学习器）。</li><li><strong>并行与串行</strong>：Bagging的各个预测函数可以并行生成，而Boosting的只能顺序生成。</li><li><strong>预测精度</strong>：Bagging的训练集是随机的，以独立同分布选取的训练样本子集训练弱分类器；而Boosting训练集的选择不是独立的，每一次选择的训练集都依赖于上一次学习的结果，根据错误率取样。因此<strong>Boosting的分类精度在大多数数据集中要优于Bagging</strong>，但是在有些数据集中，由于过拟合的原因，Boosting的精度会退化。 </li><li><strong>权重</strong>：Bagging的每个预测函数没有权重，而Boosting根据每一次训练的训练误差得到该次预测函数的权重</li></ol><h2 id="结合策略（模型融合）"><a href="#结合策略（模型融合）" class="headerlink" title="结合策略（模型融合）"></a>结合策略（模型融合）</h2><p>好处：</p><ol><li>减小泛化误差</li><li>降低陷入局部极小点的风险</li><li>假设空间有所扩大，可能学到更好的近似</li></ol><ul><li><p><strong>平均法</strong>（简单平均、加权平均）</p><pre><code>适用范围：</code></pre><ol><li>规模大的集成，学习的权重较多，加权平均法易导致过拟合</li><li>个体学习器性能相差较大时宜使用加权平均法，相近用简单平均法。 </li></ol></li><li><p><strong>投票法</strong>（绝对多数、相对多数、加权投票）</p><ol><li><p>绝对多数投票法：某标记超过半数则预测为该标记，否则<strong>拒绝预测</strong>（在可靠性要求较高的学习任务中是一个很好的机制，若必须提供预测结果则退化为相对多数投票法）</p></li><li><p>相对多数投票法：预测为得票最多的标记，若同时有多个标记的票最高，则从中随机选取一个</p></li><li><p>加权投票法：与加权平均法类似，是对<strong>预测分布概率</strong>进行加权求和，再取其中概率最大类</p><ul><li><p>类标记：使用类标记的投票亦称“硬投票”（hard voting）</p></li><li><p>类概率：使用类概率的投票亦称“软投票”（soft voting）</p></li></ul><p><strong>硬投票与软投票不能混用，</strong>若基学习器产生分类置信度，例如支持向量机的分类间隔值，需使用一些技术如Platt缩放、等分回归、等进行校准后才能作为类概率使用。若基学习器的类型不同，则其类概率值不能直接进行比较，可将类概率输出转化为类标记输出然后再投票。</p></li></ol></li><li><p><strong>学习法</strong></p><p>典型代表<strong>Stacking</strong>：先从初始数据集中训练出<strong>初级学习器</strong>，然后生成一个新数据集用于训练<strong>次级学习器</strong>，在新数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记仍被当作样例标记</p></li></ul><p><strong>Stacking注意事项</strong></p><p><strong>在训练阶段，次级训练集是利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大。一般通过使用交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。</strong>以k折交叉验证为例（如下图所示），对训练集划分为5份（5折）；对每一折Dk为测试集，D-Dk为训练集，训练第一种初级学习器5次，将5次的结果拼接起来即可得到一个N×1的矩阵，以此往复训练5种初级学习器，得到5个N×1矩阵，将其横向拼接，即可得到N×5矩阵，将其作为次级学习器的训练集来训练次级学习器。</p><p> <img src="http://image.mamicode.com/info/201901/20190108231708049828.png" alt="技术分享图片"> </p><h3 id="Stacking详细举例说明："><a href="#Stacking详细举例说明：" class="headerlink" title="Stacking详细举例说明："></a><strong>Stacking详细举例说明：</strong></h3><p><strong>假设是五折的stacking，我们有一个train数据集和一个test数据集，那么一个基本的stacking框架会进行如下几个操作：</strong></p><ol><li><p>选择基模型。我们可以有xgboost，lightGBM，RandomForest，SVM，ANN，KNN，LR等等你能想到的各种基本算法模型。</p></li><li><p>把训练集分为不交叉的五份。我们标记为train1到train5。</p></li><li><p>从train1开始作为预测集，使用train2到train5建模，然后预测train1，并保留结果；然后，以train2作为预测集，使用train1，train3到train5建模，预测train2，并保留结果；如此进行下去，直到把train1到train5各预测一遍；</p></li><li><p>把预测的结果按照train1到trian5的位置对应填补上，得到对train整个数据集在第一个基模型的一个stacking转换。</p></li><li><p>在上述建立的五个模型过程中，每个模型分别对test数据集进行预测，并最终保留这五列结果，然后对这五列取平均，作为第一个基模型对test数据的一个stacking转换。</p></li><li><p>选择第二个基模型，重复以上2-5操作，再次得到train整个数据集在第二个基模型的一个stacking转换。</p></li><li><p>以此类推。<strong>有几个基模型，就会对整个train数据集生成几列新的特征表达。同样，也会对test有几列新的特征表达。</strong></p></li><li><p><strong>一般使用LR作为第二层的模型进行建模预测。</strong></p></li></ol><h2 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h2><p>欲构建泛化能力强的集成，个体学习器应“好而不同”。</p><p><strong>根据误差-分歧分解：个体学习器准确性越高、多样性越大，则集成越好（证明见西瓜书）。</strong></p><h3 id="如何度量个体分类器的多样性？"><a href="#如何度量个体分类器的多样性？" class="headerlink" title="如何度量个体分类器的多样性？"></a>如何度量个体分类器的多样性？</h3><p>典型做法是考虑个体分类器的两两相似/不相似性。 </p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Ensemble </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XGBoost</title>
      <link href="/2019/10/18/xgboost/"/>
      <url>/2019/10/18/xgboost/</url>
      
        <content type="html"><![CDATA[<h2 id="xgboost的原理"><a href="#xgboost的原理" class="headerlink" title="xgboost的原理"></a>xgboost的原理</h2><h2 id="XGBoost与GBDT的区别"><a href="#XGBoost与GBDT的区别" class="headerlink" title="XGBoost与GBDT的区别"></a>XGBoost与GBDT的区别</h2><p><strong>损失函数的改变：（导数和正则项的认识）</strong></p><ol><li><p><strong>正则项</strong>：XGBoost显式地<strong>加入了正则项</strong>来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。正则项里包含树的叶子节点个数、每个叶子节点上输出的score的平方和。</p></li><li><p><strong>几阶导数</strong>：传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对损失函数<strong>进行了二阶泰勒展开</strong>，得到一阶和二阶导数</p></li><li><p><strong>列采样</strong>：传统的GBDT在每轮迭代的时候使用全部的数据，XGBoost则采用与RF相类似的策略，支持对数据进行采样，同时还支持列抽样，不仅能降低过拟合，还能减少计算。</p></li><li><p><strong>缺失值</strong>：传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略</p></li><li><p><strong>基学习器</strong>：传统的GBDT以CART树作为基学习器，XGBoost支持多种类型的基分类器，比如线性分类器</p></li><li><p>xgboost工具支持自定义损失函数，只要函数可一阶和二阶求导。</p></li><li><p><strong>并行（计算特征的增益）</strong>：xgboost工具支持并行。注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。<strong>xgboost的并行是在特征粒度上的</strong>。因为决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，<strong>预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。</strong>这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p></li><li><p>Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）</p></li><li><p>树节点在进行分裂时，需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。</p></li></ol><h2 id="xgboost有什么优势"><a href="#xgboost有什么优势" class="headerlink" title="xgboost有什么优势"></a>xgboost有什么优势</h2><ol><li><p>正则化</p><p>使用许多策略防止过拟合，如：正则化项、支持列抽样和Shrinkage等</p></li><li><p>并行处理</p><p>支持并行化，虽然树与树之间是串行关系，但是同层级节点可并行。具体地，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么<strong>各个特征的增益计算就可以开多线程进行</strong></p></li><li><p>高度的灵活性</p><p>XGBoost 允许用户定义自定义优化目标和评价标准</p></li><li><p>缺失值处理</p><p>XGBoost内置处理缺失值的规则，XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法</p></li><li><p>剪枝</p><p>当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。</p><p>XGBoost会一直分裂到指定的最大深度，然后回过头来剪枝。</p></li><li><p>内置交叉验证</p><p>XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数</p></li><li><p>在已有的模型基础上继续</p><p>XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势</p></li></ol><h2 id="怎么缓解过拟合的"><a href="#怎么缓解过拟合的" class="headerlink" title="怎么缓解过拟合的"></a>怎么缓解过拟合的</h2><p>​    正则化项</p><p>​    通过early_stopping提前停止训练</p><p>​    通过控制树的深度（max_depth）、树的个数、叶子结点数等参数</p><h2 id="xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？"><a href="#xgboost-gbdt在调参时为什么树的深度很少就能达到很高的精度？" class="headerlink" title="xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？"></a>xgboost/gbdt在调参时为什么树的深度很少就能达到很高的精度？</h2><p>一句话的解释：Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</p><p>随机森林和GBDT都是属于集成学习的范畴。</p><p>对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差, 因为采用了相互独立的基分类器多了以后，h的值自然就会靠近.所以对于每个基分类器来说，目标就是如何降低这个偏差,所以我们会采用深度很深甚至不剪枝的决策树。</p><p>对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差,所以对于每个基分类器来说，问题就在于如何选择方差更小的分类器，即更简单的分类器，所以选择了深度很浅的决策树。</p><h2 id="为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？"><a href="#为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？" class="headerlink" title="为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？"></a>为什么基于tree-ensemble的机器学习方法，在kaggle比赛中效果非常好？</h2><p>主要从三个方面来回答这个问题。</p><p>​    理论模型 （站在 vc-dimension 的角度）</p><p>​    实际数据</p><p>​    系统的实现 （主要基于xgboost）</p><p>通常决定一个机器学习模型能不能取得好的效果，以上三个方面的因素缺一不可。</p><p><strong>1） 站在理论模型的角度</strong></p><p>一个机器学习模型想要取得好的效果，这个模型需要满足以下两个条件：</p><p>​    模型在我们的训练数据上的表现要不错，也就是训练误差要足够小。</p><p>​    模型的自由度不能太大，以防过拟合。 </p><p>为什么 tree-ensemble 在实际中的效果很好呢？区别就在于 “模型的可控性”。</p><p>先说结论，tree-ensemble 这样的模型的可控性是好的，而像 LR 这样的模型的可控性是不够好的（或者说，可控性是没有 tree-ensemble 好的）。</p><p>我们之前说，当我们选择一个 hypothsis 后，就需要在训练数据上进行训练，从而逼近我们的 “上帝函数”。</p><p>对于LR这样的模型。如果 underfit，我们可以通过加 feature，或者通过高次的特征转换来使模型在训练数据上取得足够高的正确率。而对于 tree-ensemble来说，解决这一问题的方法是通过训练更多的 “弱弱” 的 tree. 所以，这两类模型都可以把 training error 做的足够低，也就是说模型的表达能力都是足够的。</p><p><strong>在 tree-ensemble 模型中，通过加 tree 的方式，对于模型的 vc-dimension 的改变是比较小的。而在LR中，初始的维数设定，或者说特征的高次转换对于vc-dimension的影响都是更大的。</strong>换句话说，tree-ensemble总是用一些 “弱弱” 的树联合起来去逼近 “上帝函数”，一次一小步，总能拟合的比较好。而对于LR这样的模型，我们很难去猜到这个“上帝函数”到底长什么样子（到底是2次函数还是3次函数？上帝函数如果是介于2次和3次之间怎么办呢？）。所以，一不小心我们设定的多项式维数高了，模型就 “刹不住车了”。这也就是我们之前说的，tree-ensemble 模型的可控性更好，也即更不容易 overfit.</p><p><strong>2) 站在数据的角度</strong></p><p>除了理论模型之外, 实际的数据也对我们的算法最终能取得好的效果息息相关。kaggle 比赛选择的都是真实世界中的问题。所以数据多多少少都是有噪音的。而<strong>基于树的算法通常抗噪能力更强。</strong>比如在树模型中，<strong>很容易对缺失值进行处理。除此之外，基于树的模型对于categorical feature也更加友好</strong>。</p><p>除了数据噪音之外，<strong>feature的多样性</strong>也是tree-ensemble模型能够取得更好效果的原因之一。通常在一个kaggle任务中，可能有年龄、收入特征、性别特征等等从不同 channel 获得的特征。而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一，因为 svm 本质上是属于一个几何模型，这个模型需要去定义样本之间的 kernel 或者 similarity（对于linear svm 来说，这个similarity 就是内积）。这其实和我们在之前说过的问题是相似的，我们无法预先设定一个很好的similarity。这样的数学模型使得 svm 更适合去处理 “同性质”的特征。而从不同 channel 中来的 feature 则更适合 tree-based model, 这些模型对数据的分布通常并不敏感。</p><p><strong>3) 站在系统实现的角度</strong></p><p>除了有合适的模型和数据，一个良好的机器学习系统实现往往也是算法最终能否取得好的效果的关键。一个好的机器学习系统实现应该具备以下特征：</p><p>​    正确高效的实现某种模型。</p><p>​    系统具有灵活、深度的定制功能</p><p>​    系统简单易用</p><p>​    系统具有可扩展性, 可以从容处理更大的数据。</p><p>到目前为止，xgboost 是我发现的唯一一个能够很好的满足上述所有要求的 machine learning package.</p><p>在灵活性方面，xgboost 可以深度定制每一个子分类器，并且可以灵活的选择 loss function（logistic，linear，softmax等等）。除此之外，xgboost还提供了一系列在机器学习比赛中十分有用的功能，例如 early-stop，cv等等在易用性方面，xgboost 提供了各种语言的封装，使得不同语言的用户都可以使用这个优秀的系统。</p><p>最后，在可扩展性方面，xgboost提供了分布式训练（底层采用rabit接口），并且其分布式版本可以跑在各种平台之上，例如mpi, yarn, spark等等。</p><h2 id="XGBoost如何输出概率？算法上怎么输出概率的？"><a href="#XGBoost如何输出概率？算法上怎么输出概率的？" class="headerlink" title="XGBoost如何输出概率？算法上怎么输出概率的？"></a>XGBoost如何输出概率？算法上怎么输出概率的？</h2><p>sklearn接口的xgboost 分类器：</p><p>xgboost.predict() ：输出预测的类别，默认采用0.5做阈值</p><p>xgboost.predict_proba()：输出概率</p><p>XGBoost库的分类器：</p><p>predict() 只会输出概率</p><h2 id="XGBoost怎么处理缺失值？"><a href="#XGBoost怎么处理缺失值？" class="headerlink" title="XGBoost怎么处理缺失值？"></a>XGBoost怎么处理缺失值？</h2><p>缺失数据会被分到左子树和右子树分别计算损失，选择较优的那一个。</p><p>如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分到右子树。</p><h2 id="XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？"><a href="#XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？" class="headerlink" title="XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？"></a>XGBoost中gbtree（基于树的模型）和gblinear（线性模型）的区别？</h2><p>gbtree使用基于树的模型进行提升计算，gblinear使用线性模型进行提升计算</p><h2 id="XGBoost如何寻找最优特征？是有放回还是无放回的呢？"><a href="#XGBoost如何寻找最优特征？是有放回还是无放回的呢？" class="headerlink" title="XGBoost如何寻找最优特征？是有放回还是无放回的呢？"></a>XGBoost如何寻找最优特征？是有放回还是无放回的呢？</h2><p>XGBoost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性。XGB属于boosting集成学习方法，样本是不放回的，每轮计算样本不重复。</p><h2 id="XGBoost-v-s-Boosting"><a href="#XGBoost-v-s-Boosting" class="headerlink" title="XGBoost v.s. Boosting"></a>XGBoost v.s. Boosting</h2><p>是在GBDT的基础上对boosting算法进行的改进：</p><p>GBDT是用模型在数据上的负梯度作为残差的近似值，从而拟合残差；XGBoost也是拟合的在数据上的残差，但是它是用泰勒展式对模型损失残差的近似</p><p>同时XGBoost对模型的损失函数进行的改进，并加入了模型复杂度的正则项。</p><h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>可按照max_depth，min_child_weight，colsamplt_bytree，eta的顺序一个一个调，每次调的时候其他参数保持不变</p><p>（参考<a href="https://blog.csdn.net/han_xiaoyang/article/details/52665396#commentBox）" target="_blank" rel="noopener">https://blog.csdn.net/han_xiaoyang/article/details/52665396#commentBox）</a></p><p><strong>XGBoost的作者把所有的参数分成了三类：</strong></p><p>​    通用参数：宏观函数控制。</p><p>​    Booster参数：控制每一步的booster(tree/regression)。</p><p>​    学习目标参数：控制训练目标的表现。</p><p><strong>通用参数</strong></p><p>Booster [默认gbtree]</p><p>gbtree：基于树的模型</p><p>gbliner：线性模型</p><p>silent[默认0]：当这个参数值为1时，静默模式开启，不会输出任何信息。</p><p>nthread[默认值为最大可能的线程数]</p><p><strong>Booster参数</strong></p><p>尽管有两种booster可供选择，这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。</p><p>eta学习率 [默认0.3]典型值为0.01-0.2</p><p>min_child_weight[默认1]决定最小叶子节点样本权重和。</p><p>这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。</p><p>max_depth[默认6]树的最大深度，用来避免过拟合的。典型值：3-10</p><p>max_leaf_nodes树上最大的节点或叶子的数量。可以替代max_depth的作用，因为如果生成的是二叉树，一个深度为n的树最多生成n^2个叶子</p><p>gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了<strong>节点分裂所需的最小损失函数下降值</strong>。这个参数的值和损失函数息息相关，所以是需要调整的</p><p>subsample[默认1]控制对于<strong>每棵树随机采样的比例</strong>。减小这个参数的值，算法会更加保守，避免过拟合。但如果这个值设置得过小，可能会导致欠拟合。典型值：0.5-1</p><p>colsample_bytree[默认1]用来控制每棵随机<strong>采样的列数</strong>的占比(每一列是一个特征)。典型值：0.5-1</p><p>lambda[默认1]L2正则化项系数</p><p>alpha[默认1]L1正则化项系数，可以应用在很高维度的情况下，使得算法的速度更快。</p><p>scale_pos_weight[默认1]在各类别样本十分不平衡时，把这个参数设定为一个正值，可使算法更快收敛</p><p><strong>学习目标参数</strong></p><p>这些参数用来控制理想的优化目标和每一步结果的度量方法</p><p><strong>objective[默认reg:linear]</strong></p><p>这个参数定义需要被最小化的损失函数。最常用的值有：</p><p>​    binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。</p><p>​    multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)。</p><p>​    multi:softprob和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。</p><p><strong>eval_metric[默认值取决于objective参数的取值]</strong></p><p>对于回归问题，默认值是rmse，对于分类问题，默认值是error</p><p>常用值：</p><p>​    rmse 均方根误差</p><p>​    mae 平均绝对误差</p><p>​    logloss 负对数似然函数值</p><p>​    error 二分类错误率(阈值为0.5)</p><p>​    merror 多分类错误率</p><p>​    mlogloss 多分类logloss损失函数</p><p>​    auc 曲线下面积</p><p>​    seed(默认0)</p><p>设置它可以复现随机数据的结果，也可以用于调整参数</p><h2 id="XGB为什么要用二阶信息不用一阶-：更快更准确"><a href="#XGB为什么要用二阶信息不用一阶-：更快更准确" class="headerlink" title="XGB为什么要用二阶信息不用一阶 ：更快更准确"></a>XGB为什么要用二阶信息不用一阶 ：更快更准确</h2><p>​    由于之前求最优解的过程只是对平方损失函数进行的，一阶导数是残差，二阶是常数，当损失函数是其它函数时，展开就没有这种形式了，为了能够有个统一的形式，使用泰勒二阶展开。为了统一损失函数求导的形式以支持自定义损失函数。</p><p>​    <strong>二阶信息本身能够让梯度收敛的更快更准确，可以简单认为一阶导数引导梯度方向，二阶导数引导梯度方向如何变化。</strong></p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Ensemble </tag>
            
            <tag> Classification </tag>
            
            <tag> XGBoost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Random Forest 随机森林</title>
      <link href="/2019/10/18/randomforest/"/>
      <url>/2019/10/18/randomforest/</url>
      
        <content type="html"><![CDATA[<h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>RF是Bagging算法的优化版本</p><p>其在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。</p><p>实现过程：</p><ol><li>从样本集中用Bootstrap采样选出n个样本</li><li>从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树</li><li>重复1、2两个步骤，即建立了m棵CART决策树</li><li>m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类</li></ol><h2 id="RF的节点划分"><a href="#RF的节点划分" class="headerlink" title="RF的节点划分"></a>RF的节点划分</h2><p>随机森林中的每一棵分类树为二叉树，<strong>从根节点开始依次对训练集进行划分</strong></p><p><strong>纯度度量方法是Gini准则</strong>，按照<strong>节点纯度最小原则</strong>，分裂为左节点和右节点，它们分别包含训练数据的一个子集，按照同样的规则节点继续分裂，直到满足分支停止规则而停止生长</p><p>若节点n上的分类数据全部来自于同一类别，则此节点的纯度$I(n)=0$</p><h2 id="RF的随机性体现"><a href="#RF的随机性体现" class="headerlink" title="RF的随机性体现"></a>RF的随机性体现</h2><ol><li><p>样本随机有放回采样（自助法重采样）</p><p>决策树对它们所训练的数据非常敏感，且对训练集的微小更改可以导致明显不同的树结构。随机森林利用了这一点，允许每棵单独的树从数据集中随机抽取样本，并进行替换，从而生成不同的树。这个过程被称为bagging。</p></li><li><p>随机属性选择<strong>（随机森林与普通bagging的区别）</strong></p><p>传统的决策树在选择划分属性时是在当前结点的属性集合中选择一个最优属性。</p><p>而在随机森林中，对子决策树的每个结点，先从该结点的属性集合中随机选择一个子集，然后再从这个子集中选择一个最优属性用于划分。 </p></li></ol><p>=&gt; 两个随机性的引入，使得随机森林不容易陷入过拟合 </p><h2 id="RF和Bagging对比"><a href="#RF和Bagging对比" class="headerlink" title="RF和Bagging对比"></a>RF和Bagging对比</h2><p>与 Bagging 中基学习器的多样性仅通过样本扰动(通过对初始训练集采样)而来不同</p><p>随机森林中基学习器的多样性不仅来自<strong>样本扰动</strong>，还来自<strong>属性扰动</strong></p><p>好处：</p><ol><li><p>最终集成的<strong>泛化性能</strong>可通过个体学习器之间差异度的增加而进一步提升。</p></li><li><p>随机森林的<strong>训练效率</strong>也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是<strong>确定型</strong>决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是<strong>随机型</strong>决策树，只需考虑特征的子集。</p></li></ol><p>RF的起始性能较差，特别当只有一个基学习器时。但随着学习器数目增多，随机森林通常会收敛到更低的<strong>泛化误差</strong>。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：</p><ol><li><p>训练可以高度并行化，对于大数据时代的大样本训练速度有优势（最主要的优点）</p></li><li><p>能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性</p></li><li><p>相对于Boosting系列的Adaboost和GBDT，RF实现比较简单。</p></li></ol><p>缺点：在噪声较大的分类或者回归问题上容易过拟合。</p><h2 id="特征重要程度Feature-Importance"><a href="#特征重要程度Feature-Importance" class="headerlink" title="特征重要程度Feature Importance"></a>特征重要程度Feature Importance</h2><p><strong>特征在RF所有决策树中节点分裂不纯度的平均改变量</strong></p><ul><li>节点$m$的Gini指数计算公式：</li></ul><script type="math/tex; mode=display">Gini_m=1-\sum_{k=1}^{|K|}p_{mk}^2</script><p>​        其中，$K$表示类别的总个数，$p_{mk}$表示节点m中类别$k$所占的比例</p><p>​        直观来说，是<strong>任意从节点$m$中随机抽取两个样本，其类别不一致的概率</strong></p><ul><li>特征$X_j$在节点$m$的重要性，即节点$m$分枝前后的Gini指数变化量为</li></ul><script type="math/tex; mode=display">VIM_{jm}=Gini_m-Gini_l-Gini_r</script><p>​        其中，$Gini_l$和$Gini_r$分别表示分枝后两个新节点的Gini指数。</p><ul><li>特征$X_j$在第$i$颗树的重要性为</li></ul><script type="math/tex; mode=display">VIM_{ij}=\sum_{m∈M}VIM_{jm}</script><ul><li>假设随机森林共有$n$颗树</li></ul><script type="math/tex; mode=display">VIM_{j}=\sum_{i=1}^{n}VIM_{ij}</script><ul><li>最后将所有特征求得的重要性评分进行归一化，得到特征$X_j$的重要性</li></ul><script type="math/tex; mode=display">VIM_{j}=\frac{VIM_{j}}{\sum_{i=1}^{n}VIM_{i}}</script><h2 id="LR-SVM-RF算法的区别"><a href="#LR-SVM-RF算法的区别" class="headerlink" title="LR/SVM/RF算法的区别"></a>LR/SVM/RF算法的区别</h2><p>都是监督学习算法，都是判别模型。</p><p>如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。</p><p>LR优点：</p><p>1.适合需要得到一个分类概率的场景</p><p>2.实现效率较高</p><p>3.对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决；</p><p>4.逻辑回归广泛的应用于工业问题上</p><p>LR的缺点：</p><p>1.当特征空间很大时，逻辑回归的性能不是很好；</p><p>2.不能很好地处理大量多类特征或变量；</p><p>3.对于非线性特征，需要进行转换；</p><p>4.依赖于全部的数据特征，当特征有缺失的时候表现效果不好。</p><p>SVM的优点：</p><p>1.能够处理大型特征空间</p><p>2.能够处理非线性特征之间的相互作用</p><p>3.无需依赖整个数据</p><p>SVM的缺点：</p><p>1.当观测样本很多时，效率并不是很高</p><p>2.有时候很难找到一个合适的核函数</p><p>决策树的优点：</p><p>1.直观的决策规则</p><p>2.可以处理非线性特征</p><p>3.考虑了变量之间的相互作用</p><p>决策树的缺点：</p><p>1.训练集上的效果高度优于测试集，即过拟合[随机森林克服了此缺点]</p><p>2.没有将排名分数作为直接结果</p><p>如何选择三种模型：</p><p>首当其冲应该选择的就是逻辑回归，如果它的效果不怎么样，那么可以将它的结果作为基准来参考；</p><p>然后试试决策树（随机森林）是否可以大幅度提升模型性能。即使你并没有把它当做最终模型，你也可以使用随机森林来移除噪声变量；</p><p>如果特征的数量和观测样本特别多，那么当资源和时间充足时，使用SVM不失为一种选择。</p><h2 id="python调参"><a href="#python调参" class="headerlink" title="python调参"></a>python调参</h2><p><img src="file:///C:/WINDOWS/Temp/msohtmlclip1/01/clip_image004.png" alt="img"></p><h2 id="PySpark参数"><a href="#PySpark参数" class="headerlink" title="PySpark参数"></a>PySpark参数</h2><p>class pyspark.ml.classification.RandomForestClassifier(featuresCol=’features’, labelCol=’label’, predictionCol=’prediction’, probabilityCol=’probability’, rawPredictionCol=’rawPrediction’, <strong>maxDepth=5,</strong> maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, <strong>impurity=’gini’</strong>, <strong>numTrees=20</strong>, <strong>featureSubsetStrategy</strong>=’auto’, seed=None, subsamplingRate=1.0)[source]</p>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Ensemble </tag>
            
            <tag> Classification </tag>
            
            <tag> RandomForest </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression 算法及其应用</title>
      <link href="/2019/10/18/logistic-regression-suan-fa-ji-qi-ying-yong/"/>
      <url>/2019/10/18/logistic-regression-suan-fa-ji-qi-ying-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><p>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到二分类的目的。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>逻辑回归是在线性回归的基础上，利用sigmoid函数（或称为logistic函数）</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>进行映射，代入线性回归部分</p><script type="math/tex; mode=display">z=\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n</script><p>得到二元逻辑回归模型的一般形式：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的$h_\theta(x)$就是逻辑回归返回的值，介于0到1之间，可以将其当做样本取正类的“概率”。因此对于样本$x$分类结果为正类1和负类0的概率分别为</p><script type="math/tex; mode=display">\begin{cases}P(y=1|x;\theta)=h_\theta(x)\\P(y=0|x;\theta)=1-h_\theta(x)\end{cases}</script><p>对$h_\theta(x)$进行变换可以得到对数几率的表达式</p><script type="math/tex; mode=display">ln\frac{h_\theta(x)}{1-h_\theta(x)}=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})=ln(\frac{1}{e^{-\theta^Tx}})=ln(e^{\theta^Tx})=\theta^Tx</script><p>从上式可以看出，<strong>逻辑回归的本质是在对线性回归模型的预测去逼近真实标记的对数几率</strong>。求解的关注点在于求解参数$\theta$上，通常使用极大似然估计的方法对$\theta$进行估计。</p><p>令</p><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的似然函数为</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^{m}P(y^i|x^i;\theta)=\prod_{i=1}^{m}h_\theta(x^i)^{y^i}*(1-h_\theta(x^i))^{1-y^i}</script><p>其中，$x^i$为第$i$个样本的特征做构成的向量（每个向量$n+1$维，共$m$个向量），$y^i$为第$i$个样本的标签，$m$为样本量。实际中为了简化计算，同时防止连乘所造成的浮点数下溢，通常会转化为对数似然函数</p><script type="math/tex; mode=display">l(\theta)=\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>逻辑回归所要解决的问题即为找到参数$\theta$，使得对数似然函数达到最大。</p><p>令损失函数为（忽略正则化项）</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}l(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>利用梯度下降求解参数</p><script type="math/tex; mode=display">\begin{align}\frac{∂J(\theta)}{∂\theta_j}& =-\sum_{i=1}^{m}[\frac{y^i}{h_\theta(x^i)}-\frac{1-y^i}{1-h_\theta(x^i)}]\frac{∂h_\theta(x^i)}{∂\theta_j}\\& =-\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]\frac{∂g(\theta^Tx^i)}{∂\theta_j}\\& =-\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))\frac{∂\theta^Tx^i}{∂\theta_j}\\& =-\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))x^i_j\\& =-\sum_{i=1}^{m}[y^i(1-g(\theta^Tx^i))-(1-y^i)g(\theta^Tx^i)]x^i_j\\& =-\sum_{i=1}^{m}[y^i-g(\theta^Tx^i)]x^i_j\\& =-\sum_{i=1}^{m}[y^i-h_\theta(x^i)]x^i_j\\\end{align}</script><p>因此最终得到参数迭代式</p><script type="math/tex; mode=display">\theta_j:=\theta_j+\eta\sum_{i=1}^{m}[y^i-h_\theta(x^i)]x^i_j</script><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点：</strong></p><ol><li><p>速度快，在时间和内存需求上相当高效，它可以应用于分布式数据和在线算法实现，用较少的资源处理大型数据</p></li><li><p>对线性分类问题拟合很好</p></li><li>简单易于理解，直接看到各个特征的权重</li></ol><p><strong>缺点：</strong></p><ol><li>分类精度可能不高，在非线性分类问题上表现不好</li><li>数据特征有缺失或者特征空间很大时表现效果并不好，受异常值影响大</li></ol><h2 id="与线性回归的异同"><a href="#与线性回归的异同" class="headerlink" title="与线性回归的异同"></a>与线性回归的异同</h2><p>本质是线性的，只是特征到结果映射用的是sigmoid函数，属于广义线性模型（GLM）</p><ul><li><p>相同</p><ol><li><p>都使用极大似然估计对训练样本进行建模；求解超参数时都可以使用梯度下降。</p></li><li><p>都是广义线性模型，逻辑回归本质上是一个线性回归模型，LR是以线性回归为理论支持的。</p></li></ol></li><li><p>不同</p><ol><li>本质：逻辑回归是分类，线性回归是回归，逻辑回归中$y$是因变量而非$\frac{p}{1-p}$，因变量是离散而非连续</li><li>LR形式上是线性回归，实质上是在求取输入空间到输出空间的非线性函数映射（对率函数起到将线性回归模型的预测值与真实标记联系起来的作用）</li><li>LR是直接对分类可能性进行建模，无需事先假设数据分布，而线性回归需要假设数据分布</li></ol></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数是它的极大似然函数取对数再除以样本量的相反数</strong></p><p>极大似然函数：</p><script type="math/tex; mode=display">L_\theta(x)=\prod_{i=1}^{m}h_\theta(x^i;\theta)^{y^i}*(1-h_\theta(x^i;\theta))^{1-y^i}</script><p>损失函数：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]</script><p>除以m并不改变最终求导极值结果，通过除以m可以得到<strong>平均损失值</strong>，避免<strong>样本数量对于损失值的影响</strong></p><blockquote><p><strong>Q：为什么要用极大似然函数作为损失函数？</strong></p><p>损失函数一般有四种：平方损失函数，对数损失函数，HingeLoss损失函数，绝对值损失函数。</p><p>将极大似然函数取对数以后等同于对数损失函数。</p><p><strong>在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</strong></p><p>梯度更新公式：</p><script type="math/tex; mode=display">θ_j=θ_j−(y^i−h_θ(x^i;θ))∗x^i_j</script><p>这个式子的更新速度只和$x^i_j$和$y^i$相关，和sigmoid函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 </p><p><strong>Q：为什么不选平方损失函数？</strong></p><p>其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmoid函数本身的梯度是很相关的。</p><script type="math/tex; mode=display">θ_j=θ_j-2(sigmoid(x)*(1-sigmoid(x)))x^i_j</script><p>sigmoid函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p><p>实际上也可以用最小二乘，但是最小二乘得到的权重效果比较差。</p><p>如果用最小二乘法，目标函数就是差值的平方和，<strong>是非凸的，不容易求解，很容易陷入到局部最优</strong>。</p><p>如果用极大似然估计，目标函数就是对数似然函数，是关于$(w,b)$的高阶<strong>连续可导凸函数</strong>，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。</p></blockquote><h2 id="参数求解方法"><a href="#参数求解方法" class="headerlink" title="参数求解方法"></a>参数求解方法</h2><ul><li>梯度下降法</li></ul><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼近最优解。</p><blockquote><p>梯度下降：随机梯度下降，批梯度下降，small-batch梯度下降</p><p>Q：三种方式的优劣以及如何选择最合适的梯度下降方式</p><ol><li><p>批梯度下降(BGD)：每次迭代使用所有样本来进行梯度的更新，能得到全局最优解。缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降(SGD)：每次迭代随机使用一个样本来对参数进行更新，优点是每一轮参数的更新速度大大加快，缺点是准确度下降，可能会收敛到局部最优（单个样本不能代表全体样本的趋势）。</p></li><li><p>小批量梯度下降：结合了BGD和SGD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p></li></ol></blockquote><ul><li><p>牛顿法</p><p>(待更新)</p></li><li><p>拟牛顿法</p><p>(待更新)</p></li></ul><blockquote><p>牛顿法与梯度下降法求解参数的区别：</p><p>两种方法不同在于牛顿法中<strong>多了一项二阶导数</strong>，这项二阶导数对参数更新的影响主要体现在<strong>改变参数更新方向上</strong>。如下图所示，红色是牛顿法参数更新的方向，绿色为梯度下降法参数更新方向，因为牛顿法考虑了二阶导数，因而可以<strong>找到更优的参数更新方向</strong>，在每次更新的步幅相同的情况下，可以<strong>比梯度下降法节省很多的迭代次数</strong>。</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/zdow4EB72J3r.png" alt="mark"></p></blockquote><h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>作用：需要一个单调可微的函数，把分类任务的真实标记与线性回归模型的预测值联系起来。</p><p>对于二分类问题，由线性回归得来的启发是根据特征的加权平均进行预测。很自然地想到设定一个阈值，如果加权平均大于该阈值就判为正类，反之判为负类。但<strong>阶跃函数不可导</strong>，所以<strong>引入Sigmoid函数，将样本的加权平均代入函数得到的值就是样本属于正类的概率，即将输入空间到输出空间作非线性函数映射</strong>。</p><p>Sigmoid函数形式：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid函数是一个S型的函数，函数图像：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><p>当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0。</p><p>它能够<strong>将任何实数映射到(0,1)区间</strong>（开区间，不可等于0或1），使其可用于将任意值函数转换为更适合二分类的函数。 </p><p>因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。</p><p>区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p><h2 id="共线性问题"><a href="#共线性问题" class="headerlink" title="共线性问题"></a>共线性问题</h2><p>对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。</p><p>通常做法为：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF， VIF越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。</p><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>解决方法：</p><ol><li><p>直接减少特征数，手动保留那些比较重要的特征。但是有时候舍弃特征也舍弃了有用的信息。</p></li><li><p>正则化保留所有的特征，但是减小参数θ的大小，当有很多特征，而且每一个特征都对预测结果有影响时，这种方法比较有效。</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]+\frac{\lambda}{2m}\sum_{i=1}^{m}\theta_j^2</script></li></ol><h2 id="多分类问题（待更新）"><a href="#多分类问题（待更新）" class="headerlink" title="多分类问题（待更新）"></a>多分类问题（待更新）</h2><ol><li><p>多项逻辑回归(Softmax Regression)</p><p>（二分类逻辑回归在多标签分类下的一种拓展）</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/4HnDRP6ijvH6.png" alt="mark"></p></li><li><p>one v.s. rest</p><p>k个二分类LR分类器，把标签重新整理为“第i类标签”与“非第i类标签”</p></li></ol><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a> </p><blockquote><p><em>class</em> <code>sklearn.linear_model.LogisticRegression</code>(<em>penalty=’l2’</em>, <em>dual=False</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>random_state=None</em>, <em>solver=’lbfgs’</em>, <em>max_iter=100</em>, <em>multi_class=’auto’</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>n_jobs=None</em>, <em>l1_ratio=None</em>) </p></blockquote><h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数"><a href="#1-基本模型参数" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p>fit_intercept：bool，指定是否将截距项添加到线性回归部分中。默认True。</p></li><li><p>multi_class：’auto’（默认）/‘ovr’/‘multinomial’，表示要预测的分类是二分类或一对多形式的多分类问题，还是多对多形式的多分类问题。</p><ul><li><p><strong>‘auto’</strong>：表示自动选择，会根据数据的分类情况和其他参数确定模型要处理的分类问题的类型。</p><blockquote><p>根据源码得到判定方法如下：</p><p>step 1：if solver = ‘liblinear’: multi_class = ‘ovr’</p><p>step 2：elif n_classes &gt; 2: multi_class = ‘multinomial’</p><p>step 3：else: multi_class = ‘ovr’</p></blockquote></li><li><p><strong>‘ovr’</strong>：表示当前处理的是二分类或一对多形式的多分类问题</p></li><li><p><strong>‘multinomial’</strong>：表示当前处理的是多对多形式的多分类问题</p></li></ul></li><li><p>class_weight：None（默认）/‘balanced’/dict，标签(label)的权重。</p><ul><li><p><strong>None</strong>：所有的label持有相同的权重， 所有类别的权值为1 </p></li><li><p><strong>‘balanced’</strong>：自动调整与样本中类频率成反比的权重，即<code>n_samples/(n_classes*np.bincount(y))</code></p><blockquote><p><strong>‘balanced’如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">  <span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">  y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment"># 标签值，一共16个样本</span></span><br><span class="line">  </span><br><span class="line">np.bincount(y)  </span><br><span class="line">  <span class="comment"># array([8, 6, 2], dtype=int64) 计算每个类别的样本数量，顺序按类别的出现次序</span></span><br><span class="line">  </span><br><span class="line">  class_weight = <span class="string">'balanced'</span></span><br><span class="line">  classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">  weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">  print(weight) <span class="comment"># [0.66666667 0.88888889 2.66666667]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 验证</span></span><br><span class="line">  print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">8</span>))  <span class="comment">#输出 0.6666666666666666</span></span><br><span class="line">  print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">6</span>))  <span class="comment">#输出 0.8888888888888888</span></span><br><span class="line">  print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">2</span>))  <span class="comment">#输出 2.6666666666666665</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>dict类型</strong></p><ul><li><p>对于二分类问题，可定义class_weight = {0:0.9, 1:0.1}，这样类别0的权重为0.9，类别1的权重为0.1</p><ul><li><p>对于多分类问题，定义的权重必须具体到每个标签下的每个类，其中类是key-value中的key，权重是value。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">  <span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">  y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment">#标签值，一共16个样本</span></span><br><span class="line">  </span><br><span class="line">  class_weight = &#123;<span class="number">0</span>:<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>,<span class="number">2</span>:<span class="number">5</span>&#125;   <span class="comment"># 设置</span></span><br><span class="line">  classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">  print(weight)   <span class="comment"># 输出：[1. 3. 5.]，也就是字典中设置的值</span></span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><pre><code>  （参考： https://www.cnblogs.com/qi-yuan-008/p/11992156.html )</code></pre><blockquote><p><strong>class_weight如何体现在逻辑回归的损失函数上？</strong></p></blockquote><p>  class_weight给每个类别分别设置不同的<strong>惩罚参数C</strong>。</p><p>  惩罚项C会相应的放大或者缩小某一类的损失，如果某一类C越大，这一类的损失也被（相对于其他类来说）放大，那么系统会把本次学习重点放在这一类上，使得系统尽可能的预测对这一类的输入，所以惩罚项C不会影响计算的损失，但反向学习时会相应的放大或缩小损失，间接影响学习的方向。</p><p>  (参考：<a href="https://www.zhihu.com/question/265420166/answer/293896934" target="_blank" rel="noopener">https://www.zhihu.com/question/265420166/answer/293896934</a>)</p><blockquote><p><strong>源码关于class_weight与sample_weight在LR损失函数上的具体计算方式：</strong></p></blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sample_weight *= class_weight_[le.fit_transform(y_bin)] </span><br><span class="line"><span class="comment"># 将class_weight乘到每个样本的sample_weight上</span></span><br><span class="line"><span class="comment"># sample_weight : shape (n_samples,)</span></span><br><span class="line"><span class="comment"># le即LabelEncoder，将标签标准化为0/1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">out = -np.sum(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure><h3 id="2-求解算法参数"><a href="#2-求解算法参数" class="headerlink" title="2. 求解算法参数"></a>2. 求解算法参数</h3><ul><li><p>solver：用于求解模型最优化问题的算法，可选{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认’lbfgs’。</p><ul><li><strong>‘liblinear’</strong>：使用坐标轴下降法来迭代优化损失函数。</li><li><strong>‘lbfgs’</strong>：拟牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。</li><li><strong>‘newton-cg’</strong>：牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。？</li><li><strong>‘sag’</strong>：随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅用一部分的样本来计算梯度，适用于样本量大的情况。</li><li><strong>‘saga’</strong>：线性收敛的随机优化算法的变种。</li></ul><blockquote><ul><li><p>对于数据量大小方面，’liblinear’仅限于处理二分类和一对多（OvR）问题，适用于小型数据集。’sag’和’saga’对于大型数据集来说更快（快速收敛仅在量纲大致相同的数据上得到保证），’sag’每次仅使用了部分样本进行梯度迭代，样本量少时不适合。</p></li><li><p>对于多分类问题来说，’liblinear’只能用于一对多（OvR），其它算法还可处理多对多（MvM），而多对多一般比一对多分类相对更准确一些。</p></li><li><p>对于正则化方法来说，’newton-cg’,’sag’,’lbfgs’这三种算法计算时都需要涉及到损失函数的一阶导或二阶导，因此不能用于没有连续导数的l1正则化，只能用于l2正则化。其他两种算法均可使用l1和l2正则化。</p></li></ul></blockquote></li></ul><p>（这部分还不是很了解QAQ，待补充）</p><h3 id="3-正则化参数"><a href="#3-正则化参数" class="headerlink" title="3. 正则化参数"></a>3. 正则化参数</h3><p>正则化项</p><script type="math/tex; mode=display">\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>其中$C$为正则化参数（$\lambda\ge0$)，$\alpha$为l1正则化的占比（$\alpha\in[0,1]$）。</p><ul><li>penalty： 指定正则化策略 ， {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}，默认’l2’。’elasticnet’同时包含‘l1’和‘l2’正则化。</li><li>C：正则化系数$\lambda$的倒数（乘在损失函数的前面，与乘在正则化部分前效果相同），必须是一个大于0的浮点数，默认值1.0，即默认正则项与损失函数的比值是1:1。</li><li>l1_ratio：l1正则化的占比$\rho$，取值范围[0,1]，默认为None。仅当penalty=’elasticnet’时使用，对于0&lt; l1_ratio &lt;1，惩罚是l1和l2正则化的组合。</li></ul><h3 id="4-控制迭代次数参数"><a href="#4-控制迭代次数参数" class="headerlink" title="4. 控制迭代次数参数"></a>4. 控制迭代次数参数</h3>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> LogisticRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计思考</title>
      <link href="/2019/10/18/tong-ji-si-kao/"/>
      <url>/2019/10/18/tong-ji-si-kao/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据-v-s-统计学"><a href="#大数据-v-s-统计学" class="headerlink" title="大数据 v.s. 统计学"></a>大数据 v.s. 统计学</h1><p>统计学注重的是方式方法</p><p>大数据则更关注于整个数据价值化的过程，不仅需要统计学知识，还需要具备数学知识和计算机知识</p><p>从一定角度来说，统计学为大数据进行数据价值化奠定了一定的基础</p>]]></content>
      
      
      
        <tags>
            
            <tag> statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive</title>
      <link href="/2019/10/17/hive/"/>
      <url>/2019/10/17/hive/</url>
      
        <content type="html"><![CDATA[<h1 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h1><p><strong>基于Hadoop</strong>的一个<strong>数据仓库工具</strong></p><p>数据存储在HDFS上<br>数据底层实现用MapReduce（默认使用MR，也可以使用Spark）<br>执行程序运行在Yarn上</p><p>功能：可以将结构化的数据文件映射为一张表，并提供<strong>类SQL</strong>查询功能</p><p>本质：<strong>将HiveQL转化为MapReduce程序</strong>，是一个分析引擎，相当于一个客户端</p><h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li>操作接口采用<strong>类SQL语法</strong>，<strong>提供快速开发的能力</strong>（简单、容易上手）。提供 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行，使不熟悉MapReduce 的用户也能很方便地对数据进行查询、汇总、分析。</li><li>Hive优势在于处理大数据，对实时性要求不高的场合。对于处理小数据没有优势（大量时间都在开关jvm虚拟机），因为Hive的执行延迟比较高，不支持交互式。</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li><strong>Hive的HQL表达能力有限</strong></li></ol><p>（1）迭代式算法无法表达</p><p>（2）数据挖掘方面不擅长（需要不断地对结果进行加工处理，mr在迭代方面很慢）</p><ol><li><strong>Hive的效率比较低</strong></li></ol><p>（1）Hive自动生成的MapReduce作业，通常情况下不够智能化</p><p>（2）Hive调优（包含SQL代码调优、资源调优）比较困难，粒度较粗（依赖模板，无法像mr精细化管理）</p><h1 id="Hive与数据库的区别"><a href="#Hive与数据库的区别" class="headerlink" title="Hive与数据库的区别"></a>Hive与数据库的区别</h1><p>从结构上来看，Hive 和数据库<strong>除了拥有类似的查询语言，再无类似之处</strong>。<br>数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的</p><h2 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h2><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><blockquote><p><strong>HIVE与SQL的区别</strong></p><p>hive是一种基于Hadoop的数据仓库架构，定义了简单的类SQL查询语句（HQL），当输入HQL时，hive会处理SQL将其转换为map-reduce。hive的表其实是HDFS的目录，hive的数据对应目录下的文件。</p><p>SQL是一种查询语言的标准，hive是基于hadoop的数据仓库架构，提供了类SQL的查询接口</p></blockquote><h2 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h2><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。<br>而数据库则可以将数据保存在块设备或者本地文件系统中。</p><h2 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h2><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。<br>而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据。</p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。<br>数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</p><h2 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h2><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎。</p><h2 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h2><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</p><h2 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h2><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的<br>而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。</p><h2 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h2><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据<br>数据库可以支持的数据规模较小。</p><h1 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><div class="table-container"><table><thead><tr><th>Hive数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>2byte有符号整数</td><td>20</td></tr><tr><td><strong>INT</strong></td><td>4byte有符号整数</td><td>20</td></tr><tr><td><strong>BIGINT</strong></td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td><strong>DOUBLE</strong></td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td><strong>STRING </strong></td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td>字节数组</td></tr></tbody></table></div><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table></div><h1 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h1><p> DDL(Data Definition Language)数据定义语言：</p><p>适用范围：对数据库中的某些对象(例如: database,table)进行管理，如Create,Alter, Drop</p><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>默认存储路径是/user/hive/warehouse/*.db </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive*'</span>;</span><br><span class="line">desc database db_hive;</span><br><span class="line">desc database extended db_hive;  <span class="comment">--显示数据库详细信息</span></span><br></pre></td></tr></table></figure><h2 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h2><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。</p><p><strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20170830'</span>);</span><br></pre></td></tr></table></figure></p><h2 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">--为表和列添加注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">--创建分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">--创建分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">--不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>] <span class="comment">--列分隔符</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">--指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">--指定表在HDFS上的存储位置</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="内部表和外部表区别"><a href="#内部表和外部表区别" class="headerlink" title="内部表和外部表区别?"></a>内部表和外部表区别?</h3><p>默认创建的表都是内部表。<br>Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(如/user/hive/warehouse)所定义的目录的子目录下。<br>当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;  <span class="comment">--以'\t'结尾的行格式分隔字段</span></span><br></pre></td></tr></table></figure><p><strong>被external修饰的为外部表（external table）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">EXTERNAL</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>  <span class="comment">--以'\t'结尾的行格式分隔字段 </span></span><br><span class="line">location <span class="string">'/user/t2'</span>;</span><br></pre></td></tr></table></figure><p><strong>区别：</strong> </p><ol><li>内部表数据由Hive自身管理，外部表数据由HDFS管理 </li><li>内部表数据存储的位置默认是/user/hive/warehouse，会将数据移动到数据仓库指向的路径；外部表数据的存储位置由自己制定，仅记录数据所在的路径，不移动数据</li><li>删除内部表会直接删除元数据及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除，这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ol><p><strong>相互转换：</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>); <span class="comment">--转为外部表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>); <span class="comment">--转为内部表</span></span><br></pre></td></tr></table></figure></p><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="分区表操作"><a href="#分区表操作" class="headerlink" title="分区表操作"></a>分区表操作</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition   </span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">dname <span class="keyword">string</span>, </span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2</span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">    dname <span class="keyword">string</span>, </span><br><span class="line">    loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.dept_partition <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 增加（多个）分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看分区</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment">--查看分区表结构</span></span><br><span class="line">desc formatted dept_partition;</span><br></pre></td></tr></table></figure><h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 重命名表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> old_table <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">columns</span>(deptdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">change</span> <span class="keyword">column</span> dept_old dept_new <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">replace</span> <span class="keyword">columns</span>(deptno <span class="keyword">string</span>, dname</span><br><span class="line"> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure><h1 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h1><p>DML(Data Manipulation Language 数据操控语言)用于操作数据库对象中包含的数据，也就是说操作的单位是记录。</p><p><strong>DML的操作对象：记录</strong></p><p>DML的主要语句：</p><p>Insert语句：向数据表张插入一条记录。</p><p>Delete语句：删除数据表中的一条或多条记录，也可以删除数据表中的所有记录，但是，它的操作对象仍是记录。</p><p>Update语句：用于修改已存在表中的记录的内容。</p><h2 id="数据导入-导出-清除"><a href="#数据导入-导出-清除" class="headerlink" title="数据导入/导出/清除"></a>数据导入/导出/清除</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>) </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- Hive Shell命令导出</span></span><br><span class="line">hive -e 'select * from default.student;' &gt;/opt/module/datas/export/student4.txt;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 清除表中数据（Truncate）</span></span><br><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><h3 id="drop、truncate和delete的区别"><a href="#drop、truncate和delete的区别" class="headerlink" title="drop、truncate和delete的区别"></a>drop、truncate和delete的区别</h3><ol><li>TRUNCATE和DELETE只删除数据， DROP则删除整个表（结构和数据）。</li><li><p>表和索引所占空间。当表被TRUNCATE 后，这个表和索引所占用的空间会恢复到初始大小。DELETE操作不会减少表或索引所占用的空间。 drop语句将表所占用的空间全释放掉。</p></li><li><p>delete语句为DML（data maintain Language),这个操作会被放到 rollback segment中,事务提交后才生效。如果有相应的 tigger,执行的时候将被触发。</p><p>truncate、drop是DLL（data define language),操作立即生效，原数据不放到 rollback segment中，不能回滚</p></li><li><p>在没有备份情况下，谨慎使用 drop 与 truncate。要删除部分数据行采用delete且注意结合where来约束影响范围。要删除表用drop。若想保留表而将表中数据删除，用truncate即可实现。</p></li><li><p>Truncate table 表名 速度快,而且效率高,因为: truncate table 在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 </p></li></ol><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p>SQL 语言<strong>大小写不敏感</strong></p><h2 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h2><div class="table-container"><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&gt;=B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>A IS NOT NULL</td><td>所有数据类型</td><td>如果A不等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用 IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table></div><h2 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h2><p>% 代表零个或多个字符(任意个字符)；_ 代表一个字符</p><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><h2 id="Having-与-where-区别"><a href="#Having-与-where-区别" class="headerlink" title="Having 与 where 区别"></a>Having 与 where 区别</h2><p>（1）where针对<strong>表</strong>中的列发挥作用，查询数据；having针对<strong>查询结果</strong>中的列发挥作用，筛选数据。</p><p>（2）where后面不能写分组函数，而having后面可以使用分组函数。</p><p>（3）having只用于group by分组统计语句。</p><h2 id="Join语句"><a href="#Join语句" class="headerlink" title="Join语句"></a>Join语句</h2><p>Hive只支持等值连接，<strong>不支持非等值连接</strong></p><ol><li><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。    </p></li><li><p>左（右）外连接：JOIN操作符左（右）边表中符合WHERE子句的所有记录将会被返回。</p></li><li><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。  </p></li></ol><h3 id="join大表怎么优化"><a href="#join大表怎么优化" class="headerlink" title="join大表怎么优化"></a>join大表怎么优化</h3><p>Hive的Join连接总是按照<strong>从前到后</strong>的顺序执行的</p><p><img src="/2019/10/17/hive/Users\徐佳未\AppData\Roaming\Typora\typora-user-images\1573652534350.png" alt="1573652534350"></p><p>将小表放在左边，大表放到join的右边。这样可以提高性能。更准确的说法：把重复关联键少的表放在join前面，做关联可以提高join的效率。写在关联左侧的表每有1条重复的关联键时底层就会多1次运算处理。</p><h1 id="分区与分桶"><a href="#分区与分桶" class="headerlink" title="分区与分桶"></a>分区与分桶</h1><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>按分区键的列值存储在表目录的子目录中，针对的是数据的存储路径，提供一个隔离数据和优化查询的便利方式</p><p>好处：<strong>可以更快地执行查询</strong><br>使用分区列的名称创建一个子目录，当使用where子句进行查询时，<strong>只扫描特定子目录，而不是扫描整个表</strong></p><h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><p>将表中记录按分桶键的哈希值分散进多个文件中，针对的是<strong>数据文件</strong>，将数据集分解成更容易管理的若干部分</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span>(<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置参数（否则导入数据后不分桶）</span></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 使用分桶抽样查询（对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，<strong>决定抽样的比例</strong>。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>x表示<strong>从哪个bucket开始抽取</strong>，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>注意：x的值必须小于等于y的值，否则</p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><p>CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。（COLLECT_LIST类似，不去重）</p><p><strong>e.g. </strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">','</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th>base</th><th>name</th></tr></thead><tbody><tr><td>射手座,A</td><td>大海,凤姐</td></tr><tr><td>白羊座,A</td><td>孙悟空,猪八戒</td></tr><tr><td>白羊座,B</td><td>宋宋</td></tr></tbody></table></div><h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p><strong>e.g.</strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie  to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><p>结果：<br>| movie          | category                 |<br>| ——————— | ———————————— |<br>|《疑犯追踪》  |   悬疑|<br>|《疑犯追踪》  |   动作|<br>|《疑犯追踪》  |   科幻|<br>|《疑犯追踪》  |   剧情|<br>|《Lie to me》 | 悬疑  |<br>|《Lie to me》 | 警匪  |<br>|《Lie to me》 | 动作  |<br>|《Lie to me》 | 心理  |<br>|《Lie to me》 | 剧情  |<br>|《战狼2》     |  战争 |<br>|《战狼2》     |  动作 |<br>|《战狼2》     |  灾难 |</p><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p><p>CURRENT ROW：当前行</p><p>n PRECEDING：往前n行数据</p><p>n FOLLOWING：往后n行数据</p><p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>LAG(col,n)：往前第n行数据</p><p>LEAD(col,n)：往后第n行数据</p><p>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line"><span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看顾客上次购买时间</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, </span><br><span class="line">lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1, lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询前20%时间的订单信息</span></span><br><span class="line"><span class="keyword">select</span> * </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span> <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>, ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="rank函数"><a href="#rank函数" class="headerlink" title="rank函数"></a>rank函数</h2><p>RANK() 排序相同时会重复，总数不会变</p><p>DENSE_RANK() 排序相同时会重复，总数会减少</p><p>ROW_NUMBER() 会根据顺序计算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">score;</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>name</th><th>subject</th><th>score</th><th>rank</th><th>dense_rank</th><th>row_number</th></tr></thead><tbody><tr><td>宋宋</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>1</strong></td></tr><tr><td>大海</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>2</strong></td></tr><tr><td>婷婷</td><td>英语</td><td>78</td><td><strong>3</strong></td><td><strong>2</strong></td><td><strong>3</strong></td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td><td><strong>4</strong></td><td><strong>3</strong></td><td><strong>4</strong></td></tr></tbody></table></div><h1 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h1><h2 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h2><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=<span class="keyword">none</span>;</span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>有时Hive的输入数据量是非常小的。在这种情况下，<strong>为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多</strong>。对于大多数这种情况，<strong>Hive可以通过本地模式在单台机器上处理所有的任务</strong>。对于小数据集，执行时间可以明显被缩短。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启本地mr</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h2><h3 id="小表-大表join"><a href="#小表-大表join" class="headerlink" title="小表\大表join"></a>小表\大表join</h3><p><strong>将key相对分散，并且数据量小的表放在join的左边</strong>，这样可以有效减少内存溢出错误发生的几率；</p><p>再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p><strong>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</strong></p><h3 id="大表join小表"><a href="#大表join小表" class="headerlink" title="大表join小表"></a>大表join小表</h3><h4 id="空KEY过滤"><a href="#空KEY过滤" class="headerlink" title="空KEY过滤"></a>空KEY过滤</h4><p>有时join超时是因为<strong>某些key对应的数据太多</strong>，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。</p><p>例如key对应的字段为空  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> nullidtable <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> ) n  <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id; <span class="comment">-- 更快</span></span><br></pre></td></tr></table></figure><h4 id="空KEY转换"><a href="#空KEY转换" class="headerlink" title="空KEY转换"></a>空KEY转换</h4><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以<strong>表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上</strong>。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure></p><h3 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。<strong>可以用MapJoin把小表全部加载到内存在map端进行join</strong>，避免reducer处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置自动选择Mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 大表小表的阈值设置（默认25M一下认为是小表）</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure><h3 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>并不是所有的聚合操作都需要在Reduce端完成，<strong>很多聚合操作都可以先在Map端进行部分聚合</strong>，最后在Reduce端得出最终结果。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 是否在Map端进行聚合，默认为True</span></span><br><span class="line">hive.map.aggr = true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在Map端进行聚合操作的条目数目</span></span><br><span class="line">hive.groupby.mapaggr.checkinterval = 100000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 有数据倾斜的时候进行负载均衡（默认是false）</span></span><br><span class="line">hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure></p><p>当选项设定为 true，生成的查询计划会有两个MR Job。</p><p>第一个MR Job中，<strong>Map的输出结果会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的Reduce中</strong>，从而达到负载均衡的目的；</p><p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证<strong>相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。  </p><h3 id="Count-Distinct-去重统计"><a href="#Count-Distinct-去重统计" class="headerlink" title="Count(Distinct) 去重统计"></a>Count(Distinct) 去重统计</h3><p>数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用<strong>先GROUP BY再COUNT</strong>的方式替换<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 原始</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 改进</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span><br></pre></td></tr></table></figure></p><p>虽然会多用一个Job来完成，但在数据量大的情况下，绝对是值得的。</p><h3 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h3><p>对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启动态分区功能（默认true，开启）</span></span><br><span class="line">hive.exec.dynamic.partition=true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</span></span><br><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在所有执行MR的节点上，最大一共可以创建多少个动态分区。</span></span><br><span class="line">hive.exec.max.dynamic.partitions=1000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</span></span><br><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure></p><h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><p>数据倾斜：由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p><p>主要表现：任务进度长时间维持在99%的附近，只有少量reduce子任务未完成，因为其处理的数据量和其他的 reduce 差异过大。 </p><p>数据倾斜的原因：</p><p>key 分布不均匀/业务数据本身的特性（小表join大表）/建表考虑不周全/某些 HQL 语句本身就存在数据倾斜（count(distinct)，group by不和聚集函数搭配使用的时候）</p><p><strong>目的：使map的输出数据更均匀的分布到reduce中去</strong></p><p><strong>在hive中产生数据倾斜的原因和解决方法：</strong></p><p>1）group by, 使用Hive对数据做一些类型统计的时候遇到过<strong>某种类型的数据量特别多，而其他类型数据的数据量特别少</strong>。当按照类型进行group by的时候，会将相同的group by字段的reduce任务<strong>需要的数据拉取到同一个节点进行聚合</strong>，而当其中每一组的数据量过大时，会出现其他组的计算已经完成而这里还没计算完成，其他节点的一直等待这个节点的任务执行完成，所以会看到一直map 100% reduce 99%的情况。</p><p>解决方法：设置参数set hive.map.aggr=true; set hive.groupby.skewindata=true;</p><p>原理：hive.map.aggr=true 这个配置项代表<strong>是否在map端进行聚合</strong></p><p>　　　hive.groupby.skwindata=true 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，<strong>Map 的输出结果集合会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的</strong>；第二个 MR Job 再根据预处理的数据结果按照 Group By Key分布到 Reduce 中（这个过程可以<strong>保证相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。</p><p><strong>2）map和reduce优化。</strong></p><p>1.当出现小文件过多，需要合并小文件。可以通过set hive.merge.mapfiles=true来解决。</p><p>2.单个文件大小稍稍大于配置的block块的大写，此时需要适当增加map的个数。解决方法：set mapred.map.tasks个数</p><p>3.文件大小适中，但map端计算量非常大，如select id,count(*),sum(case when…),sum(case when…)…需要增加map个数。解决方法：set mapred.map.tasks个数，set mapred.reduce.tasks个数</p><p>3）当HiveQL中包含<strong>count（distinct）</strong>时，如果数据量非常大，执行如select a, count(distinct b) from t group by a;类型的SQL时，会出现数据倾斜的问题。</p><p>解决方法：使用sum…group by代替。如select a, sum(1) from (select a, b from t group by a, b) group by a;</p><p><strong>4）当遇到一个大表和一个小表进行join操作时。</strong></p><p>解决方法：使用<strong>MapJoin</strong> 将小表加载到内存中（在Map阶段进行表之间的连接。而不需要进入到Reduce阶段才进行连接。这样就节省了在Shuffle阶段时要进行的大量数据传输。从而起到了优化作业的作用）。set hive.auto.convert.join=true;</p><p>5）<strong>遇到需要进行join的但是关联字段有数据为空</strong></p><p>解决方法1：id为空的不参与关联</p><p>比如：</p><p>select <em> from log a join users b on <em>*a.id is not null</em></em> and a.id = b.id </p><p>union all </p><p>select <em> from log a <em>*where a.id is null;</em></em> </p><p>解决方法2：给空值分配随机的key值，其核心是将这些引起倾斜的值随机分发到Reduce</p><p>如：</p><p>select * from log a left outer join users b </p><p>on </p><p>case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;</p><p>现在不需要改代码，hive提供参数可以设置。</p><p>从HDFS读取数据，初始RDD的分区数根据原文件的split个数 </p><h3 id="合理设置Map数"><a href="#合理设置Map数" class="headerlink" title="合理设置Map数"></a>合理设置Map数</h3><p>如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。因此需要<strong>减少map数</strong>。</p><p>比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。因此<strong>增加map数</strong>。</p><p>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</p><p>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</p><h3 id="合理设置Reduce数"><a href="#合理设置Reduce数" class="headerlink" title="合理设置Reduce数"></a>合理设置Reduce数</h3><p>1．调整reduce个数方法一</p><p>（1）每个Reduce处理的数据量默认是256MB</p><p>hive.exec.reducers.bytes.per.reducer=256000000</p><p>（2）每个任务最大的reduce数，默认为1009</p><p>hive.exec.reducers.max=1009</p><p>（3）计算reducer数的公式</p><p>N=min(参数2，总输入数据量/参数1)</p><p>2．调整reduce个数方法二</p><p>在hadoop的mapred-default.xml文件中修改</p><p>设置每个job的Reduce个数</p><p>set mapreduce.job.reduces = 15;</p><p>3．reduce个数并不是越多越好</p><p>1）过多的启动和初始化reduce也会消耗时间和资源；</p><p>2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</p><h3 id="小文件进行合并"><a href="#小文件进行合并" class="headerlink" title="小文件进行合并"></a>小文件进行合并</h3><p>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。</p><p>不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。在系统资源比较空闲的时候才有优势。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;       <span class="comment">--打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>; <span class="comment">--同一个sql允许最大并行度，默认为8。</span></span><br></pre></td></tr></table></figure><h3 id="严格模式"><a href="#严格模式" class="headerlink" title="严格模式"></a>严格模式</h3><p>防止用户执行那些可能意想不到的不好的影响的查询。</p><p>开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。</p><p>1)    <strong>对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。</strong>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p><p>2)    <strong>对于使用了order by语句的查询，要求必须使用limit语句。</strong>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</p><p>3)    <strong>限制笛卡尔积的查询。</strong>对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p><h1 id="union和union-all有何不同？"><a href="#union和union-all有何不同？" class="headerlink" title="union和union all有何不同？"></a>union和union all有何不同？</h1><p>1、对<strong>重复</strong>结果的处理：UNION在进行表链接后会筛选掉重复的记录，Union All不会去除重复记录。</p><p>2、对<strong>排序</strong>的处理：Union将会按照字段的顺序进行排序；UNION ALL只是简单的将两个结果合并后就返回。</p><p>3、从<strong>效率</strong>上说，UNION ALL 要比UNION快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用UNION ALL。 </p><h2 id="使用union要满足什么条件？"><a href="#使用union要满足什么条件？" class="headerlink" title="使用union要满足什么条件？"></a>使用union要满足什么条件？</h2><p>需要保证select中字段须一致，每个select语句返回的列的数量和名字必须一样</p><h1 id="count-count-1-和count-字段-的区别"><a href="#count-count-1-和count-字段-的区别" class="headerlink" title="count(*), count(1)和count(字段)的区别"></a>count(*), count(1)和count(字段)的区别</h1><p><strong>count(1)和count(*)</strong></p><p>都会对全表进行扫描，统计所有记录的条数，包括那些为null的记录</p><p>count(1)会比count(*)更快</p><p><strong>count(1) and count(字段)</strong></p><p>两者的主要区别是</p><p>（1）count(1) 会统计表中的所有的记录数，包含字段为null 的记录</p><p>（2）count(字段) 会统计该字段在表中出现的次数，不统计字段为null 的记录。</p><p>总结</p><p>此处设定count（a），其中a为变量，可以为各种值，下面根据a的不同值，得出不同的count(a)的结果</p><p>1）当a = null时，count(a)的值为0；</p><p>2）当a != null 且不是表的列名的时候，count(a)为该表的行数；</p><p>3）当a是表的列名时，count(a)为该表中a列的值不等于null的行的总数，它和2)中的差值就是该表中a列值为null的行数</p>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
