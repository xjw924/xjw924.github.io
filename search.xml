<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>类别型变量编码</title>
      <link href="/2021/01/17/bian-liang-bian-ma/"/>
      <url>/2021/01/17/bian-liang-bian-ma/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考笔记：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/248763414" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/248763414</a></li></ul></blockquote><h1 id="独热编码（One-hot-Encoding）"><a href="#独热编码（One-hot-Encoding）" class="headerlink" title="独热编码（One-hot Encoding）"></a>独热编码（One-hot Encoding）</h1><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>通常用于处理类别间<strong>不具有大小关系</strong>的特征。</p><p>one-hot编码将分类变量转换为几个二进制列，其中1代表某个输入属于该类别。</p><p><img src="https://pic4.zhimg.com/80/v2-c7c17cf095c8ae10232d7b22de5bdd9b_1440w.jpg" alt="img"></p><p>对于类别取值较多的情况下使用独热编码需要注意以下问题：</p><ol><li>使用稀疏向量来节省空间。在one-hot下特征向量只有某一维取值为1，其他位置均为0。因此可以利用向量的稀疏表示有效地节省空间，目前大部分算法均接受稀疏向量形式的输入。</li><li>配合特征选择来降低维度。高维特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在LR中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助。</li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li>one-hot 编码为数据集增加大量维度。例如，如果用一个序列来表示美国的各个州，那么 one-hot 编码会带来 50 多个维度。</li><li>one-hot 编码增加的大量维度实际上并没有太多信息，很多时候 1 散落在众多零之中，即有用的信息零散地分布在大量数据中。这会<strong>导致结果异常稀疏，使其难以进行优化</strong>。</li><li>每个信息稀疏列之间都具有<strong>线性关系</strong>。这意味着一个变量可以很容易地使用其他变量进行预测，导致高维度中出现并行性和多重共线性的问题。如果只有三、四个类，那么 one-hot 编码可能不是一个糟糕的选择。但是随着类别的增加，可能还有其他更合适的方案值得探索。</li></ol><p><img src="https://pic4.zhimg.com/80/v2-948cfefbe19db39027ec3ac60b26cc17_1440w.jpg" alt="img"></p><blockquote><p>小结：如果变量的类别数目不太多，可优先考虑。</p></blockquote><h2 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h2><h3 id="使用pandas中的get-dummies包"><a href="#使用pandas中的get-dummies包" class="headerlink" title="使用pandas中的get_dummies包"></a>使用pandas中的get_dummies包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化DataFrame</span></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">'Pet'</span>: [<span class="string">'Cat'</span>, <span class="string">'Dog'</span>, <span class="string">'Turtle'</span>, <span class="string">'Fish'</span>, <span class="string">'Cat'</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过pandas中的get_dummies生成</span></span><br><span class="line">dummies = pd.get_dummies(df[<span class="string">'Pet'</span>])</span><br><span class="line">df_new = pd.concat([df,dummies],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看结果</span></span><br><span class="line">df_new</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:right">Pet</th><th style="text-align:right">Cat</th><th style="text-align:right">Dog</th><th style="text-align:right">Fish</th><th style="text-align:right">Turtle</th></tr></thead><tbody><tr><td style="text-align:right">0</td><td style="text-align:right">Cat</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">1</td><td style="text-align:right">Dog</td><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">2</td><td style="text-align:right">Turtle</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">1</td></tr><tr><td style="text-align:right">3</td><td style="text-align:right">Fish</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">4</td><td style="text-align:right">Cat</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">0</td></tr></tbody></table></div><h3 id="pandas-get-dummies参数"><a href="#pandas-get-dummies参数" class="headerlink" title="pandas.get_dummies参数"></a>pandas.get_dummies参数</h3><p><code>pandas.get_dummies</code>(<strong>data</strong>, <em>prefix=None**</em>,<strong> <em>prefix_sep=’_’</em></strong>,<strong> <em>dummy_na=False</em></strong>,<strong> <em>columns=None</em></strong>,<strong> <em>sparse=False</em></strong>,<strong> <em>drop_first=False</em></strong>,<strong> <em>dtype=None</em></strong>)**<a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html#pandas.get_dummies" target="_blank" rel="noopener">[source]</a></p><ul><li><p>data : array-like, Series, or DataFrame，输入的数据。</p></li><li><p>prefix : string, list of strings, or dict of strings, default None.<br>get_dummies转换后，列名的前缀。默认使用原变量名作为前缀。</p></li></ul><p><strong>注意：Series里的整数会按照one-hot进行编码，但是在DataFrame里面不会</strong>（如下例中的C列）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>], <span class="string">'B'</span>: [<span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">pd.get_dummies(df)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th></th><th>C</th><th>A_a</th><th>A_b</th><th>B_a</th><th>B_b</th><th>B_c</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>1</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>3</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>], <span class="string">'B'</span>: [<span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">pd.get_dummies(df, prefix=[<span class="string">'col1'</span>, <span class="string">'col2'</span>])</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th></th><th>C</th><th>col1_a</th><th>col1_b</th><th>col2_a</th><th>col2_b</th><th>col2_c</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>1</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>2</td><td>3</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s = pd.Series([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) <span class="comment"># 对比Series中的整数列</span></span><br><span class="line">pd.get_dummies(s)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th></th><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td></tr><tr><td>2</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div><ul><li>prefix_sep：str, default ‘_’，前缀与取值之间的分割符号。也可以使用字典形式对每个变量进行指定。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>], <span class="string">'B'</span>: [<span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">pd.get_dummies(df, prefix_sep=&#123;<span class="string">'A'</span>:<span class="string">'_'</span>,<span class="string">'B'</span>:<span class="string">'/'</span>&#125;)</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:right">C</th><th style="text-align:right">A_a</th><th style="text-align:right">A_b</th><th style="text-align:right">B/a</th><th style="text-align:right">B/b</th><th>B/c</th></tr></thead><tbody><tr><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">1</td><td>0</td></tr><tr><td style="text-align:right">1</td><td style="text-align:right">2</td><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td>0</td></tr><tr><td style="text-align:right">2</td><td style="text-align:right">3</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td>1</td></tr></tbody></table></div><ul><li><p>columns : list-like, default None<br>用列表指定需要实现类别转换的列名，限定对哪些列进行转换，不写默认全部。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>], <span class="string">'B'</span>: [<span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">pd.get_dummies(df, columns=[<span class="string">'A'</span>]) <span class="comment"># 仅对A列进行one-hot转换</span></span><br></pre></td></tr></table></figure></li></ul><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:right">B</th><th style="text-align:right">C</th><th style="text-align:right">A_a</th><th style="text-align:right">A_b</th></tr></thead><tbody><tr><td style="text-align:right">0</td><td style="text-align:right">b</td><td style="text-align:right">1</td><td style="text-align:right">1</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">1</td><td style="text-align:right">a</td><td style="text-align:right">2</td><td style="text-align:right">0</td><td style="text-align:right">1</td></tr><tr><td style="text-align:right">2</td><td style="text-align:right">c</td><td style="text-align:right">3</td><td style="text-align:right">1</td><td style="text-align:right">0</td></tr></tbody></table></div><ul><li><p>dummy_na : bool, default False<br>增加一列表示缺失值，如果False就忽略缺失值。</p>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">s1 = [<span class="string">'a'</span>, <span class="string">'b'</span>, np.nan]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.get_dummies(s1)</span><br><span class="line">     a  b</span><br><span class="line">  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">1</span></span><br><span class="line">  <span class="number">2</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>pd.get_dummies(s1, dummy_na=<span class="literal">True</span>)</span><br><span class="line">     a  b  NaN</span><br><span class="line">  <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span>    <span class="number">0</span></span><br><span class="line">  <span class="number">1</span>  <span class="number">0</span>  <span class="number">1</span>    <span class="number">0</span></span><br><span class="line">  <span class="number">2</span>  <span class="number">0</span>  <span class="number">0</span>    <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>sparse：bool, default False.</p><p> 表示虚拟列是否是稀疏的，默认为False。</p><blockquote><p>相关问题：<a href="https://zhuanlan.zhihu.com/p/55160639" target="_blank" rel="noopener">使用sklearn处理高维稀疏逻辑回归问题</a></p></blockquote></li><li><p>drop_first : bool, default False<br>获得k中的k-1个类别值，去除第一个。相当于虚拟编码！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(&#123;<span class="string">'A'</span>: [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'a'</span>], <span class="string">'B'</span>: [<span class="string">'b'</span>, <span class="string">'a'</span>, <span class="string">'c'</span>], <span class="string">'C'</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]&#125;)</span><br><span class="line">pd.get_dummies(df, drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ul><div class="table-container"><table><thead><tr><th style="text-align:right"></th><th style="text-align:right">C</th><th style="text-align:right">A_b</th><th style="text-align:right">B_b</th><th style="text-align:right">B_c</th></tr></thead><tbody><tr><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">1</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">1</td><td style="text-align:right">2</td><td style="text-align:right">1</td><td style="text-align:right">0</td><td style="text-align:right">0</td></tr><tr><td style="text-align:right">2</td><td style="text-align:right">3</td><td style="text-align:right">0</td><td style="text-align:right">0</td><td style="text-align:right">1</td></tr></tbody></table></div><ul><li><p>dtype: Data type for new columns. Only a single dtype is allowed.</p><p> 指定生成虚拟列的数据类型，只能指定一种类型。</p></li></ul><h3 id="使用OneHotEncoder"><a href="#使用OneHotEncoder" class="headerlink" title="使用OneHotEncoder"></a>使用OneHotEncoder</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># one-hot 先序列化，然后再做独热编码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder, LabelEncoder</span><br><span class="line"></span><br><span class="line">store = pd.DataFrame(&#123;<span class="string">'gender'</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="string">'unknown'</span>]&#125;)</span><br><span class="line">arr=np.array(store[[<span class="string">'gender'</span>]].astype(str)).ravel() <span class="comment"># 获取所有取值</span></span><br><span class="line">lenc_code=LabelEncoder().fit_transform(arr) <span class="comment"># 对所有取值进行编码</span></span><br><span class="line">oenc_code = OneHotEncoder(sparse=<span class="literal">False</span>).fit_transform(lenc_code.reshape(<span class="number">-1</span>, <span class="number">1</span>))  <span class="comment"># 独热编码</span></span><br><span class="line">add_col=list(map(<span class="keyword">lambda</span> x:<span class="string">'gender_&#123;&#125;'</span>.format(x),np.unique(arr))) <span class="comment"># 对新列进行命名</span></span><br><span class="line">df_onehot=pd.concat([store,pd.DataFrame(oenc_code,columns=add_col)],axis=<span class="number">1</span>) <span class="comment"># 拼接到原始DataFrame</span></span><br><span class="line">df_onehot</span><br></pre></td></tr></table></figure><h1 id="虚拟编码（Dummy-Coding）"><a href="#虚拟编码（Dummy-Coding）" class="headerlink" title="虚拟编码（Dummy Coding）"></a>虚拟编码（Dummy Coding）</h1><p>和独热编码类似，但是k个类别只需要k-1个虚拟变量。比如一个特征的取值是高，中和低，那么只需要两位编码，比如只编码中和低，如果是（1,0）则是中，（0,1）则是低。（0,0）则是高。目前虚拟编码使用的没有独热编码广，通常还是使用独热编码。</p><p>实现方式： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(df, drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="序号编码（Ordinal-Encoding-Label-Encoding）"><a href="#序号编码（Ordinal-Encoding-Label-Encoding）" class="headerlink" title="序号编码（Ordinal Encoding/Label Encoding）"></a>序号编码（Ordinal Encoding/Label Encoding）</h1><p>序号编码通常用于处理类别间<strong>具有大小关系</strong>的数据。</p><p>例如成绩可以分为低、中、高三档，并且存在“高＞ 中＞ 低”的排序关系。序号编码会按照大小关系对类别型特征赋值，例如高表示为3、中表示为2、低表示为1，<strong>转换后依然保留了大小关系</strong>。</p><p>例如学历变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">edu_dict = &#123;<span class="string">"小学"</span>: <span class="number">1</span>, <span class="string">"初中"</span>: <span class="number">2</span>, <span class="string">"高中"</span>: <span class="number">3</span>, <span class="string">"大学"</span>: <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">df[<span class="string">'Edu'</span>].replace(edu_dict, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——9.Decision Tree</title>
      <link href="/2020/05/01/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-9.decision-tree/"/>
      <url>/2020/05/01/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-9.decision-tree/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-9-Decision-Tree"><a href="#Lecture-9-Decision-Tree" class="headerlink" title="Lecture 9: Decision Tree"></a>Lecture 9: Decision Tree</h1><p>上节课我们主要介绍了Adaptive Boosting。AdaBoost演算法通过调整每笔资料的权重，得到不同的hypotheses，然后将不同的hypothesis乘以不同的系数$\alpha$进行线性组合。这种演算法的优点是，即使底层的演算法$g$不是特别好（只要比乱选好点），经过多次迭代后算法模型会越来越好，起到了boost提升的效果。</p><p>本节课将在此基础上介绍一种新的aggregation算法：决策树（Decision Tree）。</p><h2 id="1-Decision-Tree-Hypothesis"><a href="#1-Decision-Tree-Hypothesis" class="headerlink" title="1. Decision Tree Hypothesis"></a>1. Decision Tree Hypothesis</h2><p>aggregation的核心就是将许多可供选择使用的比较好的hypothesis融合起来，利用集体的智慧组合成$G$，使其得到更好的机器学习预测模型。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/1-15.png" alt="img"></p><p>aggregation type有三种：uniform，non-uniform（线性组合），conditional（非线性组合）。它有两种情况：</p><ol><li>所有的$g$是已知的，即blending。对应的三种类型分别是voting/averaging，linear和stacking。</li><li>所有的$g$是未知的，只能通过手上的资料重构$g$，即learning。其中uniform和non-uniform分别对应的是Bagging和AdaBoost算法，而conditional对应的就是我们本节课将要介绍的Decision Tree算法。</li></ol><p>决策树（Decision Tree）模型是一种传统的算法，它的处理方式与人类思维十分相似。例如下面这个例子，对下班时间、约会情况、提交截止时间这些条件进行判断，从而决定是否要进行在线课程测试。如下图所示，整个流程类似一个树状结构。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/2-14.png" alt="img"></p><p>图中每个条件和选择都决定了最终的结果，Y or N。蓝色的圆圈表示树的叶子，即最终的决定。</p><p>把这种树状结构对应到一个hypothesis $G(x)$中，$G(x)$的表达式为：</p><script type="math/tex; mode=display">G(x)=\sum_{t=1}^Tq_t(x)\cdot g_t(x)</script><p>$G(x)$由许多$g_t(x)$组成，即aggregation的做法。每个$g_t(x)$就代表上图中的蓝色圆圈（树的叶子）。这里的$g_t(x)$是常数，因为是处理简单的classification问题。我们把这些$g_t(x)$称为base hypothesis。</p><p>$q_t(x)$表示每个$g_t(x)$成立的条件，代表上图中橘色箭头的部分。不同的$g_t(x)$对应于不同的$q_t(x)$，即从树的根部到顶端叶子的路径不同。图中中的菱形代表每个简单的节点。所以，这些base hypothesis和conditions就构成了整个$G(x)$的形式，就像一棵树一样，从根部到顶端所有的叶子都安全映射到上述公式上去了。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/3-12.png" alt="img"></p><p>决策树实际上就是在模仿人类做决策的过程。一直以来，决策树的应用十分广泛而且分类预测效果都很不错，而它在数学上的理论完备性不充分。</p><p>如果从另外一个方面来看决策树的形式，不同于上述$G(x)$的公式，我们可以利用条件分支的思想，将整体$G(x)$分成若干个$G_c(x)$，也就是<strong>把整个大树分成若干个小树</strong>，如下所示：</p><script type="math/tex; mode=display">G(x)=\sum_{c=1}^C[b(x)=c]\cdot G_c(x)</script><p>上式中，$G(x)$表示完整的大树，即full-tree hypothesis，$b(x)$表示每个分支条件，即branching criteria，$G_c(x)$表示第$c$个分支下的子树，即sub-tree。这种结构被称为<strong>递归型的数据结构</strong>，即将大树分割成不同的小树，再将小树继续分割成更小的子树。所以，决策树可以分为两部分：root和sub-trees。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/4-12.png" alt="img"></p><p>在详细推导决策树算法之前，我们先来看一看它的优点和缺点。首先，decision tree的优点有：</p><ol><li>模型直观，便于理解，应用广泛</li><li>算法简单，容易实现</li><li>训练和预测时，效率较高</li></ol><p>然而，decision tree也有相应的缺点：</p><ol><li>缺少足够的理论支持</li><li>如何选择合适的树结构对初学者来说比较困惑</li><li>没有单一的代表决策树的演算法</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/5-12.png" alt="img"></p><h2 id="2-Decision-Tree-Algorithm"><a href="#2-Decision-Tree-Algorithm" class="headerlink" title="2. Decision Tree Algorithm"></a>2. Decision Tree Algorithm</h2><p>我们可以用递归形式将decision tree表示出来，它的基本的算法可以写成：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/6-12.png" alt="img"></p><p>这个Basic Decision Tree Algorithm的流程可以分成四个部分：</p><ol><li>首先设定划分不同分支的标准和条件是什么；</li><li>接着将整体数据集$D$根据分支个数$C$和条件，划为不同分支下的子集$D_c$；</li><li>然后对每个分支下的$D_c$进行训练，得到相应的机器学习模型$G_c$；</li><li>最后将所有分支下的$G_c$合并到一起，组成大矩$G(x)$。</li></ol><p>但值得注意的是，这种递归的形式需要终止条件，否则程序将一直进行下去。当满足递归的终止条件之后，将会返回基本的hypothesis $g_t(x)$。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/7-13.png" alt="img"></p><p>所以，决策树的基本演算法包含了四个选择：</p><ol><li>分支个数（number of branches）</li><li>分支条件（branching criteria）</li><li>终止条件（termination criteria）</li><li>基本算法（base hypothesis）</li></ol><p>下面我们来介绍一种常用的决策树模型算法，叫做<strong>Classification and Regression Tree(C&amp;RT)</strong>。</p><p>C&amp;RT算法有两个简单的设定：</p><ol><li>分支的个数$C=2$，即<strong>二叉树（binary tree）</strong>的数据结构；也就是每次在一个维度上，只对一个特征feature将数据一分为二，左子树和右子树，分别代表不同的类别。</li><li>每个分支最后的$g_t(x)$（数的叶子）是一个<strong>常数</strong>。按照最小化$E_{in}$的目标，对于binary/multiclass classification(0/1 error)问题，看正类和负类哪个更多，$g_t(x)$取所占比例最多的那一类$y_n$；对于regression(squared error)问题，$g_t(x)$则取所有$y_n$的平均值。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/8-13.png" alt="img"></p><p>怎么切割才能让数据划分得最好呢（error最小）？C&amp;RT中使用<strong>纯净度purifying</strong>这个概念来选择最好的decision stump。purifying的核心思想就是每次切割都尽可能让左子树和右子树中同类样本占得比例最大或者$y_n$都很接近（regression），即错误率最小。比如说classifiacation问题中，如果左子树全是正样本，右子树全是负样本，那么它的纯净度就很大，说明该分支效果很好。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/9-13.png" alt="img"></p><p>根据C&amp;RT中purifying的思想，我们得到选择合适的分支条件$b(x)$的表达式如上所示。</p><p>最好的decision stump重点包含两个方面：</p><ol><li>purifying越大越好，而这里使用purifying相反的概念impurity，则impurity越小越好；</li><li>左右分支纯净度所占的权重，权重大小由该分支的数据量决定，分支包含的样本个数越多，则所占权重越大，分支包含的样本个数越少，则所占权重越小。上式中的$|D_c\ with\ h|$代表了分支$c$所占的权重。</li></ol><p>这里$b(x)$类似于error function（这也是为什么使用impurity代替purifying的原因），选择最好的decision stump，让所有分支的不纯度最小化，使$b(x)$越小越好。</p><p>不纯度Impurity如何用函数的形式量化？一种简单的方法就是类比于$E_{in}$，看预测值与真实值的误差是多少。</p><ol><li><p>对于regression问题，它的impurity可表示为：</p><script type="math/tex; mode=display">impurity(D)=\frac1N\sum_{n=1}^N(y_n-\overline{y})^2</script><p>其中，$\overline{y}$表示对应分支下所有$y_n$的均值。</p></li><li><p>对应classification问题，它的impurity可表示为：</p><script type="math/tex; mode=display">impurity(D)=\frac1N\sum_{n=1}^N[y_n\neq y^*]</script><p>其中，$y^*$表示对应分支下所占比例最大的那一类。</p></li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/10-13.png" alt="img"></p><p>以上这些impurity是基于原来的regression error和classification error直接推导的。进一步来看classification的impurity functions，如果某分支条件下，让其中一个分支纯度最大，那么就选择对应的decision stump，即得到的classification error为：</p><script type="math/tex; mode=display">1-\max_{1\leq k\leq K}\frac{\sum_{n=1}^N[y_n=k]}{N}</script><p>其中，$K$为分支个数。</p><p>上面这个式子<strong>只考虑纯度最大的那个分支</strong>，更好的做法是将所有分支的纯度都考虑并计算在内，用<strong>基尼指数（Gini index）</strong>表示：</p><script type="math/tex; mode=display">1-\sum_{k=1}^K(\frac{\sum_{n=1}^N[y_n=k]}{N})^2</script><p>Gini index的优点是<strong>将所有的class在数据集中的分布状况和所占比例全都考虑了</strong>，这样让decision stump的选择更加准确。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/11-12.png" alt="img"></p><p>对于决策树C&amp;RT算法，通常来说，上面介绍的各种impurity functions中，Gini index更适合求解classification问题，而regression error更适合求解regression问题。</p><p>C&amp;RT算法迭代终止条件有两种情况：</p><ol><li>当前各个分支下包含的所有样本y_n都是同类的，即不纯度impurity为0，表示该分支已经达到了最佳分类程度。</li><li>该特征下所有的$x_n$相同，无法对其进行区分，表示没有decision stumps。遇到这两种情况，C&amp;RT算法就会停止迭代。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/12-12.png" alt="img"></p><p>所以，C&amp;RT算法遇到迭代终止条件后就成为完全长成树（fully-grown tree）。它每次分支为二，是二叉树结构，采用purify来选择最佳的decision stump来划分，最终得到的叶子$g_t(x)$是常数。</p><h2 id="3-Decision-Tree-Heuristics-in-C-amp-RT"><a href="#3-Decision-Tree-Heuristics-in-C-amp-RT" class="headerlink" title="3. Decision Tree Heuristics in C&amp;RT"></a>3. Decision Tree Heuristics in C&amp;RT</h2><p>现在我们已经知道了C&amp;RT算法的基本流程：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/13-12.png" alt="img"></p><p>可以看到C&amp;RT算法在处理binary classification和regression问题时非常简单实用，而且，处理muti-class classification问题也十分容易。</p><p>考虑这样一个问题，有$N$个样本，如果我们每次只取一个样本点作为分支，那么在经过$N-1$次分支之后，所有的样本点都能完全分类正确。最终每片叶子上只有一个样本，有$N$片叶子，即必然能保证$E_{in}=0$。这样看似是完美的分割，但是不可避免地造成VC Dimension无限大，造成模型复杂度增加，从而出现过拟合现象。为了避免overfit，我们需要在C&amp;RT算法中引入<strong>正则化</strong>，来控制整个模型的复杂度。</p><p>考虑到避免模型过于复杂的方法是减少叶子$g_t(x)$的数量，那么可以令regularizer就为<strong>决策树中叶子的总数</strong>，记为$\Omega(G)$。正则化的目的是尽可能减少$\Omega(G)$的值。这样，regularized decision tree的形式就可以表示成：</p><script type="math/tex; mode=display">\mathop{\arg\min}_{(all\ possible\ G)}\ E_{in}(G)+\lambda\Omega(G)</script><p>我们把这种regularized decision tree称为<strong>pruned decision tree</strong>。pruned是修剪的意思，通过regularization来修剪决策树，去掉多余的叶子，更简洁化，从而达到避免过拟合的效果。</p><p>那么如何确定修剪多少叶子，修剪哪些叶子呢？假设由C&amp;RT算法得到一棵完全长成树（fully-grown tree），总共10片叶子。首先分别减去其中一片叶子，剩下9片，将这10种情况比较，取$E_{in}$最小的那个模型；然后再从9片叶子的模型中分别减去一片，剩下8片，将这9种情况比较，取$E_{in}$最小的那个模型。以此类推，继续修建叶子。这样，最终得到包含不同叶子的几种模型，将这几个使用regularized decision tree的error function来进行选择，确定包含几片叶子的模型误差最小，就选择该模型。另外，参数$\lambda$可以通过validation来确定最佳值。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/14-12.png" alt="img"></p><p>我们一直讨论决策树上的叶子（features）都是numerical features，而实际应用中，决策树的特征值可能不是数字量，而是类别（categorical features）。对于numerical features，我们直接使用decision stump进行数值切割；而对于categorical features，我们仍然可以使用decision subset，对不同类别进行“左”和“右”，即是与不是（0和1）的划分。numerical features和categorical features的具体区别如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/15-11.png" alt="img"></p><p>在决策树中预测中，还会遇到一种问题，就是当某些特征缺失的时候，没有办法进行切割和分支选择。一种常用的方法就是<strong>surrogate branch</strong>（用替代品进行切割），即寻找与该特征相似的替代feature。如何确定是相似的feature呢？做法是在决策树训练的时候，找出与该特征相似的feature，如果替代的feature与原feature切割的方式和结果是类似的，那么就表明二者是相似的，就把该替代的feature也存储下来。当预测时遇到原feature缺失的情况，就用替代feature进行分支判断和选择。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/16-11.png" alt="img"></p><h2 id="4-Decision-Tree-in-Action"><a href="#4-Decision-Tree-in-Action" class="headerlink" title="4. Decision Tree in Action"></a>4. Decision Tree in Action</h2><p>最后我们来举个例子看看C&amp;RT算法究竟是如何进行计算的。例如下图二维平面上分布着许多正负样本，我们使用C&amp;RT算法来对其进行决策树的分类。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/17-12.png" alt="img"></p><p>第一步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/18-10.png" alt="img"></p><p>第二步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/19-7.png" alt="img"></p><p>第三步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/20-6.png" alt="img"></p><p>第四步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/21-6.png" alt="img"></p><p>在进行第四步切割之后，我们发现每个分支都已经非常纯净了，没有办法继续往下切割。此时表明已经满足了迭代终止条件，这时候就可以回传base hypothesis，构成sub tree，然后每个sub tree再往上整合形成tree，最后形成我们需要的完全决策树。如果将边界添加上去，可得到下图：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/22-5.png" alt="img"></p><p>得到C&amp;RT算法的切割方式之后，我们与AdaBoost-Stump算法进行比较：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/23-3.png" alt="img"></p><p>我们之前就介绍过，<strong>AdaBoost-Stump算法的切割线是横跨整个平面的</strong>；而C&amp;RT算法的切割线是<strong>基于某个条件的，所以一般不会横跨整个平面</strong>。比较起来，虽然C&amp;RT和AdaBoost-Stump都采用decision stump方式进行切割，但是二者在细节上还是有所区别。</p><p>再看一个数据集分布比较复杂的例子，C&amp;RT和AdaBoost-Stump的切割方式对比效果如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/24-4.png" alt="img"></p><p>通常来说，由于C&amp;RT是基于条件进行切割的，所以C&amp;RT比AdaBoost-Stump分类切割更有效率。总结一下，C&amp;RT决策树有以下特点：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/25-1.png" alt="img"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了Decision Tree。首先将decision tree hypothesis对应到不同分支下的矩$g_t(x)$。然后再介绍决策树算法是如何通过递归的形式建立起来。接着详细研究了决策树C&amp;RT算法对应的数学模型和算法架构流程。最后通过一个实际的例子来演示决策树C&amp;RT算法是如何一步一步进行分类的。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——8.Adaptive Boosting</title>
      <link href="/2020/04/29/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-8.adaptive-boosting/"/>
      <url>/2020/04/29/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-8.adaptive-boosting/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-8-Adaptive-Boosting"><a href="#Lecture-8-Adaptive-Boosting" class="headerlink" title="Lecture 8: Adaptive Boosting"></a>Lecture 8: Adaptive Boosting</h1><p>上节课我们主要开始介绍Aggregation Models，目的是将不同的hypothesis得到的$g_t$集合起来，利用集体智慧得到更好的预测模型$G$。首先我们介绍了Blending，blending是将已存在的所有$g_t$结合起来，可以是uniformly，linearly，或者non-linearly组合形式。然后，我们讨论了在没有那么多$g_t$的情况下，使用bootstrap方式，从已有数据集中得到新的类似的数据集，从而得到不同的$g_t$。这种做法称为bagging。本节课将继续从这些概念出发，介绍一种新的演算法。</p><h2 id="1-Motivation-of-Boosting"><a href="#1-Motivation-of-Boosting" class="headerlink" title="1. Motivation of Boosting"></a>1. Motivation of Boosting</h2><p>我们先来看一个简单的识别苹果的例子，老师展示20张图片，让6岁孩子们通过观察，判断其中哪些图片的内容是苹果。从判断的过程中推导如何解决二元分类问题的方法。</p><p>显然这是一个监督式学习，20张图片包括它的标签都是已知的。</p><p>首先，学生Michael回答说：所有的苹果应该是圆形的。根据Michael的判断，对应到20张图片中去，大部分苹果能被识别出来，但也有错误。其中错误包括有的苹果不是圆形，而且圆形的水果也不一定是苹果。如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/1-14.png" alt="img"></p><p>上图中蓝色区域的图片代表分类错误。显然，只用“苹果是圆形的”这一个条件不能保证分类效果很好。我们<strong>把蓝色区域（分类错误的图片）放大，分类正确的图片缩小，这样在接下来的分类中就会更加注重这些错误样本</strong>。</p><p>然后，学生Tina观察被放大的错误样本和上一轮被缩小的正确样本，回答说：苹果应该是红色的。根据Tina的判断，得到的结果如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/2-13.png" alt="img"></p><p>上图中蓝色区域的图片一样代表分类错误，即根据这个苹果是红色的条件，使得青苹果和草莓、西红柿都出现了判断错误。那么结果就是把这些分类错误的样本放大化，其它正确的样本缩小化。同样，这样在接下来的分类中就会更加注重这些错误样本。</p><p>接着，学生Joey经过观察又说：苹果也可能是绿色的。根据Joey的判断，得到的结果如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/3-11.png" alt="img"></p><p>上图中蓝色区域的图片一样代表分类错误，根据苹果是绿色的条件，使得图中蓝色区域都出现了判断错误。同样把这些分类错误的样本放大化，其它正确的样本缩小化，在下一轮判断继续对其修正。</p><p>后来，学生Jessica又发现：上面有梗的才是苹果。得到如下结果：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/4-11.png" alt="img"></p><p>经过这几个同学的推论，苹果被定义为：圆的，红色的，也可能是绿色的，上面有梗。从一个一个的推导过程中，我们似乎得到一个较为准确的苹果的定义。虽然可能不是非常准确，但是要比单一的条件要好得多。也就是说把所有学生对苹果的定义融合起来，最终得到一个比较好的对苹果的总体定义。</p><p>这种做法就是我们本节课将要讨论的演算法。这些学生代表的就是简单的hypotheses $g_t$，将所有$g_t$融合，得到很好的预测模型$G$。例如，二维平面上简单的hypotheses（水平线和垂直线），这些简单$g_t$最终组成的较复杂的分类线能够较好地将正负样本完全分开，即得到了好的预测模型。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/5-11.png" alt="img"></p><p>所以，上个苹果的例子中，不同的学生代表不同的hypotheses $g_t$；最终得到的苹果总体定义就代表hypothesis $G$；而老师就代表演算法$A$，指导学生的注意力集中到关键的例子中（错误样本），从而得到更好的苹果定义。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/6-11.png" alt="img"></p><h2 id="2-Diversity-by-Re-weighting"><a href="#2-Diversity-by-Re-weighting" class="headerlink" title="2. Diversity by Re-weighting"></a><strong>2. Diversity by Re-weighting</strong></h2><p>在介绍这个演算法之前，我们先来讲一下上节课就介绍过的bagging。Bagging的核心是bootstrapping，通过对原始数据集D不断进行bootstrap的抽样动作，得到与D类似的数据集$\hat{D}_t$，每组$\hat{D}_t$都能得到相应的$g_t$，从而进行aggregation的操作。现在，假如包含四个样本的D经过bootstrap，得到新的$\hat{D}_t$如下：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/7-12.png" alt="img"></p><p>那么，对于新的$\hat{D}_t$，把它交给base algorithm，找出$E_{in}$最小时对应的$g_t$，如下图右边所示。</p><script type="math/tex; mode=display">E_{in}^{0/1}(h)=\frac14\sum_{n=1}^4[y\neq h(x)]</script><p>由于$\hat{D}_t$完全是$D$经过bootstrap得到的，其中样本$(x_1,y_1)$出现2次，$(x_2,y_2)$出现1次，$(x_3,y_3)$出现0次，$(x_4,y_4)$出现1次。引入一个参数$u_i$来表示原$D$中第$i$个样本在$\hat{D}_t$中出现的次数，如下图左边所示。</p><script type="math/tex; mode=display">E_{in}^u(h)=\frac14\sum_{n=1}^4u_n^{(t)}\cdot [y_n\neq h(x)]</script><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/8-12.png" alt="img"></p><p>参数$u$相当于是权重因子，当$\hat{D}_t$中第$i$个样本出现的次数越多的时候，那么对应的$u_i$越大，表示在error function中对该样本的惩罚越多。</p><p>所以，从另外一个角度来看bagging，它其实就是<strong>通过bootstrap的方式，来得到这些$u_i$值，作为犯错样本的权重因子，再用base algorithn最小化包含$u_i$的error function，得到不同的$g_t$。</strong>这个error function被称为bootstrap-weighted error。</p><p>这种算法叫做Weightd Base Algorithm，目的就是最小化bootstrap-weighted error。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/9-12.png" alt="img"></p><p>其实，这种weightd base algorithm我们之前就介绍过类似的算法形式。例如在soft-margin SVM中，我们引入允许犯错的项，同样可以将每个点的error乘以权重因子$u_n$。加上该项前的参数$C$，经过QP，最终得到$0\leq \alpha_n\leq Cu_n$，有别于之前介绍的$0\leq \alpha_n\leq C$。这里的$u_n$相当于每个犯错的样本的惩罚因子，并会反映到$\alpha_n$的范围限定上。</p><p>同样在logistic regression中，同样可以对每个犯错误的样本乘以相应的$u_n$，作为惩罚因子。$u_n$表示该错误点出现的次数，$u_n$越大，则对应的惩罚因子越大，则在最小化error时就应该更加重视这些点。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/10-12.png" alt="img"></p><p>其实这种example-weighted learning，我们在机器学习基石课程第8次笔记中就介绍过class-weighted的思想。二者道理是相通的。</p><p>知道了$u$的概念后，我们知道不同的$u$组合经过base algorithm得到不同的$g_t$。那么如何选取$u$，使得到的$g_t$之间有很大的不同呢？之所以要让所有的$g_t$差别很大，是因为上节课aggregation中，我们介绍过$g_t$越不一样，其aggregation的效果越好，即每个人的意见越不相同，越能运用集体的智慧，得到好的预测模型。</p><p>为了得到不同的$g_t$，我们先来看看$g_t$和$g_{t+1}$是怎么得到的：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/11-11.png" alt="img"></p><p>如上所示，$g_t$是由$u_n^{t}$得到的，$g_{t+1}$是由$u_n^{(t+1)}$得到的。如果$g_t$这个模型在使用$u_n^{(t+1)}$的时候得到的error很大，即预测效果非常不好，那就表示由$u_n^{(t+1)}$计算的$g_{t+1}$会与$g_t$有很大不同。而$g_{t+1}$与$g_t$差异性大正是我们希望看到的。</p><p>怎么做呢？方法是利用$g_t$在使用$u_n^{(t+1)}$的时候表现很差的条件，越差越好。如果在$g_t$作用下，$u_n^{(t+1)}$中的表现（即error）近似为0.5的时候，表明$g_t$对$u_n^{(t+1)}$的预测分类没有什么作用（就像抛硬币一样）。这样的做法就能最大限度地保证$g_{t+1}$会与$g_t$有较大的差异性。其数学表达式如下所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/12-11.png" alt="img"></p><p>乍看上面这个式子，似乎不好求解。但是，我们对它做一些等价处理，其中分式中分子可以看成$g_t$作用下犯错误的点，而分母可以看成犯错的点和没有犯错误的点的集合，即所有样本点。其中犯错误的点和没有犯错误的点分别用橘色方块和绿色圆圈表示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/13-11.png" alt="img"></p><p>要让分式等于0.5，显然只要<strong>将犯错误的点和没有犯错误的点的数量调成一样就可以了</strong>。也就是说，在$g_t$作用下，让犯错的$u_n^{(t+1)}$数量和没有犯错的$u_n^{(t+1)}$数量一致就行。</p><p>一种简单的方法就是利用放大和缩小的思想（本节课开始引入识别苹果的例子中提到的放大图片和缩小图片就是这个目的），将犯错误的$u_n^{t}$和没有犯错误的$u_n^{t}$做相应的乘积操作，使得二者值变成相等。例如$u_n^{t}$ of incorrect为1126，$u_n^{t}$ of correct为6211，要让$u_n^{(t+1)}$中错误比例正好是0.5，可以这样做，对于incorrect $u_n^{(t+1)}$：</p><script type="math/tex; mode=display">u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot 6211</script><p>对于correct $u_n^{(t+1)}$：</p><script type="math/tex; mode=display">u_n^{(t+1)}\leftarrow u_n^{(t)}\cdot 1126</script><p>或者利用犯错的比例来做，令weighted incorrect rate和weighted correct rate分别设为$\frac{1126}{7337}$和$\frac{6211}{7337}$。一般求解方式是令犯错率为$\epsilon_t$，在计算$u_n^{(t+1)}$的时候，$u_n^{t}$分别乘以$(1-\epsilon_t)$和$\epsilon_t$。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/14-11.png" alt="img"></p><h2 id="3-Adaptive-Boosting-Algorithm"><a href="#3-Adaptive-Boosting-Algorithm" class="headerlink" title="3. Adaptive Boosting Algorithm"></a>3. Adaptive Boosting Algorithm</h2><p>上一部分，我们介绍了在计算$u_n^{(t+1)}$的时候，$u_n^{t}$分别乘以$(1-\epsilon_t)$和$\epsilon_t$。下面将构造一个新的尺度因子：</p><script type="math/tex; mode=display">\diamond_t=\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}</script><p>那么引入这个新的尺度因子之后，对于错误的$u_n^{t}$，将它乘以$\diamond_t$；对于正确的$u_n^{t}$，将它除以$\diamond_t$。</p><p>这种操作跟之前介绍的分别乘以$(1-\epsilon_t)$和$\epsilon_t$的效果是一样的。之所以引入$\diamond_t$是因为它告诉我们更多的物理意义。因为如果$\epsilon_t\leq\frac12$，得到$\diamond_t\geq1$，那么接下来错误的$u_n^{t}$与$\diamond_t$的乘积就相当于把错误点放大了，而正确的$u_n^{t}$与$\diamond_t$的相除就相当于把正确点缩小了。这种scale up incorrect和scale down correct的做法与本节课开始介绍的学生识别苹果的例子中放大错误的图片和缩小正确的图片是一个原理，让学生能够将注意力更多地放在犯错误的点上。通过这种scaling-up incorrect的操作，能够保证得到不同于$g_t$的$g_{t+1}$。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/15-10.png" alt="img"></p><p>值得注意的是上述的结论是建立在$\epsilon_t\leq\frac12$的基础上，如果$\epsilon_t\geq\frac12$，那么就做相反的推论即可。关于$\epsilon_t\geq\frac12$的情况，我们稍后会进行说明。</p><p>从这个概念出发，我们可以得到一个初步的演算法。其核心步骤是每次迭代时利用$\diamond _t=\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}$把$u_t$更新为$u_{t+1}$。具体迭代步骤如下：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/16-10.png" alt="img"></p><p>但是，上述步骤还有两个问题没有解决：</p><ol><li>初始的$u^{(1)}$应为多少呢？一般来说，为了保证第一次$E_{in}$最小的话，设$u^{(1)}=\frac1N$即可。这样最开始的$g_1$就能由此推导。</li><li>最终的$G(x)$应该怎么求？是将所有的$g(t)$合并uniform在一起吗？一般来说并不是这样直接uniform求解，因为$g_{t+1}$是通过$g_t$得来的，二者在$E_{in}$上的表现差别比较大。所以，一般是对所有的$g(t)$进行linear或者non-linear组合来得到$G(t)$。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/17-11.png" alt="img"></p><p>接下来的内容，我们将对上面的第二个问题进行探讨，研究一种算法，将所有的$g(t)$进行linear组合。方法是计算$g(t)$的同时，就能计算得到其线性组合系数$\alpha_t$，即aggregate linearly on the fly。这种算法使最终求得$g_{t+1}$的时候，所有$g_t$的线性组合系数$\alpha$也求得了，不用再重新计算$\alpha$了。这种Linear Aggregation on the Fly算法流程为：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/18-9.png" alt="img"></p><p>如何在每次迭代的时候计算$\alpha_t$呢？我们知道$\alpha_t$与$\epsilon_t$是相关的：$\epsilon_t$越小，对应的$\alpha_t$应该越大，$\epsilon_t$越大，对应的$\alpha_t$应该越小。又因为$\diamond_t$与$\epsilon_t$是正相关的，所以，$\alpha_t$应该是$\diamond_t$的单调函数。我们构造$\alpha_t$为：</p><script type="math/tex; mode=display">\alpha_t=\ln(\diamond_t)</script><p>$\alpha_t$这样取值是有物理意义的：</p><ol><li>当$\epsilon_t=\frac12$时，error很大，跟掷骰子这样的随机过程没什么两样，此时对应的$\diamond_t=1$，$\alpha_t=0$，即此$g_t$对$G$没有什么贡献，权重应该设为零。</li><li>当$\epsilon_t=0$时，没有error，表示该$g_t$预测非常准，此时对应的$\diamond_t=\infty$，$\alpha_t=\infty$，即此$g_t$对$G$贡献非常大，权重应该设为无穷大。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/19-6.png" alt="img"></p><p>这种算法被称为Adaptive Boosting。它由三部分构成：base learning algorithm $A$，re-weighting factor $\diamond_t$和linear aggregation $\alpha_t$。这三部分分别对应于我们在本节课开始介绍的例子中的Student，Teacher和Class。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/20-5.png" alt="img"></p><p>综上所述，完整的adaptive boosting（AdaBoost）Algorithm流程如下：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/21-5.png" alt="img"></p><p>从我们之前介绍过的VC bound角度来看，AdaBoost算法理论上满足：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/22-4.png" alt="img"></p><p>上式中，$E_{out}(G)$的上界由两部分组成，一项是$E_{in}(G)$，另一项是模型复杂度$O(*)$。模型复杂度中$d_{vc}(H)$是$g_t$的VC Dimension，$T$是迭代次数，可以证明$G$的$d_{vc}$服从$O(d_{vc}(H)\cdot Tlog\ T)$。</p><p>对这个VC bound中的第一项$E_{in}(G)$来说，有一个很好的性质：如果满足$\epsilon_t\leq \epsilon\lt\frac12$，则经过$T=O(\log N)$次迭代之后，$E_{in}(G)$能减小到等于零的程度。而当$N$很大的时候，其中第二项也能变得很小。因为这两项都能变得很小，那么整个$E_{out}(G)$就能被限定在一个有限的上界中。</p><p>其实，这种性质也正是AdaBoost算法的精髓所在。只要每次的$\epsilon_t\leq \epsilon\lt\frac12$，即所选择的矩$g$比乱猜的表现好一点点，那么经过每次迭代之后，矩$g$的表现都会比原来更好一些，逐渐变强，最终得到$E_{in}=0$且$E_{out}$很小。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/23-2.png" alt="img"></p><h2 id="4-Adaptive-Boosting-in-Action"><a href="#4-Adaptive-Boosting-in-Action" class="headerlink" title="4. Adaptive Boosting in Action"></a><strong>4. Adaptive Boosting in Action</strong></h2><p>上一小节我们已经介绍了选择一个“弱弱”的算法$A$（$\epsilon_t\leq \epsilon\lt\frac12$，比乱猜好就行），就能经过多次迭代得到$E_{in}=0$。我们称这种形式为decision stump模型。下面介绍一个例子，来看看AdaBoost是如何使用decision stump解决实际问题的。</p><p>如下图所示，二维平面上分布一些正负样本点，利用decision stump来做切割。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/24-3.png" alt="img"></p><p>第一步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/25.png" alt="img"></p><p>第二步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/26.png" alt="img"></p><p>第三步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/27.png" alt="img"></p><p>第四步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/28.png" alt="img"></p><p>第五步：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/29.png" alt="img"></p><p>可以看到，经过5次迭代之后，所有的正负点已经被完全分开了，则最终得到的分类线为：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/30.png" alt="img"></p><p>另外一个例子，对于一个相对比较复杂的数据集，如下图所示。它的分界线从视觉上看应该是一个sin波的形式。如果我们再使用AdaBoost算法，通过decision stump来做切割。在迭代切割100次后，得到的分界线如下所示。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/31.png" alt="img"></p><p>可以看出，AdaBoost-Stump这种非线性模型得到的分界线对正负样本有较好的分离效果。</p><p>课程中还介绍了一个AdaBoost-Stump在人脸识别方面的应用：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/32.png" alt="img"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h2><p>本节课主要介绍了Adaptive Boosting。首先通过讲一个老师教小学生识别苹果的例子，来引入Boosting的思想，即把许多“弱弱”的hypotheses合并起来，变成很强的预测模型。然后重点介绍这种算法如何实现，关键在于每次迭代时，给予样本不同的系数$u$，宗旨是放大错误样本，缩小正确样本，得到不同的小矩$g$。并且在每次迭代时根据错误$\epsilon$值的大小，给予不同$g_t$不同的权重。最终由不同的$g_t$进行组合得到整体的预测模型$G$。实际证明，Adaptive Boosting能够得到有效的预测模型。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——7.Blending and Bagging</title>
      <link href="/2020/04/22/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-7.blending-and-bagging/"/>
      <url>/2020/04/22/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-7.blending-and-bagging/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-7-Blending-and-Bagging"><a href="#Lecture-7-Blending-and-Bagging" class="headerlink" title="Lecture 7: Blending and Bagging"></a>Lecture 7: Blending and Bagging</h1><p>上节课我们主要介绍了Support Vector Regression，将kernel model引入到regression中。首先，通过将ridge regression和representer theorem结合起来，得到kernel ridge regression。但是其解是dense的，即不部分不为零。为了得到sparse解，我们将regularized tube error和Lagrange dual结合起来，利用SVM dual的推导方法，得到support vector regression的sparse解。</p><p>从本节课开始，讲介绍Aggregation Models，即如何将不同的hypothesis和features结合起来，让模型更好。本节课将介绍其中的两个方法，一个是Blending，一个是Bagging。</p><h2 id="1-Motivation-of-Aggregation"><a href="#1-Motivation-of-Aggregation" class="headerlink" title="1. Motivation of Aggregation"></a>1. Motivation of Aggregation</h2><p>首先举个例子来说明为什么要使用Aggregation。假如你有$T$个朋友，每个朋友向你预测推荐明天某支股票会涨还是会跌，对应的建议分别是$g_1,g_2,\cdots,g_T$，那么你该选择哪个朋友的建议呢？即最终选择对股票预测的$g_t(x)$是什么样的？</p><ol><li>从$T$个朋友中选择一个最受信任，对股票预测能力最强的人，直接听从他的建议就好。这是一种普遍的做法，对应的就是validation思想，即选择犯错误最小的模型。</li><li>如果每个朋友在股票预测方面都是比较厉害的，都有各自的专长，那么就同时考虑$T$个朋友的建议，将所有结果做个投票，一人一票，最终决定出对该支股票的预测。这种方法对应的是uniformly思想。</li><li>如果每个朋友水平不一，有的比较厉害，投票比重应该更大一些，有的比较差，投票比重应该更小一些。那么，仍然对$T$个朋友进行投票，只是每个人的投票权重不同。这种方法对应的是non-uniformly的思想。</li><li>与第3种方法类似，但是权重不是固定的，根据不同的条件，给予不同的权重。比如如果是传统行业的股票，那么给这方面比较厉害的朋友较高的投票权重，如果是服务行业，那么就给这方面比较厉害的朋友较高的投票权重。</li></ol><p>以上所述的这四种方法都是将不同人不同意见融合起来的方式，接下来我们就要讨论如何将这些做法对应到机器学习中去。Aggregation的思想与这个例子是类似的，即把多个hypothesis结合起来，得到更好的预测效果。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/1-13.png" alt="img"></p><p>将刚刚举的例子的各种方法用数学化的语言和机器学习符号归纳表示出来，其中$G(x)$表示最终选择的模型。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/2-12.png" alt="img"></p><p>注意这里提到的第一种方法是通过验证集来选择最佳模型，不能使用$E_{in}(g_t)$来代替$E_{val}(g_t^-)$。经过Validation，选择最小的$E_{val}$，保证$E_{out}$最小，从而将对应的模型作为最佳的选择。</p><p>但是第一种方法只是从众多可能的hypothesis中选择最好的模型，并不能发挥集体的智慧。而Aggregation的思想是博采众长，将可能的hypothesis优势集合起来，将集体智慧融合起来，使预测模型达到更好的效果。</p><p>下面先来看一个例子，通过这个例子说明为什么Aggregation能work得更好。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/3-10.png" alt="img"></p><p>如上图所示，平面上分布着一些待分类的点。如果要求只能用一条水平的线或者垂直的线进行分类，那不论怎么选取直线，都达不到最佳的分类效果。这实际上就是上面介绍的第一种方法：validation。但是，如果可以使用集体智慧，比如一条水平线和两条垂直线组合而成的图中折线形式，就可以将所有的点完全分开，得到了最优化的预测模型。</p><p>这个例子表明，通过将不同的hypotheses均匀地结合起来，得到了比单一hypothesis更好的预测模型。这就是aggregation的优势所在，<strong>它提高了预测模型的power，起到了特征转换（feature transform）的效果</strong>。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/4-10.png" alt="img"></p><p>我们再从另外一方面来看，同样是平面上分布着一些待分类的点，使用PLA算法，可以得到很多满足条件的分类线，如下图所示：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/5-10.png" alt="img"></p><p>这无数条PLA选择出来的直线对应的hypothesis都是满足分类要求的。但是我们最想得到的分类直线是中间那条距离所有点都比较远的黑色直线，这与之前SVM目标是一致的。如果我们将所有可能的hypothesis结合起来，以投票的方式进行组合选择，最终会发现<strong>投票得到的分类线就是中间和黑色那条</strong>。这从哲学的角度来说，就是<strong>对各种效果较好的可能性进行组合，得到的结果一般是中庸的、最合适的</strong>，即对应图中那条黑色直线。所以，aggregation也<strong>起到了正则化（regularization）的效果，让预测模型更具有代表性</strong>。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/6-10.png" alt="img"></p><p>基于以上的两个例子，我们得到了<strong>aggregation的两个优势：feature transform和regularization</strong>。我们之前在机器学习基石课程中就介绍过，feature transform和regularization是对立的，还把它们分别比作踩油门和踩刹车。如果进行feature transform，那么regularization的效果通常很差，反之亦然。也就是说，单一模型通常只能倾向于feature transform和regularization之一，在两者之间做个权衡。但是aggregation却能将feature transform和regularization各自的优势结合起来，好比把油门和刹车都控制得很好，从而得到不错的预测模型。</p><h2 id="2-Uniform-Blending"><a href="#2-Uniform-Blending" class="headerlink" title="2. Uniform Blending"></a>2. Uniform Blending</h2><p>那对于我们已经选择的性能较好的一些矩$g_t$，如何将它们进行整合、合并，来得到最佳的预测模型呢？这个过程称为blending。</p><p>最常用的一种方法是<strong>uniform blending</strong>。</p><p>应用于classification分类问题，做法是将每一个可能的矩赋予权重1，进行投票，得到的$G(x)$表示为：</p><script type="math/tex; mode=display">G(x)=sign(\sum_{t=1}^T1\cdot g_t(x))</script><p>这种方法对应三种情况：</p><ol><li>每个候选的矩$g_t$都完全一样，这跟选其中任意一个$g_t$效果相同；</li><li>每个候选的矩$g_t$都有一些差别，这是最常遇到的，大都可以通过投票的形式使多数意见修正少数意见，从而得到很好的模型，如下图所示；</li><li>多分类问题，选择投票数最多的那一类即可。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/7-11.png" alt="img"></p><p>如果是regression回归问题，uniform blending的做法很简单，就是将所有的矩$g_t$求平均值：</p><script type="math/tex; mode=display">G(x)=\frac1T\sum_{t=1}^Tg_t(x)</script><p>uniform blending for regression对应两种情况：</p><ol><li>每个候选的矩$g_t$都完全一样，这跟选其中任意一个$g_t$效果相同；</li><li>每个候选的矩$g_t$都有一些差别，有的$g_t&gt;f(x)$，有的$g_t&lt;f(x)$，此时求平均值的操作可能会消去这种大于和小于的影响，从而得到更好的回归模型。</li></ol><p>因此，从直觉上来说，求平均值的操作更加稳定，更加准确。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/8-11.png" alt="img"></p><p>对于uniform blending，一般要求每个候选的矩$g_t$都有一些差别。这样，通过不同矩$g_t$的组合和集体智慧，都能得到比单一矩$g_t$更好的模型。</p><p>刚才我们提到了uniform blending for regression中，计算$g_t$的平均值可能比单一的$g_t$更稳定，更准确。下面进行简单的推导和证明。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/9-11.png" alt="img"></p><p>推导过程中注意$G(t)=avg(g_t)$。经过推导，我们发现$avg((g_t(x)-f(x))^2)$与$(G-f)^2$之间差了$avg((g_t-G)^2)$项，且是大于零的。从而得到$g_t$与目标函数$f$的差值要比$G$与$f$的差值大。</p><p>刚才是对单一的$x$进行证明，如果从期望角度，对整个$x$分布进行上述公式的整理，得到：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/10-11.png" alt="img"></p><p>从结果上来看，$avg(E_{out}(g_t))\geq E_{out}(G)$，从而证明了从平均上来说，计算$g_t$的平均值$G(t)$要比单一的$g_t$更接近目标函数$f$，regression效果更好。</p><p>我们已经知道$G$是数目为$T$的$g_t$的平均值。令包含$N$个数据的样本$D$独立同分布于$P^N$，每次从新的$D_t$中学习得到新的$g_t$，在对$g_t$求平均得到$G$，当做无限多次，即$T$趋向于无穷大的时候：</p><script type="math/tex; mode=display">\overline{g}=\lim_{T\rightarrow \infty}\ G=\lim_{T\rightarrow \infty}\ \frac1T\sum_{t=1}^Tg_t=\varepsilon_DA(D)</script><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/11-10.png" alt="img"></p><p>当$T$趋于无穷大的时候，$G=\overline{g}$，则有如下等式成立：</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/12-10.png" alt="img"></p><p>上述等式中左边表示演算法误差的期望值；右边第二项表示不同$g_t$的平均误差共识，用偏差bias表示；右边第一项表示不同$g_t$与共识的差距是多少，反映$g_t$之间的偏差，用方差variance表示。</p><p>也就是说，一个演算法的平均表现可以被拆成两项，一个是所有$g_t$的共识，一个是不同$g_t$之间的差距是多少，即<strong>bias和variance</strong>。而uniform blending的操作时求平均的过程，这样就削减弱化了上式第一项variance的值，从而演算法的表现就更好了，能得到更加稳定的表现。</p><h2 id="3-Linear-and-Any-Blending"><a href="#3-Linear-and-Any-Blending" class="headerlink" title="3. Linear and Any Blending"></a>3. Linear and Any Blending</h2><p>上一部分讲的是uniform blending，即每个$g_t$所占的权重都是1，求平均的思想。下面我们将介绍linear blending，每个$g_t$赋予的权重$\alpha_t$并不相同，其中$\alpha_t\geq0$。我们最终得到的预测结果等于所有的$g_t$线性组合。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/13-10.png" alt="img"></p><p>如何确定$\alpha_t$的值，方法是利用误差最小化的思想，找出最佳的$\alpha_t$，使$E_{in}(\alpha)$取最小值。</p><p>例如对于linear blending for regression，$E_{in}(\alpha)$可以写成下图左边形式，其中$\alpha_t$是带求解参数，$g_t(x_n)$是每个矩得到的预测值，由已知矩得到。这种形式很类似于下图右边的形式，即加上特征转换$\phi_i(x_n)$的linear regression模型。两个式子中的$g_t(x_n)$对应于$\phi_i(x_n)$，唯一不同的就是linear blending for regression中$\alpha_t\geq0$，而linear regression中$w_i$<strong>没有限制</strong>。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/14-10.png" alt="img"></p><p>这种求解$\alpha_t$的方法就像是使用two-level learning，类似于我们之前介绍的probabilistic SVM。这里，我们先计算$g_t(x_n)$，再进行linear regression得到$\alpha_t$值。总的来说，linear blending由三个部分组成：LinModel，hypotheses as transform，constraints。其中值得注意的一点就是，计算过程中可以把$g_t$当成feature transform，求解过程就跟之前没有什么不同，除了$\alpha\geq0$的条件限制。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/15-9.png" alt="img"></p><p>我们来看一下linear blending中的constraint $\alpha_t\geq0$。这个条件是否一定要成立呢？如果$\alpha_t&lt;0$，会带来什么后果呢？其实$\alpha_t&lt;0$并不会影响分类效果，只需要将正类看成负类，负类当成正类即可。</p><p>例如分类问题，判断该点是正类对应的$\alpha_t<0$，则它就表示该点是负类，且对应的$-\alpha_t>0$。如果我们说这个样本是正类的概率是-99%，意思也就是说该样本是负类的概率是99%。$\alpha_t\geq0$和$\alpha_t&lt;0$的效果是等同的一致的。所以，我们可以把$\alpha_t\geq0$这个条件舍去，这样linear blending就可以使用常规方法求解。</0$，则它就表示该点是负类，且对应的$-\alpha_t></p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/16-9.png" alt="img"></p><p>Linear Blending中使用的$g_t$是通过模型选择而得到的，利用validation，从$D_{train}$中得到$g_1^-,g_2^-,\cdots,g_T^-$。然后将$D_{train}$中每个数据点经过各个矩的计算得到的值，代入到相应的linear blending计算公式中，迭代优化得到对应$\alpha$值。最终，再利用所有样本数据，得到新的$g_t$代替$g_t^-$，则$G(t)$就是$g_t$的线性组合而不是$g_t^-$，系数是$\alpha_t$。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/17-10.png" alt="img"></p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/18-8.png" alt="img"></p><p>除了linear blending之外，还可以使用任意形式的blending。</p><ol><li>linear blending中，$G(t)$是$g(t)$的线性组合；</li><li>any blending中，$G(t)$可以是$g(t)$的任何函数形式（非线性）。这种形式的blending也叫做<strong>Stacking</strong>。any blending的优点是模型复杂度提高，更容易获得更好的预测模型；缺点是复杂模型也容易带来过拟合的危险。所以，在使用any blending的过程中要时刻注意避免过拟合发生，通过采用regularization的方法，让模型具有更好的泛化能力。</li></ol><h2 id="4-Bagging-Bootstrap-Aggregation"><a href="#4-Bagging-Bootstrap-Aggregation" class="headerlink" title="4. Bagging (Bootstrap Aggregation)"></a>4. Bagging (Bootstrap Aggregation)</h2><p>总结一些上面讲的内容，blending的做法就是将已经得到的矩$g_t$进行aggregate的操作。具体的aggregation形式包括：uniform，non-uniforn和conditional。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/19-5.png" alt="img"></p><p>现在考虑一个问题：如何得到不同的$g_t$呢？</p><ol><li>可以选取不同模型$H$；</li><li>可以设置不同的参数，例如$\eta$、迭代次数n等；</li><li>可以由算法的随机性得到，例如PLA、随机种子等；</li><li>可以选择不同的数据样本等。</li></ol><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/20-4.png" alt="img"></p><p>那如何利用已有的一份数据集来构造出不同的$g_t$呢？首先，我们回顾一下之前介绍的bias-variance，即一个演算法的平均表现可以被拆成两项，一个是所有$g_t$的共识（bias），一个是不同$g_t$之间的差距是多少（variance）。其中每个$g_t$都是需要新的数据集的。只有一份数据集的情况下，如何构造新的数据集？</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/21-4.png" alt="img"></p><p>其中，$\overline{g}$是在矩个数$T$趋向于无穷大的时候，不同的$g_t$计算平均得到的值。这里我们为了得到$\overline{g}$，做两个近似条件：</p><ol><li>有限的$T$；</li><li>由已有数据集$D$构造出$D_t$~$P^N$，独立同分布</li></ol><p>第一个条件没有问题，第二个近似条件的做法就是<strong>bootstrapping</strong>。</p><p>bootstrapping是统计学的一个工具，思想就是从已有数据集$D$中模拟出其他类似的样本$D_t$。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/22-3.png" alt="img"></p><p>bootstrapping的做法是，假设有$N$笔资料，先从中选出一个样本，再放回去，再选择一个样本，再放回去，共重复$N$次。这样我们就得到了一个新的$N$笔资料，这个新的$\breve{D_t}$中可能包含原$D$里的重复样本点，也可能没有原$D$里的某些样本，$\breve{D_t}$与$D$类似但又不完全相同。值得一提的是，抽取-放回的操作不一定非要是$N$，次数可以任意设定。例如原始样本有10000个，我们可以抽取-放回3000次，得到包含3000个样本的$\breve{D_t}$也是完全可以的。<strong>利用bootstrap进行aggragation的操作就被称为bagging。</strong></p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/23-1.png" alt="img"></p><p>下面举个实际中Bagging Pocket算法的例子。如下图所示，先通过bootstrapping得到25个不同样本集，再使用pocket算法得到25个不同的$g_t$，每个pocket算法迭代1000次。最后，再利用blending，将所有的$g_t$融合起来，得到最终的分类线，如图中黑线所示。可以看出，虽然bootstrapping会得到差别很大的分类线（灰线），但是经过blending后，得到的分类线效果是不错的，则bagging通常能得到最佳的分类模型。</p><p><img src="http://redstonewill.com/wp-content/uploads/2018/07/24-2.png" alt="img"></p><p>值得注意的是，只有当演算法对数据样本分布比较敏感的情况下，才有比较好的表现。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h2><p>本节课主要介绍了blending和bagging的方法，它们都属于aggregation，即将不同的$g_t$合并起来，利用集体的智慧得到更加优化的$G(t)$。</p><p>Blending通常分为三种情况：Uniform Blending，Linear Blending和Any Blending。其中，uniform blending采样最简单的“一人一票”的方法，linear blending和any blending都采用标准的two-level learning方法，类似于特征转换的操作，来得到不同$g_t$的线性组合或非线性组合。最后，我们介绍了如何利用bagging（bootstrap aggregation），从已有数据集D中模拟出其他类似的样本$D_t$，而得到不同的$g_t$，再合并起来，优化预测模型。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——6.Support Vector Regression</title>
      <link href="/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-6.support-vector-regression/"/>
      <url>/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-6.support-vector-regression/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-6-Support-Vector-Regression"><a href="#Lecture-6-Support-Vector-Regression" class="headerlink" title="Lecture 6: Support Vector Regression"></a>Lecture 6: Support Vector Regression</h1><p>本节课讨论如何将SVM的kernel技巧应用到regression问题上。</p><h2 id="1-Kernel-Ridge-Regression"><a href="#1-Kernel-Ridge-Regression" class="headerlink" title="1. Kernel Ridge Regression"></a>1. Kernel Ridge Regression</h2><p>首先回顾一下上节课介绍的Representer Theorem，对于任何包含正则项的L2-regularized linear model，它的最佳化解$w$都可以写成是$z$的线性组合形式，因此，也就能引入kernel技巧，将模型kernelized化。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-12.png" alt="img"></p><p>那么如何将regression模型变成kernel的形式呢？我们之前介绍的linear/ridge regression最常用的错误估计是squared error，即$err(y,w^Tz)=(y-w^Tz)^2$。这种形式对应的解是analytic solution，即可以使用线性最小二乘法，通过向量运算，直接得到最优化解。</p><p>那么接下来我们就要研究如何将kernel引入到ridge regression中去，得到与之对应的analytic solution。</p><p>我们先把Kernel Ridge Regression问题写下来：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-11.png" alt="img"></p><p>其中，最佳解$w_<em>$必然是z的线性组合。那么我们就把$w_</em>=\sum_{n=1}^N\beta_nz_n$代入到ridge regression中，将$z$的内积用kernel替换，把求$w_*$的问题转化成求$\beta_n$的问题，得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-9.png" alt="img"></p><p>ridge regression可以写成矩阵的形式，其中第一项可以看成是$\beta_n$的正则项，而第二项可以看成是$\beta_n$的error function。这样，我们的目的就是求解该式最小化对应的$\beta_n$值，这样就解决了kernel ridge regression问题。</p><p>求解$\beta_n$的问题可以写成如下形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-9.png" alt="img"></p><p>$E_{aug}(\beta)$是关于$\beta$的二次多项式，要对$E_{aug}(\beta)$求最小化解，这种凸二次最优化问题，只需要先计算其梯度，再令梯度为零即可。得到一种可能的$\beta$的解析解为：</p><script type="math/tex; mode=display">\beta=(\lambda I+K)^{-1}y</script><p>$(\lambda I+K)$的逆矩阵是否存在？答案是肯定的。因为我们之前介绍过，核函数$K$满足Mercer’s condition，它是半正定的，而且$\lambda&gt;0$，所以$(\lambda I+K)$一定是可逆的。</p><p>从计算的时间复杂上来说，由于$(\lambda I+K)$是$N$x$N$大小的，所以时间复杂度是$O(N^3)$。</p><p>$\nabla E_{aug}(\beta)$是由两项乘积构成的，另一项是$K$，会不会出现$K=0$的情况呢？其实，由于核函数$K$表征的是$z$空间的内积，一般而言，除非两个向量互相垂直，内积才为零，否则，一般情况下$K$不等于零。这个原因也决定了$(\lambda I+K)$是dense matrix，即$\beta$的解大部分都是非零值。</p><p>所以说，我们可以通过kernel来解决non-linear regression的问题。下面比较一下linear ridge regression和kernel ridge regression的关系。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-9.png" alt="img"></p><p>如上图所示，左边是linear ridge regression，是一条直线；右边是kernel ridge regression，是一条曲线。大致比较一下，右边的曲线拟合的效果更好一些。</p><p>这两种regression有什么样的优点和缺点呢？</p><ol><li>linear ridge regression：<ul><li>线性模型，只能拟合直线；</li><li>训练复杂度是$O(d^3+d^2N)$（矩阵（$\lambda I+X^TX$）是$d$x$d$的），预测的复杂度是$O(d)$，如果$N$比$d$大很多时，这种模型就更有效率。</li></ul></li><li>kernel ridge regression：<ul><li>它转换到$z$空间，使用kernel技巧，得到的是非线性模型，所以更加灵活；</li><li>训练复杂度是$O(N^3)$（矩阵（$\lambda I+K$）是$N$x$N$的），预测的复杂度是$O(N)$，均只与$N$有关。当$N$很大的时候，计算量就很大，所以，kernel ridge regression适合$N$不是很大的场合。</li></ul></li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-9.png" alt="img"></p><p>比较下来，可以说linear和kernel实际上是效率（efficiency）和灵活（flexibility）之间的权衡。</p><h2 id="2-Support-Vector-Regression-Primal"><a href="#2-Support-Vector-Regression-Primal" class="headerlink" title="2. Support Vector Regression Primal"></a>2. Support Vector Regression Primal</h2><p>我们在机器学习基石课程中介绍过linear regression可以用来做classification，那么上一部分介绍的kernel ridge regression同样可以来做classification。我们把kernel ridge regression应用在classification上取个新的名字，叫做least-squares SVM（LSSVM）。</p><p>先来看一下对于某个问题，soft-margin Gaussian SVM和Gaussian LSSVM结果有哪些不一样的地方。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-10.png" alt="img"></p><p>如上图所示，如果只看分类边界的话，soft-margin Gaussian SVM和Gaussian LSSVM差别不是很大，即分类线是几乎相同的。但是如果看Support Vector的话（图中方框标注的点），左边soft-margin Gaussian SVM的SV不多，而<strong>右边Gaussian LSSVM中基本上每个点都是SV</strong>。这是因为soft-margin Gaussian SVM中的$\alpha_n$大部分是等于零，$\alpha_n&gt;0$的点只占少数，所以SV少。而对于LSSVM，我们上一部分介绍了$\beta$的解大部分都是非零值，所以对应的每个点基本上都是SV。</p><p>SV太多会带来一个问题，就是做预测的矩$g(x)=\sum_{n=1}^N\beta_nK(x_n,x)$，如果$\beta_n$非零值较多，那么$g$的计算量也比较大，降低计算速度。基于这个原因，soft-margin Gaussian SVM更有优势。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-10.png" alt="img"></p><p>那么，针对LSSVM中dense $\beta$的缺点，我们能不能使用一些方法来的得到sparse $\beta$，使得SV不会太多，从而得到和soft-margin SVM同样的分类效果呢？下面我们将尝试解决这个问题。</p><p>方法是引入一个叫做<strong>Tube Regression</strong>的做法，即在分类线上下分别划定一个区域（中立区），如果数据点分布在这个区域内，则不算分类错误，<strong>只有误分在中立区域之外的地方才算error</strong>。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-10.png" alt="img"></p><p>假定中立区的宽度为$2\epsilon$，$\epsilon&gt;0$,那么error measure就可以写成：$err(y,s)=\max(0,|s-y|-\epsilon)$，对应上图中红色标注的距离。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-10.png" alt="img"></p><p>通常<strong>把这个error叫做$\epsilon$-insensitive error</strong>，这种max的形式跟我们上节课中介绍的hinge error measure形式其实是类似的。所以，我们接下来要做的事情就是将L2-regularized tube regression做类似于soft-margin SVM的推导，从而得到sparse $\beta$。</p><p>首先，我们把tube regression中的error与squared error做个比较：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-9.png" alt="img"></p><p>然后，将$err(y,s)$与$s$的关系曲线分别画出来：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-9.png" alt="img"></p><p>上图中，红色的线表示squared error，蓝色的线表示tube error。我们发现：</p><ol><li>当$|s-y|$比较小即$s$比较接近$y$的时候，squared error与tube error是差不多大小的。</li><li>在$|s-y|$比较大的区域，squared error的增长幅度要比tube error大很多。error的增长幅度越大，表示越容易受到noise的影响，不利于最优化问题的求解。所以，从这个方面来看，tube regression的这种error function要更好一些。</li></ol><p>现在，我们把L2-Regularized Tube Regression写下来：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-9.png" alt="img"></p><p>这个最优化问题，由于其中包含max项，并不是处处可微分的，所以不适合用GD/SGD来求解。而且，虽然满足representer theorem，有可能通过引入kernel来求解，但是也并不能保证得到sparsity $\beta$。从另一方面考虑，我们可以把这个问题转换为带条件的QP问题，仿照dual SVM的推导方法，引入kernel，得到KKT条件，从而保证解$\beta$是sparse的。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-9.png" alt="img"></p><p>所以，我们就可以把L2-Regularized Tube Regression写成跟SVM类似的形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-8.png" alt="img"></p><p>值得一提的是，系数$\lambda$和$C$是反比例相关的，$\lambda$越大对应C越小。而且该式也把$w_0$即$b$单独拿了出来，这跟我们之前推导SVM的解的方法是一致的。</p><p>现在我们已经有了Standard Support Vector Regression的初始形式，这还是不是一个标准的QP问题。我们继续对该表达式做一些转化和推导：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-8.png" alt="img"></p><p>如上图右边所示，即为标准的QP问题，其中$\xi_n^{\vee}$和$\xi_n^{\wedge}$分别表示lower tube violations和uppertube violations。这种形式叫做Support Vector Regression（SVR） primal。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-9.png" alt="img"></p><p>SVR的标准QP形式包含几个重要的参数：$C$和$\epsilon$。$C$表示的是regularization和tube violation之间的权衡。large $C$倾向于tube violation，small $C$则倾向于regularization。$\epsilon$表征了tube的区域宽度，即对错误点的容忍程度。$\epsilon$越大，则表示对错误的容忍度越大。$\epsilon$是可设置的常数，是SVR问题中独有的，SVM中没有这个参数。另外，SVR的QP形式共有$\hat{d}+1+2N$个参数，$2N+2N$个条件。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-7.png" alt="img"></p><h2 id="3-Support-Vector-Regression-Dual"><a href="#3-Support-Vector-Regression-Dual" class="headerlink" title="3. Support Vector Regression Dual"></a>3. Support Vector Regression Dual</h2><p>现在我们已经得到了SVR的primal形式，接下来将推导SVR的Dual形式。首先，与SVM对偶形式一样，先令拉格朗日因子$\alpha^{\vee}$和$\alpha^{\wedge}$，分别是与$\xi_n^{\vee}$和$\xi_n^{\wedge}$不等式相对应。这里忽略了与$\xi_n^{\vee}\geq0$和$\xi_n^{\wedge}\geq0$对应的拉格朗日因子。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-4.png" alt="img"></p><p>然后，与SVM一样做同样的推导和化简，拉格朗日函数对相关参数偏微分为零，得到相应的KKT条件：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/20-3.png" alt="img"></p><p>接下来，通过观察SVM primal与SVM dual的参数对应关系，直接从SVR primal推导出SVR dual的形式。（具体数学推导此处忽略）</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/21-3.png" alt="img"></p><p>最后，我们就要来讨论一下SVR的解是否真的是sparse的。前面已经推导了SVR dual形式下推导的解$w$为：</p><script type="math/tex; mode=display">w=\sum_{n=1}^N(\alpha_n^{\wedge}-\alpha_n^{\vee})z_n</script><p>相应的complementary slackness为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/22-2.png" alt="img"></p><p>对于分布在tube中心区域内的点，满足$|w^Tz_n+b-y_n|\lt \epsilon$，此时忽略错误，$\xi_n^{\vee}$和$\xi_n^{\wedge}$都等于零。则complementary slackness两个等式的第二项均不为零，必然得到$\alpha_n^{\wedge}=0$和$\alpha_n^{\vee}=0$，即$\beta_n=\alpha_n^{\wedge}-\alpha_n^{\vee}=0$。</p><p>所以，对于分布在tube内的点，得到的解$\beta_n=0$，是sparse的。而分布在tube之外的点，$\beta_n\neq0$。至此，我们就得到了SVR的sparse解。</p><h2 id="4-Summary-of-Kernel-Models"><a href="#4-Summary-of-Kernel-Models" class="headerlink" title="4. Summary of Kernel Models"></a>4. Summary of Kernel Models</h2><p>这部分将对我们介绍过的所有的kernel模型做个概括和总结。我们总共介绍过三种线性模型，分别是PLA/pocket，regularized logistic regression和linear ridge regression。这三种模型都可以使用国立台湾大学的Chih-Jen Lin博士开发的Liblinear库函数来解决。</p><p>另外，我们介绍了linear soft-margin SVM，其中的error function是$\hat{err}_{svm}$，可以通过标准的QP问题来求解。linear soft-margin SVM和PLA/pocket一样都是解决同样的问题。然后，还介绍了linear SVR问题，它与linear ridge regression一样都是解决同样的问题，从SVM的角度，使用$err_{tube}$，转换为QP问题进行求解，这也是我们本节课的主要内容。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/23.png" alt="img"></p><p>上图中相应的模型也可以转化为dual形式，引入kernel，整体的框图如下：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/24-1.png" alt="img"></p><p>其中SVM，SVR和probabilistic SVM都可以使用国立台湾大学的Chih-Jen Lin博士开发的LLibsvm库函数来解决。通常来说，这些模型中<strong>SVR和probabilistic SVM最为常用</strong>。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h2><p>本节课主要介绍了SVR，我们先通过representer theorem理论，将ridge regression转化为kernel的形式，即kernel ridge regression，并推导了SVR的解。但是得到的解是dense的，大部分为非零值。</p><p>所以，我们定义新的tube regression，使用SVM的推导方法，来最小化regularized tube errors，转化为对偶形式，得到了sparse的解。</p><p>最后，我们对介绍过的所有kernel模型做个总结，简单概述了各自的特点。在实际应用中，我们要根据不同的问题进行合适的模型选择。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——5.Kernel Logistic Regression</title>
      <link href="/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-5.kernel-logistic-regression/"/>
      <url>/2020/04/20/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-5.kernel-logistic-regression/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-5-Kernel-Logistic-Regression"><a href="#Lecture-5-Kernel-Logistic-Regression" class="headerlink" title="Lecture 5: Kernel Logistic Regression"></a>Lecture 5: Kernel Logistic Regression</h1><p>上节课我们主要介绍了Soft-Margin SVM，即如果允许有分类错误的点存在，那么在原来的Hard-Margin SVM中添加新的惩罚因子$C$，修正原来的公式，得到新的$\alpha_n$值。最终的到的$\alpha_n$有个上界，上界就是$C$。Soft-Margin SVM权衡了large-margin和error point之前的关系，目的是在尽可能犯更少错误的前提下，得到最大分类边界。本节课将把Soft-Margin SVM和我们之前介绍的Logistic Regression联系起来，研究如何使用kernel技巧来解决更多的问题。</p><h2 id="1-Soft-Margin-SVM-as-Regularized-Model"><a href="#1-Soft-Margin-SVM-as-Regularized-Model" class="headerlink" title="1. Soft-Margin SVM as Regularized Model"></a>1. Soft-Margin SVM as Regularized Model</h2><p>因为Soft-Margin Dual SVM更加灵活、便于调整参数，所以在实际应用中，使用Soft-Margin Dual SVM来解决分类问题的情况更多一些。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-11.png" alt="img"></p><p>Soft-Margin Dual SVM有两个应用非常广泛的工具包，分别是Libsvm和Liblinear。</p><p>下面我们再来回顾一下Soft-Margin SVM的主要内容。我们的出发点是用$\xi_n$来表示margin violation，即犯错值的大小，没有犯错对应的$\xi_n=0$。然后将有条件问题转化为对偶dual形式，使用QP来得到最佳化的解。</p><p>从另外一个角度来看，$\xi_n$描述的是点$(x_n,y_n)$距离$y_n(w^Tz_n+b)=1$的边界有多远：</p><ol><li>第一种情况是violating margin，即不满足$y_n(w^Tz_n+b)\geq1$。那么$\xi_n$可表示为：$\xi_n=1-y_n(w^Tz_n+b)\gt0$。</li><li>第二种情况是not violating margin，满足$y_n(w^Tz_n+b)\geq1$的条件，此时$\xi_n=0$。</li></ol><p>我们可以将两种情况整合到一个表达式中，对任意点：</p><script type="math/tex; mode=display">\xi_n=\max(1-y_n(w^Tz_n+b),0)</script><p>上式表明，如果有voilating margin，则$1-y_n(w^Tz_n+b)\gt0$，$\xi_n=1-y_n(w^Tz_n+b)$；如果not violating margin，则$1-y_n(w^Tz_n+b)\lt0$，$\xi_n=0$。整合之后，我们可以把Soft-Margin SVM的最小化问题写成如下形式：</p><script type="math/tex; mode=display">\frac12w^Tw+C\sum_{n=1}^N\max(1-y_n(w^Tz_n+b),0)</script><p>经过这种转换之后，表征犯错误值大小的变量$\xi_n$就被消去了，转而由一个$\max$操作代替。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-10.png" alt="img"></p><p>为什么要将把Soft-Margin SVM转换为这种unconstrained form呢？我们再来看一下转换后的形式，其中包含两项，第一项是$w$的内积，第二项关于$y$和$w,b,z$的表达式，似乎有点像一种错误估计$\hat{err}$，则类似这样的形式：</p><script type="math/tex; mode=display">\min \frac12w^Tw+C\sum\hat{err}</script><p>看到这样的形式我们应该很熟悉，因为之前介绍的L2 Regularization中最优化问题的表达式跟这个是类似的：</p><script type="math/tex; mode=display">\min \frac{\lambda}{N}w^Tw+\frac1N\sum err</script><p>既然unconstrained form SVM与L2 Regularization的形式是一致的，而且L2 Regularization的解法我们之前也介绍过，那么为什么不直接利用这种方法来解决unconstrained form SVM的问题呢？有两个原因：</p><ol><li>这种无条件的最优化问题<strong>无法通过QP解决</strong>，即对偶推导和kernel都无法使用；</li><li>这种形式中包含的max()项可能造成函数<strong>并不是处处可导</strong>，这种情况难以用微分方法解决。</li></ol><p>Hard-Margin SVM与Regularization Model是有关系的：</p><ol><li>Regularization的目标是最小化$E_{in}$，条件是$w^Tw\leq C$</li><li>Hard-Margin SVM的目标是最小化$w^Tw$，条件是$E_{in}=0$</li></ol><p>即它们的<strong>最小化目标和限制条件是相互对调的</strong>。对于L2 Regularization来说，条件和最优化问题结合起来，整体形式写成：</p><script type="math/tex; mode=display">\frac{\lambda}{N}w^Tw+E_{in}</script><p>而对于Soft-Margin SVM来说，条件和最优化问题结合起来，整体形式写成：</p><script type="math/tex; mode=display">\frac12w^Tw+CN\hat{E_{in}}</script><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-8.png" alt="img"></p><p>通过对比，我们发现L2 Regularization和Soft-Margin SVM的形式是相同的，两个式子分别包含了参数$\lambda$和$C$。Soft-Margin SVM中的large margin对应着L2 Regularization中的short $w$，也就是都让hyperplanes更简单一些。我们使用特别的$\hat{err}$来代表可以容忍犯错误的程度，即soft margin。L2 Regularization中的$\lambda$和Soft-Margin SVM中的$C$也是相互对应的，$\lambda$越大，$w$会越小，Regularization的程度就越大；$C$越小，$\hat{E_{in}}$会越大，相应的margin就越大。</p><p>所以说<strong>增大$C$，或者减小$\lambda$，效果是一致的，Large-Margin等同于Regularization，都起到了防止过拟合的作用</strong>。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-8.png" alt="img"></p><p>建立了Regularization和Soft-Margin SVM的关系，接下来我们将尝试看看是否能把SVM作为一个regularized的模型进行扩展，来解决其它一些问题。</p><h2 id="2-SVM-versus-Logistic-Regression"><a href="#2-SVM-versus-Logistic-Regression" class="headerlink" title="2. SVM versus Logistic Regression"></a>2. SVM versus Logistic Regression</h2><p>上一小节，我们已经把Soft-Margin SVM转换成无条件的形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-8.png" alt="img"></p><p>上式中第二项的$\max(1-y_n(w^Tz_n+b),0)$被设置为$\hat{err}$。下面来看$\hat{err}$与之前再二元分类中介绍过的$err_{0/1}$有什么关系。</p><ol><li>对于$err_{0/1}$，linear score $s=w^Tz_n+b$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1$，呈阶梯状。</li><li>对于$\hat{err}$，当$ys\geq0$时，$err_{0/1}=0$；当$ys&lt;0$时，$err_{0/1}=1-ys$，呈折线状。通常把$\hat{err}_{svm}$称为<strong>hinge error measure</strong>（合页损失函数）。</li></ol><p>比较两条error曲线，我们发现$\hat{err}_{svm}$始终在$err_{0/1}$的上面，则$\hat{err}_{svm}$可作为$err_{0/1}$的上界。所以，可以使用$\hat{err}_{svm}$来代替$err_{0/1}$，解决二元线性分类问题，而且$\hat{err}_{svm}$是一个凸函数，使它在最佳化问题中有更好的性质。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-9.png" alt="img"></p><p>紧接着，我们再来看一下logistic regression中的error function。逻辑回归中，$err_{sce}=\log_2(1+\exp(-ys))$，当$ys=0$时，$err_{sce}=1$。它的err曲线如下所示。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-9.png" alt="img"></p><p>很明显，$err_{sce}$也是$err_{0/1}$的上界，而$err_{sce}$与$\hat{err}_{svm}$也是比较相近的。因为当$ys$趋向正无穷大的时候，$err_{sce}$与$\hat{err}_{svm}$都趋向于零；当$ys$趋向负无穷大的时候，$err_{sce}$与$\hat{err}_{svm}$都趋向于正无穷大。正因为二者的这种相似性，我们<strong>可以把SVM看成是L2-regularized logistic regression</strong>。</p><p>总结一下，我们已经介绍过几种Binary Classification的Linear Models，包括PLA，Logistic Regression和Soft-Margin SVM。</p><ol><li>PLA是相对简单的一个模型，对应的是$err_{0/1}$，通过不断修正错误的点来获得最佳分类线。它的优点是简单快速，缺点是只对线性可分的情况有用，线性不可分的情况需要用到pocket算法。</li><li>Logistic Regression对应的是$err_{sce}$，通常使用GD/SGD算法求解最佳分类线。它的优点是凸函数$err_{sce}$便于最优化求解，而且有regularization作为避免过拟合的保证；缺点是$err_{sce}$作为$err_{0/1}$的上界，当$ys$很小（负值）时，上界变得更宽松，不利于最优化求解。</li><li>Soft-Margin SVM对应的是$\hat{err}_{svm}$，通常使用QP求解最佳分类线。它的优点和Logistic Regression一样，凸优化问题计算简单而且分类线比较“粗壮”一些；缺点也和Logistic Regression一样，当$ys$很小（负值）时，上界变得过于宽松。其实，Logistic Regression和Soft-Margin SVM都是在最佳化$err_{0/1}$的上界而已。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-9.png" alt="img"></p><p>至此，可以看出，<strong>求解regularized logistic regression的问题相当于求解soft-margin SVM的问题</strong>。反过来，如果我们求解了一个soft-margin SVM的问题，那这个解能否直接为regularized logistic regression所用？来预测结果是正类的几率是多少，就像regularized logistic regression做的一样。我们下一小节将来解答这个问题。</p><h2 id="3-SVM-for-Soft-Binary-Classification"><a href="#3-SVM-for-Soft-Binary-Classification" class="headerlink" title="3. SVM for Soft Binary Classification"></a>3. SVM for Soft Binary Classification</h2><p>接下来，我们探讨如何将SVM的结果应用在Soft Binary Classification中，得到是正类的概率值。</p><ol><li>第一种简单的方法是先得到SVM的解$(b_{svm},w_{svm})$，然后直接代入到logistic regression中，得到$g(x)=\theta(w_{svm}^Tx+b_{svm})$。这种方法直接使用了SVM和logistic regression的相似性，一般情况下表现还不错。但是，这种形式过于简单，与logistic regression的关联不大，没有使用到logistic regression中好的性质和方法。</li><li>第二种简单的方法是同样先得到SVM的解$(b_{svm},w_{svm})$，然后把$(b_{svm},w_{svm})$作为logistic regression的<strong>初始值</strong>，再进行迭代训练修正，速度比较快，最后，将得到的$b$和$w$代入到$g(x)$中。这种做法有点显得多此一举，因为并没有比直接使用logistic regression快捷多少。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-9.png" alt="img"></p><p>这两种方法都没有融合SVM和logistic regression各自的优势，下面构造一个模型，融合了二者的优势。构造的模型g(x)表达式为：</p><script type="math/tex; mode=display">g(x)=\theta(A\cdot(w_{svm}^T\Phi(x)+b_{svm})+B)</script><p>与上述第一种简单方法不同，我们<strong>额外增加了放缩因子$A$和平移因子$B$。</strong>首先利用SVM的解$(b_{svm},w_{svm})$来构造这个模型，放缩因子$A$和平移因子$B$是待定系数。然后再用通用的logistic regression优化算法，通过迭代优化，得到最终的$A$和$B$。一般来说，如果$(b_{svm},w_{svm})$较为合理的话，满足$A$&gt;0且$B\approx0$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-8.png" alt="img"></p><p>那么，新的logistic regression表达式为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-8.png" alt="img"></p><p>这个表达式看上去很复杂，其实其中的$(b_{svm},w_{svm})$已经在SVM中解出来了，实际上的未知参数只有$A$和$B$两个。归纳一下，这种Probabilistic SVM的做法分为三个步骤：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-8.png" alt="img"></p><p>这种soft binary classifier方法得到的结果跟直接使用SVM classifier得到的结果可能不一样，这是因为我们引入了系数$A$和$B$。一般来说，soft binary classifier效果更好。至于logistic regression的解法，可以选择GD、SGD等等。</p><h2 id="4-Kernel-Logistic-Regression"><a href="#4-Kernel-Logistic-Regression" class="headerlink" title="4. Kernel Logistic Regression"></a>4. Kernel Logistic Regression</h2><p>上一小节我们介绍的是通过kernel SVM在$z$空间中求得logistic regression的近似解。如果我们希望直接在$z$空间中直接求解logistic regression，通过引入kernel，来解决最优化问题，又该怎么做呢？SVM中使用kernel，转化为QP问题，进行求解，但是logistic regression却不是个QP问题，看似好像没有办法利用kernel来解决。</p><p>我们先来看看之前介绍的kernel trick为什么会work，kernel trick就是把$z$空间的内积转换到$x$空间中比较容易计算的函数。如果$w$可以表示为$z$的线性组合，即$w_<em>=\sum_{n=1}^N\beta_nz_n$的形式，那么乘积项$w_</em>^Tz=\sum_{n=1}^N\beta_nz_n^Tz=\sum_{n=1}^N\beta_nK(x_n,x)$，即其中包含了$z$的内积。也就是$w$可以表示为$z$的线性组合是kernel trick可以work的关键。</p><p>我们之前介绍过SVM、PLA、logistic regression都可以表示成$z$的线性组合，这也提供了一种可能，就是将kernel应用到这些问题中去，简化z空间的计算难度。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-8.png" alt="img"></p><p>claim：对于L2-regularized linear model，若它的最小化问题形式为如下的话，那么最优解$w_*=\sum_{n=1}^N\beta_nz_n$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-7.png" alt="img"></p><blockquote><p>下面给出简单的证明：</p><p>假如最优解$w_*=w_{||}+w_{\bot}$。其中，$w_{||}$和$w_{\bot}$分别是平行$z$空间和垂直$z$空间的部分。我们需要证明的是$w_{\bot}=0$。</p><p>利用反证法，假如$w_{\bot}\neq0$，考虑$w_*$与$w_{||}$的比较。</p><ol><li>比较第二项：$err(y,w_*^Tz_n)=err(y_n,(w_{||}+w_{\bot})^Tz_n=err(y_n,w_{||}^Tz_n)$，即第二项是相等的。</li><li>比较第一项：$w_<em>^Tw_</em>=w_{||}^Tw_{||}+2w_{||}^Tw_{\bot}+w_{\bot}^Tw_{\bot}\gt w_{||}^Tw_{||}$，即$w_<em>$对应的L2-regularized linear model值要比$w_{||}$大，这就说明$w_</em>$并不是最优解，从而证明$w_{\bot}$必然等于零，即$w_<em>=\sum_{n=1}^N\beta_nz_n$一定成立，$w_</em>$一定可以写成$z$的线性组合形式。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-7.png" alt="img"></p></blockquote><p>经过证明和分析，我们得到了结论是<strong>任何L2-regularized linear model都可以使用kernel来解决</strong>。</p><p>现在，我们来看看如何把kernel应用在L2-regularized logistic regression上。上面我们已经证明了$w_<em>$一定可以写成$z$的线性组合形式，即$w_</em>=\sum_{n=1}^N\beta_nz_n$。那么我们就无需一定求出$w_<em>$，而只要求出其中的$\beta_n$就行了。怎么求呢？直接将$w_</em>=\sum_{n=1}^N\beta_nz_n$代入到L2-regularized logistic regression最小化问题中，得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-8.png" alt="img"></p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-6.png" alt="img"></p><p>上式中，所有的$w$项都换成$\beta_n$来表示了，变成了没有条件限制的最优化问题。我们把这种问题称为kernel logistic regression，即引入kernel，将求w的问题转换为求$\beta_n$的问题。</p><p>从另外一个角度来看Kernel Logistic Regression（KLR）：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-3.png" alt="img"></p><p>上式中log项里的$\sum_{m=1}^N\beta_mK(x_m,x_n)$可以看成是变量$\beta$和$K(x_m,x_n)$的内积。上式第一项中的$\sum_{n=1}^N\sum_{m=1}^N\beta_n\beta_mK(x_n,x_m)$可以看成是关于$\beta$的正则化项$\beta^TK\beta$。所以，KLR是$\beta$的线性组合，其中包含了kernel内积项和kernel regularizer。这与SVM是相似的形式。</p><p>但值得一提的是，KLR中的$\beta_n$与SVM中的$\alpha_n$是有区别的。SVM中的$\alpha_n$大部分为零，SV的个数通常是比较少的；而KLR中的$\beta_n$通常都是非零值。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了Kernel Logistic Regression。</p><p>首先把Soft-Margin SVM解释成Regularized Model，建立二者之间的联系，其实Soft-Margin SVM就是一个L2-regularization，对应着hinge error messure。</p><p>然后利用它们之间的相似性，讨论了如何利用SVM的解来得到Soft Binary Classification。方法是先得到SVM的解，再在logistic regression中引入参数$A$和$B$，迭代训练，得到最佳解。</p><p>最后介绍了Kernel Logistic Regression，证明L2-regularized logistic regression中，最佳解$w_*$一定可以写成$z$的线性组合形式，从而可以将kernel引入logistic regression中，使用kernel思想在$z$空间直接求解L2-regularized logistic regression问题。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——4.Soft-margin SVM</title>
      <link href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-4.soft-margin-svm/"/>
      <url>/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-4.soft-margin-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-4-Soft-margin-SVM"><a href="#Lecture-4-Soft-margin-SVM" class="headerlink" title="Lecture 4: Soft-margin SVM"></a>Lecture 4: Soft-margin SVM</h1><p>上节课我们主要介绍了Kernel SVM。先将特征转换和计算内积这两个步骤合并起来，简化计算、提高计算速度，再用Dual SVM的求解方法来解决。Kernel SVM不仅能解决简单的线性分类问题，也可以求解非常复杂甚至是无限多维的分类问题，关键在于核函数的选择，例如线性核函数、多项式核函数和高斯核函数等等。但是，<strong>我们之前讲的这些方法都是Hard-Margin SVM，即必须将所有的样本都分类正确才行</strong>。这往往需要更多更复杂的特征转换，甚至造成过拟合。<strong>本节课将介绍一种Soft-Margin SVM，目的是让分类错误的点越少越好，而不是必须将所有点分类正确，也就是允许有noise存在</strong>。这种做法很大程度上不会使模型过于复杂，不会造成过拟合，而且分类效果是令人满意的。</p><h2 id="1-Motivation-and-Primal-Problem"><a href="#1-Motivation-and-Primal-Problem" class="headerlink" title="1. Motivation and Primal Problem"></a>1. Motivation and Primal Problem</h2><p>SVM可能会造成overfit，原因有两个：</p><ol><li>由于SVM模型（即kernel）过于复杂，转换的维度太多，过于powerful了；</li><li>由于我们坚持要将所有的样本都分类正确，即不允许错误存在，造成模型过于复杂。如下图所示，左边的图$\Phi_1$是线性的，虽然有几个点分类错误，但是大部分都能完全分开。右边的图$\Phi_4$是四次多项式，所有点都分类正确了，但是模型比较复杂，可能造成过拟合。直观上来说，左边的图是更合理的模型。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-9.png" alt="img"></p><p><strong>如何避免过拟合？</strong>方法是允许有分类错误的点，即把某些点当作是noise，放弃这些noise点，但是尽量让这些noise个数越少越好。回顾一下我们在机器学习基石笔记中介绍的pocket算法，pocket的思想不是将所有点完全分开，而是找到一条分类线能让分类错误的点最少。而Hard-Margin SVM的目标是将所有点都完全分开，不允许有错误点存在。为了防止过拟合，我们可以借鉴pocket的思想，即<strong>允许有犯错误的点，目标是让这些点越少越好</strong>。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-8.png" alt="img"></p><p>为了引入允许犯错误的点，我们将Hard-Margin SVM的目标和条件做一些结合和修正，转换为如下形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-6.png" alt="img"></p><p>修正后的条件中，对于分类正确的点，仍需满足$y_n(w^Tz_n+b)\geq 1$，而对于noise点，满足$y_n(w^Tz_n+b)\geq -\infty$，即没有限制。修正后的目标除了$\frac12w^Tw$项，还添加了$y_n\neq sign(w^Tz_n+b)$，即noise点的个数。参数C的引入是为了权衡目标第一项和第二项的关系，即<strong>权衡large margin和noise tolerance的关系</strong>。</p><p>我们再对上述的条件做修正，将两个条件合并，得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-6.png" alt="img"></p><p>这个式子存在两个不足的地方：</p><ol><li>最小化目标中第二项是非线性的，不满足QP的条件，所以无法使用dual或者kernel SVM来计算。</li><li>对于犯错误的点，有的离边界很近，即error小，而有的离边界很远，error很大，上式的条件和目标没有区分small error和large error。这种分类效果是不完美的。</li></ol><p>为了改正这些不足，我们继续做如下修正：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-6.png" alt="img"></p><p>修正后的表达式中，我们引入了新的参数$\xi_n$来表示每个点犯错误的程度值，$\xi_n\geq0$。通过使用error值的大小代替是否有error，让问题变得易于求解，满足QP形式要求。这种方法类似于我们在机器学习基石笔记中介绍的0/1 error和squared error。这种soft-margin SVM引入新的参数$\xi$。</p><p>至此，最终的Soft-Margin SVM的目标为：</p><script type="math/tex; mode=display">\min_{b,w,\xi}\ \frac12w^Tw+C\cdot\sum_{n=1}^N\xi_n</script><p>条件是：</p><script type="math/tex; mode=display">y_n(w^Tz_n+b)\geq 1-\xi_n\\\xi_n\geq0</script><p>其中，$\xi_n$表示每个点犯错误的程度，$\xi_n=0$，表示没有错误，$\xi_n$越大，表示错误越大，即点距离边界（负的）越大。参数$C$表示尽可能选择宽边界和尽可能不要犯错两者之间的权衡，因为边界宽了，往往犯错误的点会增加。large $C$表示希望得到更少的分类错误，即不惜选择窄边界也要尽可能把更多点正确分类；small $C$表示希望得到更宽的边界，即不惜增加错误点个数也要选择更宽的分类边界。</p><p>与之对应的QP问题中，由于新的参数$\xi_n$的引入，总共参数个数为$\hat d+1+N$，限制条件添加了$\xi_n\geq0$，则总条件个数为$2N$。</p><h2 id="2-Dual-Problem"><a href="#2-Dual-Problem" class="headerlink" title="2. Dual Problem"></a>2. Dual Problem</h2><p>接下来，我们将推导Soft-Margin SVM的对偶dual形式，从而让QP计算更加简单，并便于引入kernel算法。首先，我们把Soft-Margin SVM的原始形式写出来：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-7.png" alt="img"></p><p>然后，跟我们在第二节课中介绍的Hard-Margin SVM做法一样，构造一个拉格朗日函数。因为引入了$\xi_n$，原始问题有两类条件，所以包含了两个拉格朗日因子$\alpha_n$和$\beta_n$。拉格朗日函数可表示为如下形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-7.png" alt="img"></p><p>接下来，我们跟第二节课中的做法一样，利用Lagrange dual problem，将Soft-Margin SVM问题转换为如下形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-7.png" alt="img"></p><p>根据之前介绍的KKT条件，我们对上式进行简化。上式括号里面的是对拉格朗日函数$L(b,w,\xi,\alpha,\beta)$计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。</p><p>我们先对$\xi_n$做偏微分：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial \xi_n}=0=C-\alpha_n-\beta_n</script><p>根据上式，得到$\beta_n=C-\alpha_n$，因为有$\beta_n\geq0$，所以限制$0\leq\alpha_n\leq C$。将$\beta_n=C-\alpha_n$代入到dual形式中并化简，我们发现$\beta_n$和$\xi_n$都被消去了：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-6.png" alt="img"></p><p>这个形式跟Hard-Margin SVM中的dual形式是基本一致的，只是条件不同。那么，我们分别令拉格朗日函数$L$对$b$和$w$的偏导数为零，分别得到：</p><script type="math/tex; mode=display">\sum_{n=1}^N\alpha_ny_n=0\\w=\sum_{n=1}^N\alpha_ny_nz_n</script><p>经过化简和推导，最终标准的Soft-Margin SVM的Dual形式如下图所示：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-6.png" alt="img"></p><p>Soft-Margin SVM Dual与Hard-Margin SVM Dual基本一致，只有一些条件不同。Hard-Margin SVM Dual中$\alpha_n\geq0$，而Soft-Margin SVM Dual中$0\leq\alpha_n\leq C$，且新的拉格朗日因子$\beta_n=C-\alpha_n$。在QP问题中，Soft-Margin SVM Dual的参数$\alpha_n$同样是$N$个，但是，条件由Hard-Margin SVM Dual中的$N+1$个变成$2N+1$个，这是因为多了$N$个$\alpha_n$的上界条件。</p><h2 id="3-Messages-behind-Soft-Margin-SVM"><a href="#3-Messages-behind-Soft-Margin-SVM" class="headerlink" title="3. Messages behind Soft-Margin SVM"></a>3. Messages behind Soft-Margin SVM</h2><p>推导完Soft-Margin SVM Dual的简化形式后，就可以利用QP，找到$Q,p,A,c$对应的值，用软件工具包得到$\alpha_n$的值。或者利用核函数的方式，同样可以简化计算，优化分类效果。Soft-Margin SVM Dual计算$\alpha_n$的方法过程与Hard-Margin SVM Dual的过程是相同的。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-7.png" alt="img"></p><p>但是如何根据$\alpha_n$的值计算$b$呢？在Hard-Margin SVM Dual中，有complementary slackness条件：$\alpha_n(1-y_n(w^Tz_n+b))=0$，找到SV，即$\alpha_s&gt;0$的点，计算得到$b=y_s-w^Tz_s$。</p><p>那么，在Soft-Margin SVM Dual中，相应的complementary slackness条件有两个（因为两个拉格朗日因子$\alpha_n$和$\beta_n$）：</p><script type="math/tex; mode=display">\alpha_n(1-\xi_n-y_n(w^Tz_n+b))=0\\\beta_n\xi_n=(C-\alpha_n)\xi=0</script><p>找到SV，即$\alpha_s&gt;0$的点，由于参数$\xi_n$的存在，还不能完全计算出$b$的值。根据第二个complementary slackness条件，如果令$C-\alpha_n\neq0$，即$\alpha_n\neq C$，则一定有$\xi_n=0$，代入到第一个complementary slackness条件，即可计算得到$b=y_s-w^Tz_s$。我们把$0&lt;\alpha_s&lt;C$的点称为free SV。引入核函数后，$b$的表达式为：</p><script type="math/tex; mode=display">b=y_s-\sum_{SV}\alpha_ny_nK(x_n,x_s)</script><p>上面求解b提到的一个假设是$\alpha_s&lt;C$，这个假设是否一定满足呢？如果没有free SV，所有$\alpha_s$大于零的点都满足$\alpha_s=C$怎么办？一般情况下，至少存在一组SV使$\alpha_s&lt;C$的概率是很大的。如果出现没有free SV的情况，那么$b$通常会由许多不等式条件限制取值范围，值是不确定的，只要能找到其中满足KKT条件的任意一个$b$值就可以了。这部分细节比较复杂，不再赘述。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-6.png" alt="img"></p><p>接下来，我们看看$C$取不同的值对margin的影响。例如，对于Soft-Margin Gaussian SVM，$C$分别取1，10，100时，相应的margin如下图所示：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-5.png" alt="img"></p><p>从上图可以看出，$C=1$时，margin比较粗，但是分类错误的点也比较多，当$C$越来越大的时候，margin越来越细，分类错误的点也在减少。正如前面介绍的，$C$值反映了margin和分类正确的一个权衡。$C$越小，越倾向于得到粗的margin，宁可增加分类错误的点；$C$越大，越倾向于得到高的分类正确率，宁可margin很细。我们发现，当$C$值很大的时候，虽然分类正确率提高，但很可能把noise也进行了处理，从而可能造成过拟合。也就是说Soft-Margin Gaussian SVM同样可能会出现过拟合现象，所以参数$(\gamma,C)$的选择非常重要。</p><p>我们再来看看$\alpha_n$取不同值是对应的物理意义。已知$0\leq\alpha_n\leq C$满足两个complementary slackness条件：</p><script type="math/tex; mode=display">\alpha_n(1-\xi_n-y_n(w^Tz_n+b))=0\\\beta_n\xi_n=(C-\alpha_n)\xi=0</script><ol><li>若$\alpha_n=0$，得$\xi_n=0$。$\xi_n=0$表示该点没有犯错，$\alpha_n=0$表示该点不是SV。所以对应的点在margin之外（或者在margin上），且均分类正确。</li><li>若$0&lt;\alpha_n&lt;C$，得$\xi_n=0$，且$y_n(w^Tz_n+b)=1$。$\xi_n=0$表示该点没有犯错，$y_n(w^Tz_n+b)=1$表示该点在margin上。这些点即free SV，确定了$b$的值。</li><li>若$\alpha_n=C$，不能确定$\xi_n$是否为零，且得到$1-y_n(w^Tz_n+b)=\xi_n$，这个式表示该点偏离margin的程度，$\xi_n$越大，偏离margin的程度越大。只有当$\xi_n=0$时，该点落在margin上。所以这种情况对应的点在margin之内负方向（或者在margin上），有分类正确也有分类错误的。这些点称为bounded SV。</li></ol><p>所以，在Soft-Margin SVM Dual中，根据$\alpha_n$的取值，就可以推断数据点在空间的分布情况。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-5.png" alt="img"></p><h2 id="4-Model-Selection"><a href="#4-Model-Selection" class="headerlink" title="4. Model Selection"></a>4. Model Selection</h2><p>在Soft-Margin SVM Dual中，kernel的选择、$C$等参数的选择都非常重要，直接影响分类效果。例如，对于Gaussian SVM，不同的参数$(C,\gamma)$，会得到不同的margin，如下图所示。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-6.png" alt="img"></p><p>其中横坐标是$C$逐渐增大的情况，纵坐标是$\gamma$逐渐增大的情况。不同的$(C,\gamma)$组合，margin的差别很大。那么如何选择最好的$(C,\gamma)$等参数呢？最简单最好用的工具就是<strong>validation</strong>。</p><p>validation我们在机器学习基石课程中已经介绍过，只需要将由不同$(C,\gamma)$等参数得到的模型在验证集上进行cross validation，选取$E_{cv}$最小的对应的模型就可以了。例如上图中各种$(C,\gamma)$组合得到的$E_{cv}$如下图所示：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-4.png" alt="img"></p><p>因为左下角的$E_{cv}(C,\gamma)$最小，所以就选择该$(C,\gamma)$对应的模型。通常来说，$E_{cv}(C,\gamma)$并不是$(C,\gamma)$的连续函数，很难使用最优化选择（例如梯度下降）。一般做法是选取不同的离散的$(C,\gamma)$值进行组合，得到最小的$E_{cv}(C,\gamma)$，其对应的模型即为最佳模型。这种算法就是我们之前在机器学习基石中介绍过的V-Fold cross validation，在SVM中使用非常广泛。</p><p>V-Fold cross validation的一种极限就是Leave-One-Out CV，也就是验证集只有一个样本。对于SVM问题，它的验证集Error满足：</p><script type="math/tex; mode=display">E_{loocv}\leq \frac{SV}{N}</script><p>也就是说<strong>留一法验证集Error大小不超过支持向量SV占所有样本的比例</strong>。</p><blockquote><p>下面做简单的证明。令样本总数为$N$，对这$N$个点进行SVM分类后得到margin，假设第$N$个点$(x_N,y_N)$的$\alpha_N=0$，不是SV，即远离margin（正距离）。这时候，如果我们只使用剩下的$N-1$个点来进行SVM分类，那么第$N$个点$(x_N,y_N)$必然是分类正确的点，所得的SVM margin跟使用$N$个点的到的是完全一致的。这是因为我们假设第$N$个点是non-SV，对SV没有贡献，不影响margin的位置和形状。所以前$N-1$个点和$N$个点得到的margin是一样的。</p><p>那么，对于non-SV的点，它的$g^-=g$，即对第$N$个点，它的Error必然为零：</p><script type="math/tex; mode=display">e_{non-SV}=err(g^-,non-SV)=err(g,non-SV)=0</script><p>另一方面，假设第$N$个点$\alpha_N\neq0$，即对于SV的点，它的Error可能是0，也可能是1，必然有：</p><script type="math/tex; mode=display">e_{SV}\leq1</script><p>综上所述，即证明了$E_{loocv}\leq \frac{SV}{N}$。这符合我们之前得到的结论，即只有SV影响margin，non-SV对margin没有任何影响，可以舍弃。</p></blockquote><p>SV的数量在SVM模型选择中也是很重要的。一般来说，SV越多，表示模型可能越复杂，越有可能会造成过拟合。所以，通常选择SV数量较少的模型，然后在剩下的模型中使用cross-validation，比较选择最佳模型。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了Soft-Margin SVM。我们的出发点是与Hard-Margin SVM不同，不一定要将所有的样本点都完全分开，允许有分类错误的点，而使margin比较宽。然后，我们增加了$\xi_n$作为分类错误的惩罚项，根据之前介绍的Dual SVM，推导出了Soft-Margin SVM的QP形式。得到的$\alpha_n$除了要满足大于零，还有一个上界$C$。接着介绍了通过$\alpha_n$值的大小，可以将数据点分为三种：non-SVs，free SVs，bounded SVs，这种更清晰的物理解释便于数据分析。最后介绍了如何选择合适的SVM模型，通常的办法是cross-validation和利用SV的数量进行筛选。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——3.Kernel SVM</title>
      <link href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-3.kernel-svm/"/>
      <url>/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-3.kernel-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-3-Kernel-SVM"><a href="#Lecture-3-Kernel-SVM" class="headerlink" title="Lecture 3: Kernel SVM"></a>Lecture 3: Kernel SVM</h1><p>上节课我们主要介绍了SVM的对偶形式，即dual SVM。Dual SVM也是一个二次规划问题，可以用QP来进行求解。之所以要推导SVM的对偶形式是因为：首先，它展示了SVM的几何意义；然后，从计算上，求解过程“好像”与所在维度$\hat d$无关，规避了$\hat d$很大时难以求解的情况。但是，上节课的最后，我们也提到dual SVM的计算过程其实跟$\hat d$还是有关系的。那么，能不能完全摆脱对$\hat d$的依赖，从而减少SVM计算量呢？这就是我们本节课所要讲的主要内容。</p><h2 id="1-Kernel-Trick"><a href="#1-Kernel-Trick" class="headerlink" title="1. Kernel Trick"></a>1. Kernel Trick</h2><p>dual SVM形式如下：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-8.png" alt="img"></p><p>其中$\alpha$是拉格朗日因子，共$N$个，这是我们要求解的，而条件共有$N+1$个。我们来看向量$Q_D$中的$q_{n,m}=y_ny_mz_n^Tz_m$，看似这个计算与$\hat d$无关，但是$z_n^Tz_m$的内积中不得不引入$\hat d$。也就是说，如果很$\hat d$大，计算$z_n^Tz_m$的复杂度也会很高，同样会影响QP问题的计算效率。可以说，$q_{n,m}=y_ny_mz_n^Tz_m$这一步是计算的瓶颈所在。</p><p>问题的关键在于$z_n^Tz_m$内积求解上。我们知道，$z$是由$x$经过特征转换而来：</p><script type="math/tex; mode=display">z_n^Tz_m=\Phi(x_n)\Phi(x_m)</script><p>如果从$x$空间来看的话，$z_n^Tz_m$分为两个步骤：</p><ol><li>进行特征转换$\Phi(x_n)$和$\Phi(x_m)$；</li><li>计算$\Phi(x_n)$与$\Phi(x_m)$的内积。</li></ol><p>这种先转换再计算内积的方式，必然会引入$\hat d$参数，从而在$\hat d$很大的时候影响计算速度。那么，若把这两个步骤联合起来，是否可以有效地减小计算量，提高计算速度呢？</p><p>我们先来看一个简单的例子，对于二阶多项式转换，各种排列组合为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-7.png" alt="img"></p><p>这里提一下，为了简单起见，我们把$x_0=1$包含进来，同时将二次项$x_1x_2$和$x_2x_1$也包含进来。转换之后再做内积并进行推导，得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-5.png" alt="img"></p><p>其中$x^Tx’$是$x$空间中特征向量的内积。所以，$\Phi_2(x)$与$\Phi_2(x’)$的内积的复杂度由原来的$O(d^2)$变成$O(d)$，只与$x$空间的维度$d$有关，而与$z$空间的维度$\hat d$无关，这正是我们想要的！</p><p>至此，我们发现如果把特征转换和$z$空间计算内积这两个步骤合并起来，有可能会简化计算。因为我们只是推导了二阶多项式会提高运算速度，这个特例并不具有一般推论性。但是，我们还是看到了希望。</p><p>我们把合并特征转换和计算内积这两个步骤的操作叫做Kernel Function，用大写字母$K$表示。例如刚刚讲的二阶多项式例子，它的kernel function为：</p><script type="math/tex; mode=display">K_{\Phi}(x,x’)=\Phi(x)^T\Phi(x’)\\K_{\Phi_2}(x,x’)=1+(x^Tx’)+(x^Tx’)^2</script><p>有了kernel function之后，我们来看看它在SVM里面如何使用。在dual SVM中，二次项系数$q_{n,m}$中有$z$的内积计算，就可以用kernel function替换：</p><script type="math/tex; mode=display">q_{n,m}=y_ny_mz_n^Tz_m=y_ny_mK(x_n,x_m)</script><p>所以，直接计算出$K(x_n,x_m)$，再代入上式，就能得到$q_{n,m}$的值。</p><p>$q_{n,m}$值计算之后，就能通过QP得到拉格朗日因子$\alpha_n$。然后，下一步就是计算$b$（取$\alpha_n&gt;0$的点，即SV），$b$的表达式中包含$z$，可以作如下推导：</p><script type="math/tex; mode=display">b=y_s-w^Tz_s=y_s-(\sum_{n=1}^N\alpha_ny_nz_n)^Tz_s=y_s-\sum_{n=1}^N\alpha_ny_n(K(x_n,x_s))</script><p>这样得到的$b$就可以用kernel function表示，而与$z$空间无关。</p><p>最终我们要求的矩$g_{SVM}$可以作如下推导：</p><script type="math/tex; mode=display">g_{SVM}(x)=sign(w^T\Phi(x)+b)=sign((\sum_{n=1}^N\alpha_ny_nz_n)^Tz+b)=sign(\sum_{n=1}^N\alpha_ny_n(K(x_n,x))+b)</script><p>至此，dual SVM中我们所有需要求解的参数都已经得到了，而且整个计算过程中都没有在$z$空间作内积，即与$z$无关。我们把这个过程称为<strong>kernel trick</strong>，也就是把特征转换和计算内积两个步骤结合起来，用kernel function来避免计算过程中受$\hat d$的影响，从而提高运算速度。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-5.png" alt="img"></p><p>那么总结一下，引入kernel funtion后，SVM算法变成：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-5.png" alt="img"></p><p>分析每个步骤的时间复杂度为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-5.png" alt="img"></p><p>我们把这种引入kernel function的SVM称为kernel SVM，它是基于dual SVM推导而来的。kernel SVM同样只用SV（$\alpha_n&gt;0$）就能得到最佳分类面，而且整个计算过程中摆脱了$\hat d$的影响，大大提高了计算速度。</p><h2 id="2-Polynomial-Kernel"><a href="#2-Polynomial-Kernel" class="headerlink" title="2. Polynomial Kernel"></a>2. Polynomial Kernel</h2><p>我们刚刚通过一个特殊的二次多项式导出了相对应的kernel，其实二次多项式的kernel形式是多种的。例如，相应系数的放缩构成完全平方公式等。下面列举了几种常用的二次多项式kernel形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-6.png" alt="img"></p><p>比较一下，第一种$\Phi_2(x)$（蓝色标记）和第三种$\Phi_2(x)$（绿色标记）从某种角度来说是一样的，因为都是二次转换，对应到同一个$z$空间。但是，它们系数不同，内积就会有差异，那么就代表有不同的距离，最终可能会得到不同的SVM margin。所以，<strong>系数不同，可能会得到不同的SVM分界线</strong>。通常情况下，第三种$\Phi_2(x)$（绿色标记）简单一些，更加常用。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-6.png" alt="img"></p><p>不同的转换，对应到不同的几何距离，得到不同的距离，这是什么意思呢？举个例子，对于我们之前介绍的一般的二次多项式kernel，它的SVM margin和对应的SV如下图（中）所示。对于上面介绍的完全平方公式形式，自由度$\gamma=0.001$，它的SVM margin和对应的SV如下图（左）所示。比较发现，这种SVM margin比较简单一些。对于自由度$\gamma=1000$，它的SVM margin和对应的SV如下图（右）所示。与前两种比较，margin和SV都有所不同。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-6.png" alt="img"></p><p>通过改变不同的系数，得到不同的SVM margin和SV，如何选择正确的kernel，非常重要。</p><p>归纳一下，引入$\zeta\geq 0$和$\gamma&gt;0$，对于$Q$次多项式一般的kernel形式可表示为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-6.png" alt="img"></p><p>所以，使用高阶的多项式kernel有两个优点：</p><ol><li>得到最大SVM margin，SV数量不会太多，分类面不会太复杂，防止过拟合，减少复杂度</li><li>计算过程避免了对$\hat d$的依赖，大大简化了计算量。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-5.png" alt="img"></p><p>顺便提一下，当多项式阶数$Q=1$时，那么对应的kernel就是线性的，即本系列课程第一节课所介绍的内容。对于linear kernel，计算方法是简单的，而且也是我们解决SVM问题的首选。还记得机器学习基石课程中介绍的奥卡姆剃刀定律（Occam’s Razor）吗？</p><h2 id="3-Gaussian-Kernel"><a href="#3-Gaussian-Kernel" class="headerlink" title="3. Gaussian Kernel"></a>3. Gaussian Kernel</h2><p>刚刚我们介绍的$Q$阶多项式kernel的阶数是有限的，即特征转换的$\hat d$是有限的。但是，如果是无限多维的转换$\Phi(x)$，是否还能通过kernel的思想，来简化SVM的计算呢？答案是肯定的。</p><p>先举个例子，简单起见，假设原空间是一维的，只有一个特征$x$，我们构造一个kernel function为高斯函数：</p><script type="math/tex; mode=display">K(x,x’)=e^{-(x-x’)^2}</script><p>构造的过程正好与二次多项式kernel的相反，利用反推法，先将上式分解并做泰勒展开：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-5.png" alt="img"></p><p>将构造的$K(x,x’)$推导展开为两个$\Phi(x)$和$\Phi(x’)$的乘积，其中：</p><script type="math/tex; mode=display">\Phi(x)=e^{-x^2}\cdot (1,\sqrt \frac{2}{1!}x,\sqrt \frac{2^2}{2!}x^2,\cdots)</script><p>通过反推，我们得到了$\Phi(x)$，$\Phi(x)$是无限多维的，它就可以当成特征转换的函数，且$\hat d$是无限的。这种$\Phi(x)$得到的核函数即为Gaussian kernel。</p><p>更一般地，对于原空间不止一维的情况（$d$&gt;1），引入缩放因子$\gamma\gt0$，它对应的Gaussian kernel表达式为：</p><script type="math/tex; mode=display">K(x,x’)=e^{-\gamma||x-x’||^2}</script><p>那么引入了高斯核函数，将有限维度的特征转换拓展到无限的特征转换中。根据本节课上一小节的内容，由$K$，计算得到$\alpha_n$和$b$，进而得到矩$g_{SVM}$。将其中的核函数$K$用高斯核函数代替，得到：</p><script type="math/tex; mode=display">g_{SVM}(x)=sign(\sum_{SV}\alpha_ny_nK(x_n,x)+b)=sign(\sum_{SV}\alpha_ny_ne^{(-\gamma||x-x_n||^2)}+b)</script><p>通过上式可以看出，$g_{SVM}$有$n$个高斯函数线性组合而成，其中$n$是SV的个数。而且，每个高斯函数的中心都是对应的SV。通常我们也把高斯核函数称为径向基函数（Radial Basis Function, RBF）。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-4.png" alt="img"></p><p>总结一下，kernel SVM可以获得large-margin的hyperplanes，并且可以通过高阶的特征转换使$E_{in}$尽可能地小。kernel的引入大大简化了dual SVM的计算量。而且，Gaussian kernel能将特征转换扩展到无限维，并使用有限个SV数量的高斯函数构造出矩$g_{SVM}$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-5.png" alt="img"></p><p>值得注意的是，缩放因子$\gamma$取值不同，会得到不同的高斯核函数，hyperplanes不同，分类效果也有很大的差异。举个例子，$\gamma$分别取1, 10, 100时对应的分类效果如下：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-4.png" alt="img"></p><p>从图中可以看出，当$\gamma$比较小的时候，分类线比较光滑，当$\gamma$越来越大的时候，分类线变得越来越复杂和扭曲，直到最后，分类线变成一个个独立的小区域，像小岛一样将每个样本单独包起来了。为什么会出现这种区别呢？这是因为$\gamma$越大，其对应的高斯核函数越尖瘦，那么有限个高斯核函数的线性组合就比较离散，分类效果并不好。所以，SVM也会出现过拟合现象，$\gamma$的正确选择尤为重要，不能太大。</p><h2 id="4-Comparison-of-Kernels"><a href="#4-Comparison-of-Kernels" class="headerlink" title="4. Comparison of Kernels"></a>4. Comparison of Kernels</h2><p>目前为止，我们已经介绍了几种kernel，下面来对几种kernel进行比较。</p><p>首先，Linear Kernel是最简单最基本的核，平面上对应一条直线，三维空间里对应一个平面。Linear Kernel可以使用上一节课介绍的Dual SVM中的QP直接计算得到。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-4.png" alt="img"></p><p>Linear Kernel的优点是计算简单、快速，可以直接使用QP快速得到参数值，而且从视觉上分类效果非常直观，便于理解；缺点是如果数据不是线性可分的情况，Linear Kernel就不能使用了。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-5.png" alt="img"></p><p>然后，Polynomial Kernel的hyperplanes是由多项式曲线构成。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-3.png" alt="img"></p><p>Polynomial Kernel的优点是阶数$Q$可以灵活设置，相比linear kernel限制更少，更贴近实际样本分布；缺点是当$Q$很大时，$K$的数值范围波动很大，而且参数个数较多，难以选择合适的值。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-2.png" alt="img"></p><p>对于Gaussian Kernel，表示为高斯函数形式。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/20-2.png" alt="img"></p><p>Gaussian Kernel的优点是边界更加复杂多样，能最准确地区分数据样本，数值计算$K$值波动较小，而且只有一个参数，容易选择；缺点是由于特征转换到无限维度中，$w$没有求解出来，计算速度要低于linear kernel，而且可能会发生过拟合。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/21-2.png" alt="img"></p><p>除了这三种kernel之外，我们还可以使用其它形式的kernel。首先，我们考虑kernel是什么？实际上kernel代表的是两笔资料$x$和$x’$，特征变换后的相似性即内积。但是不能说任何计算相似性的函数都可以是kernel。</p><p>有效的kernel还需满足下面两个条件：</p><ol><li>$K$是对称的</li><li>$K$是半正定的</li></ol><p>这两个条件不仅是必要条件，同时也是充分条件。所以，只要我们构造的$K$同时满足这两个条件，那它就是一个有效的kernel。这被称为<strong>Mercer 定理</strong>。事实上，构造一个有效的kernel是比较困难的。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/22-1.png" alt="img"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h2><p>本节课主要介绍了Kernel Support Vector Machine。首先，我们将特征转换和计算内积的操作合并到一起，消除了$\hat d$的影响，提高了计算速度。然后，分别推导了Polynomial Kernel和Gaussian Kernel，并列举了各自的优缺点并做了比较。对于不同的问题，应该选择合适的核函数进行求解，以达到最佳的分类效果。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——2.Dual SVM</title>
      <link href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-2.dual-svm/"/>
      <url>/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-2.dual-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-2-Dual-SVM"><a href="#Lecture-2-Dual-SVM" class="headerlink" title="Lecture 2: Dual SVM"></a>Lecture 2: Dual SVM</h1><p>上节课我们主要介绍了线性支持向量机（Linear Support Vector Machine）。Linear SVM的目标是找出最“胖”的分割线进行正负类的分离，方法是使用二次规划来求出分类线。本节课将从另一个方面入手，研究对偶支持向量机（Dual Support Vector Machine），尝试从新的角度计算得出分类线，推广SVM的应用范围。</p><h2 id="1-Motivation-of-Dual-SVM"><a href="#1-Motivation-of-Dual-SVM" class="headerlink" title="1. Motivation of Dual SVM"></a>1. Motivation of Dual SVM</h2><p>对于非线性SVM，我们通常可以使用非线性变换将变量从$x$域转换到$z$域中。然后，在$z$域中，根据上一节课的内容，使用线性SVM解决问题即可。上一节课我们说过，使用SVM得到large-margin，减少了有效的VC Dimension，限制了模型复杂度；另一方面，使用特征转换，目的是让模型更复杂，减小$E_{in}$。所以说，<strong>非线性SVM是把这两者目的结合起来，平衡这两者的关系</strong>。</p><p>那么，特征转换下，求解QP问题在$z$域中的维度设为$\hat d +1$，如果模型越复杂，则$\hat d +1$越大，相应求解这个QP问题也变得很困难。当$\hat d$无限大的时候，问题将会变得难以求解，那么有没有什么办法可以解决这个问题呢？一种方法就是<strong>使SVM的求解过程不依赖$\hat d$</strong>，这就是我们本节课所要讨论的主要内容。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-7.png" alt="img"></p><p>比较一下，我们上一节课所讲的Original SVM二次规划问题的变量个数是$\hat d +1$，有$N$个限制条件；而本节课，我们把问题转化为对偶问题（’Equivalent’ SVM），同样是二次规划，只不过变量个数变成$N$个，有$N+1$个限制条件。<strong>这种对偶SVM的好处就是问题只跟$N$有关，与$\hat d$无关</strong>，这样就不存在上文提到的当$\hat d$无限大时难以求解的情况。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-6.png" alt="img"></p><p>如何把问题转化为对偶问题（’Equivalent’ SVM），其中的数学推导非常复杂，本文不做详细数学论证，但是会从概念和原理上进行简单的推导。</p><p>还记得我们在《机器学习基石》课程中介绍的Regularization中，在最小化$E_{in}$的过程中，也添加了限制条件：$w^Tw\leq C$。我们的求解方法是引入拉格朗日因子$\lambda$，将有条件的最小化问题转换为无条件的最小化问题：$\min\ E_{aug}(w)=E_{in}(w)+\frac{\lambda}{N}w^Tw $，最终得到的$w$的最优化解为：</p><script type="math/tex; mode=display">\nabla E_{in}(w)+\frac{2\lambda}{N}w=0</script><p>所以，在regularization问题中，$\lambda$是已知常量，求解过程变得容易。那么，对于dual SVM问题，同样可以引入$\lambda$，将条件问题转换为非条件问题，只不过$\lambda$是未知参数，且个数是$N$，需要对其进行求解。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-4.png" alt="img"></p><p>如何将条件问题转换为非条件问题？</p><p>上一节课我们介绍的SVM中，目标是：$\min \frac12w^Tw$，条件是：$y_n(w^Tz_n+b)\geq 1,\ for\ n=1,2,\cdots,N$。首先，我们令拉格朗日因子为$\alpha_n$（区别于regularization），构造一个函数：</p><script type="math/tex; mode=display">L(b,w,\alpha)=\frac12w^Tw+\sum_{n=1}^N\alpha_n(1-y_n(w^Tz_n+b))</script><p>这个函数右边第一项是SVM的目标，第二项是SVM的条件和拉格朗日因子$\alpha_n$的乘积。我们把这个函数称为拉格朗日函数，其中包含三个参数：$b，w，\alpha_n$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-4.png" alt="img"></p><p>下面，我们利用拉格朗日函数，把SVM构造成一个非条件问题：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-4.png" alt="img"></p><p>该最小化问题中包含了最大化问题，怎么解释呢？首先我们规定拉格朗日因子$\alpha_n\geq0$，根据SVM的限定条件可得：$1-y_n(w^Tz_n+b)\leq0$。</p><ol><li>如果没有达到最优解，即有不满足$1-y_n(w^Tz_n+b)\leq0$的情况，因为$\alpha_n\geq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\geq0$。对于这种大于零的情况，其最大值是无解的。</li><li>如果对于所有的点，均满足$1-y_n(w^Tz_n+b)\leq0$，那么必然有$\sum_n\alpha_n(1-y_n(w^Tz_n+b))\leq0$，则当$\sum_n\alpha_n(1-y_n(w^Tz_n+b))=0$时，其有最大值，最大值就是我们SVM的目标：$\frac12w^Tw$。因此，这种转化为非条件的SVM构造函数的形式是可行的。</li></ol><h2 id="2-Lagrange-Dual-SVM"><a href="#2-Lagrange-Dual-SVM" class="headerlink" title="2. Lagrange Dual SVM"></a>2. Lagrange Dual SVM</h2><p>现在，我们已经将SVM问题转化为与拉格朗日因子$\alpha_n$有关的最大最小值形式。已知$\alpha_n\geq0$，那么对于任何固定的$\alpha’$，且$\alpha_n’\geq0$，一定有如下不等式成立：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-4.png" alt="img"></p><p>对上述不等式右边取最大值，不等式同样成立：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-5.png" alt="img"></p><p>上述不等式表明，我们对SVM的min和max做了对调，满足这样的关系，这叫做<strong>Lagrange dual problem</strong>。</p><p>不等式右边是SVM问题的下界，我们接下来的目的就是求出这个下界。</p><p>已知$\geq$是一种弱对偶关系，在二次规划QP问题中，如果满足以下三个条件：</p><ul><li>函数是凸的（convex primal）</li><li>函数有解（feasible primal）</li><li>条件是线性的（linear constraints）</li></ul><p>那么，上述不等式关系就变成强对偶关系，$\geq$变成=，即一定存在满足条件的解$(b,w,\alpha)$，使等式左边和右边都成立，SVM的解就转化为右边的形式。</p><p>经过推导，SVM对偶问题的解已经转化为无条件形式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-5.png" alt="img"></p><p>其中，上式括号里面的是对拉格朗日函数$L(b,w,\alpha)$计算最小值。那么根据梯度下降算法思想：最小值位置满足梯度为零。</p><p>首先，令$L(b,w,\alpha)$对参数$b$的梯度为零：</p><script type="math/tex; mode=display">\frac{\partial L(b,w,\alpha)}{\partial b}=0=-\sum_{n=1}^N\alpha_ny_n</script><p>也就是说，最优解一定满足$\sum_{n=1}^N\alpha_ny_n=0$。那么，我们把这个条件代入计算$\max$条件中（与$\alpha_n\geq0$同为条件），并进行化简：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-5.png" alt="img"></p><p>这样，SVM表达式消去了$b$，问题化简了一些。</p><p>然后，再根据最小值思想，令$L(b,w,\alpha)$对参数$w$的梯度为零：</p><script type="math/tex; mode=display">\frac{\partial L(b,w,\alpha)}{\partial w}=0=w-\sum_{n=1}^N\alpha_ny_nz_n</script><p>即得到：</p><script type="math/tex; mode=display">w=\sum_{n=1}^N\alpha_ny_nz_n</script><p>也就是说，最优解一定满足$w=\sum_{n=1}^N\alpha_ny_nz_n$。那么，同样我们把这个条件代入并进行化简：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-5.png" alt="img"></p><p>这样，SVM表达式消去了w，问题更加简化了。这时候的条件有3个：</p><ol><li>all $\alpha_n\geq0$</li><li>$\sum_{n=1}^N\alpha_ny_n=0$</li><li>$w=\sum_{n=1}^N\alpha_ny_nz_n$</li></ol><p>SVM简化为只有$\alpha_n$的最佳化问题，即计算满足上述三个条件下，函数$-\frac12||\sum_{n=1}^N\alpha_ny_nz_n||^2+\sum_{n=1}^N\alpha_n$最大值时对应的$\alpha_n$是多少。</p><p>总结一下，SVM最佳化形式转化为只与$\alpha_n$有关：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-4.png" alt="img"></p><p>其中，满足最佳化的条件称之为<strong>Karush-Kuhn-Tucker(KKT)</strong>：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-4.png" alt="img"></p><p>在下一部分中，我们将利用KKT条件来计算最优化问题中的$\alpha$，进而得到$b$和$w$。</p><h2 id="3-Solving-Dual-SVM"><a href="#3-Solving-Dual-SVM" class="headerlink" title="3. Solving Dual SVM"></a>3. Solving Dual SVM</h2><p>上面我们已经得到了dual SVM的简化版了，接下来，我们继续对它进行一些优化。首先，将max问题转化为min问题，再做一些条件整理和推导，得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-3.png" alt="img"></p><p>显然，这是一个convex的QP问题，且有N个变量$\alpha_n$，限制条件有$N+1$个。则根据上一节课讲的QP解法，找到$Q,p,A,c$对应的值，用软件工具包进行求解即可。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-4.png" alt="img"></p><p>求解过程很清晰，但是值得注意的是，$q_{n,m}=y_ny_mz^T_nz_m$，大部分值是非零的，称为dense。当$N$很大的时候，例如$N=30000$，那么对应的$Q_D$的计算量将会很大，存储空间也很大。所以一般情况下，对dual SVM问题的矩阵$Q_D$，需要使用一些特殊的方法，这部分内容就不再赘述了。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-3.png" alt="img"></p><p>得到$\alpha_n$之后，再根据之前的KKT条件，就可以计算出$w$和$b$了。首先利用条件$w=\sum\alpha_ny_nz_n$得到$w$，然后利用条件$\alpha_n(1-y_n(w^Tz_n+b))=0$，取任一$\alpha_n\neq0$即$\alpha_n&gt;0$的点，得到$1-y_n(w^Tz_n+b)=0$，进而求得$b=y_n-w^Tz_n$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-3.png" alt="img"></p><p>值得注意的是，计算$b$值，$\alpha_n&gt;0$时，有$y_n(w^Tz_n+b)=1$成立。$y_n(w^Tz_n+b)=1$正好表示的是该点在SVM分类线上，即fat boundary。也就是说，<strong>满足$\alpha_n&gt;0$的点一定落在fat boundary上，这些点就是Support Vector</strong>。这是一个非常有趣的特性。</p><h2 id="4-Messages-behind-Dual-SVM"><a href="#4-Messages-behind-Dual-SVM" class="headerlink" title="4. Messages behind Dual SVM"></a>4. Messages behind Dual SVM</h2><p>把位于分类线边界上的点：support vector（candidates）。</p><p>$\alpha_n&gt;0$的点：一定落在分类线边界上，这些点为support vector（注意没有candidates）。</p><p>也就是说分类线上的点不一定都是支持向量，但是满足$\alpha_n&gt;0$的点，一定是支持向量。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-4.png" alt="img"></p><p>SV只由$\alpha_n&gt;0$的点决定，根据上一部分推导的$w$和$b$的计算公式，我们发现，$w$和$b$仅由SV的点决定，简化了计算量。这跟我们上一节课介绍的分类线只由“胖”边界上的点所决定是一个道理。</p><p>也就是说，样本点可以分成两类：</p><ol><li>一类是support vectors，通过support vectors可以求得fattest hyperplane；</li><li>另一类不是support vectors，对我们求得fattest hyperplane没有影响。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-2.png" alt="img"></p><p>回过头来，我们来比较一下SVM和PLA的w公式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19-1.png" alt="img"></p><p>我们发现，二者在形式上是相似的。$w_{SVM}$由fattest hyperplane边界上所有的SV决定，$w_{PLA}$由所有当前分类错误的点决定。$w_{SVM}$和$w_{PLA}$都是原始数据点$y_nz_n$的线性组合形式，是原始数据的代表。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/20-1.png" alt="img"></p><p>总结一下，本节课和上节课主要介绍了两种形式的SVM：</p><ol><li>Primal Hard-Margin SVM有$\hat d+1$个参数，有$N$个限制条件。当$\hat d+1$很大时，求解困难。</li><li>Dual Hard_Margin SVM有$N$个参数，有$N+1$个限制条件。当数据量$N$很大时，也同样会增大计算难度。</li></ol><p>两种形式都能得到$w$和$b$，求得fattest hyperplane。通常情况下，如果$N$不是很大，一般使用Dual SVM来解决问题。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/21-1.png" alt="img"></p><p>这节课提出的Dual SVM的目的是为了避免计算过程中对$\hat d$的依赖，而只与$N$有关。但是，Dual SVM是否真的消除了对$\hat d$的依赖呢？其实并没有。因为在计算$q_{n,m}=y_ny_mz_n^Tz_m$的过程中，由$z$向量引入了$\hat d$，实际上复杂度已经隐藏在计算过程中了。所以，我们的目标并没有实现。下一节课我们将继续研究探讨如何消除对$\hat d$的依赖。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/22.png" alt="img"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h2><p>本节课主要介绍了SVM的另一种形式：Dual SVM。我们这样做的出发点是为了移除计算过程对$\hat d$的依赖。Dual SVM的推导过程是通过引入拉格朗日因子$\alpha$，将SVM转化为新的非条件形式。然后，利用QP，得到最佳解的拉格朗日因子$\alpha$。再通过KKT条件，计算得到对应的$w$和$b$。最终求得fattest hyperplane。下一节课，我们将解决Dual SVM计算过程中对$\hat d$的依赖问题。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习技法》Note——1.Linear SVM</title>
      <link href="/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-1.linear-svm/"/>
      <url>/2020/04/19/lin-xuan-tian-ji-qi-xue-xi-ji-fa-note-1.linear-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li></ul></blockquote><h1 id="Lecture-1-Linear-SVM"><a href="#Lecture-1-Linear-SVM" class="headerlink" title="Lecture 1: Linear SVM"></a>Lecture 1: Linear SVM</h1><h2 id="1-Large-Margin-Separating-Hyperplane"><a href="#1-Large-Margin-Separating-Hyperplane" class="headerlink" title="1. Large-Margin Separating Hyperplane"></a>1. Large-Margin Separating Hyperplane</h2><p>对于线性可分的情况，可以使用PLA/pocket算法在平面或者超平面上把正负类分开。</p><p>例如对平面2D这种情况，能将正类和负类完全分开的直线通常不止一条。哪条线更好呢？</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-5.png" alt="img"></p><p>这三条直线都是由PLA/pocket算法不断修正错误点而最终产生的，整个确定直线形状的过程是随机的。</p><p>单从分类效果上看，这三条直线都满足要求，而且都满足VC bound要求，模型复杂度$\Omega(H)$是一样的，即具有一定的泛化能力。</p><p>但是，凭第一感觉，我们还是会选择第三条直线，感觉它的分类效果更好一些。那这又是为什么呢？</p><p>先给个简单解释，一般情况下，训练样本外的测量数据应该分布在训练样本附近，但与训练样本的位置有一些偏差。若要保证对未知的测量数据也能进行正确分类，<strong>最好让分类直线距离正类负类的点都有一定的距离</strong>。这样能让<strong>每个样本点附近的圆形区域是“安全”的</strong>。<strong>圆形区域越大，表示分类直线对测量数据误差的容忍性越高，越“安全”</strong>。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-3.png" alt="img"></p><p>左图的点距离分类直线的最小距离很小，它的圆形区域很小。那么，分类线对测量数据误差的容忍性就很差，测量数据与样本数据稍有偏差，很有可能就被误分。</p><p>右图的点距离分类直线的最小距离更大一些，其圆形区域也比较大。这种情况下，分类线对测量数据误差的容忍性就相对来说大很多，不容易误分。也就是说，<strong>左边分类线和右边分类线的最大区别是对这类测量误差的容忍度不同</strong>。</p><p>如果每一个训练样本距离分类线越远的话，就表示分类型可以忍受更多的测量误差（noise）。noise是造成overfitting的主要原因，而测量误差也是一种noise。所以，如果分类线对测量误差的容忍性越好的话，表示这是一条不错的分类线。那么，我们的目标就是找到这样一条最“健壮”的线，即距离数据点越远越好。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-3.png" alt="img"></p><p>上面我们用圆形区域表示分类线能够容忍多少误差，也就相当于计算点到直线的距离。距离越大，表示直线越“胖”，越能容忍误差；距离越小，表示直线越“瘦”，越不能容忍误差。<strong>越胖越好</strong>。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-3.png" alt="img"></p><p>如何定义分类线有多胖，就是看距离分类线最近的点与分类线的距离，我们把它用margin表示（图中灰色部分，注意是两遍的距离之和）。</p><p>分类线由权重$w$决定，目的就是找到使margin最大时对应的$w$值。</p><p>整体来说，我们的目标就是找到这样的分类线并满足下列条件：</p><ol><li>分类正确（分对）：$y_nw^Tx_n \gt0$</li><li>margin最大化：</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-3.png" alt="img"></p><h2 id="2-Standard-Large-Margin-Problem"><a href="#2-Standard-Large-Margin-Problem" class="headerlink" title="2. Standard Large-Margin Problem"></a><strong>2. Standard Large-Margin Problem</strong></h2><blockquote><p>如何计算<strong>点到分类线的距离</strong>？</p><p>首先，我们将权重$w(w_0,w_1,\cdots,w_d)$中的$w_0$拿出来，用$b$表示，省去$x_0$项。这样，hypothesis就变成了$h(x)=sign(w^Tx+b)$。</p><p>下面，利用图解的方式，详细推导如何计算点到分类平面的距离公式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-4.png" alt="img"></p><p>如上图所示，平面上有两个点：$x’$和$x’’$。因为这两个点都在分类平面上，所以它们都满足：</p><script type="math/tex; mode=display">w^Tx’+b=0\\w^Tx''+b=0</script><p>则有：</p><script type="math/tex; mode=display">w^T(x''-x’)=w^Tx''-w^Tx’=0</script><p>$(x’’-x’)$是平面上的任一向量，$(x’’-x’)$与$w$内积为0，表示$(x’’-x’)$垂直于$w$，那么$w$就是平面的法向量。</p><p>现在，若要计算平面外一点$x$到该平面的距离，做法是只要将向量$(x-x’)$投影到垂直于该平面的方向（即$w$方向）上就可以了。那么，令$(x-x’)$与$w$的夹角为$\theta$，距离就可以表示为：</p><script type="math/tex; mode=display">distance(x,b,w)=|(x-x’)cos(\theta)|=|\ ||x-x’||\cdot \frac{(x-x’)w}{||x-x’||\cdot ||w||}|=\frac1{||w||}|w^Tx-w^Tx’|</script><p>代入$w^Tx’=-b$，可得：</p><script type="math/tex; mode=display">distance(x,b,w)=\frac1{||w||}|w^Tx+b|</script><p>点到分类面（Separating Hyperplane）的距离已经算出来了。</p></blockquote><p>基于这个分类面，所有的点均满足：$y_n(w^Tx_n+b)&gt;0$，表示所有点都分类正确，则distance公式就可以变换成：</p><script type="math/tex; mode=display">distance(x,b,w)=\frac1{||w||}y_n(w^Tx_n+b)</script><p>那么，我们的目标形式就转换为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-4.png" alt="img"></p><p>对上面的式子还不容易求解，我们继续对它进行简化。我们知道分类面$w^Tx+b=0$和$3w^Tx+3b=0$其实是一样的。也就是说，<strong>对w和b进行同样的缩放还会得到同一分类面</strong>。所以，为了简化计算，我们令距离分类满最近的点满足$y_n(w^Tx_n+b)=1$。那我们所要求的margin就变成了:</p><script type="math/tex; mode=display">margin(b,w)=\frac1{||w||}</script><p>这样，目标形式就简化为：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-4.png" alt="img"></p><p>这里可以省略条件：$y_n(w^Tx_n+b)&gt;0$，因为满足条件$y_n(w^Tx_n+b)=1$必然满足大于零的条件。</p><p>另外，因为最小化问题我们最熟悉也最好解，所以可以把目标$\frac1{||w||}$最大化转化为计算$\frac12w^Tw$的最小化问题。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-3.png" alt="img"></p><p>如上图所示，最终的条件就是$y_n(w^Tx_n+b)\geq 1$，而我们的目标就是最小化$\frac12w^Tw$值。</p><h2 id="3-Support-Vector-Machine"><a href="#3-Support-Vector-Machine" class="headerlink" title="3. Support Vector Machine"></a><strong>3. Support Vector Machine</strong></h2><p>现在，条件和目标变成：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-3.png" alt="img"></p><p>假如平面上有四个点，两个正类，两个负类：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-2.png" alt="img"></p><p>不同点的坐标加上条件$y_n(w^Tx_n+b)\geq 1$，可以得到：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-3.png" alt="img"></p><p>最终得到的条件是：</p><script type="math/tex; mode=display">w_1\geq +1\\w_2\leq -1</script><p>而我们的目标是：</p><script type="math/tex; mode=display">\min\ \frac12w^Tw=\frac12(w_1^2+w_2^2)\geq 1</script><p>目标最小值为1，即$w_1=1, w_2=-1, b=-1$，那么这个例子就得到了最佳分类面的解，且$margin(b,w)=\frac1{||w||}=\frac1{\sqrt2}$。分类面的表达式为：</p><script type="math/tex; mode=display">x_1-x_2-1=0</script><p>最终我们得到的矩的表达式为：</p><script type="math/tex; mode=display">g_{SVM}(x)=sign(x_1-x_2-1)</script><blockquote><p>Support Vector Machine(SVM)这个名字从何而来？为什么把这种分类面解法称为支持向量机呢？<strong>这是因为分类面仅仅由分类面的两边距离它最近的几个点决定的，其它点对分类面没有影响</strong>。决定分类面的几个点称之为<strong>支持向量</strong>（Support Vector），好比这些点“支撑”着分类面。而<strong>利用Support Vector得到最佳分类面的方法，称之为支持向量机</strong>（Support Vector Machine）。</p></blockquote><p>下面介绍SVM的一般求解方法。先写下我们的条件和目标：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-2.png" alt="img"></p><p>满足：</p><ol><li>目标函数是凸函数（关于$w$的二次函数）</li><li>约束是线性的（关于$w$和$b$的一次函数）</li></ol><p>因此这是一个典型的二次规划问题，即Quadratic Programming（QP）。所以它的求解过程还是比较容易的，可以使用一些软件（例如Matlab）自带的二次规划的库函数来求解。下图给出SVM与标准二次规划问题的参数对应关系：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-2.png" alt="img"></p><p>那么，线性SVM算法可以总结为三步：</p><ol><li>计算对应的二次规划参数$Q，p，A，c$</li><li>根据二次规划库函数，计算$b，w$</li><li>将$b$和$w$代入$g_{SVM}$，得到最佳分类面</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/17-3.png" alt="img"></p><p>这种方法称为<strong>Linear Hard-Margin SVM Algorithm</strong>。如果是非线性的，例如包含$x$的高阶项，那么可以使用我们之前在《机器学习基石》课程中介绍的特征转换的方法，先作$z_n=\Phi(x_n)$的特征变换，从非线性的$x$域映射到线性的$z$域空间，再利用Linear Hard-Margin SVM Algorithm求解即可。</p><h2 id="4-Reasons-behind-Large-Margin-Hyperplane"><a href="#4-Reasons-behind-Large-Margin-Hyperplane" class="headerlink" title="4. Reasons behind Large-Margin Hyperplane"></a><strong>4. Reasons behind Large-Margin Hyperplane</strong></h2><p>从视觉和直觉的角度，我们认为Large-Margin Hyperplane的分类效果更好。<strong>SVM的这种思想其实与正则化regularization思想很类似</strong>。</p><ol><li>regularization的目标是将$E_{in}$最小化，条件是$w^Tw\leq C$；</li><li>SVM的目标是$w^Tw$最小化，条件是$y_n(w^Tx_n+b)\geq1$，即保证了$E_{in}=0$。</li></ol><p>有趣的是，<strong>regularization与SVM的目标和限制条件分别对调了</strong>。其实，考虑的内容是类似的，效果也是相近的。SVM也可以说是一种weight-decay regularization，限制条件是$E_{in}=0$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/18-1.png" alt="img"></p><p>从另一方面来看，Large-Margin会限制Dichotomies的个数。这从视觉上也很好理解，假如一条分类面越“胖”，即对应Large-Margin，那么它可能shtter的点的个数就可能越少：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/19.png" alt="img"></p><p>之前的《机器学习基石》课程中介绍过，Dichotomies与VC Dimension是紧密联系的。也就是说如果Dichotomies越少，那么复杂度就越低，即有效的VC Dimension就越小，得到$E_{out}\approx E_{in}$，泛化能力强。</p><p>下面我们从概念的角度推导一下为什么dichotomies越少，VC Dimension就越少。首先我们考虑一下Large-Margin演算法的VC Dimension，记为$d_{vc}(A_{\rho})$。$d_{vc}(A_{\rho})$与数据有关，而我们之前介绍的$d_{vc}$与数据无关。</p><p>假如平面上有3个点分布在单位圆上：</p><ol><li>如果Margin为0，即$\rho=0$，这条细细的直线可以很容易将圆上任意三点分开（shatter），就能得到它的$d_{vc}=3$。</li><li>如果$\rho\gt\frac{\sqrt3}{2}$，这条粗粗的线无论如何都不能将圆上的任一三点全完分开（no shatter），因为圆上必然至少存在两个点的距离小于$\sqrt3$，那么其对应d的$d_{vc}&lt;3$。</li></ol><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/20.png" alt="img"></p><p>那么，一般地，在$d$维空间，当数据点分布在半径为$R$的超球体内时，得到的$d_{vc}(A_{\rho})$满足下列不等式：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/21.png" alt="img"></p><p>之前介绍的Perceptrons的VC Dimension为$d+1$，这里得到的结果是Large-Margin演算法的$d_{vc}(A_{\rho})\leq d+1$。所以，由于Large-Margin，得到的dichotomies个数减少，从而VC Dimension也减少了。VC Dimension减少降低了模型复杂度，提高了泛化能力。</p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a><strong>5. 总结</strong></h2><p>本节课主要介绍了线性支持向量机（Linear Support Vector Machine）。我们先从视觉角度出发，希望得到一个比较“胖”的分类面，即满足所有的点距离分类面都尽可能远。然后，我们通过一步步推导和简化，最终把这个问题转换为标准的二次规划（QP）问题。二次规划问题可以使用Matlab等软件来进行求解，得到我们要求的$w$和$b$，确定分类面。这种方法背后的原理其实就是减少了dichotomies的种类，减少了有效的VC Dimension数量，从而让机器学习的模型具有更好的泛化能力。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark常用操作总结——建模</title>
      <link href="/2020/03/30/pyspark-chang-yong-cao-zuo-zong-jie-jian-mo/"/>
      <url>/2020/03/30/pyspark-chang-yong-cao-zuo-zong-jie-jian-mo/</url>
      
        <content type="html"><![CDATA[<h1 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h1><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>将建模所需的字段合并为一列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"></span><br><span class="line"><span class="comment"># featureList: 建模所需要的字段</span></span><br><span class="line">assembler = VectorAssembler(inputCols=featureList, outputCol=<span class="string">"features"</span>)</span><br><span class="line">resultDF = assembler.transform(df)</span><br></pre></td></tr></table></figure><h2 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.write().overwrite().save(modelPath + modelName)</span><br></pre></td></tr></table></figure><h2 id="读取模型"><a href="#读取模型" class="headerlink" title="读取模型"></a>读取模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegressionModel, RandomForestClassificationModel, GBTClassificationModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># LR</span></span><br><span class="line">lrModel = LogisticRegressionModel.load(modelPath + modelName)</span><br><span class="line"></span><br><span class="line"><span class="comment"># RF</span></span><br><span class="line">rfModel = RandomForestClassificationModel.load(modelPath + modelName)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GBDT</span></span><br><span class="line">gbdtModel = GBTClassificationModel.load(modelPath + modelName)</span><br></pre></td></tr></table></figure><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression, LogisticRegressionModel</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lrModel = lr.fit(df) <span class="comment"># df为特征转换为features一列的DataFrame</span></span><br><span class="line">resultDF = lrModel.transform(df).select(<span class="string">"label"</span>, <span class="string">"prediction"</span>, <span class="string">"probability"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截距</span></span><br><span class="line">intercept = lrModel.intercept</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二分类：因子系数</span></span><br><span class="line">featCoefficients = list(zip(featList, list(lrModel.coefficients)))</span><br><span class="line">featCoefficientDict = &#123;k: round(v,<span class="number">4</span>) <span class="keyword">for</span> (k,v) <span class="keyword">in</span> featCoefficients&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多分类：因子系数矩阵</span></span><br><span class="line">coeMatrix = lrModel.coefficientMatrix.toArray()</span><br><span class="line">zippedFeatCoefficient = [dict(zip(featList, [round(v,<span class="number">4</span>) <span class="keyword">for</span> v <span class="keyword">in</span> r])) <span class="keyword">for</span> r <span class="keyword">in</span> coeMatrix]</span><br><span class="line">labelList = [row[<span class="string">"label"</span>] <span class="keyword">for</span> row <span class="keyword">in</span> resultDF.select(<span class="string">"label"</span>).distinct().collect()]</span><br><span class="line">featCoefficientMatrix = dict(list(zip(labelList, zippedFeatCoefficient)))</span><br></pre></td></tr></table></figure><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> RandomForestClassifier, RandomForestClassificationModel</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier()</span><br><span class="line">rfModel = rf.fit(df)</span><br><span class="line">resultDF = rfModel.transform(df).select(<span class="string">"label"</span>, <span class="string">"prediction"</span>, <span class="string">"probability"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆分预测概率</span></span><br><span class="line">cols = [<span class="string">"label"</span>, <span class="string">"prediction"</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(row)</span>:</span></span><br><span class="line">    newRow = tuple([row[k] <span class="keyword">for</span> k <span class="keyword">in</span> cols]) + tuple(row.probability.toArray().tolist())</span><br><span class="line">    <span class="keyword">return</span> newRow</span><br><span class="line">resultDF = resultDF.rdd.map(extract).toDF(cols+[<span class="string">'negative_probability'</span>,<span class="string">'positive_probability'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征重要性</span></span><br><span class="line">featImportances = rfModel.featureImportances.toArray()</span><br><span class="line">featImportanceDict = &#123;k: round(v,<span class="number">4</span>) <span class="keyword">for</span> k,v <span class="keyword">in</span> list(zip(featList,featImportances))&#125;</span><br></pre></td></tr></table></figure><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> GBTClassifier, GBTClassificationModel</span><br><span class="line"></span><br><span class="line">gbdt = GBTClassifier()</span><br><span class="line">gbdtModel = gbdt.fit(df)</span><br><span class="line">resultDF = gbdtModel.transform(df).select(<span class="string">"label"</span>, <span class="string">"prediction"</span>, <span class="string">"probability"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征重要性</span></span><br><span class="line">featImportances = gbdtModel.featureImportances.toArray()</span><br><span class="line">featImportanceDict = &#123;k: round(v,<span class="number">4</span>) <span class="keyword">for</span> k,v <span class="keyword">in</span> list(zip(featList,featImportances))&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark常用操作总结——模型评估</title>
      <link href="/2020/03/30/pyspark-chang-yong-cao-zuo-zong-jie-mo-xing-ping-gu/"/>
      <url>/2020/03/30/pyspark-chang-yong-cao-zuo-zong-jie-mo-xing-ping-gu/</url>
      
        <content type="html"><![CDATA[<h1 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h1><h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">lrSummary = lrModel.summary() <span class="comment"># 注：仅lr模型有该接口</span></span><br><span class="line"></span><br><span class="line">AUC = lrSummary.areaUnderROC</span><br><span class="line"></span><br><span class="line">fMeasureDF = lrSummary.fMeasureByThreshold</span><br><span class="line">precisionDF = lrSummary.precisionByThreshold</span><br><span class="line">recallDF = lrSummary.recallByThreshold</span><br><span class="line"></span><br><span class="line">maxFMeasure = fMeasureDF.agg(&#123;<span class="string">"F-Measure"</span>: <span class="string">"max"</span>&#125;).first()[<span class="number">0</span>] <span class="comment"># F-Measure最大值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选取F-Measure最大值所对应的threshold/precision/recall</span></span><br><span class="line">curThreshold = fMeasureDF.filter(fMeasureDF[<span class="string">"F-Measure"</span>]&gt;=(maxFMeasure<span class="number">-0.000001</span>))\</span><br><span class="line">                         .filter(fMeasureDF[<span class="string">"F-Measure"</span>]&lt;=(maxFMeasure+<span class="number">0.000001</span>))\</span><br><span class="line">                         .first().threshold <span class="comment"># 由于精度问题不使用等号</span></span><br><span class="line">curPrecision = precisionDF.filter(precisionDF[<span class="string">"threshold"</span>]&gt;=(curThreshold<span class="number">-0.000001</span>))\</span><br><span class="line">                          .filter(precisionDF[<span class="string">"threshold"</span>]&lt;=(curThreshold+<span class="number">0.000001</span>))\</span><br><span class="line">                          .first().precision</span><br><span class="line">curRecall = recallDF.filter(recallDF[<span class="string">"threshold"</span>]&gt;=(curThreshold<span class="number">-0.000001</span>))\</span><br><span class="line">                    .filter(recallDF[<span class="string">"threshold"</span>]&lt;=(curThreshold+<span class="number">0.000001</span>))\</span><br><span class="line">                    .first().recall</span><br><span class="line"></span><br><span class="line">result = &#123;<span class="string">"maxFMeasure"</span>: round(maxFMeasure, <span class="number">3</span>),</span><br><span class="line">          <span class="string">"curThreshold"</span>: round(curThreshold, <span class="number">3</span>),</span><br><span class="line">          <span class="string">"curPrecision"</span>: round(curPrecision, <span class="number">3</span>),</span><br><span class="line">          <span class="string">"curRecall"</span>: round(curRecall, <span class="number">3</span>),</span><br><span class="line">          <span class="string">"AUC"</span>: round(AUC, <span class="number">3</span>)&#125;</span><br></pre></td></tr></table></figure><h2 id="二分类模型"><a href="#二分类模型" class="headerlink" title="二分类模型"></a>二分类模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = BinaryClassificationEvaluator(labelCol=<span class="string">"label"</span>, rawPredictionCol=<span class="string">"prediction"</span>) <span class="comment"># 指定标签列和预测列的列名</span></span><br><span class="line"><span class="comment"># resultDF：模型输出的预测结果</span></span><br><span class="line">areaUnderPR = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"areaUnderPR"</span>&#125;)</span><br><span class="line">areaUnderROC = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"areaUnderROC"</span>&#125;)</span><br><span class="line"></span><br><span class="line">truePositiveCount = resultDF.filter(<span class="string">"label == 1 and prediction == 1"</span>).count()</span><br><span class="line">predictedPositiveCount = resultDF.filter(<span class="string">"prediction == 1"</span>).count()</span><br><span class="line">totalPositiveCount = resultDF.filter(<span class="string">"label == 1"</span>).count()</span><br><span class="line"></span><br><span class="line">precision = <span class="number">1.0</span> * truePositiveCount / predictedPositiveCount</span><br><span class="line">recall = <span class="number">1.0</span> * truePositiveCount / totalPositiveCount</span><br><span class="line"></span><br><span class="line">result = &#123;<span class="string">"areaUnderPR"</span>: round(areaUnderPR,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"AUC"</span>: round(areaUnderROC,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"precision"</span>: round(precision,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"recall"</span>: round(recall,<span class="number">3</span>)&#125;</span><br></pre></td></tr></table></figure><h2 id="ROC曲线-PR曲线"><a href="#ROC曲线-PR曲线" class="headerlink" title="ROC曲线/PR曲线"></a>ROC曲线/PR曲线</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对positive_probability降低精度，提高运算效率</span></span><br><span class="line">trimRoundedDF = resultDF.select(<span class="string">"label"</span>,<span class="string">"positive_probability"</span>)\</span><br><span class="line">              .rdd.map(<span class="keyword">lambda</span> row: (row[<span class="string">"label"</span>], round(row[<span class="string">"positive_probability"</span>],<span class="number">4</span>)))\</span><br><span class="line">              .toDF([<span class="string">"label"</span>, <span class="string">"positive_probability"</span>])</span><br><span class="line">trimRoundedDF.cache()</span><br><span class="line"></span><br><span class="line">listOfPRF = []</span><br><span class="line">listOfTpFp = []</span><br><span class="line">steps = <span class="number">200</span> <span class="comment"># 设置threshold的遍历步长</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> [<span class="number">1.0</span>/steps*i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,steps,<span class="number">1</span>)]:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"*** Handling Threshold:&#123;threshold&#125; ***"</span>.format(threshold=threshold))</span><br><span class="line">    tmpCalcDF = trimRoundedDF.rdd.map(<span class="keyword">lambda</span> row:(int(row[<span class="string">"label"</span>]),<span class="number">1</span> <span class="keyword">if</span> row[<span class="string">"positive_probability"</span>] &gt;= threshold <span class="keyword">else</span> <span class="number">0</span>)).toDF([<span class="string">"label"</span>,<span class="string">"prediction"</span>])</span><br><span class="line">    tmpResult = tmpCalcDF.selectExpr(</span><br><span class="line">        <span class="string">"sum(case when label = 0 and prediction = 1 then 1 else 0 end) as FP"</span>,</span><br><span class="line">        <span class="string">"sum(case when label = 1 and prediction = 0 then 1 else 0 end) as FN"</span>,</span><br><span class="line">        <span class="string">"sum(case when label = 1 and prediction = 1 then 1 else 0 end) as TP"</span>,</span><br><span class="line">        <span class="string">"sum(case when label = 0 and prediction = 0 then 1 else 0 end) as TN"</span></span><br><span class="line">    ).collect()[<span class="number">0</span>]</span><br><span class="line">    actualPositive = tmpResult.TP + tmpResult.FN</span><br><span class="line">    actualNegative = tmpResult.TN + tmpResult.FP</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        precision = tmpResult.TP / (tmpResult.TP + tmpResult.FP)</span><br><span class="line">        recall = tmpResult.TP / actualPositive</span><br><span class="line">        TPR = recall <span class="comment"># 分到正样本中真实的正样本所占所有正样本的比例</span></span><br><span class="line">        FPR = tmpResult.FP / (tmpResult.FP + tmpResult.TN) <span class="comment"># 分到正样本类别中真实的负样本所占所有负样本总数的比例</span></span><br><span class="line">        fMeasure = <span class="number">2</span> * precision * recall / (precision + recall)</span><br><span class="line">        listOfPRF.append([precision,recall,fMeasure])</span><br><span class="line">        listOfTpFp.append([TPR,FPR])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="comment"># 超过阈值全部预测为0的情况</span></span><br><span class="line">        print(<span class="string">"TP: %s"</span> % tmpResult.TP)</span><br><span class="line">        print(<span class="string">"FN: %s"</span> % tmpResult.FN)</span><br><span class="line">        print(<span class="string">"TN: %s"</span> % tmpResult.TN)</span><br><span class="line">        print(<span class="string">"FP: %s"</span> % tmpResult.FP)</span><br><span class="line">        listOfPRF.append([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">        listOfTpFp.append([<span class="number">0.0</span>, <span class="number">0.0</span>])</span><br><span class="line">    <span class="comment"># 计算最大的FMeasure</span></span><br><span class="line">    maxFMeasureGroup = sorted(listOfPRF, key=<span class="keyword">lambda</span> x: x[<span class="number">-1</span>], reverse=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算PR曲线面积（PR曲线横坐标为recall，纵坐标为precision）</span></span><br><span class="line">    sortedListOfPRF = sorted(listOfPRF, key=<span class="keyword">lambda</span> x: (x[<span class="number">1</span>],x[<span class="number">0</span>]) ,reverse=<span class="literal">False</span>) <span class="comment"># 保证横纵坐标是增加的（便于计算近似梯形面积）</span></span><br><span class="line">    prArea = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps<span class="number">-1</span>):</span><br><span class="line">        prArea += (sortedListOfPRF[i][<span class="number">0</span>] + sortedListOfPRF[i+<span class="number">1</span>][<span class="number">0</span>]) / <span class="number">2</span>*(sortedListOfPRF[i+<span class="number">1</span>][<span class="number">1</span>] - sortedListOfPRF[i][<span class="number">1</span>]) <span class="comment"># 近似为梯形面积</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算ROC曲线面积AUC</span></span><br><span class="line">    AUC = <span class="number">0</span></span><br><span class="line">    sortedListOfTpFp = sorted(listOfTpFp, key=<span class="keyword">lambda</span> x: (x[<span class="number">1</span>],x[<span class="number">0</span>]) ,reverse=<span class="literal">False</span>) <span class="comment"># 保证横纵坐标是增加的（便于计算近似梯形面积）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(steps<span class="number">-1</span>):</span><br><span class="line">        AUC += (sortedListOfTpFp[i][<span class="number">0</span>] + sortedListOfTpFp[i+<span class="number">1</span>][<span class="number">0</span>]) / <span class="number">2</span>*(sortedListOfTpFp[i+<span class="number">1</span>][<span class="number">1</span>] - sortedListOfTpFp[i][<span class="number">1</span>]) <span class="comment"># 近似为梯形面积</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成PR曲线DF</span></span><br><span class="line">    prfData = [tuple(r) <span class="keyword">for</span> r <span class="keyword">in</span> sortedListOfPRF]</span><br><span class="line">    prfDF = spark.createDataFrame(prfData, schema=[<span class="string">'precision'</span>,<span class="string">'recall'</span>,<span class="string">'fMeasure'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成ROC曲线DF</span></span><br><span class="line">    rocData = [tuple(r) <span class="keyword">for</span> r <span class="keyword">in</span> sortedListOfTpFp]</span><br><span class="line">    tpfpDF = spark.createDataFrame(rocData, schema=[<span class="string">'TPR'</span>,<span class="string">'FPR'</span>])</span><br><span class="line">    </span><br><span class="line">    result = &#123;</span><br><span class="line">        <span class="string">"maxFMeasure"</span>: maxFMeasureGroup[<span class="number">-1</span>],</span><br><span class="line">        <span class="string">"precisionAtMaxFMeasure"</span>: maxFMeasureGroup[<span class="number">0</span>],</span><br><span class="line">        <span class="string">"recallAtMaxFMeasure"</span>: maxFMeasureGroup[<span class="number">1</span>],</span><br><span class="line">        <span class="string">"areaUnderPR"</span>: prArea,</span><br><span class="line">        <span class="string">"areaUnderROC"</span>: AUC,</span><br><span class="line">        <span class="string">"prfDF"</span>: prfDF,</span><br><span class="line">        <span class="string">"tpfpDF"</span>: tpfpDF</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="多分类模型"><a href="#多分类模型" class="headerlink" title="多分类模型"></a>多分类模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> MulticlassClassificationEvaluator</span><br><span class="line"></span><br><span class="line">evaluator = MulticlassClassificationEvaluator(labelCol=<span class="string">"label"</span>, rawPredictionCol=<span class="string">"prediction"</span>) <span class="comment"># 指定标签列和预测列的列名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resultDF：模型输出的预测结果</span></span><br><span class="line">accuracy = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"accuracy"</span>&#125;)</span><br><span class="line">weightedPrecision = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"weightedPrecision"</span>&#125;)</span><br><span class="line">weightedRecall = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"weightedRecall"</span>&#125;)</span><br><span class="line">f1 = evaluator.evaluate(resultDF, &#123;evaluator.metricName:<span class="string">"f1"</span>&#125;)</span><br><span class="line"></span><br><span class="line">result = &#123;<span class="string">"accuracy"</span>: round(accuracy,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"weightedPrecision"</span>: round(weightedPrecision,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"weightedRecall"</span>: round(weightedRecall,<span class="number">3</span>),</span><br><span class="line">          <span class="string">"f1"</span>: round(f1,<span class="number">3</span>)&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark常用操作总结——预处理</title>
      <link href="/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-yu-chu-li/"/>
      <url>/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-yu-chu-li/</url>
      
        <content type="html"><![CDATA[<h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> DoubleType, IntegerType, StringType, NullType</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换原始列</span></span><br><span class="line">df = df.withColumn(col+<span class="string">'_tmp'</span>, df[col].cast(DoubleType())).drop(col).withColumnRenamed(col+<span class="string">'_tmp'</span>, col)</span><br><span class="line"><span class="comment"># 类型有：DoubleType(), IntegerType(), StringType()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留原始列</span></span><br><span class="line">df = df.withColumn(col+<span class="string">'_new'</span>, df[col].cast(DoubleType()))</span><br></pre></td></tr></table></figure><h2 id="缺失值填充"><a href="#缺失值填充" class="headerlink" title="缺失值填充"></a>缺失值填充</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = df.fillna(&#123;key: DoubleFillValue <span class="keyword">for</span> key <span class="keyword">in</span> DoubleFeatureList&#125;) \</span><br><span class="line">       .fillna(&#123;key: IntFillValue <span class="keyword">for</span> key <span class="keyword">in</span> intFeatureList&#125;) \</span><br><span class="line">       .fillna(&#123;key: StringFillValue <span class="keyword">for</span> key <span class="keyword">in</span> StringFeatureList&#125;)</span><br><span class="line"><span class="comment"># fillna()函数可以使用常数或字典类型</span></span><br><span class="line"><span class="comment"># 比如fillna(0)代表全部用0填充</span></span><br><span class="line"><span class="comment"># 比如fillna(&#123;'age':20, 'sex':-1&#125;) 表示'age'字段空值采用20填充，'sex'字段空值采用-1填充</span></span><br></pre></td></tr></table></figure><p>str型字段的空值填充</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stringNaValue = <span class="string">'null'</span> <span class="comment"># 缺失值非空时的取值（缺失值有空值''和非空值）</span></span><br><span class="line">stringFillValue = <span class="number">-1</span> <span class="comment"># 填充值</span></span><br><span class="line">df = df.replace(<span class="string">''</span>, stringFillValue, subset=StringFeatureList).na.fill(stringFillValue, subset=StringFeatureList)</span><br></pre></td></tr></table></figure><h2 id="计算字段饱和度、值的个数"><a href="#计算字段饱和度、值的个数" class="headerlink" title="计算字段饱和度、值的个数"></a>计算字段饱和度、值的个数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n_sample = df.count() <span class="comment"># 样本数量</span></span><br><span class="line">featureQualityDict = &#123;feat:<span class="literal">None</span> <span class="keyword">for</span> feat <span class="keyword">in</span> featureList&#125; <span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">notNullCountSql = [<span class="string">"sum(case when &#123;f&#125; is not null then 1 else 0 end) as not_null_count_&#123;f&#125;"</span>.format(f=feat) <span class="keyword">for</span> feat <span class="keyword">in</span> featureList]</span><br><span class="line">distinctCountSql = [<span class="string">"count(distinct &#123;f&#125;) as distinct_count_&#123;f&#125;"</span>.format(f=feat) <span class="keyword">for</span> feat <span class="keyword">in</span> featureList]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用selectExpr接口进行并行计算</span></span><br><span class="line">countResultDF = df.selectExpr(*(notNullCountSql+distinctCountSql))</span><br><span class="line">countResult = countResultDF.collect()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> featureList:</span><br><span class="line">    notNullCount = countResult[<span class="string">"not_null_count_&#123;f&#125;"</span>.format(f=feat)]</span><br><span class="line">    distinctCount = countResult[<span class="string">"distinct_count_&#123;f&#125;"</span>.format(f=feat)]</span><br><span class="line">    notNullRate = round(<span class="number">1.0</span> * notNullCount / n_sample, <span class="number">3</span>)</span><br><span class="line">    featureQualityDict[feat] = &#123;<span class="string">"notNullRate"</span>: notNullRate, <span class="string">"distinctCount"</span>: distinctCount&#125;</span><br></pre></td></tr></table></figure><h2 id="计算统计指标"><a href="#计算统计指标" class="headerlink" title="计算统计指标"></a>计算统计指标</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">statisticDict = &#123;&#125;</span><br><span class="line">dataDescribeDF = df.describe(numFeatureList) <span class="comment"># 对数值型变量计算统计指标</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> numFeatureList:</span><br><span class="line">    statisticDict[feat] = &#123;&#125; <span class="comment"># 嵌套dict</span></span><br><span class="line">    median = df.approxQuantile(feat, (<span class="number">0.5</span>,), <span class="number">0</span>)[<span class="number">0</span>] <span class="comment"># 计算中位数(0.5分位数)</span></span><br><span class="line">    statisticDict[<span class="string">'feat'</span>][<span class="string">'median'</span>] = round(median, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataDescribeDF.collect():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(numFeatureList)):</span><br><span class="line">        featName = numFeatureList[i]</span><br><span class="line">        statName = row[<span class="number">0</span>].encode(<span class="string">"utf-8"</span>)</span><br><span class="line">        statValue = round(eval(row[i+<span class="number">1</span>]), <span class="number">3</span>) </span><br><span class="line">        <span class="comment"># eval()函数将去掉字符串的两个引号,将其解释为一个变量。单/双引号eval()函数都将其解释为int类型</span></span><br><span class="line">        statisticDict[featName][statName] = statValue</span><br></pre></td></tr></table></figure><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 固定数量采样</span></span><br><span class="line">sampleAmount = <span class="number">100</span> <span class="comment"># 抽取样本数量(int)</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> rand</span><br><span class="line">sample = df.withColumn(<span class="string">"rand"</span>,rand).orderBy(<span class="string">"rand"</span>).limit(sampleAmount).drop(<span class="string">"rand"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按比例采样</span></span><br><span class="line">fraction = <span class="number">0.1</span> <span class="comment"># 采样比例(double in (0,1))</span></span><br><span class="line">withReplacement = <span class="literal">False</span> <span class="comment"># 是否可放回，默认不可放回</span></span><br><span class="line">sample = df.sample(fraction = fraction, withReplacement = withReplacement, seed = <span class="number">123</span>)</span><br></pre></td></tr></table></figure><blockquote><p><strong>pyspark.sql.functions.rand</strong>(<em>seed=None</em>)<a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#rand" target="_blank" rel="noopener">[source]</a></p><p>Generates a random column with independent and identically distributed (i.i.d.) samples from <strong>U[0.0, 1.0]</strong>.</p><p><strong>pyspark.sql.functions.randn</strong>(<em>seed=None</em>)<a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/functions.html#randn" target="_blank" rel="noopener">[source]</a></p><p>Generates a column with independent and identically distributed (i.i.d.) samples from <strong>the standard normal distribution.</strong></p></blockquote><h2 id="划分样本"><a href="#划分样本" class="headerlink" title="划分样本"></a>划分样本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">splitRatio = <span class="number">0.8</span> <span class="comment"># 训练集占80%</span></span><br><span class="line">trainDF, testDF = df.randomSplit([splitRatio, <span class="number">1</span>-splitRatio], seed = <span class="number">123</span>)</span><br></pre></td></tr></table></figure><h2 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler, MinMaxScaler</span><br><span class="line"></span><br><span class="line">features = df.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先使用VectorAssembler将需要归一化的变量合并为一列，输出新的变量"inputFeatures"</span></span><br><span class="line">inputAssembler = VectorAssembler(inputCols=normFeatureList, outputCol=<span class="string">"inputFeatures"</span>)</span><br><span class="line"><span class="comment"># normFeatureList: 需要归一化的变量</span></span><br><span class="line">df = inputAssembler.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其次使用MinMaxScaler进行归一化</span></span><br><span class="line">scaler = MinMaxScaler(inputCol=<span class="string">"inputFeatures"</span>, outputCol=<span class="string">"scaledFeatures"</span>)</span><br><span class="line">scalerModel = scaler.fit(df)</span><br><span class="line">scaledData = scalerModel.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后将归一化后的合并变量列"scaledFeatures"分开</span></span><br><span class="line">scaledRDD = scaledData.select(features+[<span class="string">"scaledFeatures"</span>]).rdd.map(<span class="keyword">lambda</span> row: tuple([i <span class="keyword">for</span> i <span class="keyword">in</span> row[:<span class="number">-1</span>]]+[float(i) <span class="keyword">for</span> i <span class="keyword">in</span> row[<span class="number">-1</span>]]))</span><br><span class="line">newDF = scaledRDD.toDF(features + [(<span class="string">"normalized_"</span>+feat) <span class="keyword">for</span> feat <span class="keyword">in</span> normFeatureList])</span><br><span class="line"></span><br><span class="line"><span class="comment">#可选：替换原字段</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> normFeatureList:</span><br><span class="line">    newDF = newDF.drop(feat).withColumnRenamed(<span class="string">"normalized_"</span>+feat, feat)</span><br></pre></td></tr></table></figure><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler, StandardScaler</span><br><span class="line"></span><br><span class="line">features = df.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先使用VectorAssembler将需要标准化的变量合并为一列，输出新的变量"inputFeatures"</span></span><br><span class="line">inputAssembler = VectorAssembler(inputCols=standardFeatureList, outputCol=<span class="string">"inputFeatures"</span>)</span><br><span class="line"><span class="comment"># standardFeatureList: 需要标准化的变量</span></span><br><span class="line">df = inputAssembler.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其次使用StandardScaler进行标准化</span></span><br><span class="line">scaler = StandardScaler(inputCol=<span class="string">"inputFeatures"</span>, outputCol=<span class="string">"scaledFeatures"</span>)</span><br><span class="line">scalerModel = scaler.fit(df)</span><br><span class="line">scaledData = scalerModel.transform(df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后将归一化后的合并变量列"scaledFeatures"分开</span></span><br><span class="line">scaledRDD = scaledData.select(features+[<span class="string">"scaledFeatures"</span>]).rdd.map(<span class="keyword">lambda</span> row: tuple([i <span class="keyword">for</span> i <span class="keyword">in</span> row[:<span class="number">-1</span>]]+[float(i) <span class="keyword">for</span> i <span class="keyword">in</span> row[<span class="number">-1</span>]]))</span><br><span class="line">newDF = scaledRDD.toDF(features + [(<span class="string">"stdized_"</span>+feat) <span class="keyword">for</span> feat <span class="keyword">in</span> standardFeatureList])</span><br><span class="line"></span><br><span class="line"><span class="comment">#可选：替换原字段</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> standardFeatureList:</span><br><span class="line">    newDF = newDF.drop(feat).withColumnRenamed(<span class="string">"stdized_"</span>+feat, feat)</span><br></pre></td></tr></table></figure><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.function <span class="keyword">import</span> log10, log2, log, sqrt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可选方法如下（可增加）</span></span><br><span class="line">scaleFuncDict = &#123;</span><br><span class="line">    <span class="string">"log2"</span>: (<span class="keyword">lambda</span> x: log2(x)),</span><br><span class="line">    <span class="string">"log10"</span>: (<span class="keyword">lambda</span> x: log10(x)),</span><br><span class="line">    <span class="string">"ln"</span>: (<span class="keyword">lambda</span> x: log(x)),</span><br><span class="line">    <span class="string">"abs"</span>: (<span class="keyword">lambda</span> x: abs(x)),</span><br><span class="line">    <span class="string">"sqrt"</span>: (<span class="keyword">lambda</span> x: sqrt(x)),</span><br><span class="line">    <span class="string">"square"</span>: (<span class="keyword">lambda</span> x: x**<span class="number">2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所选的特征缩放方法</span></span><br><span class="line">scaleMethod = <span class="string">"log2"</span></span><br><span class="line"></span><br><span class="line">newDF = df</span><br><span class="line"><span class="comment"># scaleFeatureList: 需要特征缩放的变量</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> scaleFeatureList:</span><br><span class="line">    newDF = newDF.withColumn(<span class="string">"scaled_"</span>+feat, scaleFuncDict[scaleMethod](newDF[feat]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#可选：替换原字段</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> scaleFeatureList:</span><br><span class="line">    newDF = newDF.drop(feat).withColumnRenamed(<span class="string">"scaled_"</span>+feat, feat)</span><br></pre></td></tr></table></figure><h2 id="异常特征平滑"><a href="#异常特征平滑" class="headerlink" title="异常特征平滑"></a>异常特征平滑</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">newDF = df.copy()</span><br><span class="line">featureThreshMap = &#123;feat:&#123;&#125; <span class="keyword">for</span> feat <span class="keyword">in</span> softenFeatureList&#125;</span><br><span class="line"><span class="comment"># softenFeatureList: 需要进行异常特征平滑的变量</span></span><br><span class="line">softenMethod = <span class="string">"zScore"</span> <span class="comment"># or "minMaxThresh" / "minMaxPercent" </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算统计指标</span></span><br><span class="line">statisticDict = &#123;&#125;</span><br><span class="line">dataDescribeDF = df.describe(softenFeatureList) <span class="comment"># 对数值型变量计算统计指标</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataDescribeDF.collect():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(softenFeatureList)):</span><br><span class="line">        featName = softenFeatureList[i]</span><br><span class="line">        statName = row[<span class="number">0</span>].encode(<span class="string">"utf-8"</span>)</span><br><span class="line">        statValue = round(eval(row[i+<span class="number">1</span>]), <span class="number">3</span>) </span><br><span class="line">        <span class="comment"># eval()函数将去掉字符串的两个引号,将其解释为一个变量。单/双引号eval()函数都将其解释为int类型</span></span><br><span class="line">        statisticDict[featName][statName] = statValue</span><br><span class="line">       </span><br><span class="line"><span class="comment"># 1. softenMethod = "zScore"</span></span><br><span class="line">zValue = <span class="number">1.96</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> featureThreshMap:</span><br><span class="line">    mean, stddev = statisticDict[feat][<span class="string">"mean"</span>], statisticDict[feat][<span class="string">"stddev"</span>]</span><br><span class="line">    featureThreshMap[feat][<span class="string">"minThresh"</span>] = mean - zValue * stddev</span><br><span class="line">    featureThreshMap[feat][<span class="string">"maxThresh"</span>] = mean + zValue * stddev</span><br><span class="line">    <span class="comment"># 进行类型转换（转为Double类型）</span></span><br><span class="line">    newDF = newDF.withColumn(feat+<span class="string">"_tmp"</span>, df[feat].cast(DoubleType())).drop(feat).withColumnRenamed(feat+<span class="string">"_tmp"</span>, feat)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 2. softenMethod = "minMaxPercent" </span></span><br><span class="line">minPercent = <span class="number">0.25</span></span><br><span class="line">maxPercent = <span class="number">0.75</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> featureThreshMap:</span><br><span class="line">    featureThreshMap[feat][<span class="string">"minThresh"</span>], featureThreshMap[feat][<span class="string">"maxThresh"</span>] = df.approxQuantile(feat, [minPercent, maxPercent], <span class="number">0.01</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 3. softenMethod = "minMaxThresh" </span></span><br><span class="line">minThresh = <span class="number">0</span></span><br><span class="line">maxThresh = <span class="number">100</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> featureThreshMap:</span><br><span class="line">    featureThreshMap[feat][<span class="string">"minThresh"</span>], featureThreshMap[feat][<span class="string">"maxThresh"</span>] = minThresh, maxThresh</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 创建平滑UDF</span></span><br><span class="line">featureTypeMap = &#123;k:v <span class="keyword">for</span> k,v <span class="keyword">in</span> df.dtypes&#125; <span class="comment"># 需要对bigint/double类型进行转换，转换为int/float</span></span><br><span class="line">minV, maxV = featureThreshMap[feat][<span class="string">"minThresh"</span>], featureThreshMap[feat][<span class="string">"maxThresh"</span>]</span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> featureThreshMap:</span><br><span class="line">    <span class="keyword">if</span> featureTypeMap[feat] == <span class="string">"bigint"</span>:</span><br><span class="line">        featureType = IntegerType()</span><br><span class="line">        minV, maxV = int(minV), int(maxV)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        featureType = FloatType()</span><br><span class="line">        minV, maxV = float(minV), float(maxV)</span><br><span class="line">    compare = <span class="keyword">lambda</span> x: minV <span class="keyword">if</span> x &lt; minV <span class="keyword">else</span> (maxV <span class="keyword">if</span> x&gt;maxV <span class="keyword">else</span> x)</span><br><span class="line">    compareUDF = udf(compare, featureType)</span><br><span class="line">    newDF = newDF.withColumn(<span class="string">"soft_"</span>+feat, compareUDF(newDF[feat]))</span><br><span class="line">    <span class="comment"># 可选：替换原始变量</span></span><br><span class="line">    newDF = newDF.drop(feat).withColumnRenamed(<span class="string">"soft_"</span>+feat, feat)</span><br></pre></td></tr></table></figure><h2 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer, OneHotEncoder</span><br><span class="line"></span><br><span class="line">newDF = df.select(oneHotFeatList)</span><br><span class="line">colList = newDF.columns</span><br><span class="line">colAmount = len(colList)</span><br><span class="line"></span><br><span class="line"><span class="comment"># oneHotFeatList: 要进行one-hot编码的变量</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> oneHotFeatList:</span><br><span class="line">    <span class="comment"># 先对变量进行数值化编码</span></span><br><span class="line">    stringIndexer = StringIndexer(inputCol=feat, outputCol=<span class="string">"categoryIndex"</span>)</span><br><span class="line">    model = stringIndexer.fit(newDF)</span><br><span class="line">    indexed = model.transform(newDF)</span><br><span class="line">    <span class="comment"># 一个变量名生成多个变量名</span></span><br><span class="line">    categoryList = [feat+<span class="string">"_"</span>+str(cat) <span class="keyword">for</span> cat,index <span class="keyword">in</span> indexed.select(feat,<span class="string">"categoryIndex"</span>).distinct().orderBy(<span class="string">"categoryIndex"</span>).collect()]</span><br><span class="line">    <span class="comment"># 然后进行one-hot编码</span></span><br><span class="line">    encoder = OneHotEncoder(inputCol=<span class="string">"categoryIndex"</span>,outputCol=<span class="string">"categoryVec"</span>,dropLast=<span class="literal">False</span>)</span><br><span class="line">    encoded = encoder.transform(indexed)</span><br><span class="line">    newDF = encoded.select(colList+[<span class="string">"categoryVec"</span>]).rdd\</span><br><span class="line">           .map(<span class="keyword">lambda</span> row: tuple(list(row[<span class="number">0</span>:colAmount])+[float(x) <span class="keyword">for</span> x <span class="keyword">in</span> row[<span class="number">-1</span>].toArray()]))\</span><br><span class="line">           .toDF(colList+categoryList)</span><br><span class="line">    colList = colList+categoryList</span><br><span class="line">    colAmount = len(colList)</span><br></pre></td></tr></table></figure><blockquote><p>OneHotEncoder中的dropLast含义：</p><p><a href="https://stackoverflow.com/questions/39500213/why-does-sparks-onehotencoder-drop-the-last-category-by-default" target="_blank" rel="noopener">https://stackoverflow.com/questions/39500213/why-does-sparks-onehotencoder-drop-the-last-category-by-default</a></p><p><a href="https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/ml/feature/OneHotEncoder.html" target="_blank" rel="noopener">https://spark.apache.org/docs/1.5.2/api/java/org/apache/spark/ml/feature/OneHotEncoder.html</a></p></blockquote><h2 id="分箱"><a href="#分箱" class="headerlink" title="分箱"></a>分箱</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Bucketizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitRangeIntoFloatList</span><span class="params">(start, end, numOfSplits)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    返回下界start到上界end之间的等分点。</span></span><br><span class="line"><span class="string">    例：(start=1, end=5, numOfSplits=2) -&gt; [1.0,3.0,5.0]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    stepsize = (float(end)-float(start))/numOfSplits <span class="comment"># 间隔长度</span></span><br><span class="line">    resultList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numOfSplits):</span><br><span class="line">        resultList.append(round(start+stepsize*i), <span class="number">3</span>)</span><br><span class="line">    resultList.append(float(end))</span><br><span class="line">    <span class="keyword">return</span> resultList</span><br><span class="line"></span><br><span class="line">newDF = df.copy()</span><br><span class="line">numOfSplits = <span class="number">5</span> <span class="comment"># 分箱的个数</span></span><br><span class="line"><span class="comment"># bucketizedFeatureList: list, 要进行分箱的字段列表</span></span><br><span class="line">featureSplitDict = &#123;feat:[] <span class="keyword">for</span> feat <span class="keyword">in</span> bucketizedFeatureList&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> bucketizedFeatureList:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"*** Handling %s ***"</span> % feat) <span class="comment"># 每次循环可能耗时较长</span></span><br><span class="line">    <span class="comment"># 等频分箱</span></span><br><span class="line">    featSplits = newDF.approxQuantile(<span class="string">"typed_"</span>+feat, splitRangeIntoFloatList(<span class="number">0</span>,<span class="number">1</span>,numOfSplits),<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># 等距分箱</span></span><br><span class="line">    <span class="comment"># featSplits = splitRangeIntoFloatList(newDF[feat].min(),newDF[feat].max(),numOfSplits)</span></span><br><span class="line">    featSplits[<span class="number">0</span>] = float(<span class="string">"-inf"</span>)</span><br><span class="line">    featSplits[<span class="number">-1</span>] = float(<span class="string">"inf"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(featSplits))[::<span class="number">-1</span>][<span class="number">1</span>:]:</span><br><span class="line">        <span class="keyword">while</span> featSplits[i] &gt;= featSplits[i+<span class="number">1</span>]:</span><br><span class="line">            featSplits[i] -= <span class="number">0.00001</span> <span class="comment"># 避免数据倾斜</span></span><br><span class="line">    featureSplitDict[feat] = featSplits</span><br><span class="line">    bucketizer = Bucketizer(splits=featSplits, inputCol=<span class="string">"typed_"</span>+feat, outputCol=<span class="string">"discreate_"</span>+feat, handleInvalid=<span class="string">"keep"</span>)</span><br><span class="line">    newDF = bucketizer.transform(newDF)</span><br><span class="line">    newDF = newDF.drop(<span class="string">"typed_"</span>+feat)</span><br><span class="line">    <span class="comment"># 删除原始变量</span></span><br><span class="line">    newDF = newDF.drop(feat).withColumnRenamed(<span class="string">"discreate_"</span>+feat, feat)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PySpark常用操作总结——增删改查</title>
      <link href="/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-zeng-shan-gai-cha/"/>
      <url>/2020/03/22/pyspark-chang-yong-cao-zuo-zong-jie-zeng-shan-gai-cha/</url>
      
        <content type="html"><![CDATA[<h1 id="配置-amp-启动"><a href="#配置-amp-启动" class="headerlink" title="配置&amp;启动"></a>配置&amp;启动</h1><p>测试环境：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br><span class="line">--master yarn-client</span><br><span class="line">--driver-cores <span class="number">1</span></span><br><span class="line">--driver-memory <span class="number">2</span>g</span><br><span class="line">--num-executors <span class="number">2</span></span><br><span class="line">--executor-cores <span class="number">1</span></span><br><span class="line">--executor-memory <span class="number">3</span>g</span><br><span class="line"><span class="comment">#--queue ...</span></span><br><span class="line"><span class="comment">#--name ...</span></span><br></pre></td></tr></table></figure><p>可适当调大</p><p>建议：executor的cores:memory = 1:3<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> HiveContext, SparkSession, Row, DataFrame</span><br><span class="line"></span><br><span class="line">sc = SparkContext()</span><br></pre></td></tr></table></figure></p><h1 id="格式转换-amp-保存"><a href="#格式转换-amp-保存" class="headerlink" title="格式转换&amp;保存"></a>格式转换&amp;保存</h1><p>spark dataframe持久化为hive表（若表名重复则替换旧表）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df.write.saveAsTable(tableName)</span><br></pre></td></tr></table></figure><p>spark dataframe存成临时表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark_df.registerTempTable(<span class="string">"tableName"</span>) </span><br><span class="line">sql_result = spark.sql(<span class="string">"select * from tableName"</span>) <span class="comment"># 对临时表进行查询，可简化嵌套查询</span></span><br></pre></td></tr></table></figure><blockquote><p>使用registerTempTable注册表是一个临时表，生命周期只在所定义的sqlContext或hiveContext实例之中。换而言之，在一个sqlontext（或hiveContext）中registerTempTable的表不能在另一个sqlContext（或hiveContext）中使用。</p><p>而saveAsTable则是永久的，只要连接存在，spark再启的时候，这个表还是在的。</p></blockquote><p>spark dataframe转化为pandas dataframe</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pandas_df = spark_df.toPandas()</span><br></pre></td></tr></table></figure><p>注：数据量大会非常缓慢，数据量小适用</p><p>pandas dataframe转spark dataframe</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark_df = sqlContext.createDataFrame(pandas_df)</span><br></pre></td></tr></table></figure><p>本地csv转spark dataframe</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">raw_rdd = sc.text_file(<span class="string">"data.csv"</span>)</span><br><span class="line">df_spark = spark.createDataFrame(raw_rdd)</span><br></pre></td></tr></table></figure><h1 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h1><h2 id="查"><a href="#查" class="headerlink" title="查"></a>查</h2><h3 id="行元素查询操作"><a href="#行元素查询操作" class="headerlink" title="行元素查询操作"></a>行元素查询操作</h3><p>像SQL那样打印列表前20元素<br>show函数内可用int类型指定要打印的行数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.show()</span><br><span class="line">df.show(<span class="number">30</span>)</span><br></pre></td></tr></table></figure><p>以树的形式打印概要</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure><p>获取头几行到本地：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">list = df.head(<span class="number">3</span>)   <span class="comment"># Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]</span></span><br><span class="line">list = df.take(<span class="number">5</span>)   <span class="comment"># Example: [Row(a=1, b=1), Row(a=2, b=2), ... ...]</span></span><br></pre></td></tr></table></figure></p><p>查询总行数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.count()</span><br></pre></td></tr></table></figure></p><p>取别名<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(df.age.alias(<span class="string">'age_value'</span>),<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure></p><p>查询某列为null的行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">df = df.filter(isnull(<span class="string">"col_a"</span>))</span><br></pre></td></tr></table></figure></p><p>输出list类型，list中每个元素是Row类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list = df.collect()</span><br></pre></td></tr></table></figure><p>注：此方法将所有数据全部导入到本地，返回一个Array对象</p><p>查询某列为null的行：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> isnull</span><br><span class="line">df = df.filter(isnull(<span class="string">"col_a"</span>))</span><br></pre></td></tr></table></figure></p><p>输出list类型，list中每个元素是Row类：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list = df.collect()</span><br></pre></td></tr></table></figure></p><p>注：此方法将所有数据全部导入到本地，返回一个Array对象</p><p>去重set操作<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.select(<span class="string">'columns'</span>).distinct().show()</span><br></pre></td></tr></table></figure></p><p>跟py中的set一样，可以distinct()一下去重，同时也可以.count()计算剩余个数</p><p>随机抽样<br>随机抽样有两种方式，一种是在HIVE里面查数随机；另一种是在pyspark之中。</p><p>HIVE里面查数随机<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql = <span class="string">"select * from data order by rand()  limit 2000"</span></span><br></pre></td></tr></table></figure></p><p>pyspark之中<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample = result.sample(<span class="literal">False</span>,<span class="number">0.5</span>,<span class="number">0</span>) <span class="comment"># randomly select 50% of lines</span></span><br></pre></td></tr></table></figure></p><h3 id="列元素操作"><a href="#列元素操作" class="headerlink" title="列元素操作"></a>列元素操作</h3><p>获取Row元素的所有列名：</p><p>r = Row(age=11, name=’Alice’)<br>print r.columns    #  [‘age’, ‘name’]<br>1<br>2<br>选择一列或多列：select<br>df[“age”]<br>df.age<br>df.select(“name”)<br>df.select(df[‘name’], df[‘age’]+1)<br>df.select(df.a, df.b, df.c)    # 选择a、b、c三列<br>df.select(df[“a”], df[“b”], df[“c”])    # 选择a、b、c三列</p><p>重载的select方法：<br>jdbcDF.select(jdbcDF( “id” ), jdbcDF( “id”) + 1 ).show( false)<br>1<br>会同时显示id列 + id + 1列</p><p>还可以用where按条件选择<br>jdbcDF .where(“id = 1 or c1 = ‘b’” ).show()<br>1<br>— 1.3 排序 —<br>orderBy和sort：按指定字段排序，默认为升序</p><p>train.orderBy(train.Purchase.desc()).show(5)<br>Output:<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|1003160| P00052842|     M|26-35|        17|            C|                         3|             0|                10|                15|              null|   23961|<br>|1002272| P00052842|     M|26-35|         0|            C|                         1|             0|                10|                15|              null|   23961|<br>|1001474| P00052842|     M|26-35|         4|            A|                         2|             1|                10|                15|              null|   23961|<br>|1005848| P00119342|     M|51-55|        20|            A|                         0|             1|                10|                13|              null|   23960|<br>|1005596| P00117642|     M|36-45|        12|            B|                         1|             0|                10|                16|              null|   23960|<br>+———-+—————+———+——-+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>only showing top 5 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>按指定字段排序。加个-表示降序排序</p><p>— 1.4 抽样 —<br>sample是抽样函数</p><p>t1 = train.sample(False, 0.2, 42)<br>t2 = train.sample(False, 0.2, 43)<br>t1.count(),t2.count()<br>Output:<br>(109812, 109745)<br>1<br>2<br>3<br>4<br>5<br>withReplacement = True or False代表是否有放回。<br>fraction = x, where x = .5，代表抽取百分比</p><p>— 1.5 按条件筛选when / between —<br>when(condition, value1).otherwise(value2)联合使用：<br>那么：当满足条件condition的指赋值为values1,不满足条件的则赋值为values2.<br>otherwise表示，不满足条件的情况下，应该赋值为啥。</p><p>demo1</p><blockquote><blockquote><blockquote><p>from pyspark.sql import functions as F<br>df.select(df.name, F.when(df.age &gt; 4, 1).when(df.age &lt; 3, -1).otherwise(0)).show()<br>+——-+——————————————————————————————+<br>| name|CASE WHEN (age &gt; 4) THEN 1 WHEN (age &lt; 3) THEN -1 ELSE 0 END|<br>+——-+——————————————————————————————+<br>|Alice|                                                          -1|<br>|  Bob|                                                           1|<br>+——-+——————————————————————————————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>demo 2:多个when串联</p></blockquote></blockquote></blockquote><p>df = df.withColumn(‘mod_val_test1’,F.when(df[‘rand’] &lt;= 0.35,1).when(df[‘rand’] &lt;= 0.7, 2).otherwise(3))<br>1<br>between(lowerBound, upperBound)<br>筛选出某个范围内的值，返回的是TRUE or FALSE</p><blockquote><blockquote><blockquote><p>df.select(df.name, df.age.between(2, 4)).show()<br>+——-+—————————————-+<br>| name|((age &gt;= 2) AND (age &lt;= 4))|<br>+——-+—————————————-+<br>|Alice|                       true|<br>|  Bob|                      false|<br>+——-+—————————————-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>选择dataframe中间的特定行数<br>而我使用的dataframe前两种方法都没法解决。特点如下：</p></blockquote></blockquote></blockquote><p>特定列中的内容为字符串，并非数值，不能直接比较大小。<br>所选取数据为中间行，如第10~20行，不能用函数直接选取。<br>最终的解决方法如下：</p><p>首先添加行索引，然后选择特定区间内的行索引，从而选取特定中间行。<br>第一步，添加行索引。</p><p>from pyspark.sql.functions import monotonically_increasing_id</p><p>dfWithIndex = df.withColumn(“id”,monotonically_increasing_id())<br>1<br>2<br>3<br>第二步，筛选特定行。</p><p>dfWithIndex.select(dfWithIndex.name, dfWithIndex.id.between(50, 100)).show()<br>1<br>2、———— 增、改 ————<br>— 2.1 新建数据 —<br>有这么两种常规的新建数据方式：createDataFrame、.toDF()</p><p>sqlContext.createDataFrame(pd.dataframe())<br>1<br>是把pandas的dataframe转化为spark.dataframe格式，所以可以作为两者的格式转化</p><p>from pyspark.sql import Row<br>row = Row(“spe_id”, “InOther”)<br>x = [‘x1’,’x2’]<br>y = [‘y1’,’y2’]<br>new_df = sc.parallelize([row(x[i], y[i]) for i in range(2)]).toDF()<br>1<br>2<br>3<br>4<br>5<br>Row代表的是该数据集的列名。</p><p>— 2.2 新增数据列 withColumn—<br>withColumn是通过添加或替换与现有列有相同的名字的列，返回一个新的DataFrame</p><p>result3.withColumn(‘label’, 0)<br>1<br>或者案例</p><p>train.withColumn(‘Purchase_new’, train.Purchase /2.0).select(‘Purchase’,’Purchase_new’).show(5)<br>Output:<br>+————+——————+<br>|Purchase|Purchase_new|<br>+————+——————+<br>|    8370|      4185.0|<br>|   15200|      7600.0|<br>|    1422|       711.0|<br>|    1057|       528.5|<br>|    7969|      3984.5|<br>+————+——————+<br>only showing top 5 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br><strong>报错：</strong>AssertionError: col should be Column，一定要指定某现有列</p><p>有两种方式可以实现：</p><p>一种方式通过functions<br>from pyspark.sql import functions<br>result3 = result3.withColumn(‘label’,  functions.lit(0))<br>1<br>2<br>但是！！ 如何新增一个特别List??(参考：王强的知乎回复)<br>python中的list不能直接添加到dataframe中，需要先将list转为新的dataframe,然后新的dataframe和老的dataframe进行join操作, 下面的例子会先新建一个dataframe，然后将list转为dataframe，然后将两者join起来。</p><p>from pyspark.sql.functions import lit</p><p>df = sqlContext.createDataFrame(<br>    [(1, “a”, 23.0), (3, “B”, -23.0)], (“x1”, “x2”, “x3”))<br>from pyspark.sql.functions import monotonically_increasing_id<br>df = df.withColumn(“id”, monotonically_increasing_id())<br>df.show()<br>+—-+—-+——-+—-+<br>| x1| x2|   x3| id|<br>+—-+—-+——-+—-+<br>|  1|  a| 23.0|  0|<br>|  3|  B|-23.0|  1|<br>+—-+—-+——-+—-+<br>from pyspark.sql import Row<br>l = [‘jerry’, ‘tom’]<br>row = Row(“pid”, “name”)<br>new_df = sc.parallelize([row(i, l[i]) for i in range(0,len(l))]).toDF()<br>new_df.show()<br>+—-+——-+<br>|pid| name|<br>+—-+——-+<br>|  0|jerry|<br>|  1|  tom|<br>+—-+——-+<br>join_df = df.join(new_df, df.id==new_df.pid)<br>join_df.show()<br>+—-+—-+——-+—-+—-+——-+<br>| x1| x2|   x3| id|pid| name|<br>+—-+—-+——-+—-+—-+——-+<br>|  1|  a| 23.0|  0|  0|jerry|<br>|  3|  B|-23.0|  1|  1|  tom|<br>+—-+—-+——-+—-+—-+——-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32</p><h5 id="坑啊！！！其中，monotonically-increasing-id-生成的ID保证是单调递增和唯一的，但不是连续的。"><a href="#坑啊！！！其中，monotonically-increasing-id-生成的ID保证是单调递增和唯一的，但不是连续的。" class="headerlink" title="坑啊！！！其中，monotonically_increasing_id()生成的ID保证是单调递增和唯一的，但不是连续的。"></a><strong>坑啊！！！</strong>其中，monotonically_increasing_id()生成的ID保证是单调递增和唯一的，但不是连续的。</h5><p>所以，有可能，单调到1-140000，到了第144848个，就变成一长串：8845648744563，所以千万要注意！！</p><p>另一种方式通过另一个已有变量：<br>result3 = result3.withColumn(‘label’,  df.result*0 )<br>1<br>修改原有df[“xx”]列的所有值：<br>df = df.withColumn(“xx”, 1)<br>1<br>修改列的类型（类型投射）：<br>df = df.withColumn(“year2”, df[“year1”].cast(“Int”))<br>1<br>修改列名<br>jdbcDF.withColumnRenamed( “id” , “idx” )<br>1<br>— 2.3 过滤数据—</p><h5 id="过滤数据（filter和where方法相同）："><a href="#过滤数据（filter和where方法相同）：" class="headerlink" title="过滤数据（filter和where方法相同）："></a>过滤数据（filter和where方法相同）：</h5><p>df = df.filter(df[‘age’]&gt;21)<br>df = df.where(df[‘age’]&gt;21)<br>1<br>2<br>多个条件jdbcDF .filter(“id = 1 or c1 = ‘b’” ).show()</p><h5 id="对null或nan数据进行过滤："><a href="#对null或nan数据进行过滤：" class="headerlink" title="对null或nan数据进行过滤："></a>对null或nan数据进行过滤：</h5><p>from pyspark.sql.functions import isnan, isnull<br>df = df.filter(isnull(“a”))  # 把a列里面数据为null的筛选出来（代表python的None类型）<br>df = df.filter(isnan(“a”))  # 把a列里面数据为nan的筛选出来（Not a Number，非数字数据）<br>1<br>2<br>3<br>3、———— 合并 join / union ————<br>3.1 横向拼接rbind<br>result3 = result1.union(result2)<br>jdbcDF.unionALL(jdbcDF.limit(1)) # unionALL<br>1<br>2<br>— 3.2 Join根据条件 —<br>单字段Join<br>合并2个表的join方法：</p><p> df_join = df_left.join(df_right, df_left.key == df_right.key, “inner”)<br>1<br>其中，方法可以为：inner, outer, left_outer, right_outer, leftsemi.<br>其中注意，一般需要改为：left_outer</p><p>多字段join<br>joinDF1.join(joinDF2, Seq(“id”, “name”)）<br>1<br>混合字段<br>joinDF1.join(joinDF2 , joinDF1(“id” ) === joinDF2( “t1_id”))<br>1<br>跟pandas 里面的left_on,right_on</p><p>— 3.2 求并集、交集 —<br>来看一个例子，先构造两个dataframe：</p><p>sentenceDataFrame = spark.createDataFrame((<br>      (1, “asf”),<br>      (2, “2143”),<br>      (3, “rfds”)<br>    )).toDF(“label”, “sentence”)<br>sentenceDataFrame.show()</p><p>sentenceDataFrame1 = spark.createDataFrame((<br>      (1, “asf”),<br>      (2, “2143”),<br>      (4, “f8934y”)<br>    )).toDF(“label”, “sentence”)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12</p><h1 id="差集"><a href="#差集" class="headerlink" title="差集"></a>差集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).subtract(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p><p>+————+<br>|sentence|<br>+————+<br>|  f8934y|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9</p><h1 id="交集"><a href="#交集" class="headerlink" title="交集"></a>交集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).intersect(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p><p>+————+<br>|sentence|<br>+————+<br>|     asf|<br>|    2143|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</p><h1 id="并集"><a href="#并集" class="headerlink" title="并集"></a>并集</h1><p>newDF = sentenceDataFrame1.select(“sentence”).union(sentenceDataFrame.select(“sentence”))<br>newDF.show()</p><p>+————+<br>|sentence|<br>+————+<br>|     asf|<br>|    2143|<br>|  f8934y|<br>|     asf|<br>|    2143|<br>|    rfds|<br>+————+</p><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15</p><h1 id="并集-去重"><a href="#并集-去重" class="headerlink" title="并集 + 去重"></a>并集 + 去重</h1><p>newDF = sentenceDataFrame1.select(“sentence”).union(sentenceDataFrame.select(“sentence”)).distinct()<br>newDF.show()</p><p>+————+<br>|sentence|<br>+————+<br>|    rfds|<br>|     asf|<br>|    2143|<br>|  f8934y|<br>+————+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>— 3.3 分割：行转列 —<br>有时候需要根据某个字段内容进行分割，然后生成多行，这时可以使用explode方法<br>　　下面代码中，根据c3字段中的空格将字段内容进行分割，分割的内容存储在新的字段c3_中，如下所示</p><p>jdbcDF.explode( “c3” , “c3_” ){time: String =&gt; time.split( “ “ )}<br>1</p><p>4 ———— 统计 ————<br>— 4.1 频数统计与筛选 ——<br>jdbcDF.stat.freqItems(Seq (“c1”) , 0.3).show()<br>1<br>根据c4字段，统计该字段值出现频率在30%以上的内容</p><p>— 4.2 分组统计—<br>交叉分析<br>train.crosstab(‘Age’, ‘Gender’).show()<br>Output:<br>+—————+——-+———+<br>|Age_Gender|    F|     M|<br>+—————+——-+———+<br>|      0-17| 5083| 10019|<br>|     46-50|13199| 32502|<br>|     18-25|24628| 75032|<br>|     36-45|27170| 82843|<br>|       55+| 5083| 16421|<br>|     51-55| 9894| 28607|<br>|     26-35|50752|168835|<br>+—————+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>groupBy方法整合：<br>train.groupby(‘Age’).agg({‘Purchase’: ‘mean’}).show()<br>Output:<br>+——-+————————-+<br>|  Age|    avg(Purchase)|<br>+——-+————————-+<br>|51-55|9534.808030960236|<br>|46-50|9208.625697468327|<br>| 0-17|8933.464640444974|<br>|36-45|9331.350694917874|<br>|26-35|9252.690632869888|<br>|  55+|9336.280459449405|<br>|18-25|9169.663606261289|<br>+——-+————————-+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>另外一些demo：</p><p>df[‘x1’].groupby(df[‘x2’]).count().reset_index(name=’x1’)<br>1<br>分组汇总</p><p>train.groupby(‘Age’).count().show()<br>Output:<br>+——-+———+<br>|  Age| count|<br>+——-+———+<br>|51-55| 38501|<br>|46-50| 45701|<br>| 0-17| 15102|<br>|36-45|110013|<br>|26-35|219587|<br>|  55+| 21504|<br>|18-25| 99660|<br>+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>应用多个函数：</p><p>from pyspark.sql import functions<br>df.groupBy(“A”).agg(functions.avg(“B”), functions.min(“B”), functions.max(“B”)).show()<br>1<br>2<br>整合后GroupedData类型可用的方法（均返回DataFrame类型）：<br>avg(<em>cols)     ——   计算每组中一列或多列的平均值<br>count()          ——   计算每组中一共有多少行，返回DataFrame有2列，一列为分组的组名，另一列为行总数<br>max(</em>cols)    ——   计算每组中一列或多列的最大值<br>mean(<em>cols)  ——  计算每组中一列或多列的平均值<br>min(</em>cols)     ——  计算每组中一列或多列的最小值<br>sum(*cols)    ——   计算每组中一列或多列的总和<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>— 4.3 apply 函数 —<br>将df的每一列应用函数f：</p><p>df.foreach(f) 或者 df.rdd.foreach(f)<br>1<br>将df的每一块应用函数f：</p><p>df.foreachPartition(f) 或者 df.rdd.foreachPartition(f)<br>1<br>—— 4.4 【Map和Reduce应用】返回类型seqRDDs ——<br>map函数应用<br>可以参考：Spark Python API函数学习：pyspark API(1)</p><p>train.select(‘User_ID’).rdd.map(lambda x:(x,1)).take(5)<br>Output:<br>[(Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000001), 1),<br> (Row(User_ID=1000002), 1)]<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>其中map在spark2.0就移除了，所以只能由rdd.调用。</p><p>data.select(‘col’).rdd.map(lambda l: 1 if l in [‘a’,’b’] else 0 ).collect()</p><p>print(x.collect())<br>print(y.collect())</p><p>[1, 2, 3]<br>[(1, 1), (2, 4), (3, 9)]<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>还有一种方式mapPartitions：</p><p>def _map_to_pandas(rdds):<br>    “”” Needs to be here due to pickling issues “””<br>    return [pd.DataFrame(list(rdds))]</p><p>data.rdd.mapPartitions(_map_to_pandas).collect()<br>1<br>2<br>3<br>4<br>5<br>返回的是list。</p><p>udf 函数应用</p><p>from pyspark.sql.functions import udf<br>from pyspark.sql.types import StringType<br>import datetime</p><h1 id="定义一个-udf-函数"><a href="#定义一个-udf-函数" class="headerlink" title="定义一个 udf 函数"></a>定义一个 udf 函数</h1><p>def today(day):<br>    if day==None:<br>        return datetime.datetime.fromtimestamp(int(time.time())).strftime(‘%Y-%m-%d’)<br>    else:<br>        return day</p><h1 id="返回类型为字符串类型"><a href="#返回类型为字符串类型" class="headerlink" title="返回类型为字符串类型"></a>返回类型为字符串类型</h1><p>udfday = udf(today, StringType())</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>df.withColumn(‘day’, udfday(df.day))</p><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>有点类似apply,定义一个 udf 方法, 用来返回今天的日期(yyyy-MM-dd):</p><p>———— 5、删除 ————<br>df.drop(‘age’).collect()<br>df.drop(df.age).collect()<br>1<br>2<br>dropna函数：</p><p>df = df.na.drop()  # 扔掉任何列包含na的行<br>df = df.dropna(subset=[‘col_name1’, ‘col_name2’])  # 扔掉col1或col2中任一一列包含na的行</p><p>1<br>2<br>3<br>ex:</p><p>train.dropna().count()<br>Output:<br>166821<br>1<br>2<br>3<br>填充NA包括fillna</p><p>train.fillna(-1).show(2)<br>Output:<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|                -1|                -1|    8370|<br>|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|                 6|                14|   15200|<br>+———-+—————+———+——+—————+——————-+—————————————+———————+—————————+—————————+—————————+————+<br>only showing top 2 rows<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>———— 6、去重 ————<br>6.1 distinct：返回一个不包含重复记录的DataFrame<br>返回当前DataFrame中不重复的Row记录。该方法和接下来的dropDuplicates()方法不传入指定字段时的结果相同。<br>　　示例：</p><p>jdbcDF.distinct()<br>1<br>6.2 dropDuplicates：根据指定字段去重<br>根据指定字段去重。类似于select distinct a, b操作<br>示例：</p><p>train.select(‘Age’,’Gender’).dropDuplicates().show()<br>Output:<br>+——-+———+<br>|  Age|Gender|<br>+——-+———+<br>|51-55|     F|<br>|51-55|     M|<br>|26-35|     F|<br>|26-35|     M|<br>|36-45|     F|<br>|36-45|     M|<br>|46-50|     F|<br>|46-50|     M|<br>|  55+|     F|<br>|  55+|     M|<br>|18-25|     F|<br>| 0-17|     F|<br>|18-25|     M|<br>| 0-17|     M|<br>+——-+———+<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>———— 7、 格式转换 ————<br>pandas-spark.dataframe互转<br>Pandas和Spark的DataFrame两者互相转换：</p><p>pandas_df = spark_df.toPandas()<br>spark_df = sqlContext.createDataFrame(pandas_df)<br>1<br>2<br>转化为pandas，但是该数据要读入内存，如果数据量大的话，很难跑得动</p><p>两者的异同：</p><p>Pyspark DataFrame是在分布式节点上运行一些数据操作，而pandas是不可能的；<br>Pyspark DataFrame的数据反映比较缓慢，没有Pandas那么及时反映；<br>Pyspark DataFrame的数据框是不可变的，不能任意添加列，只能通过合并进行；<br>pandas比Pyspark DataFrame有更多方便的操作以及很强大<br>转化为RDD<br>与Spark RDD的相互转换：</p><p>rdd_df = df.rdd<br>df = rdd_df.toDF()<br>1<br>2<br>———— 8、SQL操作 ————<br>DataFrame注册成SQL的表：</p><p>df.createOrReplaceTempView(“TBL1”)<br>1<br>进行SQL查询（返回DataFrame）：</p><p>conf = SparkConf()<br>ss = SparkSession.builder.appName(“APP_NAME”).config(conf=conf).getOrCreate()</p><p>df = ss.sql(“SELECT name, age FROM TBL1 WHERE age &gt;= 13 AND age &lt;= 19″)<br>1<br>2<br>3<br>4<br>———— 9、读写csv ————<br>在Python中，我们也可以使用SQLContext类中 load/save函数来读取和保存CSV文件：</p><p>from pyspark.sql import SQLContext<br>sqlContext = SQLContext(sc)<br>df = sqlContext.load(source=”com.databricks.spark.csv”, header=”true”, path = “cars.csv”)<br>df.select(“year”, “model”).save(“newcars.csv”, “com.databricks.spark.csv”,header=”true”)<br>1<br>2<br>3<br>4<br>其中，header代表是否显示表头。<br>其中主函数：</p><p>save(path=None, format=None, mode=None, partitionBy=None, **options)[source]<br>1<br>Parameters:</p><p>path – the path in a Hadoop supported file system</p><p>format – the format used to save</p><p>mode –</p><p>specifies the behavior of the save operation when data already<br>exists.</p><p>append: Append contents of this DataFrame to existing data.</p><p>overwrite: Overwrite existing data.</p><p>ignore: Silently ignore this operation if data already exists.</p><p>error (default case): Throw an exception if data already exists.</p><p>partitionBy – names of partitioning columns</p><p>options – all other string options</p><p>延伸一：去除两个表重复的内容<br>场景是要，依据B表与A表共有的内容，需要去除这部分共有的。<br>使用的逻辑是merge两张表，然后把匹配到的删除即可。</p><p>from pyspark.sql import functions<br>def LeftDeleteRight(test_left,test_right,left_col = ‘user_pin’,right_col = ‘user_pin’):<br>    print(‘right data process …’)<br>    columns_right = test_right.columns<br>    test_right = test_right.withColumn(‘user_pin_right’, test_right[right_col])<br>    test_right = test_right.withColumn(‘notDelete’,  functions.lit(0))</p><pre><code># 删除其余的for col in columns_right:    test_right = test_right.drop(col)# 合并print(&#39;rbind left and right data ...&#39;)test_left = test_left.join(test_right, test_left[left_col] == test_right[&#39;user_pin_right&#39;], &quot;left&quot;)test_left = test_left.fillna(1)test_left = test_left.where(&#39;notDelete =1&#39;)# 去掉多余的字段for col in [&#39;user_pin_right&#39;,&#39;notDelete&#39;]:    test_left = test_left.drop(col)return test_left</code></pre><p>%time  test_left = LeftDeleteRight(test_b,test_a,left_col = ‘user_pin’,right_col = ‘user_pin’)<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>延伸二：报错<br>Job aborted due to stage failure: Task 3 in stage 0.0 failed 4 times, most recent failure: Lost task 3.3 in</p><p>1<br>2<br>解决方案</p><p>这里遇到的问题主要是因为数据源数据量过大，而机器的内存无法满足需求，导致长时间执行超时断开的情况，数据无法有效进行交互计算，因此有必要增加内存</p><p>参考：Spark常见问题汇总：<a href="https://my.oschina.net/tearsky/blog/629201" target="_blank" rel="noopener">https://my.oschina.net/tearsky/blog/629201</a><br>————————————————<br>版权声明：本文为CSDN博主「悟乙己」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/sinat_26917383/article/details/80500349" target="_blank" rel="noopener">https://blog.csdn.net/sinat_26917383/article/details/80500349</a></p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数据分析与数据化运营Note——Part4：数据分析实战</title>
      <link href="/2020/03/20/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part4/"/>
      <url>/2020/03/20/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part4/</url>
      
        <content type="html"><![CDATA[<h1 id="Ch4-向双11进军，数据分析实战开始"><a href="#Ch4-向双11进军，数据分析实战开始" class="headerlink" title="Ch4 向双11进军，数据分析实战开始"></a>Ch4 向双11进军，数据分析实战开始</h1><h2 id="L11-店铺的诊断分析方法"><a href="#L11-店铺的诊断分析方法" class="headerlink" title="L11　店铺的诊断分析方法"></a>L11　店铺的诊断分析方法</h2><p>三基分析法——<strong>“用户数”、“平均销售金额”、“复购率”</strong></p><ol><li><p>用户数</p><p>用户数是指成交后的买家数。通过买家数的多少，以及买家数的年均增长情况，来判断店铺当前所处的运营状态，并且是否保持增长的态势。</p></li><li><p>平均消费金额</p><p>指<strong>人均消费金额</strong>。通过人均消费金额，可以评估品牌的消费人群定位，以及盈利期望是否合理。</p></li><li><p>复购率</p><p>以服装为例，复购率高的品牌，其用户忠诚度非常高。品牌调性、产品，以及服务质量都得到用户的认同；对“付费流量”的依赖相对较低，因此可以节省更多的市场推广费用。这部分资金便可以使用到其他方面去，比如提升售后服务质量、改良产品质量等，这样便可以形成一个有益的经营循环。</p></li></ol><p>某店铺最近三年的“三基分析”数据：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/816cd7304d645951.png" alt></p><ol><li>2014年的用户数中没有三年老客，间接证明这是一家成立于2013年的店。</li><li>2014年、2015年两年期间，店铺花费了相当大的精力用于“新客引流”，并且取得了30%的用户数增长。 </li><li>从2016年开始，店铺的新老客占比接近5:5，店铺已经不再完全依赖于“新客引流”，间接证明店铺的流量结构已经趋于合理；同时复购率的增加也直接证明店铺近两年在“老客维护”方面取得不小的进步。 </li></ol><h2 id="L12-店铺的流量分析"><a href="#L12-店铺的流量分析" class="headerlink" title="L12 店铺的流量分析"></a>L12 店铺的流量分析</h2><h3 id="流量来源分析：流量从哪里来？"><a href="#流量来源分析：流量从哪里来？" class="headerlink" title="流量来源分析：流量从哪里来？"></a>流量来源分析：流量从哪里来？</h3><p>店铺流量来源及转化质量：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/68ea45f7f46be4a6.png" alt></p><p>在流量分析中还需要关注以下几个问题：</p><ol><li>店铺的流量是从什么渠道来的？</li><li>哪些渠道是主要渠道？</li><li>哪些渠道是付费/免费渠道？</li><li>哪些渠道的流量转化率最高？</li><li>当前流量的渠道分布占比是否正常？</li></ol><p>分析方法：</p><ul><li><p>参考数据</p><p><img src="https://i.bmp.ovh/imgs/2020/03/6037428acf7a6189.png" alt></p></li><li><p>用Excel的堆积图来跟踪每日流量的变化</p><p><img src="https://i.bmp.ovh/imgs/2020/03/af8a50c29d301cab.png" alt></p></li><li><p>店铺大型促销活动时的流量分析</p><p><img src="https://i.bmp.ovh/imgs/2020/03/904b58d1a4b17ad3.png" alt></p><p>通过上表，我们可以发现：</p><ol><li>无线端的流量占到全店的74%，但转化率却远远低于PC端，因此这家店铺的问题是<strong>应该想办法优化提升无线端的成交转化率</strong>。</li><li>PC端中，来自男装会场和聚划算的流量较高，占全店铺流量的18%，<strong>因此这两个渠道来源的流量应被视为重点流量</strong>，认真检查好承接页的商品及页面效果。</li></ol></li></ul><h3 id="流量路径分析：流量到哪儿去？"><a href="#流量路径分析：流量到哪儿去？" class="headerlink" title="流量路径分析：流量到哪儿去？"></a>流量路径分析：流量到哪儿去？</h3><p>一般情况下，在大促活动中的主推款会承接30%～50%的店铺流量。所以，从活动预热期开始，我们就必须挑选好本次活动的主推款，以及每天实时监测主推款的预热情况是否理想。</p><ul><li><p>要挑选哪些款作为主推款呢？</p><p>生意参谋中的流量来源去向查询：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/35f72966928fb37d.png" alt></p><p>商品温度计功能：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/19e55cf69d807cf8.png" alt></p></li><li><p>找到这些主推款之后，又需要如何监测呢？</p><p>从活动预热期开始，我们需要建立所有主推款的<strong>预热效果追踪表</strong>：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/7bd8a4a5d1621256.png" alt></p><p>表中加购倍率（=加购数/备货数）如果高于100%，则可视为预热表现良好的商品；若低于100%，则视为预热表现非常差；而若是高于500%，则可判断为非常热销的商品。据此分类，可为商品的预热策略调整提供指导意见。</p></li></ul><h2 id="L13-店铺的商品分析"><a href="#L13-店铺的商品分析" class="headerlink" title="L13 店铺的商品分析"></a>L13 店铺的商品分析</h2><p>在数据分析时，商品一般有两种状态：已销售商品和库存商品。因此，有关商品的分析也可分为两大方向：销售分析和库存分析。</p><h3 id="商品的销售结构分析"><a href="#商品的销售结构分析" class="headerlink" title="商品的销售结构分析"></a>商品的销售结构分析</h3><p>指<strong>对店铺某段时期内的所有销售商品进行分类汇总，然后再进行数据统计与分析</strong>。通过商品的销售结构分析，可以为店铺运营者<strong>梳理清楚店铺当前的主销商品，以及其销售表现</strong>，从而为运营者及时调整和优化销售策略提供可信的数据支撑。<br>在分析商品的销售结构时，可以按照商品年份、季度、波段、大类、小类、价格带、折扣带等指标来进行分类汇总。具体使用哪些指标和维度，需要根据实际分析需要来选择。</p><ul><li><p><strong>案例一：销售品类综合分析</strong> </p><p>下表是艾尚公司的三家电商店铺在第三季度的商品销售情况：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/62a67f9749039759.png" alt></p><p>我们可以获得以下信息：</p><ol><li>仅以“衬衫”与“T恤”而言，Q3季度衬衫的销售额是T恤的2～3倍；</li><li>T恤的件单价全线都比去年高50～100元。其中京东渠道的较高，达到102元；</li><li>衬衫的件单价在唯品会渠道出现了全季度同比下跌，而京东渠道则全季度同比上升。应与两个渠道不同的销售策略有密切关系；</li><li>如果将上表扩大到所有类目的数据，则可以对所有品类按销售贡献进行排名，然后据此判断各个品类的销售表现是否在应有的品类生命周期表现之内。</li></ol><p>根据这些信息，<strong>商品运营人员可以进行“运营复盘”，以销售结果来反推前期的运营策略是否正确，并加以调整与优化</strong>。譬如根据上表中“衬衫的件单价在唯品会与京东两个渠道中截然不同的表现”，商品运营人员便需要反思，在前期运营中唯品会渠道的商品折损是否没有控制好？抑或是有意将两个渠道衬衫上的商品款式“完全错开”的策略所导致的？ </p></li><li><p><strong>案例二：销售与退货分析</strong></p><p>某时间段内，店铺销售与退货分析的数据表：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/6633b8f585f87e4f.png" alt></p><ol><li>这份表可以找出各大品类中的主力销售品类。比如表中‘800元及以上的外套’销售额高达557万，销售占比达31%，这个品类无疑是当前的销售主力。但是这个价格带的退货率高达43%，因此需要特别注意加强退货挽回的措施。而平均折扣已经低至0.47，说明这批货可能都是往年的旧货，而且毛利率也不高。</li><li>这份表还可以<strong>分别用‘平均折扣’‘退货率’‘销售额’来做一个倒序排列，根据这种方式，可以找出店铺的‘高利润款’‘高退货款’‘畅销款’。然后针对不同的款来制定不同的销售策略。</strong><ol><li>400元以下的恤衫和400元以下的半截裙，平均销售折扣分别是0.78和0.84，毛利率足够高了。可是销量却没有起来，两者的销售占比还不到2%。【销售折扣（利润）】</li><li>从“高退货款”的角度来看，实销价高于800元的毛衣和恤衫，其退货率高达47%和54%；而同样是售价高于800元的半截裙退货率却只有32%。有可能这家店铺的毛衣与恤衫与其他竞品相比优势并不明显，而半截裙却是这家店铺的优势品类。【退货率】</li><li>从畅销度的角度来看，销售额靠前的分别是800元以上的外套、400～800元区间的毛衣、400～600元区间的半截裙；而且，除外套外，毛衣与半截裙的退货率与销售折扣都在合理范围之内。因此可以确定这三者的主推地位。【销量】</li></ol></li></ol></li></ul><h3 id="商品的库存结构分析"><a href="#商品的库存结构分析" class="headerlink" title="商品的库存结构分析"></a>商品的库存结构分析</h3><p>店铺聚划算活动报名盘货计划表：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/be1847c564987a9a.png" alt></p><p>此盘货计划的目的是统计库存中符合聚划算要求的商品，并计算货值，预估是否满足本次聚划算活动的业绩要求。</p><ol><li>本次计划参与聚划算的商品为217款，吊牌金额为292万。</li><li>本次参与活动的商品主要以毛织、半截裙、衬衫为主，此三大品类的备货超过整体活动备货额的2/3。</li><li>与活动款库存相比，库存中尚有约1400万的货值没有参与本次促销活动，按以往经验，这部分商品在活动促销期间，约可为店铺贡献 20%～30%的业绩。</li><li>按以往经验预估，聚划算商品的平均折扣为0.65折，活动商品的备货约是活动期间业绩目标的3.5～4.5倍；以这两个数值预估，本次活动预计可完成42万～55万的业绩。 </li></ol><p>两个关键的经验值：“预估平均折扣”与“预估备货/销售额倍数”</p><p>新品上市跟踪表：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/89f86e361e867a42.png" alt></p><p>关键指标：SPU数、消化率、落差</p><ul><li><p>SPU数：可以看出店铺上新的能力。也就是说店铺每个波段能够上新的数量多少，在某种程度上<strong>反映了店铺开发新款的能力高低</strong>。同时，SPU数与库存金额结合，也可计算出<strong>单款的库存深度</strong>。</p></li><li><p>消化率：配合落差，可以直观地看到<strong>此波段商品的销售进度是否符合预定进度</strong>。</p></li></ul><p>从上表中，我们可以得出以下分析结论：</p><ol><li>截止统计周期内，店铺共上线约300款春季款，其中春2波商品消化率达30%，在预期的销售进度计划之内； </li><li>春3波的商品消化率达42%，但是高出预期消化率（28%）许多。说明在后续的销售过程中，春3波商品可能会出现货品不足的风险；</li><li>本报表显示出的最大风险在于：春5、6、7波商品计划消化率达 65%～34%，是春2、春3波商品的将近一倍及以上，以经验判断，这些款应该是“春节款”。但是这些款的实际消化率仅24%～9%，存在非常大的销售落差；同时，春5、6、7波商品库存金额达近1000万，而期间累计销售额仅约180万，结合消化率落差判断，此处存在较大的库存风险。</li></ol><h3 id="商品ABC分级"><a href="#商品ABC分级" class="headerlink" title="商品ABC分级"></a>商品ABC分级</h3><p>商品的ABC分级法需要将分析颗粒精细到每个款式，分析相对繁杂，所以一般只有在店铺进行大型促销活动时才会用到。且只有店铺在大型促销活动时，每个单款商品所承载的流量与成交数据才足够大， ABC分级才更有意义。</p><p>商品ABC分级的重点在于“ABC的分级逻辑”。可以采用如下分级逻辑：</p><ol><li><strong>A级商品：高库存且有高转化率（转化率&gt;2%）的商品</strong>。因为这类商品既畅销，又有较深的库存作为保障，因此可以作为活动中的主推商品。需要注意的是，在挑选A类商品时，还应注意此款商品的<strong>访客不能太低</strong>，否则，没有经过“充分”流量测试的商品，其高转化率可能是“伪高转化率”。 </li><li><strong>B级商品：转化率中等（2%&gt;转化率&gt;0.65%），且经过流量测试的商品</strong>。由于这类商品经过流量测试，被证明对访客有一定吸引力，但却不如A类商品转化明显，所以可以继续保持当前的销售定位。B级商品中有两类商品需要特别注意：一是<strong>库存告急的</strong>， 这类商品需要特别注意避免超卖；二是占<strong>用了主推款陈列位置的</strong>，在大促中，宝贵的陈列位置是有限的，这类优质陈列位置需要留给A类商品使用，因此需要将其阵列位置往后移 动。</li><li><strong>C级商品：转化率低（转化率&lt;0.65%）且经过流量测试的商品</strong>。C类商品应处于店铺阵列页面的底端，基本是属于被放弃的一类商品。但是，C类商品中有一类需要特别注意，就是有<strong>高库存</strong>的，可以尝试主动改变原定策略，譬如换主图、降价等。</li></ol><p>在电商环境中，一般大促活动周期都在3天以上，因此，在经过第一天的流量测试后，迅速将商品的ABC分级表格分析结果提供给运营团队，可以帮助他们发现问题并及时调整商品运营策略，为业绩带来极大的推动作用。</p><h3 id="主推款销售追踪表"><a href="#主推款销售追踪表" class="headerlink" title="主推款销售追踪表"></a>主推款销售追踪表</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/09293d17fe54d9b1.png" alt></p><p>实销价：用于判断此主推款的消费群体与定位，一般而言，<strong>单价高的商品不适合使用直通车等付费工具进行推广</strong>；</p><p>总UV、 直通车占比、搜索流量占比等：用来判断此主推款的推广效果，流量越多，说明当前推广策略与推广渠道的选择越正确，反之就要考虑更换推广渠道或策略了；</p><p>消化率：用于判断此款<strong>是否继续作为主推的一个重要标志</strong>。如果消化高接近计划消化率，可以考虑暂时停止付费渠道的推广，以便提升营业利润； 如果消化率距离计划消化率较远，则可考虑加强付费推广的力度，以免造成库存风险。</p><ul><li><p>Q：像表格中款2这样已经超过计划消化率很多的款式应该如何调整？</p><p>调整主推款策略的关键因素：</p><ol><li>此款是临时主推，还是本身就定位为主推款。临时主推是为了临时突击，拉动此款的消化率；主推款则是承担着为店铺走量及引流等综合任务的款式。</li><li>此款库存量还剩余多少？</li><li>此款剩余销售周期还有多久？</li><li>在付费推广时，此款商品ROI是否在合理值？</li></ol></li></ul><h3 id="商品的屏效分析"><a href="#商品的屏效分析" class="headerlink" title="商品的屏效分析"></a>商品的屏效分析</h3><p>又称“商品的九宫格”数据运营法</p><p><strong>屏效</strong>：从传统零售分析中的‘坪效’借鉴过来的，指电商店铺的页面（首页/类目/二级页等）在电脑（或手机）上打开后，<strong>每个电脑（或手机）屏幕所产生的销售贡献</strong>。（一个电脑屏幕所呈现出来的商品（一般是6至8个商品）给店铺整体业绩所带来的贡献率）</p><p><strong>有效陈列面积</strong>：在聚划算活动中，店铺首页、活动二级页是承接流量较多的两大页面。所以，我们可以把店铺首页与聚划算二级页称为有效陈列页面。同理，在同一个网页中可以向下无限拉升，而用户的浏览习惯是一般只会专注于页面的前面3～5个屏幕的内容。所以，我们又把这些区域称为有效陈列面积，或者叫‘<strong>黄金陈列面积</strong>’。</p><p>黄金陈列位置所能够陈列的商品有限，因此只有尽量利用好每一个坑位的陈列机会，展现恰当的商品，才能达到更大的销售业绩。而<strong>‘屏效’就是用来评估黄金陈列位置利用率的</strong>。屏效高，则证明黄金陈列位置的商品利用率高；屏效低，则需要及时替换黄金陈列位置的商品。</p><ul><li><p>屏效分析第一步：挑选适合陈列在‘黄金陈列位置’的商品</p><p><img src="https://i.bmp.ovh/imgs/2020/03/cef736d8850c8609.png" alt></p><p>利用ABC分级的方法，从<strong>‘库存数量’与‘加购数量’</strong>两个维度，将活动期间的主推商品分为9个层级。</p><p>屏效分析其实就是通过‘商品ABC’分级方法，找出预热表现优秀且库存深厚的商品，将他们通过商品搭配的手段，陈列在‘黄金坑位’。这样，就可以从数据层面使销售更大化。</p><p>且陈列在‘黄金坑位’上的商品并不是固定不变的，而是要保持轮动：</p><ul><li>首先，既然我们把商品分为9个层级，那么陈列页面也可以分别对应的9大层级。</li><li>其次，我们要把握住活动当天的流量访问规律。<strong>在大型促销日的活动当天，流量一般有三大访问高峰期，分别是0～2点、8～9点、22 ～23点。</strong>因此，商品的轮转就需要安排在<strong>这三个流量高峰之后的一个小时内</strong>。</li></ul></li></ul><h2 id="L14-店铺的用户分析"><a href="#L14-店铺的用户分析" class="headerlink" title="L14 店铺的用户分析"></a>L14 店铺的用户分析</h2><h3 id="活跃度分析"><a href="#活跃度分析" class="headerlink" title="活跃度分析"></a>活跃度分析</h3><p>以某电商店铺的大促活动为例，验证如何通过数据来为运营提供指导。</p><p>数据分析人员应从<strong>店铺用户分类、收益预估、短信营销用户清单</strong>三个方面为本次运营提供了全面的支持</p><ul><li><p><strong>店铺用户分类</strong></p><p>在大促开始前7天，我们需要统计出店铺<strong>最近12个月内的用户活跃度数据分析表</strong>：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/2e8b441345d9a81f.png" alt></p><p>通过此表可以对店铺的“活跃用户”“沉睡用户”“即将流失客户”进行分类统计，以便于我们对店铺用户结构建立整体的认知，并且为接下来制定“用户唤醒”与“用户挽回”方案提供数据性参考。</p></li></ul><blockquote><p>关于‘<strong>活跃度</strong>’，每家公司都有不同的分类标准。一般的做法是：</p><ul><li>最近30天内有过消费的客户称为‘活跃用户’</li><li>最近（连续）60天内没有消费记录的客户称为‘沉睡用户’</li><li>最近（连续）90天内没有消费记录的用 户称为‘即将流失客户’</li></ul></blockquote><p><strong>用户激活方案</strong>：针对不同类型的用户，刺激的力度与文案的企划也应该有所不同。其中针对<strong>沉睡客户主要以“唤 醒”为主</strong>，常见的举措是<strong>告知本次促销力度，以及提供适量额度的优惠券</strong>；针对<strong>即将流失客户</strong>，除了告知促销信息与力度外，还需要<strong>提供较高额度的优惠券，另外最好再配上走心的文案</strong>。</p><ul><li><p><strong>收益预估</strong></p><p>根据“待唤醒”与“待挽留”人数，还可以<strong>推算出本次优惠券的使用率以及折损金额</strong>。譬如在大促活动中向约5000名即将流失客户发送“情怀短信”以及“高额优惠券”，发现<strong>挽回率可达1.8%左右</strong>。也就是说，5000名目标用户中，每次大促活动中激活约90名用户。按客单700元计算，在当次大促活动中被挽回用户可以为店铺贡献约6.3万元的销售额，但是更重要的是，<strong>本次挽回成功，可以使用户在接下来的一段时间内成为“沉睡用户”或者“活跃用户”</strong>，这种隐形的意义对店铺来说要更大一些。而90名用户即使人人都使用50元的代金券，店铺的<strong>折损</strong>也不过是4500元而已。虽然相比于直通车等收费流量的推广成本，这种手段的“获客成本”更高，但是胜在<strong>更精准，挽回的客户更符合品牌调性，也就更有质量</strong>。</p></li><li><p><strong>短信营销用户清单</strong></p><p>最后，需要与这些长时间不来店铺消费的用户建立起某种连接通道，<strong>最直接的“通道”是短信营销</strong>。我们需要为店铺运营者提供“待激活用户清单”。重要的是，运营人员需要撰写“走心营销短信”，短信内容是否走心，是决定用户能不能被“唤醒”的关键。</p></li></ul><h3 id="建立物流地图"><a href="#建立物流地图" class="headerlink" title="建立物流地图"></a>建立物流地图</h3><p>了解订单主要集中在哪些省份之后，再结合各大快递公司在不同地区的收费标准，就可以为店铺选择最优的快递方案。</p><p>电商企业在与快递公司合作时，由于订单量大，是有“优惠价”的。但每家店铺的订单量不一致，因此具体到每家电商店铺都有不同的价格。但是，不管如何优惠，<strong>每家快递公司的价格基数</strong>都是公开的，因此根据它来选择合作快递公司同样是可行的。</p><p>在实际应用场景中，选择多渠道快递公司合作的话，将节省更多的费用。而且，如果将这一策略扩大至全年，将会节省近百万的快递费用。</p><h2 id="L15-店铺的活动分析"><a href="#L15-店铺的活动分析" class="headerlink" title="L15 店铺的活动分析"></a>L15 店铺的活动分析</h2><ul><li>案例：跟踪过去三年双11当天的流量与销售数据，发现它们有着很明显的共性：<ol><li>从时段来看，全天大约共有三个时间段为销售及流量的高峰期：<strong>分别是0点至2点、8点至9点，以及21点至23点。这三个时间段大致可以代表三种购物特性：凌晨秒抢、上班购买、晚间捡漏。</strong>因此，建议营销节奏也可以据此来进行相应的铺排。</li><li>从销售占比来看，0点至2点这两个小时内，销售占比高达60% ～70%，这段时间的购买主要是以预热期间提前加购的用户为主。根据这一特性，我们可以做出全天的销售预测：<strong>首先将截至凌晨3点的销售额除以60%或70%，以此计算出全天的预估销售额。然后与当天的计划销售额加以对比，判断两者之间是否存在缺口。</strong>如果达不到预期，则应想办法在接下来的两个流量高峰中予以补救；如果达到预期，也应及时 检查商品的销售结构是否尚保持相对完整。重点监测动销率、售罄率等关键指标，降低主力商品牌出现断货的风险。</li></ol></li></ul><h3 id="可控因素"><a href="#可控因素" class="headerlink" title="可控因素"></a>可控因素</h3><p>促销活动中的可控因素，是指在做促销活动时，我们能够人为控制并且借此来改善促销业绩的一些因素。</p><p>对于一次大型促销活动而言，可以量化的影响活动的重要因素有5个：优惠券、资源位流量、老客户激活、承接页流量、加购商品监控。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/3a9ebef0ebde44bd.png" alt></p><h3 id="优惠券"><a href="#优惠券" class="headerlink" title="优惠券"></a>优惠券</h3><p>优惠券是电商促销活动中的常规武器，一般分为<strong>无门槛优惠券</strong>和<strong>满减优惠券</strong>两种。</p><p>优惠券的作用在于<strong>打消顾客在购买时对于价格的顾虑</strong>。把顾客从冷静理性的购物状态带入感性、冲动的购物状态。这样，便达到了刺激顾客购买欲望的目的——这一点，在那些对价格敏感的客户群体中尤其有效。但是缺点也同样明显，就是会显著拉低销售利润。控制不好，甚至会导致销售亏损。</p><p><strong>设置正确合理的优惠券面额：</strong></p><ul><li><p><strong>方法一：平均客单价设置法</strong></p><p>店铺的平均客单价代表了店铺目前用户群体的消费力水平。譬如某店铺的平均客单价为270元，那么，说明此店铺客群的消费力大致也是在270元左右（一般是学生或刚入职场的年轻人）。因此，我们可以把优惠券的门槛设置为300元，这样便可以<strong>达到鼓励用户进行高客单消费的目的</strong>。</p><p>利用平均客单价方法设置优惠券时，首先要规划本次共设几档优惠券，然后参考店铺的平均客单价，<strong>以再搭一个单品即可达到优惠门槛为佳</strong>。</p><p>举例说明：<br>A店铺在筹备三八妇女节的活动时，计划设置三档优惠券，用以提 升活动效果。那么，应该如何设置呢？</p><p>首先，<strong>预估一下活动期间的平均客单价，并且将其分为上中下三档</strong>。比如，连带率（连带率=销售的件数/交易的次数，反映的是顾客平均单次消费的产品件数）低于1.6时，客单价为多少？连带率低于3.0时，客单价为多少？连带率高于3.0时客单价为多少？根<strong>据此三个阶梯来划分优惠券的三大门槛</strong>。由于是促销期间，客单价肯定会比日常略低，因此可以根据活动商品的平均折扣，或者过去同类活动来预估。</p><p>其次，<strong>精选一些可以提升连带销售量的单品作为搭配产品</strong>，在首页或者活动二级页中组成搭配专区，以便于给用户凑单。这样可鼓励用户达到使用优惠券的门槛。披肩、毛巾、内搭等是常见的搭配产品。</p><p>最后，<strong>“平均客单价+搭配物件价格-可以承受的销售折损金额”便是优惠券的面额档级</strong>。如店铺连带率低于1.6时，客单价为340元，搭配物的价格为59元，可以承受的折损是不低于活动价的9.8折。那么，优惠券高则可以设置为“80元，满399元可用”，低则可设置为“40元，满399元可用”。</p></li><li><p><strong>方法二：价格带宽度设置法</strong></p><p>此种方法要求先将店铺统计周期内的销售价格带罗列出来，然后根据价格带的宽度分布来设置优惠券的层次与门槛。</p><p>某店铺10月销售价格带分布：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/ec8de154809b3c90.png" alt></p><p>假设把主销价格带分为三段，则可以分为300～499、500 ～699、800～899三段。根据这一分类，假设同样需要设置三档优惠券，则可以按图所示进行设置。</p></li></ul><p><strong>优惠券跟踪分析：</strong></p><p>在大促之前，数据部门更重要的职能是——<strong>需要跟踪优惠券的领用与使用情况来判断店铺需要承担的折损与能够带来的销售业绩</strong>。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/6e8fc85d94611916.png" alt></p><p>首先，综合“类型”“面额&amp;档级”“展现位置”“发放时间”四列信息，可以了解到<strong>店铺运营人员对优惠券的定位与运营手段</strong>。以“无门槛50元”的优惠券为例，这是运营人员在活动期间的最后几小时冲刺业绩所用的，因此可以看到它仅在活动结束前3小时才推出。这是运营上的解读。</p><p>其次，从数据层面，<strong>需要跟踪优惠券的领用情况与使用情况</strong>。在预热期间，每天观察优惠券的领用率，以便评估活动的预热效果是否足够理想。同时，如果发现优惠券的领用超过预期，在预热尚未结束而优惠券的领用率却达到85%以上时，可提醒业务部门增加优惠券的发放数量。</p><p>同时，还可以根据领用率来<strong>评估优惠券的投放位置与投放时间是否正确有效</strong>。譬如在表中所示的“无门槛50元”优惠券，“首页”的领用率达到了100%， 而“活动二级页”的领用率仅43%，说明同样的优惠券，在首页投放的效果比在二级页中投放的效果更好。因此，若下次再有类似优惠券时，在折损允许的范围内，可以建议运营人员在“首页”上投放更多的优惠券；而<strong>在活动开始后，则需要跟踪优惠券的使用量与使用率。根据优惠券的使用率来计算本次活动中优惠券所导致的折损，以此评估本次活动中优惠券的折损是否在预算范围之内</strong>。</p><blockquote><p>怎样根据使用率来计算优惠券的折损呢？怎样判断这个折损是否合理呢？</p><p>优惠券带来的折损金额与ROI预估：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/74c402b54bdfbd05.png" alt></p><p>本次活动折损金额约为12万元，这是所有优惠券使用后带来的销售损失，但与此同时，这些优惠券也带来了约226万的业绩。以此计算，ROI达到18.9。 在大促活动中，<strong>优惠券的ROI如果低于15，一般是不太理想的</strong>。</p></blockquote><h2 id="L16-店铺的双11年终大促"><a href="#L16-店铺的双11年终大促" class="headerlink" title="L16 店铺的双11年终大促"></a>L16 店铺的双11年终大促</h2><p>屏效分析（一小时后）：</p><p><img src="https://ftp.bmp.ovh/imgs/2020/03/962e6fbfd78dde89.png" alt></p><p>主推款：商品ABC分析法（1点、11点、22点）：</p><p><img src="https://ftp.bmp.ovh/imgs/2020/03/8d7b90db4091887e.png" alt></p><p>在每一波销售高峰中，以更有吸引力且库存深厚的商品去迎接集中化的流量。</p><p>淘宝渠道商品断码统计（21点）：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/5e1a9a401ecd5d7c.png" alt></p><p>*注：期内共有333个SKU产生销售，销售后，其中出现断码的SKU达到153个。断码率达：46%（断码：当M码或L码库存为0时，判断为断码）</p><p>平台最近四小时仅动销了333个SPU，全店商品约有1200个，也就是说，最近四小时的动销率仅28%左右。而动销过的商品中，已经出现断码的商品竟然高达46%。原因：淘宝渠道的商品调换轮转太慢导致。调整策略：优先调整首页与活动二级页面的前5屏</p><p>天猫店铺预售情况分析（22点）</p><p><img src="https://i.bmp.ovh/imgs/2020/03/fb29fd44f3780a1e.png" alt></p><p>注：可催付的金额约为32万元；“下单但未付订金”的订单金额达约65万，可短信提醒，或派送定向优惠券；</p><p>调整策略：预售款尾款没有支付的客户，以及仅下单未付订单的客户的名单，包括手机号与淘宝ID给抓取出来，给了天猫渠道经理和客服经理，提醒按照名单尽快安排催付。</p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数据分析与数据化运营Note——Part3：数据表格</title>
      <link href="/2020/03/20/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part3/"/>
      <url>/2020/03/20/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part3/</url>
      
        <content type="html"><![CDATA[<h1 id="Ch3-表作骨，美化为肉，方法是灵魂"><a href="#Ch3-表作骨，美化为肉，方法是灵魂" class="headerlink" title="Ch3 表作骨，美化为肉，方法是灵魂"></a>Ch3 表作骨，美化为肉，方法是灵魂</h1><p>本章主要讲解如何快速有效地构建一份适用的数据表格，并且选择恰当的数据分析方法来达到分析目的。</p><h2 id="L9-快速建立实用美观的数据表"><a href="#L9-快速建立实用美观的数据表" class="headerlink" title="L9 快速建立实用美观的数据表"></a>L9 快速建立实用美观的数据表</h2><h3 id="快速构成实用的数据表"><a href="#快速构成实用的数据表" class="headerlink" title="快速构成实用的数据表"></a>快速构成实用的数据表</h3><ol><li><p>三种常见的Excel表格类型</p><p><img src="https://i.bmp.ovh/imgs/2020/03/385e7ec81b201ac4.png" style="zoom:50%;"></p><p><img src="https://i.bmp.ovh/imgs/2020/03/8daf519980122f53.png" alt></p><p><img src="https://i.bmp.ovh/imgs/2020/03/95146dde24db3591.png" alt></p><ul><li>一维表：指表格的每一行都是一条独立而完整的信息；主要用于基础数据的存储，因为一维的格式方便把数据导入到各种数据处理软件；</li><li>二维表和三维表中，每表格的‘行’必须与‘列’上的字段结合起来，才能够形成一条完整而独立的数据。不同点在于，<strong>二维表中，‘行’只需要与‘列’上的一个字段结合；而三维表中，‘行’需要与‘列’的两个字段结合</strong>。</li><li>二维表主要用于各种简单场景下的数据分析，需要对品类进行销售分析，而不用区分渠道时，便是采用了二维表的方式</li><li>三维表主要用于比较复杂的数据分析需求，如对各渠道与各品类的交叉式销售分析</li></ul></li><li><p>[指标—维度]快速构表法 </p><p>把‘维度’放在数据表格的第一列，而‘指标’放在表格的第一行。当指标与维度交叉时，就会形成数据。</p><ul><li><p>指标：用来衡量某一事物发展程度的。比如我们要衡量店铺的业绩好坏，便需要用到销售量、 销售额、完成率等指标。指标可又分‘绝对指标’与‘相对指标’。 比如‘销售量’‘销售额’便是绝对指标，而‘完成率’‘转化率’等便是相对指标。</p></li><li><p>例如：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/6df3ce1aeb829578.png" alt></p><p>首先，从构表方法来讲，这个表格是用‘指标–维度’的方法构成的。它包括三个指标，分别是‘销量’‘销额’和‘完成率’，然后包含了五个维度，其中‘6月’‘同比’‘MTD’‘YTD’是时间维度；‘竞品’是空间维度。</p><p>从数据来解释，这家店铺6月销售额为1600万，超额完成了当月销售目标。但是1～6月的累积完成率仅96%，说明此前5个月是存在‘销售缺口’的；同样，年度完成率仅37%，结合MTD基本达标 （96%）的情况来看，证明这家店铺把大部分的销售目标‘押宝’在后面半年中。</p></li></ul></li></ol><h3 id="建立报表输出标准"><a href="#建立报表输出标准" class="headerlink" title="建立报表输出标准"></a>建立报表输出标准</h3><p>数据分析的过程：</p><ol><li>数据收集：做数据分析之前需要先收集能够被使用的数据，比如商品资料、 销售明细等数据。</li><li>数据处理：需要清洗的数据层出不穷，如果不清洗干净，便会导致分析的结果出现非常大的偏差。</li><li>数据建模：把一个个清洗好的数据放置于一个数据表格的模板中。需要注意的是，这个数据表格的结构与逻辑，必须确保是能够满足分析目的。</li><li>报告出样：当我们根据设定好的“模型”把数据填充好后，数据报告便基本出样了。此时，更至关重要的是，我们要对数据报告进行反复解读，确保数据报告的“说服逻辑”是顺畅而且有理有据的。</li><li>优化报告：再有用的数据与数据化建议，只有在美化后才更能打动用户。同样，假设一份数据报告过长，数据分析师便非常有必要把数据分析的重要结论摘抄下来，单独形成一页，并放置在数据报告的前文。</li></ol><p>建立报表输出标准：</p><ol><li>表格的行与列，分别用相同色系，但色差相邻的两个颜色填充。这样，可以达到立体化的视觉效果，便于用户阅读。同时，关于色系的选择，在销售类报表中建议使用暖色调的色系；在退货或成本相关的报表中，建议使用冷色调的色系。</li><li>表格的字体：标题用“宋体”11号字体、表格中其他汉字及数字均用“微软雅黑”9号字体。</li><li>关于数字的处理：表格中所有数字，均要使用千分位记数法， 同时过大的数字无须保留小数位，过小的数字可以保留1～2位小数，另外，表格中的数字的单位一定要在表格中备注好。</li><li>关于小计与合计：表格中所有小计类数据均为斜体，并加单下划线；所有合计类数据，需要加粗，加双下划线，并且用淡灰色填充。</li><li>关于表中重点/异常数据：对于表格中的重点数据，或是异常数据，必须用亮黄色填充，并用红色字体凸显，如此可引导表格的读者迅速发现数据重点，或异常数据。</li><li>关于百分比：这也是表格优化的一种技巧。在实际分析场景中，我们常把结构复杂、数据绝对值过多的表格，转化为百分比表格。 这样，我们一眼就可以从表格中找出重点数值与异常数值。</li></ol><h2 id="L10-简单而实用的三大分析方法"><a href="#L10-简单而实用的三大分析方法" class="headerlink" title="L10 简单而实用的三大分析方法"></a>L10 简单而实用的三大分析方法</h2><p>本节课主要介绍数据分析的三大基础方法，分别是对比分析、细分分析、转化分析。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><ol><li>绝对值对比与相对值对比</li></ol><p>在电商数据分析中，一般绝对值指正数之间的对比较多，如销售额、退货额等；</p><p>相对值对比，则是指转化率、完成率等这类相对数之间的对比。</p><ol><li><p>环比</p><p>环比是指<strong>统计周期内的数据与上期数据的比较</strong>，比如2017年6月数据与2017年5月数据的比较。</p><p>在电商数据分析中，由于每个自然月之间的销售差额比较大，如果采用绝对指标，便很难通过对比观察到业务的变化。因此，<strong>一般会采用相对指标来做环比分析</strong>。</p></li><li><p>同比</p><p>同比是指<strong>统计周期内数据与去年同期数据之间的对比</strong>，比如2017年6月数据与2016年6月数据的比较。</p><p>在电商分析中，同比是应用最广泛的数据分析方法。通过同比，我们能大致判断店铺的运营能力在最近一年中，是保持增长还是呈下滑趋势。同时，也可以根据同比增长趋势，来制订初步的销售计划。</p></li><li><p>横向对比与纵向对比</p><p>所谓横向对比与纵向对比，是指空间与时间两个不同的维度之间的对比。</p><p>横向对比是空间维度的对比，指<strong>同类型的不同对象在统一的标准下进行的数据对比</strong>。如“本店”与“竞品”之间的对比；</p><p>纵向对比是时间维度的对比，指<strong>同一对象在不同时间轴上的对比</strong>。如前面提到的“同比”“环比”都是纵向对比。</p></li><li><p>份额</p><p>严格地说，“份额”属于横向对比的一种。在某些情况下，数据表格中多一个“份额”，会让表格清晰明了许多。</p><p>假设我们要分析“某品牌天猫、京东、唯品会三大渠道”的“上衣、下衣、连衣裙和其他”在“Q1～Q4季度”的销售趋势和表现。但是，如表1这般的数据却不能直观告诉我们每个销售类别在不同渠道和不同季度的销售趋势是什么。因此，在数据分析中便需要加入表2这样的“份额”分析表格。如此，我们便可一目了然地掌握每个类别在不同渠道、不同时期的销售趋势。因此也就达到了数据分析的目的。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/b6d94f77f721b5b1.png" alt></p></li></ol><h3 id="细分"><a href="#细分" class="headerlink" title="细分"></a>细分</h3><p>细分分析法，常用于为分析对象找到更深层次的问题根源。难点在于我们要理解从哪个角度进行‘细分’与‘深挖’才能达到分析目的。在分析之前，选择正确的‘细分’方法便非常重要。</p><ol><li><p>分类分析</p><p>就是指对所有需要被分析到的数据单元，按照某种标准打上标签，再根据标签进行分类，然后使用汇总或者对比的方法来进行分析。</p><p>在服装行业中，常用于做分类分析的标签有“类目”“价格带”“折扣带”“年份”“季节”等。通过从“年份”“季节”的维度来对商品库存进行细分，我们可以轻松地知道有多少货属于“库存”，有多少货属于“适销品”；通过从“折扣带”的维度来对销售流水进行细分，我们可以大致知道店铺的盈利情况；通过从“类目”的维度对销售流水和库存同时进行细分，我们可以知道统计周期内品类的销售动态与库存满足度。</p></li><li><p>人—货—场</p><p>“人—货—场”能够为人提供宏观视野的分析。其原理类似于分类分析，即将所有需要被分析到的数据单元，打上“人”“货”“场”的标签，然后再进行相应的数据分析与处理。</p><p>在实际应用场景中，‘人—货—场’分析法往往被灵活运用在<strong>初步诊断某一竞品店铺</strong>时。如下图所示是利用“人—货—场”逻辑方法来分析竞品店铺的主流思路。在分析之前，先用“人—货—场”的方式罗列出来， 把所有能够想到的有用的“分支”都罗列出来，然后查漏补缺、标注重要与非重要。最后，再按此思路来进行分析。便可达到事半功倍的分析效果。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/caf642c9a846bb9e.png" alt></p></li><li><p>杜邦分析</p><p>在电商中，杜邦分析常被用于寻找销售变化的细小因素之中。 下图便是根据杜邦分析原理，<strong>将所有影响到销售额的量化指标都统计出来</strong>的一种常用分析方法。此种方法，有助于我们从细小的数据颗粒中找到影响销售变化的元素。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/0c201d5a6a0fa9c2.png" alt></p></li></ol><h3 id="转化"><a href="#转化" class="headerlink" title="转化"></a>转化</h3><p>转化分析常用于页面跳转分析、用户流失分析等业务场景。</p><p>转化分析的表现形式一般是选用漏斗模型，如下图所示，模拟了某电商店铺的流量转化情况，并以漏斗图的形式展现出来。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/0a0f02b9115b267a.png" style="zoom:67%;"></p><ol><li>转化分析方法的前提，是我们需要<strong>首先确定一条“转化路径”</strong>（如图左侧的路径所示），这条路径就是我们的“解题方法”，是决定我们接下来的分析能否达成目标的重要因素。</li><li>当“转化路径”确定后，我们需要把“路径”中的各个“节点”罗列出来，并把节点下的重要数据统计出来。</li><li>最后，根据路径把各节点的数据用漏斗图的形式表达出来。 </li></ol><p>同时，转化分析还可用于店铺微观方面的“转化”洞察。譬如在某一次店铺举行大促活动时，我们需要分析<strong>大促期间“活动二级页”的流量转化效果如何</strong>。此时，我们便可以参照如下图所示的漏斗模型。 </p><p><img src="https://i.bmp.ovh/imgs/2020/03/409afd5883505179.png" style="zoom:80%;"></p><p>在以上案例中，我们将转化路径定义为“活动页→详情页→支付页面（下单）→支付成功（购买）”四个节点。然后统计每个页面的流量到达数量，于是得出漏斗图。</p><p>通过此图，可以清晰明确地诊断出此次活动二级页在“下单→付款”环节转化率仅40%，存在一定问题。在支付界面的流量跳失，很可能是价格过高所致。</p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数据分析与数据化运营Note——Part2：数据指标</title>
      <link href="/2020/03/19/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part2/"/>
      <url>/2020/03/19/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part2/</url>
      
        <content type="html"><![CDATA[<h1 id="Ch2-像“堆积木”一样认识数据指标"><a href="#Ch2-像“堆积木”一样认识数据指标" class="headerlink" title="Ch2 像“堆积木”一样认识数据指标"></a>Ch2 像“堆积木”一样认识数据指标</h1><h2 id="L7-能够诊断业务的KOL数据指标"><a href="#L7-能够诊断业务的KOL数据指标" class="headerlink" title="L7 能够诊断业务的KOL数据指标"></a>L7 能够诊断业务的KOL数据指标</h2><h3 id="以运营为导向的业务框架"><a href="#以运营为导向的业务框架" class="headerlink" title="以运营为导向的业务框架"></a>以运营为导向的业务框架</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/99e1350022a77389.png" alt></p><p>在实际运营场景中，一次正常的运营闭环一般会经历以上的7个步骤：</p><ol><li>首先，<strong>运营者们会制订本次的销售目标、销售计划</strong>（包括促销方案）；在销售计划中，运营者需要着重提出，他们需要的商品资源与推广需求；</li><li>然后，<strong>商品与市场推广的负责人会根据运营的需求提供相应的解决方案</strong>，并协商达成一致；</li><li>同时，<strong>视觉部门</strong>的同事会根据促销方案、活动主题等<strong>设计</strong>店铺的活动二级页、详情页等。他们的页面设计直接影响到活动的促销效果；</li><li>于是，进入下面的<strong>客服接待、订单处理、物流发货</strong>等环节。</li><li>最后，<strong>财务</strong>对这一时期的销售业绩与平台对账，以便初步核算出期内业务达标率等财务指标。</li></ol><h3 id="能够诊断业务的KPI指标"><a href="#能够诊断业务的KPI指标" class="headerlink" title="能够诊断业务的KPI指标"></a>能够诊断业务的KPI指标</h3><p>运营导向的业务框架下各项重要数据指标：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/85ac5d7106aeb816.png" style="zoom:150%;"></p><ol><li><p><strong>运营模块</strong></p><ul><li><p>重要职能：</p><ul><li><p>负责达成整个品牌的业绩目标</p></li><li><p>控制运营成本</p></li></ul></li><li><p>数据指标：</p><ul><li>从数据指标角度来评估运营能力时，需要<strong>避免使用单一指标来评估组织的运营能力</strong></li><li>一般使用<strong>业绩达标率、业绩增长率、销售利润额</strong>三个指标来进行综合评估。</li></ul></li><li><p>例如：</p><ul><li><p>运营团队A在2016Q1季度完成销售2000万，达标率110%，同增2%，净利率15%。 </p></li><li><p>运营团队B在2016Q1季度完成销售2000万，达标率95%，同增55%，净利率10%。 </p></li><li><p>在A、B两个运营团队中，</p><ol><li><p>如果我们只看“销售达标率”一个指标，无疑会认为运营团队A更优秀，因为达标率达到110%；</p></li><li><p>如果我们看“销售达标率”“增长率”两个指标，就会认为团队B更优秀。因为虽然B团队达标率才95%，但<strong>增长迅速，证明团队采取了有效的运营手段来冲刺业绩</strong>；</p></li><li><p>如果我们同时看三个指标，团队A业绩爆标了，但增长率才2%，<strong>暴露了它增长乏力的事实</strong>，而它的净利润额却达到300万，远高于团队B的200万。我们能说团队A比团队B更优秀吗？</p></li><li><p>反之，团队B虽然没有达标，但达标率95%也已经达到业绩及格线，而且增长率达55%，说明此团队找到了业绩增长的钥匙。而它的利润额却才200万，<strong>证明此团队为了达成高业绩而牺牲了一部分利润</strong>。我们也不能简单地定义团队B比团队A更优秀。</p></li></ol></li><li><p>从实例中可以看出，<strong>如果我们使用单一指标来评判业绩好坏，在许多场景下，都会产生不合理的结论</strong>。甚至有时还会出现，即使同时使用了两三项指标也不能对业绩进行准确评估的情况，这时我们就要反思，是分析方向不对，还是使用的指标精细度不够了。</p></li></ul></li></ul></li><li><p><strong>商品模块</strong></p><ul><li><p>重要职能：</p><ul><li><p>商品企划</p><p>商品企划是指提前将一个销售周期（一般是指一个季度）的商品需求进行品类、价格带、风格等结构性的规划，并制订有前瞻性的商品销售进度计划。</p></li><li><p>商品运营</p><p>商品运营是指从商品入库、商品上架，到制定商品主推策划、商品流通规则、商品折损保护等一系列的运营动作。以“更高的利润，销售更多商品”为目的。</p><p>既要保护品牌调性（不能过度频繁打折，不能过多爆款），又要帮助运营走量。</p></li></ul></li><li><p>数据指标：</p><ul><li>对商品企划而言，主要对当季库存率（量/额）负责。从更细致的角度而言，就是要随时跟踪<strong>品类与主推款的售罄率</strong>：各品类的销售进度是否与预期一致？主推款的销售进度是否与预期一致？如果超出预期，是否需要及时补货？如果不达预期，是否需要提前促销？</li><li>对商品运营而言，<strong>商品的周转天数、新品动销率/售罄率、活动动销率/售罄率、销售折损</strong>都是非常重要的数据指标。周转天数越低，证明商品流动越快，则仓储成本更低、资金周转越灵活；销售折损越低，代表商品以更高的价格成交，销售利润就越高；动销率与售罄率则需要根据不同的商品生命周期与销售环境来考虑。 </li></ul></li></ul></li><li><p><strong>市场模块</strong></p><ul><li><p>重要职能：</p><ul><li><p>市场推广</p><p>主要指通过天猫的直通车、钻展，京东的京东快车、京选展位，以及第三方工具百度推广、淘宝客、今日头条等渠道来实现产品或者品牌的推广，从而达到为产品或品牌引流的目的。</p><p>尽管市场推广<strong>几乎都是以付费渠道与方式为主</strong>，付费的转化率也相对较高；但市场推广也存在免费方式，如百度贴吧、品牌微博等，一个注重品牌建设的公司，对这些渠道的数据分析也不能忽视。</p></li><li><p>会员维护</p><p>凡是在店铺内购买过一次的用户，几乎都被称为会员。因为电商的优势在于凡购买过一次，都可以在店铺中留下用户的收货人名称与联系方式，<strong>可以凭借这两项信息建立用户档案，并进行客户维护</strong>。有效的用户运营可以降低品牌的市场推广费用。</p></li><li><p>活动策划</p><p>商品部会从商品折损上维护品牌调性；而市场部则需要<strong>从品牌形象、品牌风格与定位上维护品牌调性</strong>。因此，但凡公司的重大促销活动，一般都会交由市场部<strong>进行活动主题的包装和策划</strong>。而且活动策划、包装得好，对活动的销售也会有较大的正面影响。</p></li></ul></li><li><p>数据指标：</p><ul><li><p><strong>ROI（投入产出比）、付费用户销售额、付费流量转化率三个指标</strong></p></li><li><p>许多公司可能会采用单一的ROI来考核市场推广能力，这并不是最合理的。因为当投入达到一定阶段时，ROI必然会下降，但此时投入所带来的产出却还是增加的。所以，如果单纯考核ROI，那么，有丰富经验的推广团队在把ROI做到理想值后，便会止步不前，为此就会浪费掉后续追加推广费用来带来的销售增量机会。</p></li><li><p>举个例子：</p><p>  假设在推广某件产品时有两个推广方案可供选择：</p><ol><li><p>A方案：投入2万元推广费用时，预估能够带来4万元销售业绩，此时ROI为1:2。 </p></li><li><p>B方案：投入2.5万元推广费用时，预估能够带来4.5万元销售业绩， 此时ROI降为1:1.8。</p><p>于是问题出现了，如果为了单纯追求高ROI值，推广团队必然会选择A方案投放；而如果选择B方案投放，则可以为该产品带来5千元的销售增量。假设这件产品件单价为500元，便是10件产品的额外销售收入；再假设这10件衣服分别由10位新客户购买，则又意味着该品牌损失了把10位新客户转化为老客户的机会。这样推算下去，选择A方案的损失无疑是巨大的。</p><p>所以不建议选择单一指标 （ROI）来考核，而<strong>应该结合ROI、付费用户销售额、付费流量转化率三个指标来看。</strong></p></li></ol></li></ul></li></ul></li><li><p><strong>视觉编辑模块</strong></p><ul><li><p>重要职能：视觉部的重要性体现在它对店铺转化漏斗的设计，以及能够显著提升详情页转化率上面。</p><ul><li><p>店铺视觉</p></li><li><p>详情页逻辑设计</p><p>详情页的重要作用在于建立访客对于产品的信任，因此这种用户的信任力直接影响到详情页的转化率。好的详情页逻辑可以为店铺直接带来销售提升。</p></li><li><p>页面框架设计</p><p>店铺的页面逻辑是否符合用户的浏览习惯，店铺的商品分类标签是否可供用户精准而及时地找到想要买的衣服，主推商品在店铺页面中是否被突出陈列……</p></li></ul></li><li><p>数据指标：</p><p>由于访客在店铺的浏览行为是动态的、不断变化的，因此很难用某一个单一的指标来衡量其成效。因此，行业都会采用<strong>“流量漏斗”+“热力图”</strong>的方式来分析与诊断。</p><ul><li><p>流量漏斗需要根据制定好的流量浏览路径来分析，不同的分析场景可以制定不同的流量路径。</p><p>比如，我们需要分析客户从详情页到支付购买之间的转化情况，此时首先要制定好一条类似于“详情页—加入购买车—生成订单—支付订单—交易完成界面”的用户浏览路径，然后把每一个关键页面的流量数统计出来，这样就可以制作出这条路径的流量漏斗图了：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/0545da09b4202260.png" alt></p></li><li><p>热力图用于诊断具体的页面结构设计是否合理。譬如我们通过流量漏斗发现在某一个页面的流量跳失率特别高，需要判断这个页面的跳失原因时，一般会采用热力图的方式。</p></li></ul></li></ul></li><li><p><strong>客服、仓储、财务模块</strong></p><ul><li><p>重要职能：从运营角度而言，客服、仓储、财务属于销售末端的支持部门。</p><ul><li><p>客服模块负责用户进店之后的咨询、成交引导，以及用户购买之后的售后服务。</p></li><li><p>仓储也是一个重要的支持部门， 商家未及时发货则会面临积分赔偿、用户投诉等各种风险；同时，仓储的另一个重要性也体现在需要尽快对仓库的大批量到货（包括用户的退货到货）进行入库，以便及时销售（或二次销售）。</p></li><li><p>财务模块比传统经营环境下的工作更复杂。由于电商行业的“退货滞后性”，导致同一时间内的营收收入始终不能等同于实 际财务收入。如果财务团队能够及时把上月的实际运营盈利情况告知运营团队，将会对运营产生非常大的帮助；财务的另一项职能是需要协助客户的售后人员进行退款操作。</p></li></ul></li><li><p>数据指标：</p><ul><li>对于客服而言，有三个重要指标：<ul><li>一是<strong>咨询转化率</strong>，就是指在店铺咨询过的访客中，有多少人最终成交了；</li><li>二是<strong>人效</strong>，是指经过咨询转化得到的业绩除以客服总人数的人均业绩，人效是体现客服团队贡献值的一个重要指标；</li><li>三<strong>是服务质量得分</strong>，这是店铺DSR评分中的一项，主要受店铺好评数、差评数、投诉率等影响。</li></ul></li><li>对于仓储与物流而言，有两大指标需要重点关注：<ul><li>一是<strong>日均发货单数</strong>，在销售订单能够满足的前提，以满负荷的工作状态下，日均能够发出多少个订单是一项重要的能力。</li><li>二是<strong>库存准确率</strong>，也就是盘点差异。所有实体零售行业的仓储都必须对盘点差异负责，这是仓储模块最基本的要求。</li></ul></li><li>由于财务很少直接参与到电商的一线运营中，因此对于运营有直接影响的指标较少。行业中几乎所有公司都会把用户申请退货之后的退款工作安排给财务团队，所以<strong>退款及时率</strong>应该是唯一对运营有直接影响作用的一项指标，同时它也是属于店铺DSR评分中的一项。 </li></ul></li></ul></li></ol><h2 id="L8-人、货、场下的数据指标库"><a href="#L8-人、货、场下的数据指标库" class="headerlink" title="L8 人、货、场下的数据指标库"></a>L8 人、货、场下的数据指标库</h2><h3 id="有关“人”的那些指标"><a href="#有关“人”的那些指标" class="headerlink" title="有关“人”的那些指标"></a>有关“人”的那些指标</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/d0f760ef14746b1e.png" alt></p><p>“人”可以分为“客服”与“用户”两类：</p><ul><li>“客服”是指客服团队，如售前、售后</li><li>“用户”按照成交状态又可以细分为“流量”与“成交用户”：<ul><li><strong>用户</strong>：凡是在店铺内有过成交记录的；有ID与联系方式可以作为单个的“个体”被追溯和联络</li><li><strong>流量</strong>：只要登录过店铺的；流量的定义明显要高于用户；流量只能作为“群体”而被统计</li></ul></li></ul><p>相关指标：</p><ol><li><p>流量来源：</p><p>流量来源分为自主、免费、付费、淘外、其他这五类。</p><p>但是在实际数据分析中，一致的口径都是使用<strong>主动、免费、付费</strong>三个来源的数据来交流。</p></li><li><p>新客/老客：</p><p>“新客户”是指在店铺内第一次成交的客户， 反之则称为“老客户”；</p><p>“<strong>新老客户比</strong>”是衡量店铺用户质量的一个重要占比。<strong>新客占比过高，说明店铺在统计周期内获客能力不错，但老客回头率过低。</strong></p></li><li><p>活跃/沉睡用户数：</p><p>根据客户生命周期，可以将客户大致分为<strong>新客户、活跃客户、沉睡客户、流失客户</strong>四大类。</p><p>活跃客户和沉睡客户是在做用户质量分析时需要重点关注的；<strong>活跃客户是指××天内有成交记录且购买次数&gt;N次的客户</strong>，活跃客户数量越大，店铺的主动和免费流量就越多；<strong>沉睡客户是指连续××天内没有购买记录的客户</strong>，用户运营团队需要定期执行沉睡客户的“唤醒计划”，因此，针对沉睡客户的监控显得非常有必要。</p></li></ol><h3 id="有关“货”的那些指标"><a href="#有关“货”的那些指标" class="headerlink" title="有关“货”的那些指标"></a>有关“货”的那些指标</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/dd2956179e757da4.png" alt></p><p>商品分析分为四大类：库存分析、配货需求与有效性分析、销售分析、退货分析。</p><ul><li><p>实例1：商品整体库存分析</p><p>某店铺Q1季度库存分析表：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/c6b79dbddb00dcef.png" alt></p><p>通过这份表，不仅可以看出此店铺的库存结构，并且可以判断每个品类的销售进度是否符合预期，以此来评估库存风险。所以，此表既是库存结构分析表，又可作为库存风险预警之用。</p><blockquote><p>SKU = Stock Keeping Unit（库存量单位）</p><ol><li>SKU是指一款商品，每款都有出现一个SKU，便于电商品牌识别商品。</li><li>一款商品多色，则是有多个SKU。例如：iPhone X 64G 银色 则是一个SKU。</li></ol><p>SPU = Standard Product Unit （标准产品单位）SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。例如：iPhone X 可以确定一个产品即为一个SPU。</p><p>库存占比=库存量/总产量</p><p>动销率=店铺中有销量的商品数/全店所有商品数</p></blockquote></li><li><p>实例2：商品销售分析表</p><p><img src="https://i.bmp.ovh/imgs/2020/03/4b39c894ede52b15.png" alt></p><p>这份报表在电商店铺做大型促销活动时，非常有用。</p><p>它通过转化率、加购数、目前库存数这三个核心指标来监控店铺内的所有商品，帮助店铺运营者对所有单品进行“爆旺平滞”的分类，然后制定不同的销售策略；同时，这份表也可以及时发现“潜在畅销”款，提前规避“超卖”风险。 </p><p>如表中“支付转化率”一列中用黑体标注的，便是销售表现较差的款，不仅转化率低，而且库存相对较高。而“销售件数”一列中，用黑体标注的则是有“超卖”风险的。此款UV高达9万以上，售价相对较低， 单款销量上千件，但库存只余150件。以经验判断，此款应该是店铺某个“引流款”，长期投放直通车所致。因此，当通过数据发现此款后，便需要马上提醒商品人员进行库存补货，或者提醒推广人员暂停此款的直通车投放。否则便会有“超卖”风险。</p></li></ul><h3 id="有关“场”的那些常用指标"><a href="#有关“场”的那些常用指标" class="headerlink" title="有关“场”的那些常用指标"></a>有关“场”的那些常用指标</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/af1263f9d022ab5d.png" alt></p><p>场，就是指卖场。 在电商中，场主要由‘页面’与‘促销活动’构成，并体现在‘销售业绩’上。 所以，有关‘场’的指标，可以分为‘销售’‘页面’‘促销’三类。</p><ol><li><p>关于销售额与净销售额</p><p><strong>销售额</strong>是指统计周期内销售业绩的总和，<strong>净销售额</strong>是指统计周期内销售业绩<strong>减去期内退货额</strong>的业绩总和。</p><p>但是在实际分析场景中，这两个指标如何取舍？</p><ol><li>公司制定的本月销售目标是600万，可是这是销售额，还是净销售额呢？</li><li>在周销售报表中，销售额是70万，净销售额是-20万，净销售怎么会是负数呢？</li><li>不是说行业退货率是20%～30%吗，怎么上月退货率高达65%呢？</li></ol><p>以上三个问题来自同一个原因：<strong>电商滞后15天的退货周期，以及普遍高达20%以上的退货率</strong>。</p><p>问题1）：是数据统计口径的问题，一般在制定销售目标时，<strong>行业内都会使用“净销售额”</strong>，而在<strong>公众媒体上都会使用“销售额”</strong>。 </p><p>问题2）：是由于本周收到了金额高达90万的退货而导致净销售额为负；<br>问题3）：同样是由于期内收到的退货额过高所致的。如6月份店铺做了多个品牌团活动，而7月是淡季一场品牌团都没做。于是便会出现6月的退货大量出现在7月，于是拉高了7月的退货率。</p><blockquote><p>针对这种现象，行业内有一种“订单to订单”退货率（又简称A to A退货率，取“Apple to Apple”的意思）的计算方法，是按订单号来统计退货的一种方式，即不管客户滞后多久退货，只是退货所关联的订单号是属于同一个月的，便将退货额统计出来，并除以此月内销售额。</p></blockquote></li><li><p>关于业绩达标率</p><p>业绩达标率=销售额/计划额×100%。</p><p>往往需要衍生出更细致的分析维度，比如滚动达标率、YTD.%、 MTD.%等。</p><p>例如，某品牌2016年3月销售简报：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/b149793eb489bbb5.png" alt></p><p>年度滚动达标率 = 1～3月销售额/1～3月销售目标 ×100%，体现了累积销售进度的滚动达成情况</p><p>YTD.% = 1～3月销售额/全年销售目标×100% ，体现了累积销售额的年度达成情况</p><p>以天猫渠道为例：</p><ol><li>天猫3月份销售额约为102万，达标率仅96%，将近达标；</li><li>本月虽然没有达标，但得益于前两个月超额完成业绩目标，因此截至3月天猫渠道的滚动达标率为102%，说明前三个月，店铺的销售进度尚在预定进度之中；</li><li>在2016年已经过去3个月的情况下，天猫YTD进度只完成18%， 但滚动达标率达102%。说明天猫渠道把较多的销售业绩目标“押宝”在了后面的几个月中。</li></ol></li></ol><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>数据指标就像海量的积木零件，而优秀的数据分析师就像积木建筑师，需要从海量的积木零件中挑选出适用的，然后组装构造出一座理想的房子。</p><p><img src="https://i.bmp.ovh/imgs/2020/03/e3d2b83fa3b272a5.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商数据分析与数据化运营Note——Part1：建立电商运营的“上帝视角”</title>
      <link href="/2020/03/18/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part1/"/>
      <url>/2020/03/18/dian-shang-shu-ju-fen-xi-yu-shu-ju-hua-yun-ying-note-part1/</url>
      
        <content type="html"><![CDATA[<ul><li><p>优秀的数据分析师</p><p>有数据逻辑，结构化思维，商业认知能力，这是成为一个优秀数据分析师的必备技能。简单来说，就是对运营，数据，工具这三种能力的综合运用。</p></li><li><p>数据分析的价值在哪里？</p><p>能够为企业的运营提供<strong>更合理</strong>的运营建议。数据分析不是万能的，从数学角度来讲，数据分析只是一种让人理性思考和决策的工具与方法。而商业经营并不是纯靠理性的博弈就能赢的。除了理性的数据支撑，还需要丰富的情感驱动。所以数据只能做到更合理，但不一定会更有效。</p></li></ul><h1 id="Ch1-建立电商运营的“上帝视角”"><a href="#Ch1-建立电商运营的“上帝视角”" class="headerlink" title="Ch1 建立电商运营的“上帝视角”"></a>Ch1 建立电商运营的“上帝视角”</h1><h2 id="L1-服装与电商发展近史"><a href="#L1-服装与电商发展近史" class="headerlink" title="L1 服装与电商发展近史"></a>L1 服装与电商发展近史</h2><p>服装行业最重要的特点是<strong>周期</strong>——服装的生命周期、风格的生命周期，就连时尚趋势也是有生命周期的。</p><p>国内服装电商的12年发展历程：</p><p><img src="https://s1.ax1x.com/2020/03/18/8wQoIf.md.png" alt="8wQoIf.md.png"></p><ul><li><p>第一阶段：2003年～2007年电商1.0时代[流量为王] </p><p>电商处于C2C的时代，主要以草根卖家为主，运营方式也以刷单、打爆款、砸推广这样简单粗暴的手段为主。</p></li><li><p>第二阶段：2008年～2013年电商2.0时代[数据化运营] </p><p>许多传统服装品牌在这一阶段纷纷加入电商大军。受到“正规军”商家的正面影响，<strong>电商的运营逐渐趋向规范化和体系化</strong>。于是，“精细化运营”“数据化运营”的概念应运而生。数据分析师与数据分析技能便成为电商运营的重要拼图。</p></li><li><p>第三阶段：2014年～2016年电商3.0时代[内容营销]</p><p>结合视频直播、VR技术、网红号召等多种形式，为电商卖家打开新的营销思路，以便在流量稳定的情况下，提升商家的成交转化率。内容营销给商家带来的一个普遍影响是：<strong>所有商家都更加重视电商店铺的视觉效果，商家们在页面结构布局、顾客访问路径优化等方面有了很大的提升</strong>。</p></li></ul><h2 id="L2-数据分析不是“造火箭”"><a href="#L2-数据分析不是“造火箭”" class="headerlink" title="L2 数据分析不是“造火箭”"></a>L2 数据分析不是“造火箭”</h2><ul><li><p><strong>什么是电商数据分析师？</strong></p><p>数据分析师是<strong>把“运营、数据、工具”这三种能力综合运用，为电商店铺运营解决某一具体问题的职业</strong>。</p><p>一名合格的数据分析师必须有这三方面的能力：<strong>懂运营业务、有结构化思维、精通一两门数据工具</strong>。</p><p><img src="https://s1.ax1x.com/2020/03/18/8wllSe.md.png" alt="8wllSe.md.png"></p></li><li><p>数据报告加工流程</p><p><img src="https://s1.ax1x.com/2020/03/18/8wluFK.md.png" alt="8wluFK.md.png"></p><ol><li>明确目的：做任何数据分析之前，我们必须先了解<strong>本次数据分析的目的是什么</strong>。是诊断，还是预测，或者是总结？</li><li>构思结构/逻辑：针对本次分析的目的，我们需要<strong>从哪些角度来构建数据分析逻辑</strong>？用线性式的因果逻辑，还是用分布式相关逻辑？</li><li>开始分析：确定了报表的说服逻辑之后，本次分析需要用到<strong>哪些维度的数据</strong>？<strong>取值范围与口径</strong>是怎样的？这些数据能够得出什么样的<strong>结论</strong>？这些问题都需要在数据分析过程中考虑，最重要的是，对每次整理好的数据都要进行<strong>结论验证</strong>。</li><li>制作/美化报告：对整理好的数据进行取舍，按报表的说服逻辑排序，并编写相关文字观点。适当的美化，<strong>让报告更具有可读性</strong>，是非常有必要的。</li></ol></li></ul><h2 id="L3-电商运营就是“开飞机”"><a href="#L3-电商运营就是“开飞机”" class="headerlink" title="L3 电商运营就是“开飞机”"></a>L3 电商运营就是“开飞机”</h2><ul><li><p>什么是电商？</p><p><strong>电商=零售，零售=成交。</strong></p><p>所有的数据分析，都要以辅助运营、 提升业绩为目标。  </p><p>公司所有部门、所有人都是围绕“成交”两个字在转。比如：网站的页面设计要尽量满足用户使用习惯，活动策划要戳中用户痛点或是利益点，广告投放要选择精准的用户群体，商品配发要符合用户需求，包括我们所做的数据分析报告，也要以提升销售为目标。</p></li><li><p>电商分析最基本的公式</p><p><strong>UV * 转化率 * 客单价 = 销售额</strong></p><p>UV：访客；转化率：买单人数与访客的比值。</p><p>该公式的具体应用：</p><ol><li><p><strong>监测店铺运营状态</strong></p><p>当我们突然发现店铺的指标值不正常时，便应该引起警惕并且尽快找出原因。需要掌握不同时期下店铺的UV、转化率、客单价这些基本数据。</p></li><li><p><strong>制作年度运营目标</strong></p><p>UV、连带率、客单价三个因数，与销售额成正比。当三个因数中的每个因数都有细小的变化时，对销售额就会产生巨大的影响。这就是数学上的乘积效应。</p><p>利用这个原理，我们便可以根据各项指标的增长（或负增长）趋势，来预测来年的销售额。然后与财务目标，以及公司的商品与推广策略去匹配，便能大致预测我们的销售增长是否符合公司预期。</p><p>如果符合，我们的增长点体现在哪些指标上面？对应的这些指标是哪些业务部门在负责？他们在来年有哪些举措来确保这些指标的增长；如果不符合，又是哪些指标的增长达不到公司要求，是什么原因导致的？原因是否合理……</p></li></ol></li><li><p><strong>飞机模型图</strong>（核心业务部门的分工）</p><p><img src="https://s1.ax1x.com/2020/03/18/8wQ5Zt.md.png" alt="8wQ5Zt.md.png"></p><ul><li>运营部是驾驶员，他们负责根据外部（电商平台）的气候变化情况与飞机自身 （企业内部）的健康状态进行匹配，选择最适合的飞行高度与飞机路线。</li><li>商品部与市场部则是两个机翼，负责事业部在飞行过程中的方向选择。如果商品备货过重，则飞机会向右侧倾斜；如果市场推广费过高， 则飞机会向左侧倾斜；如果商品部与市场部都失去应有的作用，则飞机 失去双翼，就会变成滑翔机，只能随着机舱外风吹的方向前进。</li><li>设计部是尾翼，负责飞机在飞行过程中的平衡，如果品牌的风格不稳定，品牌就会陷入左右摇摆、飘忽不定的局面。</li><li>客服部负责在机舱内照顾好已经登机的客人。</li><li>商业智能部是飞行指挥塔，负责随时监测飞机内部的各项飞行指标，同时还要对飞行途中的外部气候做出准确的监测与预判，然后随时与飞行驾驶员沟通，及时提供飞行警示信息。同样，商业智能部也要随时监测企业经营过程中的一切指标，包括商品、流量、销售额、利润等，并且还要监测外部的行业情报、竞争对手情报，然后针对这些数据进行分析，提交可供决策人员参考的运营分析报告。 </li></ul></li></ul><h2 id="L4-在正确的渠道卖正确的货"><a href="#L4-在正确的渠道卖正确的货" class="headerlink" title="L4 在正确的渠道卖正确的货"></a>L4 在正确的渠道卖正确的货</h2><ul><li><p>什么是运营？</p><p>“机关枪”的比喻：<strong>“平台属性”“品牌调性”“商品属性”</strong></p><p>平台属性：平台就像一把机关枪，不停向你们发射子弹，子弹就是流量。我们要做的不是躲避流量，而是要接住流量。那么你们能接住多少子弹呢？这就要求你们要了解机关枪的属性。它一个弹夹（一次销售高峰）有多少子弹？射程（访问深度）多少？射速（停留时间）多少？什么时候换弹（销售高峰什么时候结束）？</p><p>品牌调性：平台同时有很多把性能不一样的机关枪在发射子弹，那么根据我们品牌的调性与风格，我们去哪一把机关枪前接子弹的成功率最高？</p><p>商品属性：用哪些商品才能接到更多的子弹？子弹就是流量，流量就是人，那么这把机关枪射击出来的子弹，它喜欢什么样的商品呢？</p><p>只有理解了这三点，并且能找到正确的匹配，才是真正优秀的运营。</p></li></ul><h3 id="根据渠道特性匹配商品属性"><a href="#根据渠道特性匹配商品属性" class="headerlink" title="根据渠道特性匹配商品属性"></a>根据渠道特性匹配商品属性</h3><ul><li><p>天猫销售分析</p><p><img src="https://s1.ax1x.com/2020/03/18/8wQbRg.md.png" alt="8wQbRg.md.png"></p><ol><li>天猫的新旧商品销售为7:3，就是说天猫是一个以新品为主的销售渠道。</li><li>在新品中，天猫有一半（38%/71%）销售是以接近正价，也就是大于8折的价格销售出去的。证明这家店铺的新品很被消费者所接受，因此<strong>品牌调性与平台属性契合度比较高</strong>。</li><li>需要注意的是，这家店铺的新品销售中，退货率接近35%，这意味着这家店铺每卖10件新品，其中有4件可能被退回，个人感觉有点太高了。</li></ol></li><li><p>京东销售分析</p><p><img src="https://s1.ax1x.com/2020/03/18/8wQqzQ.md.png" alt="8wQqzQ.md.png"></p><ol><li>京东的商品新旧比接近6∶4，也算是以新品为主的销售渠道。</li><li>不过有些不同的是，京东的新品中，以6～7折商品销售为主，6 ～7折的商品应该是处于商品生命周期末端的产品。所以，这个数据说明对于服装正价新品的消费力，京东不如天猫，也就是说，天猫以正价新品为主，京东以打折新品为主。</li><li>京东的退货率约在25%左右，这一点远远低于天猫的35%。证明在同一单成交记录中，京东的经营成本要小于天猫。因为每一笔退货都会给品牌商带来售后、物流上的经营成本。</li></ol></li><li><p>唯品会销售分析</p><p><img src="https://s1.ax1x.com/2020/03/18/8wQOMj.md.png" alt="8wQOMj.md.png"></p><ol><li>在唯品会中，新品的销售贡献不到10%，也就是说，唯品会是几乎完全以旧货为主的销售渠道。</li><li>在旧货中，唯品会的主销折扣带在3～5折，几乎占了总销售额的一半。3～5折的商品几乎是以微利的方式清仓出货的。所以，唯品会为需要清仓的商品提供了一个较好的销售渠道。</li><li>即使是清仓，品牌也不能不赚钱。所以，在旧货中，还有40%的销售业绩来自于旧品6～7折的货。旧品卖到6～7折，自然可以给品牌提供足够的利润空间了。这些货同样是旧品，为什么它们能够卖到6～7折呢？这是从旧品中精细挑选出来的比较应季的款式。</li><li>用商品运营的专业术语来讲，<strong>6～7折是盈利款，而3～5折是走量款</strong>。盈利款与走量款的销售占比能够达到4∶5，这是品牌与唯品会平台之间达成共识的结果。简单来说，品牌方与唯品会平台的共同目标， 就是“唯品会既让品牌达到清仓甩货的目的，又能保证品牌赚钱”。这就是唯品会的属性。</li></ol></li></ul><blockquote><p>总结：</p><ul><li>首先，天猫渠道是一个适合新品销售的渠道，并且它的正价新品消化能力也很不错，所以，我们可以将新品的商品重点向天猫渠道倾斜；</li><li>其次，京东渠道就显得比较均衡了，它的新旧货占比接近5∶5，而且京东的新品销售集中在6折左右，说明京东平台的新品销售利润率并没有天猫渠道高，而且京东的旧品销售也可以占到50%的销售，说明京东同时也承担了一部分库存清理的功能；</li><li>最后，唯品会的特征是最明显的，它本身就定位于一个专门做特卖的网站，所以我们往上面铺的货也几乎都是旧货，而且是超低折扣的旧货。</li></ul></blockquote><h2 id="L5-图解渠道的运营节奏"><a href="#L5-图解渠道的运营节奏" class="headerlink" title="L5 图解渠道的运营节奏"></a>L5 图解渠道的运营节奏</h2><p>产品生命周期图：</p><p><img src="https://s1.ax1x.com/2020/03/18/8w3bRO.md.png" alt="8w3bRO.md.png"></p><p>任何一个产品在其销售过程中，都会经历“介入—成长—成熟—衰退”四个阶段。</p><h3 id="天猫的运营节奏"><a href="#天猫的运营节奏" class="headerlink" title="天猫的运营节奏"></a>天猫的运营节奏</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/2407cbbf046ecce5.png" alt></p><p>把营销活动细致地规划到了每周的时间维度中，并且在每周应该做哪些品类的营销，以怎样的主题来营销都有了前瞻性的规划。</p><p>天猫的活动分为四大级别：</p><ol><li><strong>SS级是指天猫平台方策划的最大型营销活动</strong>。这其中包括双11、双12等重磅活动，也包括“春上新”“春清仓”之类的服装生命周期相关的主题活动。实际上，<strong>SS级活动的名称，被称之为“服装节点”，也正是因为这些活动都是根据服装的生命周期规律来制定和策划的</strong>。从这个意义讲，双11、双12的核心本质就是秋冬大清仓。</li><li><strong>S级是天猫平台特意留出来的一些可供品牌合作的空白活动档期</strong>，如××品牌日、品牌周年庆等活动。S级活动一般会配备品牌团资源，天猫也会从各个渠道优先将资源分配给正在做品牌合作的商家。</li><li>A级活动是天猫平台方根据<strong>不同风格的品牌</strong>主动策划和包装的不同主题的活动。商家参加这些活动需要先报名，入围后天猫才会匹配相关资源、预估会场流量等。</li><li>B级活动是天猫平台根据<strong>不同服装品类</strong>而主动策划和包装的一系列活动。如“连衣裙节”“T恤节”等。有些以某一品类为主打的服装品牌会对这一系列活动十分看重，比如有个品牌是以连衣裙或大衣为主打产 品，那么这个品牌一定会想方设法在连衣裙节或大衣节的活动中，拿到更好的会场资源。</li></ol><p>需要注意的是，这四大分级并不是绝对的。譬如某品牌为了在某个A类或者B类活动中完成冲击业绩的目标，此时往往会选择在做活动的同时，再叠加一个品牌团。此时，A类活动或者B类活动就会成为这个品牌的S级活动。</p><p><strong>“7减8清9上新”</strong>：7月是春夏装减价促销的月份，8月是清仓大促销的月份，9月便开始秋冬款的上新。这个规律贯穿了天猫整个年度营销活动的节奏之中。</p><h3 id="京东的运营节奏"><a href="#京东的运营节奏" class="headerlink" title="京东的运营节奏"></a>京东的运营节奏</h3><p><img src="https://i.bmp.ovh/imgs/2020/03/255324ce6476ecdd.png" alt></p><ul><li>京东的活动分为三大级别，这个与天猫的四大级别的划分逻辑类似，这里就不再详细描述了。</li><li>京东的营销活动有个特点，就是重要活动是呈现<strong>“一头一尾盆地状”</strong>策划的。比如“双11”之后，紧跟着会出现一个“双11返场”的活动， 而“圣诞活动”也是“双12”的返场活动。这样策划的结果，就会使商家呈现出“两头高，中间低”的盆地式销售趋势。而天猫则是“倒三角形”式的销售趋势。这个区别呈现出来的特征是，<strong>天猫的活动流量是有计划地爆发式分配给商家的；而京东的活动流量是按计划均分式分配给商家的</strong>。</li><li>京东营销活动节奏的另一个特点是，没有像天猫一样，留给商家明显的“品牌合作”机会。天猫全年给所有商家留了13次品牌合作的机会，包括这13次品牌合作分布的时间节点都列得很清楚。而京东的营销运营中有关“品牌合作”的策划并没有体现出来，从侧面说明了，京东平台的营销活动策划可能没有天猫那么严谨。</li></ul><h2 id="L6-建立店铺的说服逻辑与购买路径"><a href="#L6-建立店铺的说服逻辑与购买路径" class="headerlink" title="L6 建立店铺的说服逻辑与购买路径"></a>L6 建立店铺的说服逻辑与购买路径</h2><h3 id="电商的说服逻辑"><a href="#电商的说服逻辑" class="headerlink" title="电商的说服逻辑"></a>电商的说服逻辑</h3><ul><li><p><strong>什么是说服逻辑？</strong></p><p>能够条理清晰、有效地说服他人的一套思维方式。</p><p>“卖是表达、买是认同”：</p><ul><li>买是认同：用户之所以买了我的产品，便是对我产生了认同。这份认同可能是对品牌、产品的认同，更可能是对价格、服务，甚至某一种情怀的认同。</li><li>卖是表达：作为卖方，我们应该学会表达我们的品牌、产品、价格、服务，甚至是情怀。唯有这样，才能让用户找到“购买”的理由。</li></ul></li><li><p><strong>电商店铺是怎样来说服用户成交？</strong></p><ol><li><p>电商靠的是视觉呈现：90%的图片+9%的文案+1%的咨询</p></li><li><p>视觉的呈现必须有条理、有结构</p></li></ol></li><li><p><strong>首页的作用：</strong></p><ol><li>增加用户信任感：突出品牌形象，建立用户对品牌的信任感；</li><li>流量分配：做好流量梳理，让用户更精准地找到目标需求产品；</li><li>抓住用户利益点：抓住利益点，让用户找到留下来的理由。</li></ol></li><li><p><strong>首页的构成：</strong></p><ol><li><p><strong>店招</strong></p><ul><li><p>向用户展示品牌实力，是最容易让用户产生信任的手段。譬如“2012～2016年度天猫女装总冠军”。</p></li><li><p>向用户发名片：一键收藏店铺。</p></li><li><p>店内搜索框：大多数店铺都会把搜索框设计在店招中。因为店招有置顶的作用，这样可以避免某些用户因为在首页的目录导航中找不到想要的衣服，而直接跳失离店。</p></li></ul><p>相关指标：</p><ul><li>店铺收藏总数：收藏数越多，说明此品牌的粉丝越多，店铺用户运营的基础越好，可断定此店铺转化率也不会太差。</li><li>搜索栏点击数：搜索栏使用得越多（热力图可看出），说明搜索栏摆放位置更合理。但同时也说明<strong>店铺首页的页面逻辑和商品的主推逻辑相对不合理</strong>。如果搜索栏的点击量远远高于首页中的导航栏和入口图等其他位置，这时必须要检查店铺的首页逻辑是否合理。</li></ul></li><li><p><strong>导航栏</strong></p><ul><li>让想找不同类别或风格服装的用户能够快速找到相应的服装。</li><li>根据不同季度的热销程度与店铺主推的优先级别，会将优先级高的类目尽量放在导航栏的前面，有些还会用红色、黄色等耀眼的颜色来显示。这是为了使主推商品尽可能吸引更多的点击。</li><li>流量分配的作用很多都是通过导航栏来实现的。</li></ul></li><li><p><strong>POP</strong></p><p>POP一般分静态和轮播两种形式：</p><ul><li>在店铺内有特大型促销活动（如双11、双12）时，一般会使用活动主题的POP静态呈现，这样可以尽量避免用户跳失；</li></ul></li></ol><ul><li>如果在平时，则会使用轮播的方式来呈现，一般是3～4张左右。<strong>多个POP应当达到不同的作用，否则既是一种空间浪费，也是一种信息的重复性骚扰</strong>。</li></ul><ol><li><p><strong>豆腐块（入口图）</strong></p><p>豆腐块的作用是，将同一类主推单品集中在一起，并以同一个入口图的形式展现出来。一般的店铺会有6～8个豆腐块，这些豆腐块串联在一起，就是这个店铺所呈现给用户的商品运营逻辑。</p></li></ol></li><li><p>首页前三屏的流量会占首页整体流量的40%以上。 所以，对于首页的说服逻辑来说，<strong>前三屏是最重要的</strong>。而三屏之后，也就是在豆腐块后面的区域，可以看到都是以单品的展示为主。这种展现仅仅是为了提升店铺主推商品的曝光率，对于店铺的整体说服逻辑没有直接作用。</p></li></ul><h3 id="电商的购买路径"><a href="#电商的购买路径" class="headerlink" title="电商的购买路径"></a>电商的购买路径</h3><p>我们需要熟知店铺内的几条主要的购买路径（一般有2～3条），并进行数据分析与研究。当发现店铺的整体购买转化率连续走低时，便可以从这些购买路径中找出原因， 并提出优化与测试方案。</p><p><strong>用户流量路径</strong>：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/8d47a83084a98ad1.png" alt></p><p>“站内流量”：指电商平台本身的流量</p><p>“站外流量”：是指用户是从电商平台之外的渠道进入店铺的。站外流量在店铺流量总和的占比中很少（一般不超过2%），所以不是研究的主要流量类型。</p><p><strong>免费流量与付费流量的路径有很大不同</strong>，这样的分类便于针对某一特定路径进行刻意的分析，然后进行优化。譬如，业界常见的做法之一是针对某个单品直通车的页面来源与去向进行数据分析，如某项数据指标不达标，便会考虑优化此单品的详情页。</p><p>长期的数据跟踪结果显示， <strong>流量第一次出现在店铺时，在首页登录的流量占到70%以上</strong>。因此，首页的“说服逻辑”是否有效便显得非常重要。首页、需要投放直通车的单品详情页、品牌团活动时想买人数多的单品详情页、大型活动时为活动准备的活动二级页，这些页面都是需要被重点监控的重要分析对象。</p><p><strong>“页面分析”工作的核心就是：抓住店铺内的主要购买路径，并分析路径中重要的页面，优化页面的说服逻辑。</strong></p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>精益数据分析Note——第一篇 别再欺骗自己了</title>
      <link href="/2020/03/17/jing-yi-shu-ju-fen-xi-note-di-yi-pian-bie-zai-qi-pian-zi-ji-liao/"/>
      <url>/2020/03/17/jing-yi-shu-ju-fen-xi-note-di-yi-pian-bie-zai-qi-pian-zi-ji-liao/</url>
      
        <content type="html"><![CDATA[<h1 id="第一篇-别再欺骗自己了"><a href="#第一篇-别再欺骗自己了" class="headerlink" title="第一篇 别再欺骗自己了"></a>第一篇 别再欺骗自己了</h1><h2 id="Ch1-我们都在说谎"><a href="#Ch1-我们都在说谎" class="headerlink" title="Ch1 我们都在说谎"></a>Ch1 我们都在说谎</h2><ul><li><p><strong>精益创业模式</strong></p><p>你无法衡量的东西，你也无法管理。</p></li><li><p><strong>最小可行化产品</strong>（Minimum Viable Product；MVP）</p><p>指足以向市场传达你所主张的价值的最小化产品。</p></li><li><p><strong>专人接待式最小可行化产品</strong></p><p>如：正在考虑创建一种拼车服务，则可以试着用人工牵线搭桥这种原始方式将司机和乘客联系在一起，而并不是先考虑是否能开发出一款配对司机与乘客的应用软件。有时并不需要为了产品（即使是最小可行化产品）的开发浪费时间与金钱。</p><p>优点：</p><ol><li>可以<strong>在短时间内以最低的成本帮你尽快测试自己的想法</strong>。</li><li>通过这种专人接待方法<strong>了解用户真正使用的产品，并优化自己的业务流程</strong>。</li></ol><p>启示：</p><ul><li>有时，增长来自于自己未曾想到的方面。在你认为找到了值得一试的想法时，思<strong>考如何以最小的投入快速完成测试</strong>，然后事先为成功下定义，并明确如果直觉准确的话下一步要如何走。</li></ul></li><li><p><strong>精益分析思维</strong></p><p>指提出正确的问题，并重点关注那项可达成期望结果的关键指标。</p></li></ul><h2 id="Ch2-创业的记分牌"><a href="#Ch2-创业的记分牌" class="headerlink" title="Ch2 创业的记分牌"></a>Ch2 创业的记分牌</h2><h3 id="什么是好的数据指标"><a href="#什么是好的数据指标" class="headerlink" title="什么是好的数据指标"></a>什么是好的数据指标</h3><ul><li><p>衡量数据指标好坏的一些重要准则：</p><ol><li>好的数据指标是<strong>比较性</strong>的。如果能比较某数据指标在<strong>不同的时间段、用户群体、竞争产品</strong>之间的表现，就能更好的洞察产品的实际走向。</li><li>好的数据指标是<strong>简单易懂</strong>的。如果人不能很容易记住或讨论指标，那么通过改变它来改变公司的作为很困难。</li><li>好的数据指标是一个<strong>比率</strong>。比率之所以是一个好的指标，有以下几个原因：<ol><li>比率的可操作性强，是行动的向导（开车是速度可以告知状态以便调整确保按时抵达，而里程不可）</li><li>比率是一个天生的比较性指标（可以比较当前速度和前一小时速度，判断是加速还是减速）</li><li>比率还适用于各种因素间的相生和相克（正相关和负相关）</li></ol></li><li>好的指标会<strong>改变行为</strong>。这是<strong>最重要</strong>的评判标准：随着指标的变化，你是否会采取相应的措施。<ol><li>日销售额之类的“会计”指标，有助于进行更准确的财务预测</li><li>“试验指标”，有助于优化产品、定价以及市场定位，会极大影响接下来的动作。需要在收集数据之前就确定好针对不同情况的应变措施。</li></ol></li></ol></li><li><p>另外，数据指标之间的<strong>耦合现象</strong>也值得注意，例如转化率是和购买所需时间相绑定的，二者结合可以的出很多关于现金流的信息。类似地，病毒传播系数（平均每个用户邀请来的新用户数）和病毒传播周期（用户完成一次邀请所需的时间）共同推动产品的普及率。</p></li></ul><h3 id="定性指标与量化指标"><a href="#定性指标与量化指标" class="headerlink" title="定性指标与量化指标"></a>定性指标与量化指标</h3><p>定性指标：非结构化的、经验性的、揭示性的、难以归类的；</p><p>量化指标：涉及很多数值和统计数据，提供可靠的量化结果，但缺乏直观的洞察。</p><p>如果定量数据回答的是“什么”和“多少”这样的问题，那定性数据回答的就是“为什么”。</p><p>定量数据排斥主观因素，定性数据吸纳主观因素。</p><h3 id="虚荣指标与可付诸行动指标"><a href="#虚荣指标与可付诸行动指标" class="headerlink" title="虚荣指标与可付诸行动指标"></a>虚荣指标与可付诸行动指标</h3><p>虚荣指标表面上很美，但不能为公司带来丝毫的改变；如果有一个数据，却不知道如何根据它采取行动，该数据就仅仅是一个虚荣指标。每当看到一个指标，就应该下意识问自己：“依据这个指标，我如何改变当前的商业行为？”如果回答不了这个问题，那么就不用纠结这个指标了。</p><p>可付诸行动指标：可以帮助选出一个行动方案，从而指导你的商业行为。关键在于：你是根据收集到的数据行动。</p><p>例如，“总用户注册数”就是一个虚荣指标，这个数字只会随着时间增长。”总活跃用户数”稍微好些，但前提是对“活跃用户”定义正确，否则还是一个虚荣指标。</p><p>真正应该关注的可付诸行动指标，是<strong>“活跃用户占总用户数的百分比”（活跃用户占比）</strong>。这个指标揭示了产品的用户参与度。另外一个值得关注的指标：<strong>“单位时间内新用户的数量”（新用户增速）</strong>，它对比较不同营销手段的优劣往往很有帮助。</p><ul><li><strong>8个需要提防的虚荣指标</strong><ol><li>点击量</li><li>页面浏览量（PV值）</li><li>访问量</li><li>独立访客数</li><li>粉丝/好友/赞的数量</li><li>网站停留时间/浏览页数</li><li>收集到的用户邮件地址数量</li><li>下载量</li></ol></li></ul><h3 id="探索性指标与报告性指标"><a href="#探索性指标与报告性指标" class="headerlink" title="探索性指标与报告性指标"></a>探索性指标与报告性指标</h3><p>探索性指标：是推测性的；</p><p>报告性指标：时刻对公司的日常运营、管理性活动保持信息通畅、步调一致。</p><blockquote><p>唐纳德理论：世界上的事物可以分为这样几类：我们知道我们知道的，我们知道我们不知道的；我们不知道我们知道的，我们不知道我们不知道的。</p><p>我们知道我们知道的：不确定的事实，要经过数据的检验</p><p>我们知道我们不知道的：意味着某种度量行为。因为我们知道我们不知道这一类指标的具体值，所以要度量。这类指标可用于核算或衡量试验的结果。</p><p>我们不知道我们知道的：直觉，需要评估并训练以提高效率</p><p>我们不知道我们不知道的：是探索，蕴含着自身独特的优势。</p></blockquote><p>数据分析在唐纳德理论中的应用：</p><ol><li>检验手头上的事实和假设，以确保不是在自欺欺人，计划是切实可行的；</li><li>验证我们的直觉，把假设变成证据；</li><li>为业务预测表、瀑布式开发流程图和董事会议提供数据；</li><li>帮助我们发现黄金机遇，大展宏图。</li></ol><h3 id="案例分析：“妈妈圈”的成功之路"><a href="#案例分析：“妈妈圈”的成功之路" class="headerlink" title="案例分析：“妈妈圈”的成功之路"></a>案例分析：“妈妈圈”的成功之路</h3><p>在”朋友圈”拥有1000万用户时，发现一个问题：只有很少的用户在真正地使用这个产品。</p><p><strong>数据分析启示</strong></p><p>想要让社区产品极速启动就需要相当高的用户参与度。不温不火的用户没办法让你的产品直冲云霄。</p><p>更好的做法是：<strong>在一个更小的、更容易触及的目标市场中培养更多具有黏性的高活跃度用户</strong>。病毒式传播需要专注。</p><h3 id="先见性指标与后见性指标"><a href="#先见性指标与后见性指标" class="headerlink" title="先见性指标与后见性指标"></a>先见性指标与后见性指标</h3><p>先见性指标：用于预言未来；</p><p>后见性指标：解释过去，能提示问题的所在。</p><p>创业之初，所拥有的数据不足以预测未来，这时先关注后见性数据。如果要启用先见性数据，需要首先进行同期群分析并比较客户对照组在不同时间段的表现。</p><p>有关账号注销和产品退货的数据是很重要的指标，只是比较滞后。用户流失量是很重要的数据指标。</p><p>“指示剂”无处不在，在一个企业级软件公司，就销售业绩而言，季度订单量是一个后见性指标，而新增潜在客户量是一个先见性指标。任何一个在B2B销售领域工作过的人都会告诉你，除了培养有价值的潜在客户外，还需要对潜在客户转化率和销售周期有很好的把握。</p><p>在一个公司中，某一团队的后见性指标有时是另一个团队的先见性指标。例如，季度订单量对于销售部门来说是一个后见性指标，对于财务部门来说，是一个可以指示营收预期的先见性指标。</p><h3 id="相关性指标与因果性指标"><a href="#相关性指标与因果性指标" class="headerlink" title="相关性指标与因果性指标"></a>相关性指标与因果性指标</h3><p>相关性指标：两个指标总是一同变化</p><p>因果性指标：其中一个指标的变化会导致另一个指标的变化。</p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>精益数据分析Note——第二篇 找到当前的正确指标</title>
      <link href="/2020/03/17/jing-yi-shu-ju-fen-xi-note-di-er-pian-zhao-dao-dang-qian-de-zheng-que-zhi-biao/"/>
      <url>/2020/03/17/jing-yi-shu-ju-fen-xi-note-di-er-pian-zhao-dao-dang-qian-de-zheng-que-zhi-biao/</url>
      
        <content type="html"><![CDATA[<h1 id="第二篇-找到当前的正确指标"><a href="#第二篇-找到当前的正确指标" class="headerlink" title="第二篇 找到当前的正确指标"></a>第二篇 找到当前的正确指标</h1><h2 id="Ch5-数据分析框架"><a href="#Ch5-数据分析框架" class="headerlink" title="Ch5 数据分析框架"></a>Ch5 数据分析框架</h2><ol><li><p>戴夫.麦克卢尔 的<strong>海盗指标说</strong></p><p>麦克卢尔将创业公司最需要关注的指标分为五大类：</p><p><strong>获取用户Acquisition、提高活跃度 Activation、提高留存率 Retention、获取营收 Revenue、自传播 Referral</strong></p><p><img src="https://images2018.cnblogs.com/blog/1425945/201807/1425945-20180727111421656-1691619468.jpg" alt="img"></p><p>描述了用户/客户/访客须经历的五个环节，以便企业获取价值。</p></li><li><p>埃里克.莱斯的<strong>增长引擎</strong>传说</p><p>驱动创业增长的三大引擎，它们都着各自对应的关键绩效指标KPI。</p><ul><li><p><strong>黏着式增长引擎</strong></p><p>重点是让客户成为回头客，并且持续使用你的产品。如果产品的用户黏着性不大，流失率就会很高，用户参与度不理想。<strong>用户参与度</strong>是预测产品成功的最佳指示剂之一。</p><p>衡量黏着性最重要的KPI：<strong>客户留存率</strong>。除此之外，流失率和使用频率也是非常重要的指标。</p></li><li><p><strong>病毒式增长引擎</strong></p><p>所谓病毒式传播归根结底就是：让声名传播出去。</p><p>病毒式传播之所以吸引人，在于它的<strong>指数性</strong>本质。</p><p>关键指标是<strong>病毒式传播系数</strong>，即每个用户所带来的新用户数。这个指标衡量的是每一个病毒传播周期的新用户量。</p><p>还要衡量哪些用户行为形成了一个<strong>病毒传播周期</strong>。还有一些其他因素也与病毒性相关，包括用户完成一次邀请所需的时间（或叫病毒传播周期）以及病毒性的类别。</p></li><li><p><strong>付费式增长引擎</strong></p><p>第三种驱动增长的引擎是付费。在确知产品具有黏着性和病毒性前就开动这一引擎，是过于仓促的行为。</p><p>赚钱是识别一个商业模式是否可持续的终极指标。从客户身上赚的钱 <strong>&gt;</strong> 获取客户的花费，并可以一直做下去，就是可持续的。</p><p>只有反过来把营收的一部分再用于获取用户时，营收才有助于增长，这样就有了一个可调节的业务增长机器。机器上的两个调节旋钮是<strong>客户终生价值（CLV）</strong>和<strong>客户获取成本（CAC）</strong>。取决于多久才可以让客户付清你获取他所花的成本。一种衡量方法是看<strong>客户盈亏平衡时间</strong>，即收回获取一位用户的成本所需时间。</p></li></ul></li><li><p>肖恩.埃利斯的<strong>创业增长金字塔</strong></p><p>着眼于创业公司在找到产品与市场契合点<strong>之后</strong>该如何增长。</p><p>该框架的最大问题：如何确定已经达到产品与市场的契合点？　　　</p><p>为此肖恩设计了一份问卷，问卷中最重要的问题是：“如果不能再使用这个产品或服务，你的感受是什么？”如果40%（或以上）的人回答他们会“非常失望”，就说明你已经找到了契合点。</p></li><li><p>长漏斗</p><p>长漏斗是一种分析方法，能够帮助理解最初是如何获得客户的注意力的，以及客户从最初得知该网站到发生你所期望的行为的全过程。</p></li><li><p>精益数据分析阶段和关隘模型</p><p>在了解以上的数据框架后，需要用一个模型来确立创业通常经历的几个不同阶段，并确定你是否应进入下一阶段的“关隘”指标。</p><p>创业的五个独立阶段：<strong>移情、黏性、病毒性、营收和扩展</strong>。</p></li></ol><blockquote><p>总结：</p><p>侧重于获取和转化用户的行为：如海盗指标和长漏洞框架<br>帮助你了解在何时以何种方式增长：如增长引擎和创业增长金字塔框架<br>帮助你厘清商业模式及其组成部分：如创业画布框架</p></blockquote><h2 id="Ch6-第一关键指标的约束力"><a href="#Ch6-第一关键指标的约束力" class="headerlink" title="Ch6  第一关键指标的约束力"></a>Ch6  第一关键指标的约束力</h2><ul><li>创业在不同的阶段有不同的指标，同一阶段最好专注于某一个指标，这一指标称为<strong>第一关键指标（OMTM，One Metric That Matters）</strong>。在实际的工作中，可以选择尽量少的指标作为日常跟踪对象。</li><li>使用第一关键指标的四大理由：<ol><li>它回答了现阶段最重要的问题</li><li>它促使你得出初始的基线并建立清晰的目标</li><li>它关注的是整个公司层面的健康（数据呕吐：形容想一次汇报太多事情）</li><li>它鼓励一种实验文化：高频率地开展开发–测量–认知循环，高度倡导试验精神</li></ol></li><li>定立初始成功：明确最应该关注的指标以外，还需要定立初始基准，即确定什么数字算是成功，什么算是未达标</li><li>挤压玩具：一个地方是优势，另一个地方一定会显示出劣势，这就是下一个需要攻克的目标了，无论当前目标是什么，都需要做好<strong>随时改变</strong>的准备。</li></ul><h2 id="Ch7-你所在的商业领域"><a href="#Ch7-你所在的商业领域" class="headerlink" title="Ch7 你所在的商业领域"></a>Ch7 你所在的商业领域</h2><ul><li><p>创业增长中的杠杆：</p><ol><li><strong>更多的商品</strong>意味着推出新产品和新服务（新产品研发）</li><li><strong>更多的人</strong>意味着获取更多的用户</li><li><strong>更频繁</strong>意味着高用户黏性、低流失率以及反复使用</li><li><strong>更多的钱</strong>意味着追加销售、将用户愿意支付的价格最大化</li><li><strong>更有效率</strong>意味着降低完成以及支持服务的成本，尽量少打广告，多利用营销来降低获取客户的成本</li></ol></li><li><p>关于“人”</p><p>不是每个用户都是你想要的那种。需要区分哪些是真正有价值的客户，哪些是路过的、好奇的，哪些是有害的客户。<br>接下来要通过改进产品，尽可能增长优质用户比例，同时驱逐劣质用户。<br>不要执迷于用户数，而应该为好的用户做好优化，并根据这些用户来判定好的行为是什么。</p></li><li><p>商业模式拼接书</p><p><strong>商业模式</strong>是一些因素的混合体，不要把营销模式和商业模式混为一谈。<br>主要组成：获取渠道、销售手段、营收来源、产品类型、送达模式<br><img src="https://img-blog.csdnimg.cn/20190115213108305.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNzM1MDI4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li></ul><h2 id="Ch8-商业模式一：电子商务"><a href="#Ch8-商业模式一：电子商务" class="headerlink" title="Ch8 商业模式一：电子商务"></a>Ch8 商业模式一：电子商务</h2><ol><li><p>早期电子商务模式的简单“转化漏斗”模型：</p><p>浏览网页–驻足于某件商品–点击购买按钮–提供详细信息–完成此次交易</p></li><li><p>如今电子商务已经没有简单</p><ul><li>变化1：大家更多是通过<strong>站外搜索</strong>找到结果，而非站内的导航——<strong>搜索关键字</strong>变得更加重要。</li><li>变化2：电商商家通过<strong>推荐</strong>引擎来预测买家要买的东西，以历史购买记录为基础，向用户推荐信息。</li><li>变化3：电商一直在优化网站性能，比如划分来访流量并区别对待来源不同的访客，从而找到最优的产品、内容和价格。</li></ul></li></ol><ul><li>变化4：购买流程早在访问网站前就已经开始，比如社交软件等，使买家行为难以跟踪。</li></ul><ol><li><p>不同的电商模式：</p><p>年度重复购买率=去年购买过的买家中有多少比例今年仍在此购物</p><ul><li><strong>用户获取模式</strong>：客户的重复购买率较低（小于<strong>40%</strong>），应致力于发展新客户。但对于卖眼镜以及运动装备的电商，回头客会较少，可以把精力放在客户推荐机制上。</li><li><strong>混合模式</strong>：客户重复购买率为40%-60%，应兼顾新客户获取与回头客的招揽。</li><li><strong>忠诚度模式</strong>：客户重复购买率超过60%，将经营重心放在客户忠诚度上，即鼓励更加频繁地消费。</li><li>注：三种模式没有优劣之分，重点要找到适合自己的商业模式。</li></ul></li><li><p>关注的指标</p><ul><li><strong>转化率</strong>：访客发生购买行为的比例。是电商健康程度的<strong>最基本</strong>指标之一，可以按人群、商品、访客来源等多重标准讨论转化率。很大程度取决于电商类型。</li><li><strong>年均购买率</strong>：每位买家的年均购买次数。<strong>90天内重复购买率</strong>是判断电商所属类型的绝好指示剂。</li><li><strong>平均购物车大小</strong>：买家下单时平均每单的钱数。一般倾向于把客户获取成本看作是相对固定的，因此订单量越大，利润率越高。</li><li><strong>弃买率</strong>：弃买率=1-转化率，买家开始购买流程后放弃购买的比率。客户可能在任何一步放弃购买，因此把弃买率分解到每一步是是十分必要的。</li><li><strong>客户获取成本</strong>：获取一位客户所需的平均成本。</li><li><strong>平均每位客户营收</strong>（客户终身价值）：平均每位客户终其一生在该网站消费的总金额。包含了其他关键指标的综合指标，是<strong>衡量网站健康程度的唯一标准</strong>。转化率与平均每位客户营收寻求平衡。</li><li><strong>关键词和搜索词</strong>：有助于了解相邻产品与市场。因为搜索引擎有竞价排名，所以应该要找性价比最高的。需要弄清楚大家在找些什么。如果有大量的搜索指向某一特定分类，则需要调整商品分类或者将这个分类放到首页。</li><li><strong>推荐接受率</strong>：从推荐的商品中获得了多少额外的营收。</li><li><strong>病毒性</strong>：口碑以及平均每个访客带来的分享次数。</li><li><strong>邮件列表点入率</strong>：邮件中链接的点击率以及招揽回头客的能力。电子邮件是把双刃剑，一次失败的邮件推广所造成的退订和推广获取的利润。</li></ul></li><li><p>线上线下相结合</p><ul><li>运送时间：提高整体的运营效率，有助于获取对质量和速度有更高要求的客户</li><li>库存可供率：商品缺货时，销售量也随之下降；建议根据销售量来分配库存，做到库存和销量之间的均衡 </li></ul></li><li><p>图说电子商务：一位买家在电商网站中的历程，以及每一步的关键指标。</p><p><img src="https://img-blog.csdnimg.cn/20190119203351417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyNzM1MDI4,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" style="zoom:200%;"></p></li></ol><blockquote><p>总结：</p><ul><li>明确应关注忠诚度还是客户的获取</li><li>站内外搜索成为寻找购买商品的常用方式</li><li>真正起作用的是平均每位客户营收=转化率*重复购买率*购物车的大小</li><li>不要忽视现实问题，如送货、物流、库存等</li></ul></blockquote><h2 id="Ch9-商业模式二：SaaS"><a href="#Ch9-商业模式二：SaaS" class="headerlink" title="Ch9 商业模式二：SaaS"></a>Ch9 商业模式二：SaaS</h2><ol><li><p>SaaS简介</p><p>指按需提供软件的公司，通常以网站的形式出现，如Gmail。</p><p>大部分的 SaaS提供商以<strong>月费或者年费</strong>的形式获取收益。成本为<strong>实际硬件消耗</strong>，如存储空间、占用带宽等。</p><p>对于SaaS而言，增加一个用户的边际成本是可以忽略不计的，因此大多数的SaaS采用<strong>免费增值模式</strong>来获取客户。</p></li><li><p>关注指标</p><ul><li>眼球：网站吸引访客的效果如何</li><li>参与度：有多少访客注册成为了免费版或者试用版的用户</li><li>黏性：有多少客户真正在使用你的产品</li><li>转化率：有多少用户成为了付费用户</li><li>平均每位客户营收：单位时间内平均每位客户带来的营收</li><li>客户获取成本：获取一位付费客户的所需成本</li><li>病毒性：客户邀请他人或向他人推荐公司产品的可能性以及所需时间</li><li>追加销售：是什么促使客户支付更多费用，以及这种情况发生的频率</li><li>系统正常运行时间和可靠性：公司会面临多少用户投诉、问题升级或者服务争端问题</li><li>流失率：单位时间内流失的用户和付费客户人数</li><li>终身价值：客户使用产品期间的付费总额。</li></ul></li><li><p>案例：对于产品生命周期的探索</p><ul><li>在开始优化各种复杂的财务指标之前，先把营收做好。但注意不要忽视成本，因为发展的关键是盈利能力。</li><li>当付费引擎状态良好，客户获取成本只占客户终身价值的一小部分时，即可加大投入，开始扩张。低占比是付费投入回报率的积极信号。</li><li>大多是SaaS公司均依靠月再发收入（客户月以继月地支付费用）获取收益。该指标是公司成功的重要基础。</li></ul></li><li><p>一些指标的衡量</p><ul><li><p><strong>参与度</strong>：用于衡量参与度的终极指标是<strong>日活跃量</strong>。</p><p>将产品的早期版本推向市场，测试客户的反应，然后找出对产品反响最大的人群。着重发展这部分人群。</p><p>此外，部分专业应用未必是天天使用的，因此日活量可以对应变为月活量，<strong>找准适合自己产品的衡量线</strong>最为重要。</p><p><strong>“跨越鸿沟”</strong>：早期用户即可代表主流群体，然后以他们为起点，扩大商品市场。</p><p>衡量参与度时，不要只关注访问率等原始数据，要试着看用户的使用规律。用处有两点：1、找到产品的改进点；2、判断某项产品改动是否奏效，可以先测试部分用户的反应，然后将测试结果与对照组进行比较。</p><p>不仅需要告知产品或服务的黏性如何，还应指出哪些客户留了下来、付出是否的得到了回报。</p></li><li><p><strong>流失率</strong>：流失率之在一段时间内流失掉的用户比例，单位时间可以是周、月或者季度，但是所有指标的时间单位应该保持一致性，这样才具有可比性。</p><p>免费客户流失和付费客户流失应该分开衡量。免费：用户注销账户或再也没回来使用过；付费：注销账号并停止支付费用，或降级到免费版。</p><p>非活跃用户：90天（或更短）内没有登录的用户。</p><p>计算公式：一段时间内流失的用户数/这段时间开始时的用户数<br>修正公式：一段时间内流失的用户数/[(这段时间开始时的用户数+时间段结束时的用户数)/2]</p><p>其他修正：以断代来衡量流失率（以注册时间为基础比较）；以天为单位计算流失率</p><p><img src="https://i.bmp.ovh/imgs/2020/04/8cb505b068218538.png" style="zoom:150%;"></p></li></ul></li></ol><ol><li><p>案例分析：ClearFit放弃月费模式后获得了10倍增长</p><p>SaaS并不意味着一定要采用年费或月费的模式。如果服务存续性十分短暂，也许按件计费的方式会更好些。</p><p>低价位并不一定带来好结果，客户可能会低估产品或服务的质量。</p></li><li><p>SaaS的难题：免费增值、分级收费以及其他定价模式</p><ul><li><p>免费增值模式和付费模式，各有优缺点</p><p>付费模式可以有效控制成本，更具有预测性且能够即时明确所提供这些服务是否具有相应的价值。<br>免费模式可以了解用户的目的并以此为基础进行产品的迭代。</p><p>这些客户群之间的差异可以使分析变得非常复杂。</p></li><li><p>如何分级定价？<br>对使用量的需求因人而异，因此所付金额也会随着时间的推移而变化，因此需要不断尝试让用户升级到更昂贵的版本。</p></li></ul></li></ol><blockquote><p>总结：</p><ul><li>尽管免费增值模式可见度高，但是其实际上是一种销售策略，需谨慎使用。</li><li>在SaaS中，流失率等于一切。如果忠实客户的形成速度要高于用户流失的速度，你就可以生存下来。</li><li>需在用户转换成付费用户前衡量其参与度，并在客户流失以前对其活动进行分析，以采用先见性的措施。</li><li>很多人会把SaaS模式和订阅等同起来，但是完全可以采用许多其他方式来销售软件，有时还比订阅模式有效的多。</li></ul></blockquote><h2 id="Ch10-商业模式三：免费移动应用"><a href="#Ch10-商业模式三：免费移动应用" class="headerlink" title="Ch10 商业模式三：免费移动应用"></a>Ch10 商业模式三：免费移动应用</h2><p>移动应用的开发者通过以下几种方式在应用内赚钱：</p><ul><li>可下载内容（例如新的地图或车型）</li><li>角色天赋、虚拟外观定制和游戏内容（宠物或虚拟角色的一套衣服）</li><li>优势（更好的武器、装备升级等）</li><li>节省时间：付费即可获得原地复活功能，而无需从墓地跑到阵亡点，一种被许多大型多人网页游戏所采取的盈利手段</li><li>跳过冷却/等待时间：付费即可瞬间充满平时需要一天才能充满的能量条</li><li>追加销售至付费成本：有些应用会推出功能不完整的免费版本</li><li>游戏中的广告</li></ul><p>此类公司关注的重要指标如下：</p><ul><li><p>下载量：应用的已下载数量，应用商店排名和评分等相关指标</p></li><li><p>客户获取成本（CAC）：获取一位用户和付费客户的所需成本</p></li><li><p>应用运行率：有多少下载用户真正开启了该项应用，并注册了账号</p></li><li><p>活跃用户/玩家比例：每天/每月保持活跃在线的用户比例，日活（DAU）和月活（MAU）</p></li><li><p>付费用户率：有多少用户曾支付过费用</p></li><li><p>首次付费时间：用户激活多久后才会开始付费</p></li><li><p>用户平均每月营收（monthly Average Revenue Per User，APRU）：该指标是购买和广告的收入总和，通常还包括特定于某个应用程序的信息，例如哪一屏或哪个物品最能吸引用户购买。此外还需跟踪ARPPU，即平均每位付费用户营收（Average Revenue Per Paying User）</p></li><li><p>点评率</p></li><li><p>病毒性：平均每位用户可以邀请多少新客户</p></li><li><p>流失率：卸载应用或一定时间段内没有开启过应用的用户比例</p></li><li><p>客户终身价值</p></li></ul><p>对于移动开发者而言，<strong>应用商店几乎是应用成功普及的最重要因素</strong>。</p><p>计算移动应用的所有必要指标：</p><p><img src="https:////upload-images.jianshu.io/upload_images/11486274-41d3146d2e0bb69f.png?imageMogr2/auto-orient/strip|imageView2/2/w/376/format/webp" alt="img" style="zoom:150%;"></p><p>移动应用商业模式图解：</p><p><img src="https:////upload-images.jianshu.io/upload_images/11486274-1e6379e9c5aa1ecb.png?imageMogr2/auto-orient/strip|imageView2/2/w/419/format/webp" alt="img" style="zoom:150%;"></p><blockquote><p>总结：</p><ul><li>移动应用的盈利模式有很多种</li><li>大部分营收来自于一小部分用户，应将该部分用户单独划归一组进行分析处理。（“鲸鱼”玩家与其他玩家）</li></ul></blockquote><h2 id="Ch11-商业模式四：媒体网站"><a href="#Ch11-商业模式四：媒体网站" class="headerlink" title="Ch11 商业模式四：媒体网站"></a>Ch11 商业模式四：媒体网站</h2><p>广告是一些媒体网站的保底变现方式。那些通过广告盈利颇丰的网站通常更为关注网站的具体内容，并努力提高特定访客的重复访问率。</p><p>广告收入可以有很多种形式：</p><ol><li>出售广告位</li><li>达成赞助协议</li><li>点击量</li><li>后续销售的提成</li></ol><p>媒体网站最为关注的是<strong>点击率和展示率</strong>，其次是<strong>访客在线时长、页面浏览数量以及独立访客数</strong>。</p><p>媒体关注的指标：</p><ul><li>访客与流失率：访客人数及忠诚度</li><li>广告库存：可供变现的广告次数</li><li>广告价格：有时以印象成本计算，即以网页内容和来访人群为基础，计算网站通过展示次数而获得的收入</li><li>点击率</li><li>内容与广告间的平衡：实现广告库存与媒体内容的平衡，以最大化网站的总体性能。</li></ul><h2 id="Ch12-商业模式五：用户生成内容"><a href="#Ch12-商业模式五：用户生成内容" class="headerlink" title="Ch12 商业模式五：用户生成内容"></a>Ch12 商业模式五：用户生成内容</h2><p><strong>用户生成内容网站（UGC）</strong>：将用户聚集在一起，组成在线社区并生成用户内容。如Facebook、Reddit和Twitter。<br>此类商业模式需要关注<strong>优质内容的生成</strong>，此内容不仅局限于帖子的发布与上传，还包括投票、评论、不良内容举报以及其他有价值的活动。</p><p>UGC指优质内容与糟糕内容之间以及内容生成者和潜水者之间的比例。这是一个<strong>参与度漏斗</strong>，与传统电商模式的转化漏斗十分相似。二者之间的唯一差别在于，转化漏斗的最终目的是购买行为的发生，而参与度漏斗则旨在<strong>不断提高用户参与度，让潜水者参与投票，投票者参与评论等</strong>。</p><p>从毫无参与度可言，到只碰巧来过一次的访客，再到深度用户，用户参与度的高低形成了一个天然漏斗。</p><p>网站的核心功能之一便是获取一次性访客，将其转换成注册用户，并最终使为网站的内容作出贡献。</p><p>值得关注的几个关键指标：</p><ul><li>活跃访客数：访客回放频率，以及每次来访的停留时间。还有距上次访问的平均时间（需事先排除超出某临界值如30天的用户数据）。</li><li>内容生成：以某种方式与内容进行互动的访客比例，包括生成内容以及顶/踩行为等。在发展初期，网站的最初内容需要事先准备好一部分，克服鸡生蛋、蛋生鸡的问题。</li><li>参与度漏斗的变化：网站是否有效地增加了用户参与度。比较各级别的用户参与度（环比）。</li><li>生成内容的价值：内容的商业价值，如捐款或广告收入等。最好按照用户群或流量来源分开比较。</li><li>内容的分享和病毒性：内容是如何分享的，分享又是如何有利于网站发展的</li><li>消息提醒的有效性：应用的未来不是移动，而是消息提醒。游戏规则的颠覆者。通过消息提醒不断将用户召回应用，是保持用户参与度的必备要素之一。</li></ul><p>UGC领域发生的三大变革：</p><ul><li>无处不在的签到，智能设备负责记录地理位置的变化并将其分享到网上</li><li>电子钱包：可存放积分、票据、会员数据</li><li>近场通信，摇一摇电子设备就能完成支付或分享信息</li></ul><p>要点：</p><ul><li>对于UGC而言，访客参与度意味着一切</li><li>80/20定律存在于你希望用户完成的所有活动当中</li><li>使用邮件或其他召回方式提高用户回访率</li><li>欺诈防范的工作量十分巨大</li></ul><h2 id="Ch13-商业模式六：双边市场"><a href="#Ch13-商业模式六：双边市场" class="headerlink" title="Ch13 商业模式六：双边市场"></a>Ch13 商业模式六：双边市场</h2><ul><li><p>双边市场是电商网站的一个变种。本模式中，公司通过帮助买家和卖家在网上达成交易来盈利。（如如淘宝、闲鱼、滴滴、美团）</p></li><li><p>双边市场具体定义如下：</p><ol><li>卖家需负责商品的上架与推广。</li><li>市场负责人对每笔交易采取不干预政策。</li><li>买卖双方之间存在利益冲突。</li></ol></li><li><p>双边市场面临着一个特殊的问题，即必须要同时吸引买家和买家。重点关注有钱的一方，通常这一方指的是买家，找到一群愿意花钱的人后，再找到一群愿意挣钱的人要容易更多。</p></li><li><p>首先利用最小市场证明自己的供需状况，以及对买卖双方交易的渴望。然后从中找寻盈利的方法。最后，根据交易规模、频率以及其他商业特质来决定应关注的指标，但归根结底就是交易的营收。</p></li><li><p>如果想从买家这一边做起，则可能需要先选择部分初始库存以供出售，或者在确保库存来源的前提下先接订单，再延时向买家发送货物。先创造需求，在实现市场供应。</p></li><li><p>将双边市场的关键指标分为三类，即买方活动、卖方活动和交易活动。想赚钱的人不难找，难找的是想要花钱的人。</p></li><li><p>建立双边市场的第一步是创建库存（供应）或受众（需求）的能力，这也是应首要衡量的内容。双边市场建设初期，需要关注的指标主要围绕吸引力、参与度以及关注群体的发展状况。</p></li><li><p>关注的数据：</p><ol><li>网站访客数据（潜在买家）</li><li>活跃买家人数。把在过去30天内搜索过商品的买家定义为活跃买家</li><li>卖家人数和在售商品数量的增长情况</li><li>活跃卖家和在售商品的数量与占比</li><li>销售额、满意度与营收</li><li>在售商品质量。</li></ol></li><li><p>关注的指标：</p><ol><li>买卖双方的人数增长。在双边市场的早期阶段尤为重要。当谈及可持续的竞争优势时，需求要强过供给</li><li>库存增长速率。关注每位卖家的在售商品数量，并判断该数值是否得到了增长。</li><li>搜索有效性。买家的搜索内容，以及该内容是否与所建库存相匹配。搜索次数与商品信息的点击次数之比，也是转化漏斗的重要一环。</li><li>转化漏斗。应衡量满意的交易数量。</li><li>评分以及欺诈迹象。用户可基于自身的交易体验为交易对象评分。</li></ol></li><li><p>欺诈与信任是双边市场的另一大问题。你不想承担商品或服务的运送责任，但需确保市场内具有可靠的信誉系统。</p></li><li><p>双边市场的另一大问题在于网络平台内交易的维持。大额的房屋交易不适合线上支付，也很难阻止双方私下交易，从而造成交易中介费的流失。</p></li></ul><blockquote><p>双边市场面临中同时吸引买家和卖家、欺诈与信任、网络平台内交易的维持三大问题。</p><p>关于吸引买家和卖家的问题，先吸引买家，需求永大于供给，满足买家的需求，在汇集更多的卖家，初期可以采用雇佣制；</p><p>关于欺诈与信任，我们在淘宝时会关注店铺的等级、评价、销量等，同时淘宝也采用预付的政策，我们购买物品，所支付的金额在存在在支付宝，待我们收到货后在支付给卖家，同时出台了七天无理由退款；</p><p>关于网站平台内交易的维持，淘宝定时和不定时的活动，双十一、聚划算等，吸引买家。</p></blockquote><h2 id="Ch14-创业阶段的划分"><a href="#Ch14-创业阶段的划分" class="headerlink" title="Ch14 创业阶段的划分"></a>Ch14 创业阶段的划分</h2><ul><li>每家创业公司都必须经历多个阶段的磨练。磨练过程从发现问题开始，历经解决方案的创造及其有效性的核查，并以口碑营销以及资金的筹集结束。<ol><li>移情。你需要深入目标市场，着手解决人们关心的问题，从而促使消费者愿意为你的商品买单。</li><li>黏性。黏性来自好的产品。你需要了解自己能否找到已发现问题的解决方案。</li><li>病毒性。在保证产品或服务的黏性后，即可开始口碑营销。你要以对产品或服务感兴趣的新访客为对象，测试网站的用户获取能力和新手流程，因为你已经从现有用户处获得了品质保证。同时病毒性也可加大付费推广的效力。</li><li>营收。该阶段应着手盈利事宜，但并不意味着此前不存在收费行为。</li><li>规模性。盈利后，公司即可从自身发展模式切换至市场扩张模式。你需要从新的垂直领域和地理位置获取更多的客户。同时，还可投资不同的分销渠道，帮助增长用户基础。</li></ol></li></ul><blockquote><p>创业公司发展历程即为，发现用户需求→解决用户需求→完善用户需求，吸引更多用户→盈利→扩大市场。发现并解决用户需求是基础，如果无法满足，公司也会很快倒闭。</p></blockquote><ul><li>之所以注重公司所处的创业阶段，是因为应关注指标在很大程度上受特定阶段的影响。过早的关注某指标或优化无关紧要的内容必然会导致创业失败。</li></ul><h2 id="Ch15-阶段1：移情"><a href="#Ch15-阶段1：移情" class="headerlink" title="Ch15 阶段1：移情"></a>Ch15 阶段1：移情</h2><ul><li>创业伊始，你投入时间找寻对人们重要的内容。你要做的是尝试了解他人所想，学会换位思考。</li><li>将重点放在定性反馈的收集上，并主要通过有关问题和解决方案的用户访谈来完成这一工作。旨在找到一个值得解决的问题，以及足以获取早期用户的解决方案。</li><li>问题（或想法）的发现往往源于聆听。毕竟人们很喜欢抱怨自己遇到的问题。但请对抱怨内容持保留态度。需要积极主动地聆听，从而找到隐藏在各种抱怨背后的真相或规律。</li><li><p>想法的得出只是一个开始，再将想法付诸实践之前，应使其历经一段时间的沉淀。找到一群朋友或可信任的顾问，这些人均与当前问题息息相关或对其内容有所涉猎，从而可通过互相的交流快速核实想法的现实程度。</p></li><li><p>精益创业的第一阶段旨在判断问题是否足以让足够多的人感到困扰，以及了解目前是如何试图解决这一问题的。</p><ol><li>问题足以让人感到困扰。人充满惰性。这就要求人们的处境足够窘迫，从而不得不按照你的希望去做事。</li><li>有足够多的人感到困扰。只为一个人解决问题的行为叫作咨询。营销者希望受众能够达到群内同质化（即群内成员的喜好存在一定的共性）以及群际异质化（即可按照某种特定方式，利用定制信息区分盒瞄准各细分市场）</li><li>他们已在试图解决这一问题。如果问题真实存在并得到充分认识，则人们一定会想办法加以解决。</li></ol></li><li><p>在进入下一阶段以前，需逐一验证以上三点。数据分析在验证过程中发挥着关键作用。首先，利用定性指标检测所发现问题是否值得付诸行动。建议先于15位潜在客户进行对话。定性数据的关键在于规律和规律的识别。开展成功客户访谈的几点建议：力争面对面地访谈；选择中立场所；避免录音；确保有一个脚本。</p></li><li><p>如何避免引导受访者</p><ol><li>不表明自己的意图。带有倾向性的措辞，例如“你是否同意……”就是这样一种暗示。这也是开放性问题可在客户开发的早期发挥作用的原因。先入为主也是一大影响因素。其他社会暗示源于你的外表。</li><li>保持问题的真实性。要想得到真实的回答，方法之一就是使受访者感到不自在。问题问得越具体，得到的答案就越真实。要想防止受访者为取悦你而给出不真实的答案，还可要求其模仿自己朋友的所作所为。</li><li>刨根问底。客户开发访谈中有一个很棒的技巧，即连续追问三次为什么，反复询问为什么，实际上是在迫使受访者解释自己为何如此作答。</li><li>寻找其他蛛丝马迹。肢体语言往往更能传达出人们的感受与情感。</li></ol></li><li><p>聚合性和发散性问题访谈</p><ol><li>聚合性问题访谈可为你将来的行动设定明确的方向，但要冒一定的风险，即由于过分关注某些自认为重要的问题，而无法让受访者说出对他们而言可能更加重要的问题。</li><li>发散性问题访谈则要随机得多，其目的在于扩大可用于解决方案的搜索范围。风险在于，你关注的问题数量太多、范围太广，没有让受访者重点关注其中的某个问题。</li></ol></li><li><p>人们目前在如何解决问题</p><ol><li>有多少人从未尝试着解决问题？</li><li>有多少人主动提供了“足够好”的解决方案？</li></ol></li><li><p>“典型的一天”故事板是一种还原客户日常生活的方法。故事板是一种视觉表现方式，并且可以使你在客户的一天当中自由地穿梭，以判断你的解决方案在哪里才能发挥出最大的作用。</p></li></ul><h2 id="Ch16-阶段2：黏性"><a href="#Ch16-阶段2：黏性" class="headerlink" title="Ch16 阶段2：黏性"></a>Ch16 阶段2：黏性</h2><ul><li><p>最小可行化产品的黏性</p><ol><li>现如今的关注点完全落在了黏性和参与度上。</li><li>你需要的不只是用户深度参与的表征，还要有证据表明你的产品正逐步成为用户生活中必不可少且难以替代的一部分</li><li>你的第一要务是打造一组核心功能，以保证用户的频繁使用与功能的成功应用，即便受用群体只是一小群早期用户。</li></ol></li><li><p>在步入病毒性阶段以前，你需要证明两件事：一是人们是否在如你所料地使用着产品？二是人们是否从你的产品中得到了足够多的价值？</p></li><li><p>迭代最小可行化产品是一项艰难而又烦冗的工作，迭代是渐变式的，而转型是突变式的。在迭代中，你的目标是提高跟踪中的核心指标。</p></li><li><p>最终的目标是留存率。用户对产品（以及产品的其他潜在用户）的参与度越高，越有可能长久地使用该款产品。</p></li><li><p>开发功能前七问：</p><ol><li>这个功能有什么帮助。黏性阶段重点关注的是留存率。</li><li>你能衡量这一功能的效果吗？围绕功能展开的实验要求对功能的影响做出衡量，同时这种影响必须是可量化的。</li><li>功能开发要多久。时间是一去不复返的珍贵资源。</li><li>这一功能是否会使产品变得太过复杂。复杂是产品的坟墓。</li><li>这一新功能会带来多大的风险。包括与新功能对代码基的影响有关的技术分风险，人们对于新功能如何反馈的用户风险，以及新功能如何推动产品未来发展的相关风险。</li><li>这项新功能有多创新。最好选择下大赌注、孤注一掷、尝试更激进的试验并开发更具颠覆性的产品，尤其是在初期用户期望并不高时更是如此。</li><li>用户说他们想要什么。不要只根据用户反馈就过度优先某功能，用户的行为要比话语更为可靠。</li></ol></li></ul><blockquote><p>新增加一项功能，需要考虑这项功能的可行性，可以带来多大的黏性、需要多久的时间、复杂性、风险度、创新性、效果。首先需要满足我们的目标，到底是为了什么，其次要试验，看看可操作性，整体效果。新增加的一项功能可能是有用的，但是产品现在所处的时期到底有多大的用处，需要结合多方面进行思考。</p></blockquote><ol><li>如何处理用户反馈。</li></ol><p>1)    在查看用户反馈时，一方面用户反馈存在着严重的取样偏差。另一方面他们并不知道自己对于你的价值。</p><p>2)    想改善你对用户反馈的解读，三点建议：第一提前计划好测试，并在测试前厘清自己究竟想要知道些什么。第二选择特定的交谈对象。第三在收集数据的同时快速评审结果。</p><p>读者笔记：在分析用户反馈的时候，首先有目的的，我们此次关注点是什么，并不是要分析所有的用户反馈，其次我们分析哪类人群，是轻度还是重度，最后是及时反馈，随时收集随时总结。</p><ol><li>下面这些信号说明你已拥有最小可行愿景。</li></ol><p>1)    你在打造一个平台。如果你创造的环境内可以创建其他事物</p><p>2)    你有重复性收费的能力。</p><p>3)    你形成了自然的阶梯定价。</p><p>4)    你与一场颠覆性的革命息息相关。</p><p>5)    用户自发成为拥护者。</p><p>6)    你能引发一场价格战。</p><p>7)    你正处于一场环境变革之中。</p><p>8)    你拥有一种可持续的压倒性优势。</p><p>9)    你的边际成本逐步降为零。</p><p>10)    公司模式中有固有的网络效应。</p><p>11)    你有多种赚钱方式。</p><p>12)    你因客户的盈利而盈利。</p><p>13)    你的周围会形成一个生态系统。</p><p>作者：Joan_shallot<br>链接：<a href="https://www.jianshu.com/p/1362764820a0" target="_blank" rel="noopener">https://www.jianshu.com/p/1362764820a0</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习基石》Note——Part4：How Can Machines Learn Better?</title>
      <link href="/2020/03/17/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part4/"/>
      <url>/2020/03/17/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part4/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li><li><a href="https://beader.me/mlnotebook/index.html" target="_blank" rel="noopener">https://beader.me/mlnotebook/index.html</a></li><li><a href="https://me.csdn.net/github_36324732" target="_blank" rel="noopener">https://me.csdn.net/github_36324732</a></li><li><a href="https://zhuanlan.zhihu.com/ml-note" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/ml-note</a></li></ul></blockquote><h1 id="How-Can-Machines-Learn-Better"><a href="#How-Can-Machines-Learn-Better" class="headerlink" title="How Can Machines Learn Better?"></a>How Can Machines Learn Better?</h1><h2 id="L13-Hazard-of-Overfitting"><a href="#L13-Hazard-of-Overfitting" class="headerlink" title="L13 Hazard of Overfitting"></a>L13 Hazard of Overfitting</h2><h3 id="What-is-Overfitting"><a href="#What-is-Overfitting" class="headerlink" title="What is Overfitting?"></a>What is Overfitting?</h3><p>假设平面上有5个点，目标函数是2阶多项式：</p><ul><li>如果hypothesis是2阶多项式，那么这5个点很靠近这个hypothesis，$E_{in}$很小。</li><li>如果hypothesis是4阶多项式，那么这5点会完全落在hypothesis上，$E_{in}=0$，但是$E_{out}$很大。</li><li>根据VC Bound理论，阶数越大，即VC Dimension越大，就会让模型复杂度更高，$E_{out}$更大。</li></ul><p><img src="https://i.bmp.ovh/imgs/2020/03/655d3663a730bb9e.png" style="zoom: 67%;"></p><p>VC曲线：</p><p><img src="https://i.bmp.ovh/imgs/2020/03/b222e9809f91a19b.png" style="zoom:67%;"></p><ul><li>在$d_{VC} = d_{VC}^*$的时候， $E_{out}$取到最小值；</li><li>当$d_{VC} &gt; d_{VC}^<em>$，也就是向右移动时，$E_{in} \downarrow ,E_{out} \uparrow$，这就是<em>*过拟合</em></em>（overfitting）；</li><li><p>当$d_{VC} &lt; d_{VC}^<em>$，也就是向左移动时，$E_{in} \uparrow ,E_{out} \uparrow$，这就是<em>*欠拟合</em></em>（underfitting）。</p></li><li><p><strong>overfitting是VC dimension过大的过程</strong>；<strong>bad generalization是VC dimension过大的结果</strong></p></li></ul><p>造成overfitting的原因是什么（把overfit比喻为出车祸）：</p><ul><li>使用过大的$d_{VC}$（车开太快）</li><li>有noise（路面颠簸）</li><li>数据量N不足（对路况的观察不够）</li></ul><p>也就是说，<strong>VC Dimension、noise、N这三个因素是影响过拟合现象的关键</strong>。</p><h3 id="The-Role-of-Noise-and-Data-Size"><a href="#The-Role-of-Noise-and-Data-Size" class="headerlink" title="The Role of Noise and Data Size"></a>The Role of Noise and Data Size</h3><p>我们看这样一个在二维平面上的例子，离散的圆圈是数据集，目标函数是蓝色的曲线——</p><p><img src="https://i.bmp.ovh/imgs/2020/03/0c2d07c942e574c0.png" alt></p><p>左图：目标函数是x的10阶多项式。数据没有完全落在曲线上，是因为加入了noise。<br>右图：目标函数是x的50阶多项式。没有noise的情况下，数据就全部落在曲线上。</p><p>现在我们分别用二阶多项式和十阶多项式来对两个问题进行拟合，得到下面的结果——</p><p><img src="https://i.bmp.ovh/imgs/2020/03/67389ba96094d800.png" alt></p><p>我们能看到在两个问题中$g_{10}$都发生了过拟合，反而$g_2$却表现得相对不错。</p><p>这种现象产生的原因，从哲学上来说，就是“<strong>以退为进</strong>”。有时候，简单的学习模型反而能表现的更好。</p><p>下面我们来看看发生这样现象的具体原因：</p><p><img src="https://i.loli.net/2018/07/24/5b572f1d8cda6.png" alt="img"></p><p>以上是用Learning Curves来看两个模型在有noise的情况，具体可以复习笔记：基石Lecture 9 - Linear Regression。$E_{in}$和$E_{out}$可以表示为：</p><script type="math/tex; mode=display">E_{in}=\text{noise level}\ast (1-\frac{d+1}N)\\E_{out}=\text{noise level}\ast (1+\frac{d+1}N)</script><p>本节的实验问题中，数据量N不大，即对应于上图中的灰色区域。</p><ul><li>左图的灰色区域中，因为d=2，$E_{in}$和$E_{out}$比较接近；右图中的灰色区域中，d=10，$E_{in}$很小，而$E_{out}$很大；</li><li>这就解释了之前2阶多项式模型的总是更接近，泛化能力更好。</li></ul><p>值得一提的是，如果数据量N很大的时候，上面两图中$E_{in}$和$E_{out}$都比较接近，但是对于高阶模型，z域中的特征很多的时候，需要的样本数量N很大，且容易发生维度灾难。</p><p>在第二个noiseless的问题中，我们发现仍然是2阶的模型拟合的效果更好一些，明明没有noise，为什么是这样的结果呢？ </p><p>其实，<strong>当模型很复杂的时候，这种复杂度本身就会引入一种‘noise’</strong>。所以，这种高阶无noise的问题，也可以类似于10阶多项式的目标函数加上noise的情况，只是二者的noise有所不同，下面将会详细解释。</p><h3 id="Deterministic-Noise"><a href="#Deterministic-Noise" class="headerlink" title="Deterministic Noise"></a>Deterministic Noise</h3><p>下面我们介绍一个更细节的实验来说明什么时候小心overfit会发生。</p><p>假设我们产生的数据分布由两部分组成：第一部分是目标函数$f(x)$，$Q_f$阶多项式；第二部分是噪声$\epsilon$，服从Gaussian分布。接下来我们分析的是noise强度不同对overfitting有什么样的影响。总共的数据量是N。</p><p><img src="https://i.loli.net/2018/07/24/5b572f49ac10c.png" alt="img"></p><ul><li>$Q_f$：$f(x)$的复杂度</li><li>$\sigma^2$：噪音的复杂程度</li></ul><p>那么下面我们分析不同的$(N,\sigma^2)$和$(N,Q_f)$对overfit的影响。overfit可以量化为$E_{out}-E_{in}$。结果如下：</p><p><img src="https://i.loli.net/2018/07/24/5b572f5b5fc76.png" alt="img"></p><p>上图中红色越深，代表overfit程度越高。</p><p>左图中阶数$Q_f$固定为20，横坐标代表样本数量$N$，纵坐标代表噪声水平$\sigma^2$。红色区域集中在$N$很小或者$\sigma^2$很大的时候，也就是说$N$越小，$\sigma^2$越大，越容易发生overfit。</p><p>右边图中$\sigma^2=0.1$，横坐标代表样本数量$N$，纵坐标代表目标函数阶数$Q_f$。红色区域集中在$N$很小或者$Q_f$很大的时候，也就是说$N$越大，$Q_f$越大，越容易发生overfit。这部分两图基本相似。</p><p>从上面的分析，我们发现：</p><ul><li>$\sigma^2$对overfit是有很大的影响的，我们把这种noise称之为<strong>stochastic noise</strong>。</li><li>$Q_f$即模型复杂度也对overfit有很大影响，而且二者影响是相似的，所以我们把这种称之为<strong>deterministic noise</strong>。之所以把它称为noise，是因为模型高复杂度带来的影响。</li></ul><p>总结，有四个因素会导致发生overfitting：</p><ul><li><strong>data size $N \downarrow$</strong></li><li><strong>stochastic noise $\sigma^2\uparrow$</strong></li><li><strong>deterministic noise $Q_f\uparrow$</strong></li><li><strong>excessive power $\uparrow$</strong>（右图左下角部分）</li></ul><p>我们刚才解释了如果目标函数$f(x)$的复杂度很高的时候，那么跟有noise也没有什么两样。<strong>因为目标函数很复杂，那么再好的hypothesis都会跟它有一些差距，我们把这种差距称之为deterministic noise。</strong>deterministic noise与stochastic noise不同，但是<strong>效果一样</strong>。其实<strong>deterministic noise类似于一个伪随机数发生器</strong>，它不会产生真正的随机数，而只产生伪随机数。它的值与hypothesis有关，且固定点x的deterministic noise值是固定的。</p><h3 id="Dealing-with-Overfitting"><a href="#Dealing-with-Overfitting" class="headerlink" title="Dealing with Overfitting"></a>Dealing with Overfitting</h3><p>现在我们知道了什么是overfitting，和overfitting产生的原因，那么如何避免overfitting呢？避免overfitting的方法主要包括：</p><ul><li><p><strong>start from simple model</strong></p></li><li><p><strong>data cleaning/pruning</strong></p></li><li><p><strong>data hinting</strong></p></li><li><p><strong>regularization</strong></p></li><li><p><strong>validataion</strong></p></li></ul><p>介绍简单的data cleaning/pruning和data hinting两种方法：</p><ul><li><p>data cleaning/pruning：</p><ul><li>对训练数据集里label明显错误的样本进行修正（data cleaning）</li><li>或者对错误的样本看成是noise，进行剔除（data pruning）。</li><li>data cleaning/pruning关键在于如何准确寻找label错误的点或者是noise的点，而且如果这些点相比训练样本N很小的话，这种处理效果不太明显（不一定有用）。</li></ul></li><li><p>data hinting</p><p>针对N不够大的情况，如果没有办法获得更多的训练集，那么data hinting就可以对已知的样本进行简单的处理、变换，从而获得更多的（虚拟）样本。</p><p>举个例子，数字分类问题，可以对已知的数字图片进行轻微的平移或者旋转，从而让N丰富起来，达到扩大训练集的目的。这种额外获得的例子称之为virtual examples。但是要注意一点的就是，<strong>新获取的virtual examples可能不再是iid某个distribution（分布改变）</strong>。所以新构建的virtual examples要尽量合理，且是独立同分布的。</p></li></ul><h2 id="L14-Regularization"><a href="#L14-Regularization" class="headerlink" title="L14 Regularization"></a>L14 Regularization</h2><h3 id="Regularized-Hypothesis-Set"><a href="#Regularized-Hypothesis-Set" class="headerlink" title="Regularized Hypothesis Set"></a>Regularized Hypothesis Set</h3><p>那么如何对过拟合现象进行修正，使hypothesis更接近于target function呢？一种方法就是regularized fit。</p><p><img src="https://i.loli.net/2018/07/24/5b57423e5693a.png" alt="img"></p><p>这种方法得到的红色fit曲线，更接近目标函数，它的阶数要更低一些。那么问题就变成了我们要把高阶（10阶）的hypothesis sets转换为低阶（2阶）的hypothesis sets。</p><p>不同阶数的hypothesis存在如下包含关系：</p><p><img src="https://i.loli.net/2018/07/24/5b57424cddae5.png" alt="img"></p><p>我们发现10阶多项式hypothesis sets里包含了2阶多项式hypothesis sets的所有项，那么在$H_{10}$中加入一些限定条件，使它近似为$H_2$即可。这种函数近似曾被称之为<strong>不适定问题（ill-posed problem）</strong>。</p><p>如何从10阶转换为2阶呢？首先，$H_{10}$可表示为：</p><script type="math/tex; mode=display">H_{10}=w_0+w_1x+w_2x^2+w_3x^3+\cdots+w_{10}x^{10}</script><p>而H_2可表示为：</p><script type="math/tex; mode=display">H_2=w_0+w_1x+w_2x^2</script><p>所以，如果限定条件是$w_3=w_4=\cdots=w_{10}=0$，那么就有$H_2=H_{10}$。也就是说，<strong>对于高阶的hypothesis，为了防止过拟合，我们可以将其高阶部分的权重w限制为0，这样，就相当于从高阶的形式转换为低阶，fit波形更加平滑，不容易发生过拟合。</strong></p><p><a href="https://i.loli.net/2018/07/24/5b5742587fdec.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/07/24/5b5742587fdec.png" alt="img"></a></p><p>那有一个问题，令$H_{10}$高阶权重$w$为0，为什么不直接使用$H_2$呢？这样做的目的是拓展我们的视野，为即将讨论的问题做准备。</p><p>刚刚我们讨论的限制是$H_{10}$高阶部分的权重$w$限制为0，这是比较苛刻的一种限制。下面，我们把这个限制条件变得更宽松一点，即令任意8个权重$w$为0，并不非要限定$w_3=w_4=\cdots=w_{10}=0$，这个Looser Constraint可以写成：</p><script type="math/tex; mode=display">\sum_{q=0}^{10}[[w_q\neq0]]\leq3</script><p>也就只是限定了$w$不为0的个数，并不限定必须是高阶的w。这种hypothesis记为$H_2^′$，称为<strong>sparse hypothesis set</strong>，它与$H_2$和$H_{10}$的关系为：</p><script type="math/tex; mode=display">H_2\subset H_2’\subset H_{10}</script><p>Looser Constraint对应的hypothesis应该更好解一些，但事实是sparse hypothesis set $H_2^′$被证明也是NP-hard，求解非常困难。所以，还要转换为另一种易于求解的限定条件。</p><p>那么，我们寻找一种更容易求解的宽松的限定条件Softer Constraint，即：</p><script type="math/tex; mode=display">\sum_{q=0}^{10}w_q^2=||w||^2\leq C</script><p>其中，$C$是常数，也就是说，所有的权重$w$的平方和的大小不超过$C$，我们把这种hypothesis sets记为$H(C)$。</p><p>$H_2^′$与$H(C)$的关系是，它们之间有重叠，有交集的部分，但是没有完全包含的关系，也不一定相等。对应$H(C)$，$C$值越大，限定的范围越大，即越宽松：</p><script type="math/tex; mode=display">H(0)\subset H(1.126)\subset \cdots \subset H(1126)\subset \cdots \subset H(\infty)=H_{10}</script><p>当$C$无限大的时候，即限定条件非常宽松，相当于没有加上任何限制，就与$H_{10}$没有什么两样。$H(C)$<strong>称为regularized hypothesis set</strong>，这种形式的限定条件是可以进行求解的，我们把求解的满足限定条件的权重$w$记为$w_{REG}$。接下来就要探讨如何求解$w_{REG}$。</p><h3 id="Weight-Decay-Regularization"><a href="#Weight-Decay-Regularization" class="headerlink" title="Weight Decay Regularization"></a>Weight Decay Regularization</h3><p>现在，针对H(c)，即加上限定条件，我们的问题变成：</p><p><img src="https://i.loli.net/2018/07/24/5b574276c5ab2.png" alt="img"></p><p>我们的目的是计算$E_{in}(w)$的最小值，限定条件是$||w^2||\leq C$。这个限定条件从几何角度上的意思是，权重$w$被限定在半径为$\sqrt C$的圆内，而球外的$w$都不符合要求。</p><p>用矩阵来表达：</p><p><img src="https://i.loli.net/2018/07/24/5b5742869f669.png" alt="img"></p><p>下面用一张图来解释在限定条件下，最小化$E_{in}(w)$的过程：</p><p><a href="https://i.loli.net/2018/07/24/5b57429140f3c.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2018/07/24/5b57429140f3c.png" alt="img"></a></p><p>如上图所示，假设在空间中的一点$w$，根据梯度下降算法，$w$会朝着$-\nabla E_{in}$的方向移动（图中蓝色箭头指示的方向），在没有限定条件的情况下，$w$最终会取得最小值$w_{lin}$，即“谷底”的位置。</p><p>现在，加上限定条件，即$w$被限定在半径为$\sqrt C$的圆内，如图中红色圆圈所示$w^Tw=C$。那么，这种情况下，$w$不能到达$w_{lin}$的位置，最大只能位于圆上，沿着圆的切线方向移动（图中绿色箭头指示的方向）。</p><p>与绿色向量垂直的向量（图中红色箭头指示的方向）是圆切线的法向量，即$w$的方向。</p><p><strong>那么随着迭代优化过程，只要$-\nabla E_{in}$与$w$方向不平行，那么根据向量知识，$-\nabla E_{in}$一定在$w$点切线方向上有不为零的分量，即$w$点会继续移动</strong>。只有当$-\nabla E_{in}$与红色法向量平行的时候，$-\nabla E_{in}$在切线方向上没有不为零的分量了，也就表示这时$w$达到了最优解的位置。</p><p>有了这个平行的概念，我们就得到了获得最优解需要满足的性质：</p><script type="math/tex; mode=display">\nabla E_{in}(w_{REG})+\frac{2\lambda}{N}w_{REG}=0</script><p>上面公式中的$\lambda$<strong>称为Lagrange multiplier，是用来解有条件的最佳化问题常用的数学工具</strong>，$\frac2N$是方便后面公式推导。那么我们的目标就变成了求解满足上面公式的$w_{REG}$。</p><p>之前我们推导过，线性回归的$E_{in}$的表达式为：</p><script type="math/tex; mode=display">E_{in}=\frac1N\sum_{n=1}^N(x_n^Tw-y_n)^2</script><p>计算$E_{in}$梯度，并代入到平行条件中，得到：</p><script type="math/tex; mode=display">\frac2N(Z^TZw_{REG}-Z^Ty)+\frac{2\lambda}Nw_{REG}=0</script><p>这是一个线性方程式，直接得到$w_{REG}$为：</p><script type="math/tex; mode=display">w_{REG}=(Z^TZ+\lambda I)^{-1}Z^Ty</script><p>上式中包含了求逆矩阵的过程，因为$Z^TZ$是半正定矩阵，如果$\lambda$大于零，那么$Z^TZ+\lambda I$一定是正定矩阵，即一定可逆。统计学上把这叫做<strong>ridge regression</strong>，可以看成是linear regression的进阶版。</p><p>如果对于更一般的情况，例如逻辑回归问题中，$\nabla E_{in}$不是线性的，那么将其代入平行条件中得到的就不是一个线性方程式，$w_{REG}$不易求解。下面我们从另一个角度来看一下平行等式：</p><script type="math/tex; mode=display">\nabla E_{in}(w_{REG})+\frac{2\lambda}{N}w_{REG}=0</script><p>已知$\nabla E_{in}$是$E_{in}$对$w_{REG}$的导数，而$\frac{2\lambda}{N}w_{REG}$也可以看成是$\frac{\lambda}Nw_{REG}^2$的导数。那么平行等式左边可以看成一个函数的导数，导数为零，即求该函数的最小值。也就是说，问题转换为最小化该函数：</p><script type="math/tex; mode=display">E_{aug}(w)=E_{in}(w)+\frac{\lambda}Nw^Tw</script><p>该函数中第二项就是限定条件regularizer，也称为weight-decay regularization。我们把这个函数称为<strong>Augmented Error</strong>，即$E_{aug}(w)$。</p><p>如果$\lambda$不为零，对应于加上了限定条件，若$\lambda$等于零，则对应于没有任何限定条件，问题转换成之前的最小化$E_{in}(w)$。</p><p>下面给出一个曲线拟合的例子，$\lambda$取不同的值时，得到的曲线也不相同：</p><p><img src="https://i.loli.net/2018/07/24/5b5742b7058e4.png" alt="img"></p><p>从图中可以看出，当$\lambda=0$时，发生了过拟合；当$\lambda=0.0001$时，拟合的效果很好；当$\lambda=0.01$和$\lambda=1$时，发生了欠拟合。我们可以把$\lambda$看成是一种penality，即对hypothesis复杂度的惩罚，<strong>$\lambda$越大，$w$就越小，对应于$C$值越小，即这种惩罚越大，拟合曲线就会越平滑，高阶项就会削弱，容易发生欠拟合。</strong>$\lambda$一般取比较小的值就能达到良好的拟合效果，过大过小都有问题，但究竟取什么值，要根据具体训练数据和模型进行分析与调试。</p><p><img src="https://i.loli.net/2018/07/24/5b5742c5c7df3.png" alt="img"></p><p>事实上，这种regularization不仅可以用在多项式的hypothesis中，还可以应用在logistic regression等其他hypothesis中，都可以达到防止过拟合的效果。</p><p>我们目前讨论的多项式是形如$x,x^2,x^3,\cdots,x^n$的形式，若x的范围限定在[-1,1]之间，那么可能导致$x^n$相对于低阶的值要小得多，则其对于的$w$非常大，相当于要给高阶项设置很大的惩罚。为了避免出现这种数据大小差别很大的情况，可以使用<strong>Legendre Polynomials</strong>代替$x,x^2,x^3,\cdots,x^n$这种形式，Legendre Polynomials各项之间是正交的，用它进行多项式拟合的效果更好。关于Legendre Polynomials的概念这里不详细介绍。</p><h3 id="Regularization-and-VC-Theory"><a href="#Regularization-and-VC-Theory" class="headerlink" title="Regularization and VC Theory"></a>Regularization and VC Theory</h3><p>下面我们研究一下Regularization与VC理论之间的关系。Augmented Error表达式如下：</p><script type="math/tex; mode=display">E_{aug}(w)=E_{in}(w)+\frac{\lambda}Nw^Tw</script><p>VC Bound表示为：</p><script type="math/tex; mode=display">E_{out}(w)\leq E_{in}(w)+\Omega(H)</script><p><strong>其中$w^Tw$表示的是单个hypothesis的复杂度，记为$\Omega(w)$；而$\Omega(H)$表示整个hypothesis set的复杂度。</strong></p><p><strong>根据Augmented Error和VC Bound的表达式，$\Omega(w)$包含于$\Omega(H)$之内，所以，$E_{aug}(w)$比$E_{in}$更接近于$E_{out}$，即更好地代表$E_{out}$，$E_{aug}(w$)与$E_{out}$之间的误差更小。</strong></p><p><img src="https://i.loli.net/2018/07/24/5b5742dfcb9c4.png" alt="img"></p><p>根据VC Dimension理论，整个hypothesis set的$d_{VC}=\breve d+1$，这是因为所有的$w$都考虑了，没有任何限制条件。而引入限定条件的$d_{VC}(H(C))=d_{EFF}(H,A)$，即有效的VC dimension。也就是说，$d_{VC}(H)$比较大，因为它代表了整个hypothesis set，但是$d_{EFF}(H,A)$比较小，因为由于regularized的影响，<strong>限定了$w$只取一小部分</strong>。其中A表示regularized算法。当$\lambda \gt 0$时，有：</p><script type="math/tex; mode=display">d_{EFF}(H,A)\leq d_{VC}</script><p>这些与实际情况是相符的，比如对多项式拟合模型，<strong>当$\lambda=0$时，所有的$w$都给予考虑，相应的$d_{VC}$很大，容易发生过拟合。当$\lambda$&gt;0且越来越大时，很多$w$将被舍弃，$d_{EFF}(H,A)$减小，拟合曲线越来越平滑，容易发生欠拟合。</strong></p><h3 id="General-Regularizers"><a href="#General-Regularizers" class="headerlink" title="General Regularizers"></a>General Regularizers</h3><p>那么通用的Regularizers，即$\Omega(w)$，应该选择什么样的形式呢？一般地，我们会朝着目标函数的方向进行选取。有三种方式：</p><ul><li><p><strong>target-dependent</strong>：依据目标函数设定</p></li><li><p><strong>plausible</strong>：平滑</p></li><li><p><strong>friendly</strong>：方便优化</p></li></ul><p><img src="https://i.loli.net/2018/07/24/5b5742fb6b430.png" alt="img"></p><p>其实这三种方法跟之前error measure类似，其也有三种方法：</p><ul><li><p><strong>user-dependent</strong></p></li><li><p><strong>plausible</strong></p></li><li><p><strong>friendly</strong></p></li></ul><p>regularizer与error measure是机器学习模型设计中的重要步骤。</p><p>接下来，介绍两种Regularizer：L2和L1。</p><p>L2 Regularizer一般比较通用，其形式如下：</p><script type="math/tex; mode=display">\Omega(w)=\sum_{q=0}^Qw_q^2=||w||_2^2</script><p>这种形式的regularizer计算的是$w$的平方和，是凸函数，比较平滑，易于微分，容易进行最优化计算。</p><p>L1 Regularizer的表达式如下：</p><script type="math/tex; mode=display">\Omega(w)=\sum_{q=0}^Q|w_q|=||w||_1</script><p>L1计算的不是w的平方和，而是绝对值和，即长度和，也是凸函数。已知$w^Tw=C$围成的是圆形，而$||w||_1=C$围成的是正方形，那么在正方形的四个顶点处，是<strong>不可微分</strong>的（不像圆形，处处可微分）。根据之前介绍的平行等式推导过程，对应这种正方形，<strong>它的解大都位于四个顶点处</strong>，若$-\nabla E_{in}$不与其平行，那么$w$就会向顶点处移动，顶点处的许多$w$分量为零，所以，L1 Regularizer的解是稀疏的，称为sparsity。优点是计算速度快。</p><p><img src="https://i.loli.net/2018/07/24/5b57432c1e60a.png" alt="img"></p><p>下面来看一下$\lambda$如何取值，首先，若stochastic noise不同，那么一般情况下，$\lambda$取值有如下特点：</p><p><img src="https://i.loli.net/2018/07/24/5b57433f1809c.png" alt="img"></p><p>从图中可以看出，stochastic noise越大，$\lambda$越大。</p><p>另一种情况，不同的deterministic noise，$\lambda$取值有如下特点：</p><p><img src="https://i.loli.net/2018/07/24/5b5743534e379.png" alt="img"></p><p>从图中可以看出，deterministic noise越大，$\lambda$越大。</p><p>以上两种noise的情况下，都是noise越大，相应的$\lambda$也就越大。这也很好理解，如果在开车的情况下，路况也不好，即noise越多，那么就越会踩刹车，这里踩刹车指的就是regularization。但是大多数情况下，noise是不可知的，这种情况下如何选择$\lambda$？这部分内容，我们下节课将会讨论。</p><h2 id="L15-Validation"><a href="#L15-Validation" class="headerlink" title="L15 Validation"></a>L15 Validation</h2><p>如何保证训练的模型具有良好的泛化能力？</p><h3 id="Model-Selection-Problem"><a href="#Model-Selection-Problem" class="headerlink" title="Model Selection Problem"></a>Model Selection Problem</h3><p>机器学习模型建立的过程中有许多选择，不同的选择搭配，有不同的机器学习效果。我们的目标就是找到最合适的选择搭配，构建最佳的机器学习模型。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/1-4.png" alt="img"></p><p>假设有$M$个模型，对应有$H_1,H_2,\cdots,H_M$，即有$M$个hypothesis set，演算法为$A_1,A_2,\cdots,A_M$，共$M$个。我们的目标是从这$M$个hypothesis set中选择一个模型$H_{m^<em>}$，通过演算法$A_{m^</em>}$对样本集$D$的训练，得到一个最好的矩$g_{m^<em>}$，使其$E_{out}(g_{m^</em>})$最小。所以，问题的关键就是机器学习中<strong>如何选择到最好的矩$g_{m^*}$。</strong></p><ul><li><p>方法一：对$M$个模型分别计算使$E_{in}$最小的矩$g$，再横向比较，取其中<strong>能使$E_{in}$最小</strong>的模型的矩$g_{m^*}$：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-3.png" alt="img"></p><p>但是$E_{in}$足够小并不能表示模型好，反而<strong>可能表示训练的矩$g_{m^*}$发生了过拟合</strong>，泛化能力很差。而且这种“模型选择+学习训练”的过程，它的VC Dimension是$d_{VC}(H_1\cup H_2)$，模型复杂度增加。总的来说，泛化能力差，用$E_{in}$来选择模型是不好的。</p></li><li><p>方法二：如果有这样一个独立于训练样本的测试集，将$M$个模型在测试集上进行测试，选取$E_{test}$<strong>最小</strong>的模型作为最佳模型：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-3.png" alt="img"></p><p>这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到：</p><script type="math/tex; mode=display">E_{out}(g_{m^*})\leq E_{test}(g_{m^*})+O(\sqrt \frac{log M}{N_{test}})</script><p>由上式可以看出，模型个数$M$越少，测试集数目越大，那么$O(\sqrt \frac{log M}{N_{test}})$越小，即$E_{test}(g_{m^<em>})$越接近于$E_{out}(g_{m^</em>})$。</p></li></ul><p>比较这两种方法：</p><ul><li><p>第一种方法使用$E_{in}$作为判断基准，使用的数据集就是训练集$D$本身；不仅使用$D$来训练不同的$g_m$，而且又使用$D$来选择最好的$g_{m^<em>}$，那么$g_{m^</em>}$对未知数据并不一定泛化能力好。举个例子，这相当于老师用学生做过的练习题再来对学生进行考试，那么即使学生得到高分，也不能说明他的学习能力强。所以最小化$E_{in}$的方法并不科学。</p></li><li><p>第二种方法使用$E_{test}$作为判断基准，使用的是独立于训练集$D$之外的测试集。相当于新的考试题能更好地反映学生的真实水平，所以最小化$E_{test}$更加理想。</p></li><li><p>但是，我们拿到的都是训练集$D$，测试集是拿不到的。所以，寻找一种折中的办法，<strong>我们可以使用已有的训练集$D$来创造一个验证集validation set，即从$D$中划出一部分$D_{val}$作为验证集</strong>。$D$另外的部分作为训练集，$D_{val}$独立开来，用来测试各个模型的好坏，最小化$E_{val}$，从而选择最佳的$g_{m^*}$。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-1.png" alt="img"></p></li></ul><h3 id="Validation"><a href="#Validation" class="headerlink" title="Validation"></a>Validation</h3><p>从训练集$D$中抽出一部分$K$个数据作为验证集$D_{val}$，前提是保证$D_{val}$独立同分布(iid)于$P(x,y)$，也就是说$D_{val}$的选择是从$D$中平均随机抽样得到的。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/5-1.png" alt="img"></p><p>$D$中去除$D_{val}$后的数据就是供模型选择的训练数据$D_{train}$，其大小为$N-k$。</p><p>使用$D_{train}$训练模型，得到$g_m^-$，使用$g_m^-$对$D_{val}$进行验证，得到如下Hoffding不等式：</p><script type="math/tex; mode=display">E_{out}(g_m^-)\leq E_{val}(g_m^-)+O(\sqrt \frac{log M}{K})</script><p>假设有$M$种模型hypothesis set，$D_{val}$的数量为$K$，那么从每种模型$m$中得到一个在$D_{val}$上表现最好的矩，再横向比较，从$M$个矩中选择一个最好的$m^*$作为我们最终得到的模型。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-1.png" alt="img"></p><p>现在由于数量为$N$的总样本$D$的一部分$K$作为验证集，那么只有$N-k$个样本可供训练。从$D_{train}$中得到最好的$g_{m^<em>}^-$，而总样本$D$对应的最好的矩为$g_{m^</em>}$。根据之前的leraning curve很容易知道，<strong>训练样本越多，得到的模型越准确，其hypothesis越接近target function</strong>，即$D$的$E_{out}$比$D_{train}$的$E_{out}$要小：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-2.png" alt="img"></p><p>所以，我们通常的做法是通过$D_{val}$来选择最好的矩$g_{m^<em>}^-$对应的模型$m^</em>$，再<strong>对整体样本集$D$使用该模型进行训练</strong>，最终得到最好的矩$g_{m^*}$。</p><p>总结使用验证集进行模型选择的整个过程：</p><ul><li>先将$D$分成两个部分，一个是训练样本$D_{train}$，一个是验证集$D_{val}$</li><li>若有$M$个模型，那么分别对每个模型在$D_{train}$上进行训练，得到矩$g_{m}^-$</li><li>再用$D_{val}$对每个$g_{m}^-$进行验证，选择表现最好的矩记为$g_{m^*}^-$</li><li>最后使用该模型对整个$D$进行训练，得到最终的$g_{m^*}$。</li></ul><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-2.png" alt="img"></p><p>不等式关系满足：</p><script type="math/tex; mode=display">E_{out}(g_{m^*})\leq E_{out}(g_{m^*}^-)\leq E_{val}(g_{m^*}^-)+O(\sqrt \frac{log M}{K})</script><p>下面我们举个例子来解释这种模型选择的方法的优越性：</p><p>假设有两个模型：一个是5阶多项式$H_{\Phi_5}$，一个是10阶多项式$H_{\Phi_{10}}$。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-2.png" alt="img"></p><p>图中，横坐标表示验证集的样本数量$K$，纵坐标表示$E_{out}$大小。</p><ul><li>黑色水平线表示没有验证集，完全使用$E_{in}$进行判断基准。这种方法的$E_{out}$较大，而且与$K$无关。</li><li>黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，$E_{out}$很小，与$K$无关，实际中很难得到这条虚线。</li><li>红色曲线表示使用验证集，但是最终选取的矩是$g_{m^<em>}^-$，其趋势是随着$K$的增加，它对应的$E_{out}$先减小再增大，当$K$大于一定值的时候，甚至会超过黑色水平线。这是因为随着$K$的增大，$D_{val}$增大，但可供模型训练的$D_{train}$在减小，那得到的$g_{m^</em>}^-$不具有很好的泛化能力。</li><li>蓝色曲线表示也使用验证集，最终选取的矩是$g_{m^*}$，其趋势是随着$K$的增加，它对应的$E_{out}$先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。</li></ul><p>那么，如何设置验证集K值的大小呢？</p><ul><li>当$K$值很大时，$E_{val}\approx E_{out}$，但是$g_{m}^-$与$g_m$相差很大；</li><li>当$K$值很小时，$g_m^-\approx g_m$，但是$E_{val}$与$E_{out}$可能相差很大。</li></ul><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-1.png" alt="img"></p><p>所以有个折中的办法，通常设置$k=\frac N5$。值得一提的是，划分验证集，通常并不会增加整体时间复杂度，反而会减少，因为$D_{train}$减少了。</p><h3 id="Leave-One-Out-Cross-Validation"><a href="#Leave-One-Out-Cross-Validation" class="headerlink" title="Leave-One-Out Cross Validation"></a>Leave-One-Out Cross Validation</h3><p>假如考虑一个极端的例子，$K=1$，也就是说验证集大小为1，每次只用一组数据对$g_m$进行验证。这样做的优点是$g_m^-\approx g_m$，但是$E_{val}$与$E_{out}$可能相差很大。</p><p>为了避免$E_{val}$与$E_{out}$相差很大，<strong>每次从$D$中取一组作为验证集，直到所有样本都作过验证集</strong>，共计算$N$次，最后对验证误差求平均，得到$E_{loocv}(H,A)$，这种方法称之为<strong>留一法交叉验证</strong>，表达式为：</p><script type="math/tex; mode=display">E_{loocv}(H,A)=\frac1N\sum_{n=1}^Ne_n=\frac1N\sum_{n=1}^Nerr(g_n^-(x_n),y_n)</script><p>这样求平均的目的是为了让$E_{loocv}(H,A)$尽可能地接近$E_{out}(g)$。</p><p>下面用一个例子图解留一法的过程：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/12-2.png" alt="img"></p><p>如上图所示，要对二维平面上的三个点做拟合，上面三个图表示的是线性模型，下面三个图表示的是常数模型。对于两种模型，分别使用留一交叉验证法来计算$E_{loocv}$，<strong>计算过程都是每次将一个点作为验证集，其他两个点作为训练集，最终将得到的验证误差求平均值</strong>，就得到了$E_{loocv}(linear)$和$E_{loocv}(constant)$，比较两个值的大小，取值小对应的模型即为最佳模型。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/13-1.png" alt="img"></p><p>接下来，我们从理论上分析<strong>Leave-One-Out方法的可行性</strong>，即$E_{loocv}(H,A)$是否能保证$E_{out}$的矩足够好？</p><p>假设有不同的数据集$D$，它的期望分布记为$\varepsilon_D$，则其$E_{loocv}(H,A)$可以通过推导，等于$E_{out}(N-1)$的平均值。由于$N-1$近似为$N$，$E_{out}(N-1)$的平均值也近似等于$E_{out}(N)$的平均值。具体推导过程如下：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/14-2.png" alt="img"></p><p>最终我们得到的结论是$E_{loocv}(H,A)$的期望值和$E_{out}(g^-)$的期望值是相近的，这代表得到了比较理想的$E_{out}(g)$，Leave-One-Out方法是可行的。</p><p>举一个例子，使用两个特征：Average Intensity和Symmetry加上这两个特征的非线性变换（例如高阶项）来进行手写数字识别。平面特征分布如下图所示：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/15-1.png" alt="img"></p><p>Error与特征数量的关系如下图所示：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/16-1.png" alt="img"></p><p>从图中我们看出，随着特征数量的增加，$E_{in}$不断减小，$E_{out}$先减小再增大，虽然$E_{in}$是不断减小的，但是它与的差距$E_{out}$越来越大，发生了过拟合，泛化能力太差。</p><p>而$E_{cv}$与$E_{out}$的分布基本一致，能较好地反映$E_{out}$的变化。所以，我们只要使用Leave-One-Out方法得到使$E_{cv}$最小的模型，就能保证其$E_{out}$足够小。</p><h3 id="V-Fold-Cross-Validation"><a href="#V-Fold-Cross-Validation" class="headerlink" title="V-Fold Cross Validation"></a>V-Fold Cross Validation</h3><p>Leave-One-Out可能的问题：</p><ul><li>计算量，假设$N=1000$，那么就需要计算1000次的$E_{loocv}$，再计算其平均值。当$N$很大的时候，计算量是巨大的，很耗费时间。</li><li>稳定性，例如对于二分类问题，取值只有0和1两种，预测本身存在不稳定的因素，那么对所有的$E_{loocv}$计算平均值可能会带来很大的数值跳动，稳定性不好。</li></ul><p>所以，这两个因素决定了Leave-One-Out方法在实际中并不常用。</p><p>针对Leave-One-Out的缺点，我们对其作出了改进。将$N$个数据分成$V$份（例如V=10），计算过程与Leave-One-Out相似。这样可以减少总的计算量，又能进行交叉验证，得到最好的矩，这种方法称为<strong>V-折交叉验证</strong>。其实Leave-One-Out就是V-折交叉验证的一个极端例子。</p><script type="math/tex; mode=display">E_{cv}(H,A)=\frac1V\sum_{v=1}^VE_{val}^{(V)}(g_V^-)</script><p>所以，<strong>一般的Validation使用V-折交叉验证来选择最佳的模型</strong>。</p><p>值得一提的是Validation的数据来源也是样本集中的，所以并不能保证交叉验证的效果好，它的模型一定好。只有样本数据越多，越广泛，那么Validation的结果越可信，其选择的模型泛化能力越强。</p><h2 id="L16-Three-Learning-Principles"><a href="#L16-Three-Learning-Principles" class="headerlink" title="L16 Three Learning Principles"></a>L16 Three Learning Principles</h2><h3 id="Occam’s-Razor"><a href="#Occam’s-Razor" class="headerlink" title="Occam’s Razor"></a>Occam’s Razor</h3><p>奥卡姆剃刀定律（Occam’s Razor）：“切勿浪费较多东西去做用较少的东西同样可以做好的事情。” “如无必要，勿增实体”，就像剃刀一样，将不必要的部分去除掉。</p><p>Occam’s Razor反映到机器学习领域中，指的是在所有可能选择的模型中，我们应该<strong>选择能够很好地解释已知数据并且十分简单的模型</strong>。</p><p>问题：什么模型称得上是简单的？为什么简单模型比复杂模型要好？</p><ul><li><p><strong>什么模型称得上是简单的？</strong></p><ul><li><p><strong>简单的hypothesis $h$</strong>，简单的hypothesis就是指<strong>模型使用的特征比较少</strong>，例如多项式阶数比较少。</p></li><li><p><strong>模型$H$包含的hypothesis数目有限</strong>，不会太多，这也是简单模型包含的内容。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/2-4.png" alt="img"></p></li></ul><p>simple hypothesis $h$和simple model $H$是紧密联系的。如果hypothesis的特征个数是$l$，那么$H$中包含的hypothesis个数就是$2^l$，也就是说，hypothesis特征数目越少，$H$中hypothesis数目也就越少。</p><p>所以，为了让模型简单化，我们可以<strong>一开始就选择简单的model，或者用regularization，让hypothesis中参数个数减少</strong>，都能降低模型复杂度。</p></li><li><p>为什么简单模型比复杂模型要好？</p><p>机器学习的目的是“找规律”，即分析数据的特征，总结出规律性的东西出来。</p><p>假设现在有一堆没有规律的杂乱的数据需要分类，要找到一个模型，让它的$E_{in}=0$是很难的。但是如果是很复杂的模型，也有可能将其分开。如果使用某种简单的模型就可以将数据分开，那表明数据本身应该符合某种规律性。相反地，<strong>如果用很复杂的模型将数据分开，并不能保证数据本身有规律性存在，也有可能是杂乱的数据，因为无论是有规律数据还是杂乱数据，复杂模型都能分开。</strong>这就不是机器学习模型解决的内容了。所以，模型选择中，我们应该尽量先选择简单模型，例如最简单的线性模型。</p></li></ul><h3 id="Sampling-Bias"><a href="#Sampling-Bias" class="headerlink" title="Sampling Bias"></a>Sampling Bias</h3><p>抽样的样本会影响到结果。如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。</p><p>从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。</p><h3 id="Data-Snooping"><a href="#Data-Snooping" class="headerlink" title="Data Snooping"></a>Data Snooping</h3><p>之前的课程，我们介绍过在模型选择时应该尽量避免偷窥数据，因为这样会使我们人为地倾向于某种模型，而不是根据数据进行随机选择。所以，$\Phi$应该自由选取，最好不要偷窥到原始数据，这会影响我们的判断。</p><p>事实上，数据偷窥发生的情况有很多，不仅仅指我们看到了原始数据。什么意思呢？其实，当你在使用这些数据的任何过程，都是间接地偷看到了数据本身，然后你会进行一些模型的选择或者决策，这就增加了许多的model complexity，也就是引入了污染。</p><p>下面举个例子来说明。假如我们有8年的货比交易数据，我们希望从这些数据中找出规律，来预测货比的走势。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/3-2.png" alt="img"></p><p>现在有两种训练模型的方法：</p><ul><li>一种是使用前6年数据进行模型训练，后2年数据作为测试，图中蓝色曲线表示后2年的预测收益；</li><li>另一种是直接使用8年数据进行模型训练，图中红色曲线表示后2年的预测收益情况。</li></ul><p>很明显，使用8年数据进行训练的模型对后2年的预测的收益更大，似乎效果更好。但是这是一种自欺欺人的做法，<strong>因为训练的时候已经拿到了后2年的数据，用这样的模型再来预测后2年的走势是不科学的</strong>。这种做法也属于间接偷窥数据的行为。直接偷窥和间接偷窥数据的行为都是不科学的做法，并不能表示训练的模型有多好。</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/4-2.png" alt="img"></p><p>还有一个偷窥数据的例子，比如对于某个基准数据集$D$，某人对它建立了一个模型$H_1$，并发表了论文。第二个人看到这篇论文后，又会对$D$，建立一个新的好的模型$H_2$。这样，不断地有人看过前人的论文后，建立新的模型。其实，后面人选择模型时，已经被前人影响了，这也是偷窥数据的一种情况。也许你能对$D$训练很好的模型，但是<strong>可能你仅仅只根据前人的模型，成功避开了一些错误，甚至可能发生了overfitting或者bad generalization</strong>。所以，机器学习领域有这样一句有意思的话“If you torture the data long enough, it will confess.”所以，我们不能太“折磨”我们的数据了，否则它只能“妥协”了。</p><p>在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据：</p><ul><li>第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。</li><li>第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。</li></ul><h3 id="Power-of-Three"><a href="#Power-of-Three" class="headerlink" title="Power of Three"></a>Power of Three</h3><p>对16节课做个简单的总结</p><p>首先，我们介绍了跟机器学习相关的三个领域：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/6-2.png" alt="img"></p><p>我们还介绍了三个理论保证：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/7-3.png" alt="img"></p><p>然后，我们又介绍了三种线性模型：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/8-3.png" alt="img"></p><p>同时，我们介绍了三种重要的工具：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/9-3.png" alt="img"></p><p>还有我们本节课介绍的三个锦囊妙计：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/10-3.png" alt="img"></p><p>最后，我们未来机器学习的方向也分为三种：</p><p><img src="http://47.94.229.135/wp-content/uploads/2018/07/11-2.png" alt="img"></p><p>完结~</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX相关</title>
      <link href="/2020/03/12/latex-xiang-guan/"/>
      <url>/2020/03/12/latex-xiang-guan/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据化管理——洞悉零售及电子商务运营Note</title>
      <link href="/2020/03/11/shu-ju-hua-guan-li-dong-xi-ling-shou-ji-dian-zi-shang-wu-yun-ying-note/"/>
      <url>/2020/03/11/shu-ju-hua-guan-li-dong-xi-ling-shou-ji-dian-zi-shang-wu-yun-ying-note/</url>
      
        <content type="html"><![CDATA[<p>如何快速识别真假数值？</p><ul><li>尾数法（首尾法）：只看最后一位数字（第一位数字），尾数（首位数）相互加减乘除后的结果必须满足相应的算术规律。</li><li>数位法：通过位数判断，如4位数乘以3位数的结果应该是6位数或7位数。</li><li>极值法：在求和运算中，最大值能左右运算。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Books </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DataAnalysis </tag>
            
            <tag> Books </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习基石》Note——Part2：Why Can Machines Learn?</title>
      <link href="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part2/"/>
      <url>/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li><li><a href="https://beader.me/mlnotebook/index.html" target="_blank" rel="noopener">https://beader.me/mlnotebook/index.html</a></li><li><a href="https://me.csdn.net/github_36324732" target="_blank" rel="noopener">https://me.csdn.net/github_36324732</a></li><li><a href="https://zhuanlan.zhihu.com/ml-note" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/ml-note</a></li></ul></blockquote><h1 id="Why-Can-Machines-Learn"><a href="#Why-Can-Machines-Learn" class="headerlink" title="Why Can Machines Learn?"></a>Why Can Machines Learn?</h1><h2 id="VC-Dimension"><a href="#VC-Dimension" class="headerlink" title="VC Dimension"></a>VC Dimension</h2><h3 id="Dichotomy"><a href="#Dichotomy" class="headerlink" title="Dichotomy"></a>Dichotomy</h3><p>从中任意选取一个方程，让这个对进行二元分类，输出一个结果向量，比如对4个点进行预测，输出，这样的一个输出向量我们称它为一个dichotomy。</p><p>我们把一个dichotomy对应的所有直线方程视为一类，则effective number of lines=dichotomy的数量。显然这个dichotomy的数量小于等于所有数据点的排列组合数的，例如下图中画大叉的那幅图对应的排列组合，就不能成为一个dichotomy，因为它们无法由任何一条直线方程产生。如果存在三点共线的情况，则dichotomy的数量会更少。</p><p>2D Perceptrons：</p><p><img src="https://beader.me/mlnotebook/section2/images/4points14lines.png" alt="img"></p><h3 id="Growth-Function"><a href="#Growth-Function" class="headerlink" title="Growth Function"></a>Growth Function</h3><p>成长函数：有效线的数量，描述的是“<strong>最多</strong>”能产生的dichotomy种数。是有限的，以$2^N$为上界。</p><script type="math/tex; mode=display">m_\mathcal H(N)=\max_{x_1,x_2,\cdots,x_N\in\mathcal X}|\mathcal H(x_1,x_2,\cdots,x_N)|</script><p>在确定的情况下，growth function是一个与N相关的函数。以下是几种常见的Hypothesis Set的成长函数。</p><ol><li><p>Positive Rays</p><p><img src="https://beader.me/mlnotebook/section2/images/postive_rays.png" alt="img"></p><p>输入空间为一维实数空间。大于threshold a的预测+1，否则预测-1。</p><p>当N=4时，共能产生5个不同的dichotomies。如下图：</p><p><img src="https://beader.me/mlnotebook/section2/images/positive_rays_dichotomies.png" alt="img"></p><p>成长函数为</p><script type="math/tex; mode=display">m_\mathcal H(N)=N+1</script></li><li><p>Positive Intervals</p><p><img src="https://beader.me/mlnotebook/section2/images/positive_intervals.png" alt="img"></p><p>当N=4时，共能产生11种dichotomies。</p><p><img src="https://beader.me/mlnotebook/section2/images/positive_intervals_dichotomies.png" alt="img"></p><p>成长函数为</p><script type="math/tex; mode=display">m_\mathcal H(N)=C_{N+1}^{2}+1=\frac{1}{2}N^2+\frac{1}{2}N+1</script></li><li><p>Convex Sets</p><p><img src="https://beader.me/mlnotebook/section2/images/convex_sets.png" alt="img"></p><p>任选k个点，在这k个点组成的convex多边形包围内的所有点都预测+1，否则预测-1。这N个点的任意一种排列组合都能成为一个dichotomy。因此Convex Sets的成长函数为：</p><script type="math/tex; mode=display">m_\mathcal H(N)=2^N</script><p>当作用于有N个inputs时，产生的dichotomies数量等于这N个点的排列组合数时（也就是上述这种情况），我们就称这N个inputs被shatter掉了，或者说产生的dichotomies把这N个点的排列组合给shatter了。</p><blockquote><p>shatter 的原意是「打碎」，在此指「N個點的所有(碎片般的)可能情形都被產生了」。</p><p>从打游戏过关的角度去理解：“是把散弹枪，在每个关卡(level N)中，他可以有发小子弹（每发小子弹对应一种dichotomy），而你面临的是个敌人。你得一枪打出去shatter掉所有人。对于这把散弹枪来说，第一关和第二关都还好，第三关6发小子弹shatter不掉8个人，于是它就break了。”</p></blockquote></li></ol><h3 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h3><p>第一个小于$2^N$的点以及之后的点均是break point。令$k$为最小的那个break point。</p><ol><li>Positive Rays成长函数的break point为2。</li><li>Positive Intervals成长函数的break point为3。</li><li>Convex Sets不管N多大都可以去shatter掉那N个点，因此它的成长函数没有break point。</li><li>2D Perceptrons的break point为4，当N=4时，它不能够shatter，最多只能产生14种dichotomies（而非16种），因此2D Perceptrons成长函数的break point为4。</li></ol><p>因此结合break point和成长函数，猜测：</p><ul><li>没有break point：$m_\mathcal H(N)=2^N$，肯定正确</li><li>k(min break point)：$m_\mathcal H(N)=O(N^{k-1})$，可能是多项式而非指数！（后验证猜想是正确的）</li></ul><h3 id="Restriction-of-Break-Point"><a href="#Restriction-of-Break-Point" class="headerlink" title="Restriction of Break Point"></a>Restriction of Break Point</h3><ul><li>限制：有些成长函数很容易找到，比如前面说到的Positive Rays、Positive Intervals以及Convex Sets；有些则没有那么容易，比如2D perceptrons，我们无法直接看出它的成长函数是什么。</li><li><p>考虑：能否利用break point得到成长函数的upper bound呢？</p></li><li><p>先用例子来看看，当我们完全不知道是什么，只知道它的break point k时，作用于“<strong>最多最多</strong>”可以产生多少个dichotomies。注意这里我用了两个“<strong>最多</strong>”，由于我们无法确切知道成长函数，因此我们用这个break point推算出的这个dichotomies的数量仍然是个高估值，这个高估值实际上是任何break point为k的作用于所真实产生的dichotomies数量的上界 (upper bound)。</p></li><li><p>举例说明，假设我们不知道某个的成长函数，但知道它的break point k=2，那么作用于N=3的时，“<strong>最多最多</strong>”能产生多少种dichotomies？</p><ul><li><p>从k=2我们可以知道，任意2个数据点都不能被shatter，意思就是我产生的dichotomies不能完全包含任何2个数据点所有的排列组合。</p><p><img src="https://beader.me/mlnotebook/section2/images/n3k2d4s.png" alt="img"></p><p>上图右边两列被shatter了。k=2，即任意2个点不能被shatter，因此不可能产生这4种dichotomies。那我们换一个dichotomy试试看。</p><p><img src="https://beader.me/mlnotebook/section2/images/n3k2d4.png" alt="img"></p><p>换了一个dichotomy之后就行了，检查任意的两个点都没有被shatter，看来这4种dichotomies是可以的。</p></li><li><p>5个dichotomies的情形这里就不再画出来了，很容易看出不管增加怎样的dichotomy进去，都会有两个点被shatter掉。因此这里“<strong>最多最多</strong>”只能有4种dichotomies。因此，k=2时的upper bound是4。</p></li></ul></li><li><p>我们用Bounding Function $B(N,k)$来表示：break point为$k$时，任意的作用于size为$N$时，所能产生的dichotomies的数量的上限（“<strong>最多最多</strong>”）。比如刚刚得出的结论可以表示为，当$k=2,N=3$时，$B(N,k)=B(3,2)=4$。</p></li><li><p>从以上可以看出，虽然很多时候我们无法直接得到成长函数，但如果我们知道它的break point是多少，我们似乎还是有办法算出这个的上界的。于是乎我们就有了新的目标，不去直接研究成长函数，转而去研究$B(N,k)$。</p></li></ul><h3 id="Bounding-Function"><a href="#Bounding-Function" class="headerlink" title="Bounding Function"></a>Bounding Function</h3><p>考察$B(N,k)$的取值情况：</p><ul><li>$k=1$时，1个点（2种排列组合）都没有办法shatter，因此恒等于1。</li><li>$k&gt;N$时，一定能shatter掉$2^N$个点，此时无任何条件限制。</li><li>$k=N$时，从所有的排列组合中移除掉一个（否则会shatter掉），剩下的都可以作为dichotomies，因此它产生的dichotomies的数量“<strong>最多最多</strong>”可以是$2^N-1$。</li></ul><p>  因此我们可以得到下表：</p><p><img src="https://beader.me/mlnotebook/section2/images/bnk_table.png" alt="img"></p><p>如何求取其他部分？</p><ul><li><p>以$B(4,3)$为例，采用穷举法得到所有可能的dichotomies：</p><p><img src="https://beader.me/mlnotebook/section2/images/n4k3d11.png" alt="img"></p><p>整理一下后得到：</p><p><img src="https://beader.me/mlnotebook/section2/images/n4k3d11_ordered2.png" alt="img"></p><p>其中橙色的部分是成对的，紫色的部分是单个出现的。</p><p>去掉$x_4$并去重，可以得到：</p><p><img src="https://beader.me/mlnotebook/section2/images/n4k3d7.png" alt="img"></p><p>$\alpha+\beta$部分可以成为这3个点的dichotomies，因为对于$B(4,3)$任3个点不能够被shatter，所以$\alpha+\beta$部分这3个点也不能够被shatter，从而有</p><script type="math/tex; mode=display">\alpha+\beta\le B(3,3)</script><p>再单独观察$\alpha$部分：</p><p><img src="https://beader.me/mlnotebook/section2/images/n4k2_alpha.png" alt="img"></p><p>因为对于$B(4,3)$任3个点不能够被shatter，所以$\alpha$部分这3个点中任2个点也不能够被shatter，从而有</p><script type="math/tex; mode=display">\alpha\le B(3,2)</script></li></ul><p>  结合以上两个结论，可以得出</p><script type="math/tex; mode=display">  B(4,3)=2\alpha+\beta\le B(3,3)+B(3,2)</script><p>  这样就能够把前面那张表给填完整：</p><p>  <img src="https://beader.me/mlnotebook/section2/images/bnk_table_full.png" alt="img"></p><p>  从而得到了Bounding Function的upper bound</p><ul><li><p>可以推广上述结论，得到</p><script type="math/tex; mode=display">B(N,k)\le B(N-1,k)+B(N-1,k-1)</script><p>（可以证明上述不等号其实可以取等号）</p><blockquote><p>思路可以归纳为：</p><p>成长函数（Growth Function）&lt;上限函数（Bounding Function）&lt;上限的上限（Bounding Function的upper bound）（最高次项为$N^{k-1}$的多项式）</p><p>从而证明霍夫丁不等式中的M是有上限的。</p></blockquote></li><li><p>思路总结：上一篇说的learning的可行性，讲到：如果遇上bad sample，与就会差很多，此时learning不可行。遇到bad sample的概率与中方程的数量以及中的数据量有关。然而中方程的数量往往是无穷的（比如2D Perceptrons的是平面上所有的直线），本篇则继续阐述，方程的数量看上去是无穷的，但真正有效(effective)的方程的数量却是有限的，我们可以用成长函数来描述作用于会产生多少种有效的方程。但实际上我们很难确切知道各种的成长函数究竟长什么样子，我们只好通过break point去寻找成长函数的upper bound。</p></li></ul><h3 id="VC-Bound"><a href="#VC-Bound" class="headerlink" title="VC Bound"></a>VC Bound</h3><p>我们设想利用有限的$m_{\mathcal{H}}(N)$来替换无限的大$M$，得到$\mathcal{H}$遇到Bad Sample的概率上界：</p><script type="math/tex; mode=display">\mathbb{P}_\mathcal{D}[BAD\ D]=\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \epsilon]\leq 2m_{\mathcal{H}}(N)\cdot exp(-2\epsilon ^2N)</script><p>其中$\mathbb{P}_\mathcal{D}[BAD\ D]$是$\mathcal{H}$中所有有效的方程(Effective Hypotheses)遇到Bad Sample的联合概率，即$\mathcal{H}$中存在一个方程遇上bad sample，则说$\mathcal{H}$遇上bad sample。</p><p>但事实上上面的不等式是不严谨的，为什么呢？</p><p>$m_{\mathcal{H}}(N)$描述的是$\mathcal{H}$作用于数据量为$N$的资料$\mathcal{D}$，有效的方程数，因此$\mathcal{H}$当中每一个$h$作用于$\mathcal{D}$都能算出一个$E_{in}$来，$E_{in}$的可能取值是一个有限的数。</p><p>但在out of sample的世界里(总体)，往往存在无限多个点，平面中任意一条直线，随便转一转就能产生一个不同的$E_{out}$来。<strong>$E_{in}$的可能取值是有限个的，而$E_{out}$的可能取值是无限的，无法直接套用union bound，我们得先把上面那个无限多种可能的$E_{out}$换掉。</strong>那么如何把$E_{out}$变成有限个呢？</p><ul><li>Step 1: 用$E_{in}^{’}$替换$E_{out}$</li></ul><p>假设我们能从总体当中再获得一份$N$个样本的验证资料(verification set)$\mathcal{D}’$，对于任何一个$h$我们可以算出它作用于$\mathcal{D}’$上的$E_{in}^{’}$。<strong>由于$\mathcal{D}’$也是总体的一个样本，因此如果$E_{in}$和$E_{out}$离很远，有非常大的可能$E_{in}$和$E_{in}^{’}$也会离得比较远。</strong></p><p><img src="https://beader.me/imgs/vc-dimension-two/pdf_of_ein.png" alt="img"></p><p>事实上，当$N$很大的时候，$E_{in}$和$E_{in}^{’}$可以看做服从以$E_{out}$为中心的近似正态分布(Gaussian)，如上图。</p><p>$[|E_{in}-E_{out}|\text{ is large}]$这个事件取决于$\mathcal{D}$，如果$[|E_{in}-E_{out}|\text{ is large}]$，则如果我们从总体中再抽一份$\mathcal{D}^{‘}$出来，有50%左右的可能性会发生$[|E_{in}-E_{in}^{‘}|\text{ is large}]$，还有大约50%的可能$[|E_{in}-E_{in}^{’}|\text{ is not large}]$。</p><p>从而得到下式（没有进行严格的推导）：</p><script type="math/tex; mode=display">\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 2\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]</script><p>这样一来我们就把无限多种的$E_{out}$换成了有限多种的$E_{in}^{‘}$。</p><ul><li>Step 2：重新构成$\mathcal{H}$</li></ul><p>因为$\mathcal{D}$与$\mathcal{D}^{’}$的大小相等，都为$N$，因此我们手中一共有$2N$笔数据，这样$\mathcal{H}$作用于$\mathcal{D}+\mathcal{D}^{’}$最多能产生$m_{\mathcal{H}}(2N)$种dichotomies，此时又可以使用union bound了。</p><p><img src="https://i.loli.net/2020/03/13/d8VGeU7pDJL3tsn.png" alt="微信截图_20200313124748.png" style="zoom:67%;"></p><p>用固定的$h$来看$E_{in}$和$E_{in}^{’}$之间的差别：</p><script type="math/tex; mode=display">2\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}] \le 2m_{\mathcal{H}}(2N) \mathbb{P}[\text{fixed h s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]</script><ul><li>Step 3：使用Hoeffding inequality</li></ul><p>前面的动作相当于先从总体中抽出$2N$笔数据，把这$2N$笔数据当成一个比较小的bin，然后在这个bin中抽取$N$笔作为$\mathcal{D}$，剩下的$N$笔作为$\mathcal{D}^{’}$，$\mathcal{D}$和$\mathcal{D}^{’}$之间是没有交集的。在我们想象出来的这个small bin当中，整个bin的错误率为$\frac{E_{in}+E_{out}}{2}$，又因为：</p><script type="math/tex; mode=display">|E_{in}-E_{in}^{'}|\gt \frac{\epsilon}{2} \Leftrightarrow |E_{in} - \frac{E_{in}+E_{in}^{'}}{2}|\gt \frac{\epsilon}{4}</script><p>所以不等式就又可以使用Hoeffding inequality了：</p><script type="math/tex; mode=display">2m_{\mathcal{H}}(2N) \mathbb{P}[\text{fixed h s.t. } |E_{in}(h)-E_{in}^{'}(h)|\gt \frac{\varepsilon}{2}]\le2m_{\mathcal{H}}(2N)·2\exp(-2(\frac{\epsilon}{4})^2N)</script><p>最终整理得到：</p><script type="math/tex; mode=display">\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4m_{\mathcal{H}}(2N)\cdot \exp(-\frac{1}{8}\varepsilon ^2N)</script><p>上式右侧千辛万苦得出来的这个bound就叫做VC Bound。</p><h3 id="VC-Dimension-1"><a href="#VC-Dimension-1" class="headerlink" title="VC Dimension"></a>VC Dimension</h3><p>VC Bound所描述的是在给定数据量N以及给定的Hypothesis Set的条件下，遇到坏事情的概率的上界，即$E_{in}$与$E_{out}$差很远的概率，最多是多少。</p><p>因为寻找所有Hypothesis Set的成长函数是困难的，因此我们再利用$N^{d_{vc}}$来bound住所有VC Dimension为$d_{vc}$的Hypothesis Set的成长函数。所以对于任意一个从$\mathcal{H}$中的$g$来说，如果$k$存在，有：</p><script type="math/tex; mode=display">\mathbb{P}[\exists h \in \mathcal{H}\text{ s.t. } |E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4(2N)^{k-1}\cdot \exp(-\frac{1}{8}\varepsilon ^2N)</script><p>因此说想让机器真正学到东西，并且学得好，有三个条件：</p><ol><li>$\mathcal{H}$的$d_{vc}$是有限的，即存在break point $k$，这样VC Bound才存在。(good $\mathcal{H}$)</li><li>$N$足够大(对于特定的$d_{vc}$而言)，这样才能保证上面不等式的bound不会太大。(good $\mathcal{D}$)</li><li>算法$\mathcal{A}$有办法在$\mathcal{H}$中顺利地挑选一个使得$E_{in}$最小的方程$g$。(good $\mathcal{A}$)</li></ol><blockquote><p>为什么要费那么大的力气来讲这个VC Bound和VC Dimension呢？因为对于初学者来说，最常犯的错误就是只考虑到了第3点，而忽略掉了前两点，往往能在training set上得到极好的表现，但是在test set中表现却很烂。关于算法$\mathcal{A}$的部分会在后续的笔记当中整理，目前我们只关心前面两点。</p></blockquote><p>假设空间$\mathcal H$的VC Dimension记为$d_{VC}(\mathcal H)$，是这个假设空间最多能够shatter掉的点的数量。</p><ul><li>如果该假设空间中存在$N$个点能够shatter他们，则$d_{VC}(\mathcal H)\ge N$。</li><li>如果该假设空间中任意$N$个点都不能够shatter他们，则$d_{VC}(\mathcal H)&lt; N$。</li><li>不难看出与break point k的关系，有$k=d_{vc}+1$</li></ul><p>因此我们用下式来描述成长函数的上界（$N\ge 2,d_{VC}\ge 2）$：</p><script type="math/tex; mode=display">m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i}</script><p>上式右边事实上是最高项为$d_{vc}$的多项式，利用数学归纳法可得：</p><script type="math/tex; mode=display">m_{\mathcal{H}}(N)\leq \sum_{i=0}^{d_{vc}} \binom {N}{i} \leq N^{d_{vc}}+1</script><p>对于以下几个$\mathcal{H}$，由于之前我们已经知道了他们的成长函数，因此可以根据$m_{\mathcal{H}}(N)\leq N^{d_{vc}}$，直接得到他们的VC Dimension：</p><ul><li>positive rays: $m_{\mathcal{H}}(N)=N+1$，看N的最高次项的次数，知道$d_{vc}=1$</li><li>positive intervals: $m_{\mathcal{H}}(N)=\frac{1}{2}N^2+\frac{1}{2}N+1$，$d_{vc}=2$</li><li>convex sets: $m_{\mathcal{H}}(N)=2^N$，$d_{vc}=\infty$</li><li>2D Perceptrons: $m_{\mathcal{H}}(N)\leq N^3\;for\;N\geq 2$，所以$d_{vc}=3$</li></ul><p>由于convex sets的$d_{vc}=\infty$，不满足上面所说的第1个条件，因此不能用convex sets这个$\mathcal{H}$来学习（不是好的假设空间）。</p><p>但这里要回归本意，通过成长函数来求得$d_{vc}$没有太大的意义，引入$d_{vc}$很大的一部分原因是，我们想要得到某个Hypothesis Set的成长函数是困难的，希望用$N^{d_{vc}}$来bound住对应的$m_{\mathcal{H}}(N)$。对于陌生的$\mathcal{H}$，如何求解它的$d_{vc}$呢？下面用自由度的角度进行解释。</p><p>对于比较简单的模型，可以从它最多能够shatter的点的数量，得到$d_{vc}$，但对于一些较为复杂的模型，寻找能够shatter掉的点的数量，就不太容易了。此时我们可以通过模型的自由度，来近似的得到模型的$d_{vc}$。</p><p>定义自由度是：<strong>模型当中可以自由变动的参数的个数</strong>，即我们的机器需要通过学习来决定模型参数的个数。</p><p>譬如：</p><ul><li>Positive Rays，需要确定1个threshold，这个threshold就是机器需要根据$\mathcal{D}$来确定的一个参数，则Positive Rays中自由的参数个数为1，$d_{vc}=1$</li><li>Positive Intervals，需要确定左右2个thresholds，则可以由机器自由决定的参数的个数为2，$d_{vc}=2$</li><li>d-D Perceptrons，$d$维的感知机，可以由机器通过学习自由决定的参数的个数为$d+1$（别忘了还有个$w_0$），$d_{vc}=d+1$</li></ul><h3 id="Model-Complexity"><a href="#Model-Complexity" class="headerlink" title="Model Complexity"></a>Model Complexity</h3><p>learning的问题应该关注的两个最重要的问题是：</p><ol><li>能不能使$E_{in}$与$E_{out}$很接近；</li><li>能不能使$E_{in}$足够小。</li></ol><ul><li>对于相同的$\mathcal{D}$而言，$d_{vc}$小的模型，其VC Bound比较小，比较容易保证$E_{in}$与$E_{out}$很接近，但较难做到小的$E_{in}$。试想，对于2D Perceptron，如果规定它一定要过原点($d_{vc}=2$)，则它就比没有规定要过原点($d_{vc}=3$)的直线更难实现小的$E_{in}$，因为可选的方程更少。2维平面的直线，就比双曲线($d_{vc}=6$)，更难实现小的$E_{in}$。</li><li>对于相同的$\mathcal{D}$而言，$d_{vc}$大的模型，比较容易实现小的$E_{in}$，但是其VC Bound就会很大，很难保证模型对$\mathcal{D}$之外的世界也能有同样强的预测能力。</li></ul><p>令之前得到的VC Bound为$\delta$，坏事情$[|E_{in}(g)-E_{out}(g)|\gt \varepsilon]$发生的概率小于$\delta$，则好事情$[|E_{in}(g)-E_{out}(g)|\leq \varepsilon]$发生的概率就大于$1-\delta$，这个$1-\delta$在统计学中又被称为置信度，或置信水平。</p><script type="math/tex; mode=display">\mathbb{P}[|E_{in}(h)-E_{out}(h)|\gt \varepsilon]\leq 4(2N)^{d_{vc}}\cdot \exp(-\frac{1}{8}\varepsilon ^2N)=\delta</script><p>因此$E_{in}$、$E_{out}$又有下面的关系：</p><script type="math/tex; mode=display">|E_{in}(h)-E_{out}(h)|\le\sqrt{\frac{8}{N}\ln(\frac{4(2N)^{d_{vc}}}{\delta})}</script><p>令$\Omega (N,\mathcal{H},\delta)=\sqrt{…}$，即上式的根号项为来自模型复杂度的，模型越复杂，$E_{in}$与$E_{out}$离得越远。</p><p><img src="https://beader.me/imgs/vc-dimension-three/model_complexity_curve.png" alt="model_complexity_curve.png"></p><p>随着$d_{vc}$的上升，$E_{in}$不断降低，而$\Omega$项不断上升，他们的上升与下降的速度在每个阶段都是不同的，因此我们能够寻找一个二者兼顾的，比较合适的$d_{vc}^{*}$，用来决定应该使用多复杂的模型。</p><p>反过来，如果我们需要使用$d_{vc}=3$这种复杂程度的模型，并且想保证$\varepsilon = 0.1$，置信度$1-\delta =90\%$，我们也可以通过VC Bound来求得大致需要的数据量$N$。通过简单的计算可以得到理论上，我们需要$N\approx 10,000d_{vc}$笔数据。</p><p>但VC Bound事实上是一个极为宽松的bound，因为它对于任何演算法$\mathcal{A}$，任何分布的数据，任何目标函数$f$都成立，所以经验上，常常认为$N\approx 10d_{vc}$就可以有不错的结果。</p><h2 id="Noise-and-Error"><a href="#Noise-and-Error" class="headerlink" title="Noise and Error"></a>Noise and Error</h2><p>首先我们认为存在一个未知的真理$f$，认为 $\mathcal{D}$中的$y$就是$f$作用于$x$产生的，因此虽然无法直接得到$f$，但若能找到一个和$f$表现差不多的函数，也算是能学到东西。</p><p>但在现实世界中，我们拿到的$\mathcal{D}$并不是完美的，会有noise的存在。在Learning中，noise主要表现为以下几种形式：</p><p>在Learning中，noise主要表现为以下几种形式：</p><ul><li>noise in y : 本来应该是圈圈的，却被标记为叉叉</li><li>noise in y : 输入$\mathcal{X}$完全相同的点，既有被标记圈圈的，也有被标记叉叉的</li><li>noise in x : 输入$\mathcal{X}$本身就存在问题，譬如100被写成了100万</li></ul><p>$f$是一个“确定性”(deterministic)的模型，但$noise$是一个随机发生的东西，他们两个共同作用的结果，就成了一个“概率性”(probabilistic)的东西：</p><p>对于某个样本 $x$，理想状态下，应该有$y=f(x)=+1$，但由于某种noise的存在，该noise会有30%的概率会转换$f(x)$的结果(把+1变成-1或把-1变成+1)。因此在$\mathcal{D}$中，该样本有70%的概率表现出$y=+1$，30%的概率表现出。</p><h3 id="Error-Measure"><a href="#Error-Measure" class="headerlink" title="Error Measure"></a>Error Measure</h3><p>在把learning的工作交给机器的时候，必须让机器明白你学习的目标，譬如你想让什么什么最大化，或者什么什么最小化。通常的做法是把每一个预测值与真实值之间的误差(error)看成一种成本，机器要做的，就是在$\mathcal{H}$中，挑选一个能使总成本最低的函数。</p><p>之前一直说的$E_{in}(h)$，就是$h$作用于$\mathcal{D}$中每一笔数据，所产生的成本之和：</p><script type="math/tex; mode=display">E_{out}(g)=\varepsilon[[g(x)\neq f(x)]]</script><p>这种误差衡量方式称为”pointwise measure”，即对每个点记录误差，总误差为所有点产生的误差之和。</p><p>针对不同的问题与不同的使用环境，我们可以设计不同的误差衡量方法，下面是常见的pointwise误差的定义：</p><ul><li><p>0/1 error ，通常用于分类问题：</p><script type="math/tex; mode=display">err(\widetilde{y},y)=[\widetilde{y}\neq y]</script></li><li><p>squared error，通常用于回归问题：</p><script type="math/tex; mode=display">err(\widetilde{y},y)=(\widetilde{y}-y)^2</script></li><li><p>absolute error：</p><script type="math/tex; mode=display">err(\widetilde{y},y)=|\widetilde{y}-y|</script></li></ul><p>之前提到的二元分类问题，就是对判断错误的点，记误差为1，判断正确的点，记误差为0；不管是把$y=+1$的猜错成$-1$，或是把$y=-1$的猜错成$+1$，其产生的误差都为1。</p><p>但在实际应用中，这个误差的定义可以很灵活，例如下面指纹验证的例子：</p><ul><li><p>中情局的门禁系统，利用指纹判断是内部工作人员，才允许进入。这种情形下，若是把好人当坏人，代价并不高，无非就是请工作人员多按一次指纹的功夫，但如果把坏人当好人，损失可就大了。针对这种需求，下面这个error的衡量办法可能更加合理。</p><p><img src="https://i.loli.net/2020/03/13/InbO4kXmYZaxE7R.png" alt="微信截图_20200313175204.png" style="zoom:50%;"></p><p>其中权重1000可以视为：将$y=-1$时将数据复制了1000倍（过采样），从而也可以转化为等权重的问题</p><p>（随机地重复选取这些$y=-1$的样本1000次）</p></li></ul><p>总结一下，先根据问题的不同选择合适的误差衡量方式，0/1 error还是squared error或者是其他针对某一场景特殊设计的error？把$h$作用于$\mathcal{D}$中所有点的error加总起来就成了一个cost function，也就是$E_{in}(h)$，接着要设计一个最优化算法$\mathcal{A}$，它能够从$\mathcal{H}$中挑选出能够使$E_{in}$最小的方程$g$，learning就完成了。</p><p>对于不同类型的cost function，通常会使用不同的最优化算法。对于某些cost function，很容易实现$E_{in}$最小，但对于某些cost function，寻找最小的$E_{in}$是困难的，比如用0/1 error来衡量误差，要minimize $E_{in}$就是个NP Hard问题。可以将问题转化为求解闭式解、凸优化的问题。</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习基石》Note——Part1：When Can Machines Learn?</title>
      <link href="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part1/"/>
      <url>/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li><li><a href="https://beader.me/mlnotebook/index.html" target="_blank" rel="noopener">https://beader.me/mlnotebook/index.html</a></li><li><a href="https://me.csdn.net/github_36324732" target="_blank" rel="noopener">https://me.csdn.net/github_36324732</a></li><li><a href="https://zhuanlan.zhihu.com/ml-note" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/ml-note</a></li></ul></blockquote><h1 id="When-Can-Machines-Learn"><a href="#When-Can-Machines-Learn" class="headerlink" title="When Can Machines Learn?"></a>When Can Machines Learn?</h1><h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><p><strong>技巧</strong>：某种表现的增进（e.g. prediction accuarcy）</p><p><strong>机器学习</strong>：</p><ul><li>从数据出发，经过电脑的计算后，最终得到某种表现的增进（data→ML→improved performance measure）</li><li>一种构建复杂规则的路径，授机以渔(?)</li></ul><p><strong>机器学习条件</strong>：</p><ul><li>存在某些潜在的模式可以学习</li><li>有一定的难以具体言明的规则（not easily programmable）</li><li>一定的数据资料</li></ul><p><strong>机器学习的组成部分</strong>：</p><ul><li>输入： $x\in X$</li><li>输出： $y\in Y$</li><li>目标函数 $f$：$X\rightarrow Y$（未知的目标函数）</li><li>数据，即训练样本， $D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\}$</li><li>假设函数 $g$： $X\rightarrow Y$ （能够学习到的函数），$g$与$f$越接近越好</li><li><img src="https://www.zhihu.com/equation?tex=g%5Cin+H+%3D+%5Cleft%5C%7B+h_1%2Ch_2...h_n+%5Cright%5C%7D" alt="[公式]"> ，是属于假设空间$H$的一个假设，$H$称为假设函数集，包含了好与不好的各种$h$（假设），反映数据中可能存在的规律，从输入到输出的函数</li></ul><p><strong>机器学习 v.s. 数据挖掘 v.s. 人工智能 v.s. 统计</strong></p><ul><li>机器学习的目的是从数据资料出发，假设空间中选出一个最接近目标函数$f$的函数$g$，使得$g\approx f$</li><li>数据挖掘：用数据找到隐含在资料中的有用信息，与机器学习密不可分，相互帮助（但有时候并不是以找到函数$g$为目标）</li><li>人工智能：机器学习是实现人工智能的一种方法</li><li>统计：使用数据来进行推论，$g$就是一个推论结果，统计是实现机器学习的一种方法；由于统计是从数学出发的，统计会更注重数学假设，而非计算。</li></ul><h2 id="感知机算法"><a href="#感知机算法" class="headerlink" title="感知机算法"></a>感知机算法</h2><p>Perceptron Learning Algorithm（PLA）</p><p>首先要保证在这些得到的数据中，假设函数可以预测的很好</p><p>根据预测与实际结果，不断地去修正决策平面</p><script type="math/tex; mode=display">sign(w^T_tx_n)\neq y_n \\w_{t+1}\leftarrow w_t+y_{n(t)}x_{n(t)}</script><p><img src="https://i.loli.net/2020/03/09/wLcn5qdYjgOCkl9.png" alt="未命名1583747872.png"></p><p>$w$是决策平面的法向量</p><p>当发生错误时，比如，当实际标签为正$y=+1$而预测为负，则决策平面会更接近$x$；当实际标签为负$y=-1$而预测为正，则决策平面会更远离$x$。</p><p>不断更新决策平面，直到不犯错误，最终得到的$w_{PLA}$是所要求的$g$。</p><p>因此又称PLA为“知错能改算法”。</p><blockquote><p>感知器学习算法对$w$的更新方法，首先$w$和$x$的内积就表示两个向量的夹角大小，假如$x$的类别是+1，那么内积也是正的，如果初始的$w_0$使得这个值为负，那么代表$w_0$和$x$之间的夹角是钝角，太大了，就旋转$w_0$到$w_1$， 使得内积的值是正的，这就对$w_0$完成了一次更新</p></blockquote><p>终止条件：线性可分(linear separable)，即存在一个向量$w_f$使得能够完美地区分开正负样本</p><p>由于函数间隔大于0（均可完全区分，且最近点离平面的也有一定的距离）：</p><script type="math/tex; mode=display">y_{n(t)}w_f^Tx_{n(t)}\ge \min_n y_{n}w_f^Tx_{n}>0</script><p>因此可以推导</p><script type="math/tex; mode=display">\begin{align}w^T_fw_{t+1}&=w^T_f(w_{t}+y_{n(t)}x_{n(t)})\\&\ge w^T_fw_{t}+\min_n y_{n}w_f^Tx_{n}\\&> w^T_fw_{t} +0\end{align}</script><p>可以看出随着$w_t$的更新，是在不断接近$w_f$</p><p>（衡量向量接近的一个方法是内积，越大越接近）</p><h2 id="机器学习的分类"><a href="#机器学习的分类" class="headerlink" title="机器学习的分类"></a>机器学习的分类</h2><p>根据输出类型可以划分为：</p><ul><li>分类（二元，多元）</li><li>回归</li><li>结构化学习（如标识词性，NLP）</li></ul><p>根据是否具有标签可分为：</p><ul><li>有监督</li><li>无监督（聚类、密度分析、异常值检测）</li><li>半监督（有标注+无标注混合）</li><li>强化学习（通过对错误进行惩罚/对正确进行奖励，从而实现对序列化的信息进行学习）</li></ul><p>根据训练的形式可分为：</p><ul><li>批学习（batch）：一批样本一起学习，填鸭式’duck feeding’</li><li>在线学习（online）：一个样本学习一次，一条一条地</li><li>主动学习（active）：让机器有问问题的能力</li></ul><p>根据输入空间（特征）的类别可分为：</p><ul><li>具体的（concrete）</li><li>原始的（raw）：通常需要人或机器去将抽象转化为具体</li><li>抽象的（abstract）：通常也需要进行特征转换、提取等</li></ul><h2 id="机器学习可行性论证"><a href="#机器学习可行性论证" class="headerlink" title="机器学习可行性论证"></a>机器学习可行性论证</h2><blockquote><p><strong>第一部分：证明适用于训练集的$g$同样适用于整个输入空间</strong></p></blockquote><ul><li><strong>No Free Lunch（没有免费午餐定理）</strong>：即使对于已有的样本算法完全能够预测正确，但对于未知的样本来说并不能做到。</li><li><p>这表明：<strong>学习可能是做不到的</strong>，在训练集中可以求得一个最佳假设$g$，但是在训练集外的样本中，$g$可能和目标函数$f$相差甚远。</p></li><li><p>引入<strong>Hoeffding’s Inequality（霍夫丁不等式）</strong></p><script type="math/tex; mode=display">P(|\nu-\mu|> \varepsilon) \le 2\exp(-2\varepsilon^2N)</script><ul><li>$|\nu-\mu|$表示$\nu$与$\mu$的接近程度</li><li>随着样本量$N$增大，$\nu$与$\mu$相差较大的概率不断变小，<strong>$\nu$与$\mu$相等的结论“可能近似正确”（Probably Approximately Correct，PAC）</strong></li><li>用$E_{in}(h)$和$E_{out}(h)$表示一个确定的假设函数$h$在样本内的错误率和样本外的错误率，则有： $P(|E_{in}(h)-E_{out}(h)|&gt; \varepsilon) \le 2\exp(-2\varepsilon^2N)$</li><li>如果求得一个$h$，使得$E_{in}(h)$很小，且$E_{in}(h)\approx E_{out}(h)$，则可以推出$E_{out}(h)$很小，则可以推出$h\approx f$ </li></ul></li></ul><p><strong>通过大数定理由样本估计总体，可以增加样本数量使得估计与真实更加接近。证明了在一个$h$的情况下，当样本的数量足够多，该$h$在样本集上的表现可以代表它与真实$f$的接近程度。</strong></p><ul><li><p>但$h$只是假设空间的一种，不能由$h\approx f$得到$g\approx f$。（#不能由部分可行来推断总体可行）</p></li><li><p>因此算法要能够自由的从中挑选$h$，这里就需要添加一个<strong>验证流程(Verification Flow)</strong>，这个流程使用历史数据来判断某个够不够好。选出一个最小的$E_{in}(h)$，把挑选出的最好的称为final hypothesis，从而得到$g\approx f$。</p><p><img src="https://beader.me/mlnotebook/section2/images/verification_flow.png" alt="img"></p></li></ul><p><strong>以上证明了机器学习在训练样本中学习得到的$g$在整个输入空间中也可行。</strong></p><blockquote><p><strong>第二部分：证明训练样本的误差不会导致选择到的$g$对整个输入空间有误差</strong></p></blockquote><ul><li><p>当$h$有多个时，要从中挑选最好的$h$，但验证所用的数据只是来自于总体的一个样本 (sample)，会存在抽样误差。譬如你想知道一枚硬币抛出正面的概率是多少，于是扔了5次，有一定的可能你连续扔了5个正面出来，这时候说抛出正面的概率是1，这当然是行不通的，因此你扔的这5次硬币，就是一个<strong>bad sample</strong>。</p></li><li><p>凡是由于抽样误差所造成样本分布与总体分布相差很大的样本，$E_{in}(h)$和$E_{out}(h)$差很远，我们都可以称之为<strong>bad sample</strong>。</p></li><li><p>由于数据不完美有噪声，在假设$h$有很多个的情况下，会倾向于选择让$E_{in}(h)$最小的那个$h$，导致恶化。</p></li><li><p>但当数据量足够多时，肯定能有数据使得机器学习$g$时不踩雷，即在每个$h$上$E_{in}(h)\approx E_{out}(h)$，能正确学习到g。（例如样本$D_{1126}$）</p><p><img src="https://pic4.zhimg.com/80/v2-4d9f3d54a18a3edefecff9f3f47d3437_1440w.jpg" alt="img"></p><ul><li>上图包含了$M$个假设$h$，而不好的$D$不是由单一假设就能确定的，而是只要有一个假设在此抽样$D$上表现不好则该抽样被标记为不好的。求训练样本不好的几率是多少：</li></ul><p><img src="https://pic4.zhimg.com/80/v2-574d528dd3031280c049da6a3000b0df_1440w.jpg" alt="img"></p><ul><li>当训练样本量$N$足够大且$M$有限大，则概率可以小于一个足够小的数字，可以说每一个假设$h$都是安全的，因此肯定会存在一个$D_{1126}$这样的样本。</li></ul></li><li><p>所以如果通过机器学习找出了$g$满足<script type="math/tex">E_{in}(g)\approx 0</script> ，则PAC规则可以保证$E_{out}(g)\approx 0$</p></li><li><p>此时，我们就能把选到的最小$E_{in}(h)$的$h$当做$g$，那么$E_{out}(h)\approx 0$，从而达到了学习的效果。</p></li></ul><p>此处留坑：这里假定$M$有限大，但如果像感知机一样$M=\infin$呢？</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>林轩田《机器学习基石》Note——Part3：How Can Machines Learn?</title>
      <link href="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/"/>
      <url>/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>课程：</p><ul><li><a href="https://www.bilibili.com/video/av12463015" target="_blank" rel="noopener">https://www.bilibili.com/video/av12463015</a></li></ul><p>参考笔记：</p><ul><li><a href="http://redstonewill.com/" target="_blank" rel="noopener">http://redstonewill.com/</a></li><li><a href="https://beader.me/mlnotebook/index.html" target="_blank" rel="noopener">https://beader.me/mlnotebook/index.html</a></li><li><a href="https://me.csdn.net/github_36324732" target="_blank" rel="noopener">https://me.csdn.net/github_36324732</a></li><li><a href="https://zhuanlan.zhihu.com/ml-note" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/ml-note</a></li></ul></blockquote><h1 id="How-Can-Machines-Learn"><a href="#How-Can-Machines-Learn" class="headerlink" title="How Can Machines Learn?"></a>How Can Machines Learn?</h1><p>接下来要讲的就是各种error measurement的区别以及针对它们如何设计最优化的算法。通过设计出来的算法，使得机器能够从$\mathcal{H}$(Hypothesis Set)当中挑选可以使得cost function最小的$h$作为$g$输出。</p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>本篇以线性回归为例，从方程的形式、误差的衡量方式、如何最小化$E_{in}$的角度出发，并简单分析了Hat Matrix的性质与几何意义。</p><h3 id="方程的形式"><a href="#方程的形式" class="headerlink" title="方程的形式"></a>方程的形式</h3><script type="math/tex; mode=display">h(x)=w^Tx</script><p>和perceptron相差一个sign（perceptron是$h(x)=sign(w^Tx)$）。</p><h3 id="误差的衡量"><a href="#误差的衡量" class="headerlink" title="误差的衡量"></a>误差的衡量</h3><p>平方误差(squared error)：</p><script type="math/tex; mode=display">err(\hat{y},y)=(\hat{y}-y)^2</script><h3 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h3><script type="math/tex; mode=display">E_{in}(w)=\frac{1}{N}\sum_{n=1}^{N}(h(x_n)-y_n)^2</script><p>$h(x)$是一个以$x$为变量的方程，而$E_{in}(w)$变成了一个以$w$为变量的方程。这样一来，我们就把“在$\mathcal{H}$中寻找能使平均误差最小的方程”这个问题，转换为“求解一个函数的最小值”的问题。使得$E_{in}(w)$最小的$w$，就是我们要寻找的那个最优方程的参数。</p><h3 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h3><p>用矩阵形式表示：</p><p>$X$与$y$来源于$\mathcal{D}$，是固定不变的，因此它是一个以$w$为变量的函数。我们需要解使得$E_{in}$最小的$w$，即</p><script type="math/tex; mode=display">\underset{w}{min}\,E_{in}(w)=\frac{1}{N}\begin{Vmatrix}Xw-y\end{Vmatrix}^2</script><p>$E_{in}(w)$是一个连续(continuous)、处处可微(differentiable)的凸函数(convex)：</p><p><img src="https://beader.me/imgs/linear-regression/wlin.png" alt="img"></p><p>对于这一类函数，只需要解其一阶导数为0时的解即可。</p><p>令$\nabla E_{in}(w)=0$，可得最佳解：</p><script type="math/tex; mode=display">w_{LIN}=(X^TX)^{-1}X^Ty</script><ul><li>当$X^TX$可逆的时候，记$(X^TX)^{-1}X^T$为pseudo-inverse（伪逆）矩阵$X^{\dagger}$</li><li>当$X^TX$不可逆的时候，再用其他方式定义$X^{\dagger}$（不展开）</li></ul><h3 id="帽子矩阵"><a href="#帽子矩阵" class="headerlink" title="帽子矩阵"></a>帽子矩阵</h3><p>用以$w_{LIN}$为参数的线性方程对原始数据做预测，可以得到拟合值$\hat{y}=Xw_{LIN}=XX^{\dagger}y$。这里称</p><script type="math/tex; mode=display">H=XX^{\dagger}=X(X^TX)^{-1}X^T</script><p>为Hat Matrix（帽子矩阵）。$H$为$y$带上了帽子，成为$\hat{y}$。</p><h4 id="H-的几何含义"><a href="#H-的几何含义" class="headerlink" title="$H$的几何含义"></a>$H$的几何含义</h4><p><img src="https://beader.me/imgs/linear-regression/geoview_hatmatrix.png" alt="img"></p><p>这张图展示的是在$N$维实数空间$\mathbb{R}^N$中，注意这里是$N$为样本量，$y$为真实值，$\hat{y}$为预测值。$X$中包含$d+1$个column。</p><ul><li>$\hat{y}=Xw_{LIN}$是$X$的一个线性组合，$X$中每个column对应$\mathbb{R}^N$下的一个向量，共有$d+1$个这样的向量，因此$\hat{y}$在这$d+1$个向量所构成的$span$(平面)上。</li><li>事实上我们要做的就是在这个平面上找到一个向量$\hat{y}$使得他与真实值之间的距离$|y-\hat{y}|$最短。不难发现当$\hat{y}$是$\color{purple}{y}$在这个平面上的投影时，即$y-\hat{y}\perp span$时，$|y-\hat{y}|$最短。</li><li><strong>因此Hat Matrix $H$为$y$戴上帽子转化为$\hat{y}$，所做的就是投影这个动作，寻找$span$上$y$的投影。</strong></li><li>$Hy=\hat{y}$，$(I-H)y=y-\hat{y}$。($I$为单位矩阵)</li></ul><h4 id="H-的性质"><a href="#H-的性质" class="headerlink" title="$H$的性质"></a>$H$的性质</h4><ul><li><p>对称性(symetric)，即$H=H^T$</p></li><li><p>幂等性(idempotent)，即$H^2=H$</p></li><li><p>半正定(positive semi-definite)，即所有特征值为非负数：0或1</p></li><li><p>$trace(I-H) = N-(d+1)$。$trace$为矩阵的迹，一个矩阵的$trace$等于该矩阵的所有特征值(Eigenvalues)之和。$X$的维度为$d+1$维。用物理意义来解释这个式子：</p><p><img src="https://beader.me/imgs/linear-regression/geoview_hatmatrix_noise.png" alt="geoview_hatmatrix_noise.png"></p><p>假设$y$由$f(X)\in span+noise$构成的。有$y=f(X)+noise$。</p><p>之前讲到$H$作用于某个向量，会得到该向量在$span$上的投影，而$I-H$作用于某个向量，会得到那条与$span$垂直的向量，在这里就是图中的$y-\hat{y}$，即$(I-H)noise=y-\hat{y}$。这个$y-\hat{y}$是真实值与预测值的差，其长度就是就是所有点的平方误差之和。于是就有：</p><script type="math/tex; mode=display">E_{in}(w_{LIN})=\frac{1}{N}||y-\hat{y}||^2\\=\frac{1}{N}||(I-H)noise||^2=\frac{1}{N}trace(I-H)||noise||^2=\frac{1}{N}(N-(d+1))||noise||^2</script><p>因此，就平均而言，有：</p><script type="math/tex; mode=display">\overline{E_{out}}=\text{noise level}·(1+\frac{d+1}{N})\\\overline{E_{in}}=\text{noise level}·(1-\frac{d+1}{N})</script><p>花这么大力气是为了什么，又回到之前learning可行性的话题了。</p><p><img src="https://beader.me/imgs/linear-regression/linear_regression_learning_curve.png" alt="img" style="zoom:80%;"></p><p>$\overline{E_{in}}$和$\overline{E_{out}}$都向$\sigma ^2$(noise level)收敛，并且他们之间的差异被$\frac{2(d+1)}{N}$给bound住了。虽然与VC Bound不同，VC Bound是通过最差错误概率来推导$E_{in}$与$E_{out}$接近，这里是从平均的角度，仍然能证明线性回归算法能够学习！</p></li></ul><h3 id="线性回归与线性分类"><a href="#线性回归与线性分类" class="headerlink" title="线性回归与线性分类"></a>线性回归与线性分类</h3><p>实际中进行线性分类时，可以先做一个regression来求一个初始化参数值，然后再应用诸如PLA/Pocket这类的算法降低error。这样可以提升分类效率。</p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>之前提过的二元分类器如PLA，其目标函数为$f(x)=sign(w^Tx)\in\{-1,+1\}$，输出要么是-1要么是+1，是一个硬分类器。而Logistic Regression是一个软分类器，它的输出是$y=+1$的概率，因此Logistic Regression的目标函数是$f(x)=P(+1|x)\in [0,1]$。</p><h3 id="方程形式"><a href="#方程形式" class="headerlink" title="方程形式"></a>方程形式</h3><p>使用</p><script type="math/tex; mode=display">h(x)=\frac{1}{1+e^{-w^Tx}}</script><p>来近似Target Function$f(x)=P(+1|x)$</p><h3 id="误差衡量"><a href="#误差衡量" class="headerlink" title="误差衡量"></a>误差衡量</h3><blockquote><p>为什么不能像Linear Regression一样使用平方误差？</p><p>如果使用平方误差，每个点产生的误差是：</p><script type="math/tex; mode=display">err(h,x_n,y_n)=y_n(1-\theta(w^Tx))^2+(1-y_n)(\theta(w^Tx))^2</script><p>此时cost function，$E_{in}(w)=\sum{err}$就是一个关于$w$的非凸函数(non-convex)：</p><p><img src="https://beader.me/imgs/logistic-regression/non_convex.png" alt="img"></p><p>非凸函数由于存在很多个局部最小点，因此很难去做最优化(解全局最小)。所以Logistic Regression没有使用平方误差来定义error，而是使用极大似然法来估计模型的参数。</p></blockquote><p>Logistic Regression的目标函数的输出是，在已知$x$的条件下，$y=+1$的概率，因此在已知$x$的条件下，$y=+1$的概率是$f(x)$，$y=-1$的概率是$1-f(x)$:</p><p>考虑训练样本$\mathcal{D}=\{(x_1,+1),(x_2,-1),…,(x_N,-1)\}$，并不是每次抽样都能抽到一模一样的$\mathcal{D}$，抽到这么一份样本是由于各种的机缘巧合。那么我们能抽到这么一份$\mathcal{D}$的概率取决于两部分：</p><ol><li>抽到样本$x_1,…,x_N$的概率；</li><li>这些样本对应的$y_1,…,y_N$等于$+1$的概率</li></ol><p>对于目标函数$f$，抽到$\mathcal{D}$的概率只取决于第1部分，而我们无法知道$f$，即第2部分也是未知的，因此我们称在$h$的作用下抽出$\mathcal{D}$的概率为“似然性”。如果 $h\approx f$，则 $likelihood(h)\approx \text{probability using }f$，并且我们认为在$f$的作用下，产生$\mathcal{D}$这样的样本的概率通常是非常的大的。</p><p>所以有：</p><script type="math/tex; mode=display">\text{if }h\approx f\text{, then }\; likelihood(h)\approx(\text{probability using }f)\approx\text{large}</script><p>则理想的hypothesis就是能使得似然函数最大的那个$h$：</p><script type="math/tex; mode=display">g=\underset{h}{argmax}\;likelihood(h)</script><p>当$h$是logistic函数的时候，即$h(x)=\theta(w^Tx)$，由于logistic函数的中心对称性，有:</p><script type="math/tex; mode=display">1-h(x)=h(-x)</script><p>所以有：</p><script type="math/tex; mode=display">1-h(x_n)=h(-x_n)=h(y_nx_n)</script><p>因此：</p><script type="math/tex; mode=display">likelihood(logistic\;h)\propto \prod_{n=1}^{N}h(y_nx_n)</script><p>我们的目标是想找到一个似然性最大的方程:</p><script type="math/tex; mode=display">\underset{h}{max}\;\;{likelihood(logistic\;h) \propto}\prod_{n=1}^{N}h(y_nx_n)\propto\prod_{n=1}^{N}\theta(y_nw^Tx_n)\propto\frac{1}{N}\sum_{n=1}^{N}\ln\theta(y_nw^Tx_n)</script><p>转化成与参数$w$有关的形式（求解上式最大值，等价于求解下式的最小值）：</p><script type="math/tex; mode=display">\underset{w}{min}E_{in}(w)=\frac{1}{N}\sum_{n=1}^{N}\ln(1+\exp(-y_nw^Tx_n))</script><p>求和符号后面的部分就是在极大似然估计下，logistic方程的误差函数，这种形式的误差函数称为cross entropy error:</p><script type="math/tex; mode=display">err(w,x_n,y_n)=\ln(1+\exp(-y_nw^Tx_n))</script><h3 id="参数求解-1"><a href="#参数求解-1" class="headerlink" title="参数求解"></a>参数求解</h3><p>那么如何能够最小化$E_{in}(w)$呢？按照之前Linear Regression的逻辑，由于它是凸函数，如果我们能解出一阶微分(梯度)为0的点，这个问题就解决了。</p><p>先来看看$E_{in}(w)$在$w_i$方向上的偏微分：</p><p><img src="https://beader.me/imgs/logistic-regression/deriv_costf_logistic.png" alt="img"></p><p>再把偏微分方程中的$x_{n,i}$换成向量的形式，就得到$E_{in}(w)$的一阶微分:</p><script type="math/tex; mode=display">\triangledown E_{in}(w)=\frac{1}{N}\sum_{n=1}^{N}\theta(-y_nw^Tx_n)(-y_nx_n)</script><p>和之前的Linear Regression不同，它不是一个线性的式子，要求解$\triangledown E_{in}(w)=0$这个式子，是困难的。那么该使用何种方法实现$E_{in}(w)$最小化呢？</p><p>这里可以使用类似PLA当中的，通过迭代的方式来求解，这种方法又称为梯度下降法(Gradient Descent)。</p><p><img src="https://beader.me/imgs/logistic-regression/iterative_opt.png" alt="img"></p><p>有点类似一个小球，往山谷方向滚，直至山谷。每一步我们只要决定两个东西：1、滚动的方向；2、滚动的步长。</p><p>滚动的方向好决定，即在该点一阶微分后的向量所指的方向；步长${\eta}$比较难决定，太小了更新太慢，太大了容易矫枉过正:</p><p><img src="https://beader.me/imgs/logistic-regression/choise_of_eta.png" alt="img"></p><p>一个比较好的做法是让 $\eta$ 与 $||\triangledown E_{in}(w_t)||$ 成一定的比例。</p><p>完整的梳理下梯度下降法(Gradient Descent)：</p><p>initialize $w_0$</p><p>For t = 0, 1, …</p><ol><li><p>compute</p><script type="math/tex; mode=display">\triangledown E_{in}(w_t)=\frac{1}{N}\sum_{n=1}^{N}\theta(-y_nw^T_tx_n)(-y_nx_n)</script></li><li><p>update by</p><script type="math/tex; mode=display">w_{t+1} \leftarrow w_t - \eta\triangledown E_{in}(w_t)</script></li></ol><p>…until $E_{in}(w_{t+1})=(\approx)0$ or enough iterations</p><p>return $\text{last }w_{t+1}\text{ as }g$</p><h2 id="Linear-Models-for-Classification"><a href="#Linear-Models-for-Classification" class="headerlink" title="Linear Models for Classification"></a>Linear Models for Classification</h2><p>前面介绍了三种线性模型：PLA、Linear Regression与Logistic Regression。之所以称他们是线性模型，是因为这三种分类模型的方程中，都含有一个相同的部分，该部分是各个特征的一个线性组合，也可以称这个部分叫做线性评分方程（linear scoring function）：</p><script type="math/tex; mode=display">s=w^Tx</script><p>对比这三种模型：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\llloverview.png" alt="img"></p><ul><li>Linear Classification模型：取$s$的符号作为结果输出，使用0/1 error作为误差衡量方式，但它的cost function，也就是$E_{in}(w)$是一个离散的方程，并且该方程的最优化是一个NP-hard问题。</li><li>Linear Regression模型：直接输出评分方程，使用平方误差square error作为误差衡量方式，好处是$E_{in}(w)$是一个凸二次曲线，非常方便求最优解（可通过矩阵运算一次得到结果，一步登天）。</li><li>Logistic Regression模型：输出的是评分方程经过sigmoid的结果，使用cross-entropy作为误差衡量方式，其$E_{in}(w)$是一个光滑的凸函数，可以使用gradient descent的方式求最佳解。</li></ul><p>Linear Regression和Logistic Regression的输出是一个实数，而不是一个Binary的值，他们能用来解分类问题吗？可以，只要定一个阈值，高于阈值的输出+1，低于阈值的输出-1就好。既然Linear Regression和Logistic Regression都可以用来解分类问题，并且在最优化上，他们都比Linear Classification简单许多，<strong>我们能否使用这两个模型取代Linear Classification呢？</strong></p><p><strong>三个模型的区别在于误差的衡量</strong>：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\lll_error_function.png" alt="img"></p><p>这里$y$是一个binary的值，要么是-1，要么是+1。</p><p>注意到三个模型的error function都有一个$ys$的部分，也叫做<strong>分类正确性分数 (classification correctness score)</strong>。其中$s$是模型对某个样本给出的分数，$y$是该样本的真实值。</p><p>当$y=+1$时，$s$越大越好；当$y=-1$时，$s$越小越好。所以总的来说，我们希望$ys$尽可能大。因此这里希望给较小的$ys$较大的cost，给较大的$ys$较小的cost即可。因此，不同模型的本质差异，就在于这个cost该怎么给。</p><p>既然这三个error function都与$ys$有关，我们可以以$ys$为横坐标，$err$为纵坐标，把这三个函数画出来。</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\lll_error_function_vis.png" alt="img"></p><ul><li><p>sqr(squre error)为Linear Regression的误差函数。可以看出：</p><ul><li>在$ys$较小的时候很大，符合情况；</li><li>在$ys$较大的时候$err_{sqr}$同样很大，这点不是很理想，因为我们希望$ys$大的时候cost要小</li><li>尽管如此，至少在$err_{sqr}$小的时候（在$ys=1$附近），$err_{0/1}$也很小，因此可以拿来做error function。</li></ul></li><li><p>ce(cross entropy)为Logistic Regression的误差函数。可以看出：</p><ul><li><p>$err_{ce}$是一个单调递减的函数，形态有一点像$err_{0/1}$，但来的比较平缓。</p></li><li><p>注意到$err_{ce}$有一部分是小于$err_{0/1}$的，我们希望$err_{ce}$能成为$err_{0/1}$的一个upper bound（目的一会儿会说到），只要将$err_{ce}$做一个换底的动作，将以$e$为底的对数换成以$2$为底的对数：</p><script type="math/tex; mode=display">\text{scaled ce : err}_{sce}(s,y)=\log_2(1+\exp(-ys))</script><p>换底过后三种误差函数的图像如下：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\lll_error_function_scale.png" alt="img"></p><p>事实上这里做scale的动作并不会影响最优化的过程，它只是让之后的推导证明更加容易一些。</p></li></ul></li></ul><p>回到问题：能不能拿Linear Regression或Logistic Regression来替代Linear Classification？</p><ul><li><p>为什么会想做这样的替代？</p><p>Linear Classification在分类上做的很好，但在最优化上是NP-hard问题，而Linear Regression与Logistic Regression在最优化上比较容易。如果他们在分类能力上的表现能够接近Linear Classification，用他们来替代Linear Classification来处理分类的问题，就是件皆大欢喜的事。</p></li><li><p>为何要对$err_{se}$进行换底，将其scale成$err_{0/1}$的upper bound？</p><ul><li>目的就是为了让这几个模型的观点在某个方向上是一致的，即：$err_{sqr}$或$err_{sce}$低的时候，$err_{0/1}$也低。</li><li>通俗一点讲：假设某种疾病有两种检测方法A和B。A方法检查结果为阳性时，则患病，为阴性时，则未患病。B方法的效率差一些，对于一部分患病的人，B方法不一定结果为阳性，但只要B的结果为阳性，再用A来检查，A的结果一定也为阳性。这么一来，我们就可以说，如果B方法的结果为阳性的时候，我们就没有必要使用A方法再检查一次了，它的效率是和A相同的。</li><li><p>从理论上来说明，我们能得到如下的三个性质：</p><script type="math/tex; mode=display">err_{0/1}(s,y)\leq err_{SCE}(s,y)=\frac1{ln2}err_{CE}(s,y)\\E_{in}^{0/1}(w)\leq E_{in}^{SCE}(w)=\frac1{ln2}E_{in}^{CE}(w)\\E_{out}^{0/1}(w)\leq E_{out}^{SCE}(w)=\frac1{ln2}E_{out}^{CE}(w)</script><p>由VC理论可得：</p><script type="math/tex; mode=display">E_{out}^{0/1}(w)\leq E_{in}^{0/1}(w)+\Omega^{0/1}\leq \frac1{ln2}E_{in}^{CE}(w)+\Omega^{0/1}\\E_{out}^{0/1}(w)\leq \frac1{ln2}E_{out}^{CE}(w)\leq \frac1{ln2}E_{in}^{CE}(w)+\frac1{ln2}\Omega^{CE}</script><p>从而我们就能够把LinReg和LogReg 作为Linear Classification的上界。即如果使用$err_{sqr}$或$err_{sce}$来衡量一个模型分类分得好不好的时候，如果他们认为分得好，那么如果使用$err_{0/1}$，它也会认为分得好。</p></li></ul></li></ul><p>对比下在处理分类问题时，使用PLA，Linear Regression以及Logistic Regression的优缺点：</p><ul><li><p><strong>PLA</strong></p><ul><li>优点：高效（一个样本更新一次）；数据是线性可分时，$E_{in}^{0/1}$保证可以降到最低</li><li>缺点：数据不是线性可分时，要额外使用pocket技巧，较难做最优化</li></ul></li><li><p><strong>Linear Regression</strong></p><ul><li>优点：在这三个模型中最容易做最优化</li><li>缺点：在$ys$很大或很小时，这个bound是很宽松的，没有办法保证$E_{in}^{0/1}$能够很小</li></ul></li><li><p><strong>Logistic Regression</strong></p><ul><li>优点：较容易最优化</li><li>缺点：当$ys$是很小的负数时，bound很宽松</li></ul></li></ul><p>所以我们常常可以使用Linear Regresion跑出的$w$作为(PLA/Pocket/Logistic Regression)的$w_0$（用于初始化），然后再使用$w_0$来跑其他模型，这样可以加快其他模型的最优化速度。同时，由于拿到的数据常常是线性不可分的，我们常常会去使用Logistic Regression而不是PLA+pocket。</p><h3 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h3><p>我们知道PLA与Logistic Regression都是通过迭代的方式来实现最优化的，区别在于：</p><ul><li>PLA每次迭代只需要针对一个点进行错误修正，时间复杂度为$O(1)$</li><li>Logistic Regression每一次迭代都需要遍历所有样本点，时间复杂度为$O(N)$</li></ul><p>这样一来，数据量大的时候，由于需要计算每一个点，Logistic Regerssion就会很慢了。那么可不可以每次只看一个点，即不要公式中先求和再取平均数的那个部分呢？随机取一个点$n$，它对梯度的贡献为：</p><script type="math/tex; mode=display">\triangledown _w err(w,x_n,y_n)</script><p>我们把它称为随机梯度（stochastic gradient），而真实的梯度可以认为是随机抽出一个点的梯度值的<strong>期望</strong>：</p><script type="math/tex; mode=display">\triangledown_w E_{in}(w) = \underset{random\,n}\epsilon\triangledown_w err(w,x_n,y_n)</script><p>因此我们可以把随机梯度当成是在真实梯度上增加一个均值为0的noise：</p><script type="math/tex; mode=display">\text{stochastic gradient} = \text{true gradient} + \text{zero-mean ‘noise’ directions}</script><p>从单次迭代上来看，好像确实会对每一步找到正确的梯度方向有影响，但是从整体期望值来看，与真实的梯度方向没有差太多，同样能找到差不多好的位置。我们把这种方法称为<strong>随机梯度下降，Stochastic Gradient Descent (SGD)</strong>：</p><script type="math/tex; mode=display">w_{t+1} \leftarrow w_t + \eta \underbrace{ {\theta({-y_nw_t^Tx_n} }){(y_nx_n)} }_{-{\triangledown_{err}(w_t,x_n,y_n)} }</script><p>和之前说到的Gradient Descent相比：</p><ul><li>SGD的好处在于时间复杂度大幅减小（每次只随机地看一个点），在数据量很大的时候可以很快得得到结果；且可以应用到在线学习（online learning）上。</li><li>缺点在于，当noise比较大的时候，会较不稳定。</li></ul><p>我们看到SGD logistic regression和PLA有非常相似的地方：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\20180805195816307.png" alt="img" style="zoom:50%;"></p><ul><li>都是对当前的与进行修正，可把SGD logistic regression称之为’soft’ PLA（更新速度随着错误的多少变动）；</li><li>当$\eta=1$且足够大的时候，PLA近似于SGD。</li></ul><p>关于SGD算法的两点经验（rule-of-thumb）：</p><ul><li>SGD的终止迭代条件一般是让迭代次数足够多；</li><li>更新速度$\eta$是根据实际情况来决定的，一般$\eta=0.1$就可以了。</li></ul><h3 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h3><p>我们现在已经有办法使用线性分类器解决二元分类问题，但有的时候，我们需要对多个类别进行分类，即模型的输出不再是0和1两种，而会是多个不同的类别。那么如何套用二元分类的方法来解决多类别分类的问题呢？</p><p><strong>利用二元分类器来解决多类别分类问题主要有两种策略，OVA(One vs. ALL)和OVO(One vs. One)。</strong></p><ol><li><p><strong>One-Versus-All(OVA)</strong></p><p>假设原问题有四个类别，那么每次我把其中一个类别当成圈圈，其他所有类别当成叉叉，建立二元分类器，循环下去，最终我们会得到4个分类器。</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\ova-1584414720961.png" alt="img"></p><p>做预测的时候，分别使用这四个分类器进行预测，预测为圈圈的那个模型所代表的类别，即为最终的输出。譬如正方形的那个分类器输出圈圈，菱形、三角形、星型这三个分类器都说是叉叉，则我们认为它是正方形。</p><p>优点：简单高效，并且可以和Logistic Regression的方法搭配使用；</p><p>缺点：当数据类别$k$很大时，那么二分的正类和负类的数量差别就很大，数据不平衡unbalance，这样就不容易得出正确结果。另外，在选取某一类概率最大时，所有类别的概率之和不一定为1，虽然实际中影响不大，但统计上有更加严谨的方法——multinomial logistic regression。</p></li><li><p><strong>One-Versus-One(OVO)</strong></p><p>在类别较多的时候，如果使用OVA方法，则又会遇到数据不平衡(unbalance)的问题，你拿一个类别作为圈圈，其他所有类别作为叉叉，那么圈圈的比例就会非常小，而叉叉的比例非常高。为了解决这个不平衡的问题，我们可以利用OVO，即每次只拿两个类别的数据出来建立分类器，如下图。</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\ovo.png" alt="img"></p><p>一笔新数据进来之后，分别使用这六个模型进行预测，投票得票数最多的那个类别，作为最终的输出。</p><p>优点：更加高效，因为虽然需要进行的分类次数增加了，但是每次只需要进行两个类别的比较，也就是说单次分类的数量减少了；而且一般不会出现数据unbalanced的情况，比较稳定；可以和任何的binary classification方法搭配使用。</p><p>缺点：分类次数增加到$C_k^2$，时间复杂度和空间复杂度可能都比较高。</p></li></ol><p>总之，OVA和OVO是两个多类别分类方法，都是非常不错的多类别分类算法，在$k$不大的时候比较推荐OVO，减少分类次数。</p><h2 id="Nonliear-Transformation"><a href="#Nonliear-Transformation" class="headerlink" title="Nonliear Transformation"></a>Nonliear Transformation</h2><p>前面所谈到的分类模型，都是基于线性的，即我们假设数据是线性可分，或者至少看起来用一条线来做分类是不错的。但现实中我们的数据往往不那么容易得能用一条线区分开来。</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\linear-vs-nonlinear.png" alt="img"></p><p>可以发现$\mathcal{D}$并不是一个线性可分的数据，但使用一个圆圈，却可以很好得把圈圈和叉叉区分开来。这个”圆圈分类器”可以是下面这种形式:</p><script type="math/tex; mode=display">h_{SEP}(x)=sign(-x_1^2-x_2^2+0.6)</script><p>于是我们就能把这个式子再写成下面的形式</p><script type="math/tex; mode=display">h(x)=sign(\tilde{w}_0\cdot z_0+\tilde{w}_1\cdot z_1+\tilde{w}_2\cdot z_2)=\tilde{w}^Tz</script><p>如果我们只看包含$z$的部分，它事实上依然是一个线性方程。即$\mathcal{X}$空间下的一个圆圈，对应到$\mathcal{Z}$空间下的一条直线。</p><p><img src="https://beader.me/imgs/nonlinear-transformation/two-space.png" alt="img"></p><p>这个转换的过程称为nonlinear feature transform，用符号$\Phi$表示，$\Phi$把两个互相独立的空间给联系了起来:</p><script type="math/tex; mode=display">(1,x_1^2,x_2^2)=\Phi(x)=(z_0,z_1,z_2)=z</script><p>$\mathcal{X}$空间下的每个点，都对应$\mathcal{Z}$空间下的某个点，同样$\mathcal{X}$空间下的二次曲线方程，都对应$\mathcal{Z}$空间下的某个一次直线方程。</p><script type="math/tex; mode=display">h(x)=sign({\tilde{w}_0}+{\tilde{w}_1}{x_1^2}+{\tilde{w}_2}{x_2^2})=sign({\tilde{w}^T}{\Phi}(x))={\tilde{h}({z})}</script><p>这样以来，两个空间产生了关联，前面说的$\Phi$就是这两个空间的纽带，在这个纽带下，$\mathcal{Z}$空间下的不同直线也就对应$\mathcal{X}$下的不同形态的分类器。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$\tilde{w}$</th><th style="text-align:left">X空间下的曲线形态</th></tr></thead><tbody><tr><td style="text-align:center">(0.6,−1,−1)</td><td style="text-align:left">circle(圈圈在内部，叉叉在外部)</td></tr><tr><td style="text-align:center">(−0.6,+1,+1)</td><td style="text-align:left">circle(圈圈在外部，叉叉在内部)</td></tr><tr><td style="text-align:center">(0.6,−1,−2)</td><td style="text-align:left">ellipse椭圆</td></tr><tr><td style="text-align:center">(0.6,−1,+2)</td><td style="text-align:left">hyperbola双曲线</td></tr><tr><td style="text-align:center">(0.6,+1,+2)</td><td style="text-align:left">所有点都判断为圈圈</td></tr></tbody></table></div><p>更加一般化，我们把一次空间$\mathcal{X}$映射到二次空间$\mathcal{Z}$的时候，还会保留其一次项（图形中心不一定在原点），即下面这个样子的映射才是完整的二次映射:</p><script type="math/tex; mode=display">\Phi(x)=(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)</script><p>这样一来，这个完整版的$\mathcal{Z}$空间的直线，就可以代表$\mathcal{X}$空间下的所有二次曲线了。</p><p>我们为何要做这个非线性变换？逻辑是这样的：</p><ul><li>在$\mathcal{X}$空间中，我们使用一条直线，很难把圈圈和叉叉分开</li><li>但是如果我们使用一条曲线，可以很容易做到</li><li>可我们只学过找最优“直线”的算法，没有学过找最佳“曲线”的算法</li><li>我们有一个纽带$\Phi$，它能够把$\mathcal{X}$空间中的所有曲线，对应到$\mathcal{Z}$空间中的曲线，反过来也可以根据$\mathcal{Z}$空间的一条直线，找到$\mathcal{X}$空间中对应的曲线</li><li>所以我们可以把$\mathcal{X}$空间下的原始数据$\mathcal{D}$映射到$\mathcal{Z}$空间下</li><li>然后在$\mathcal{Z}$空间下寻找最佳的“直线”，找直线这件事我们已经学过该怎么做了</li><li>找到这条最佳的“直线”后，把它对应回$\mathcal{X}$空间，就是我们刚开始说的需要找的那条最佳的“曲线”了。</li></ul><p><img src="https://beader.me/imgs/nonlinear-transformation/nonlinear-transform-steps.png" alt="img"></p><p>实际操作上分以下三步：</p><ol><li>变换原始数据 ${(x_n,y_n)}\Rightarrow {(z_n=\phi(x_n),y_n)}$</li><li>利用之前学过的算法，使用${(z_n,y_n)}$训练线性模型，得到$\tilde{w}$</li><li>得到分类器方程$g(x)=sign({\tilde{w}^T}{\Phi_2}(x))$</li></ol><h3 id="非线性变换的代价"><a href="#非线性变换的代价" class="headerlink" title="非线性变换的代价"></a>非线性变换的代价</h3><p>d维向量$x$经过Q次多项式变换（Q-th Order Polynomial Transform）:</p><script type="math/tex; mode=display">\tilde{d}=C_{Q+d}^Q=C_{Q+d}^d=O(Q^d)</script><p>以上是Q次完整变换，当然我们不一定会需要完整项，譬如之前那个圆圈，就舍弃了$x_1x_2$项。若考虑完整变化，原来的$1+d$维向量经过变换，就成了$1+\tilde{d}=1+\binom{Q+d}{Q}$维向量，复杂度由$O(d)$变为$O(Q^d)$。</p><ol><li><p>计算变复杂，需要的存储空间增大</p><p>这点很容易理解，如之前的例子，原始数据$(1,x_1,x_2)$经过变换之后变成$(1,x_1,x_2,x_1^2,x_1x_2,x_2^2)$，需要多一倍的空间来存转换后的数据，同时参数的增加，也增大了计算量。</p></li><li><p>模型复杂度增大</p><p>之前有讲到自由度与VC Dimension的关系，线性模型的$d_{vc}\approx 自由度 \approx \tilde{d}+1$。如果Q非常大的话，则模型的$d_{vc}$也会非常大，我们知道$d_{vc}$越大，越容易实现小的$E_{in}$，同时，也会造成$|E_{out}-E_{in}|$的加大，即模型的泛化能力(generalization)变差。</p></li></ol><h4 id="模型的泛化问题-Generalization-Issue"><a href="#模型的泛化问题-Generalization-Issue" class="headerlink" title="模型的泛化问题(Generalization Issue)"></a>模型的泛化问题(Generalization Issue)</h4><p><img src="https://beader.me/imgs/nonlinear-transformation/phi1-vs-phi4.png" alt="img"></p><p>上图是分别使用原始数据进行训练以及进行4次非线性变换后的数据进行训练的结果对比。从视觉上看，虽然右图（经过4次非线性变换的模型）能够把圈圈叉叉完全分开，但显然这种模型过于复杂了。</p><p>很多时候我们会面临模型泛化能力和分类能力的权衡取舍，即：</p><div class="table-container"><table><thead><tr><th style="text-align:center">$\tilde{d}(Q)$</th><th style="text-align:center">$E_{out}(g)$能否与$E_{in}(g)$很接近?</th><th style="text-align:center">$E_{in}(g)$是否足够小?</th></tr></thead><tbody><tr><td style="text-align:center">高</td><td style="text-align:center">×</td><td style="text-align:center">√</td></tr><tr><td style="text-align:center">低</td><td style="text-align:center">√</td><td style="text-align:center">×</td></tr></tbody></table></div><p>那么如何选择合适的复杂度呢？暂且不讨论10维的数据有没有办法用眼睛看，就拿前面2维的例子来说，用眼睛看来选择模型是件很危险的事情。</p><p>$d_{vc}$是比较难判断的，因为你是在“看过”数据之后，由你大脑选择的一个模型，这里要考虑到你大脑“选择模型”产生的一个复杂度，因为如果重新选取一部分数据，你有可能就不再挑选正圆模型了，事实上你的大脑不知不觉地参与到了模型参数估计上。（human learning√ machine learning ×）</p><h3 id="Structured-Hypothesis-Sets"><a href="#Structured-Hypothesis-Sets" class="headerlink" title="Structured Hypothesis Sets"></a>Structured Hypothesis Sets</h3><p>通常我们说的非线性变换，指的是多项式变换(Polynomial Transform)。用符号$\Phi_Q$来表示$Q$次多项式变换：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\20180806215127288.png" alt="img"></p><p>可以发现$\Phi_{i}(x)$中包含了$\Phi_{i-1}(x)$，因此他们对应的Hypothesis Set也有如下关系：</p><p><img src="/2020/03/08/lin-xuan-tian-ji-qi-xue-xi-ji-shi-note-part3/1Study\hexo\source\_posts\林轩田《机器学习基石》笔记Part3.assets\20180806215426586.png" alt="img"></p><p>假如我们分别对原始数据进行$i$次非线性多项式变换并训练模型，$g_i$表示使用$i$次非线性多项式变换后的数据所训练出的最优模型，$\color{blue}{g_i=argmin_{h\in \mathcal{H}_i}E_{in}(h)}$则有：</p><script type="math/tex; mode=display">H_{\Phi_0} \subset H_{\Phi_1} \subset H_{\Phi_2} \subset \cdots \subset H_{\Phi_Q} \\d_{VC}(H_0)\leq d_{VC}(H_1)\leq d_{VC}(H_2)\leq \cdots \leq d_{VC}(H_Q)\\E_{in}(g_0)\geq E_{in}(g_1)\geq E_{in}(g_2)\geq \cdots \geq E_{in}(g_Q)</script><p>通常在进行高次非线性变换的时候，应该特别小心，因为$d_{vc}$上升很快，极容易造成overfitting。</p><p>比较安全的做法是，先尝试不做非线性变换，即使用$\mathcal{H}_{\phi_1}$，如果效果足够好了，就不需要进行非线性变换，如果效果不够好，再慢慢尝试使用复杂更高的模型。</p><p><img src="https://beader.me/imgs/nonlinear-transformation/model_complexity_curve.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>剑指offer题目Python解答</title>
      <link href="/2020/02/14/jian-zhi-offer-ti-mu-python-jie-da/"/>
      <url>/2020/02/14/jian-zhi-offer-ti-mu-python-jie-da/</url>
      
        <content type="html"><![CDATA[<h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><h2 id="二维数组中的查找"><a href="#二维数组中的查找" class="headerlink" title="二维数组中的查找"></a>二维数组中的查找</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>矩阵是有序的，从左下角来看，向上数字递减，向右数字递增，</p><p>因此从左下角开始查找：</p><ul><li><p>当要查找数字比左下角数字大时：右移</p></li><li><p>当要查找数字比左下角数字小时：上移</p></li></ul><p>如果超出边界，则说明二维数组中不存在target元素。 </p><h3 id="解答"><a href="#解答" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># array 二维列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Find</span><span class="params">(self, target, array)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        rows=len(array)<span class="number">-1</span></span><br><span class="line">        cols=len(array[<span class="number">0</span>])<span class="number">-1</span></span><br><span class="line">        i=rows</span><br><span class="line">        j=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i&gt;=<span class="number">0</span> <span class="keyword">and</span> j&lt;=cols:</span><br><span class="line">            <span class="keyword">if</span> array[i][j]&lt;target:</span><br><span class="line">                j+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> array[i][j]&gt;target:</span><br><span class="line">                i-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="数组中重复的数字"><a href="#数组中重复的数字" class="headerlink" title="数组中重复的数字"></a>数组中重复的数字</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。</p><h3 id="解答-1"><a href="#解答-1" class="headerlink" title="解答"></a>解答</h3><ul><li><p>方法1：使用count函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]</span></span><br><span class="line">    <span class="comment"># 函数返回True/False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">duplicate</span><span class="params">(self, numbers, duplication)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> numbers:</span><br><span class="line">            <span class="keyword">if</span> numbers.count(i)&gt;<span class="number">1</span>:</span><br><span class="line">                duplication[<span class="number">0</span>]=i</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></li><li><p>方法2：使用Counter函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]</span></span><br><span class="line">    <span class="comment"># 函数返回True/False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">duplicate</span><span class="params">(self, numbers, duplication)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">        <span class="keyword">if</span> len(numbers) == len(set(numbers)): <span class="comment"># 如果不存在重复数字</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        duplication[<span class="number">0</span>] = Counter(numbers).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure></li><li><p>方法3：使用dict（31ms），时间O(1)，空间O(n) </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 这里要特别注意~找到任意重复的一个值并赋值到duplication[0]</span></span><br><span class="line">    <span class="comment"># 函数返回True/False</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">duplicate</span><span class="params">(self, numbers, duplication)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        dic = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> numbers:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> dic <span class="keyword">and</span> dic[i] == <span class="number">1</span>:</span><br><span class="line">                duplication[<span class="number">0</span>] = i</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            dic[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="构建乘积数组"><a href="#构建乘积数组" class="headerlink" title="构建乘积数组"></a>构建乘积数组</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个数组<code>A[0,1,...,n-1]</code>，请构建一个数组<code>B[0,1,...,n-1]</code>，其中B中的元素<code>B[i] = A[0]*A[1]*...*A[i-1]*A[i+1]*...*A[n-1]</code>。不能使用除法。（注意：规定B[0]和B[n-1] = 1）</p><h3 id="解法"><a href="#解法" class="headerlink" title="解法"></a>解法</h3><ul><li>方法1：剑指offer给出方法</li></ul><p><img src="https://i.loli.net/2020/03/07/37nuMo4KaDfmCRO.jpg" alt="841505_1472459965615_8640A8F86FB2AB3117629E2456D8C652.jpg" style="zoom: 25%;"></p><p> <strong>B[i]的值可以看作上图的矩阵中每行的乘积。</strong> </p><p>下三角用连乘可以很容求得，上三角，从下向上也是连乘。 </p><p>因此我们的思路就很清晰了，先算下三角中的连乘，即我们先算出B[i]中的一部分，然后倒过来按上三角中的分布规律，把另一部分也乘进去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(self, A)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        head = [<span class="number">1</span>]</span><br><span class="line">        tail = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(A)<span class="number">-1</span>):</span><br><span class="line">            head.append(A[i]*head[i])</span><br><span class="line">            tail.append(A[-i<span class="number">-1</span>]*tail[i])</span><br><span class="line">        <span class="keyword">return</span> [head[j]*tail[-j<span class="number">-1</span>] <span class="keyword">for</span> j <span class="keyword">in</span> range(len(head))]</span><br></pre></td></tr></table></figure><ul><li>方法2：直观方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">multiply</span><span class="params">(self, A)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        B=[i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(A))]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> B:</span><br><span class="line">            B[i]=<span class="number">1</span>  <span class="comment"># 将B中的元素每次初始化为1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> (A[:i]+A[i+<span class="number">1</span>:]):  <span class="comment"># 遍历A中除了i以外的所有元素</span></span><br><span class="line">                B[i]*=j</span><br><span class="line">        <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure><h1 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h1><h2 id="从尾到头打印链表"><a href="#从尾到头打印链表" class="headerlink" title="从尾到头打印链表"></a>从尾到头打印链表</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个链表，按链表从尾到头的顺序返回一个ArrayList。</p><h3 id="解答-2"><a href="#解答-2" class="headerlink" title="解答"></a>解答</h3><p>简单列表保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回从尾部到头部的列表值序列，例如[1,2,3]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printListFromTailToHead</span><span class="params">(self, listNode)</span>:</span></span><br><span class="line">        res=[]</span><br><span class="line">        <span class="keyword">while</span> listNode:</span><br><span class="line">            res.append(listNode.val)</span><br><span class="line">            listNode=listNode.next</span><br><span class="line">        <span class="keyword">return</span> res[::<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><h2 id="链表中环的入口结点"><a href="#链表中环的入口结点" class="headerlink" title="链表中环的入口结点"></a>链表中环的入口结点</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。</p><h3 id="解答-3"><a href="#解答-3" class="headerlink" title="解答"></a>解答</h3><p>遍历这个链表，把链表每个元素记录在list里，然后一旦遇到了重复节点则返回该结点，不然不存在返回None </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">EntryNodeOfLoop</span><span class="params">(self, pHead)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        stack=[]</span><br><span class="line">        p = pHead</span><br><span class="line">        <span class="keyword">while</span> p:</span><br><span class="line">            <span class="keyword">if</span> p <span class="keyword">in</span> stack:</span><br><span class="line">                <span class="keyword">return</span> p</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stack.append(p)</span><br><span class="line">            p=p.next</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><h2 id="删除链表中重复的结点"><a href="#删除链表中重复的结点" class="headerlink" title="删除链表中重复的结点"></a>删除链表中重复的结点</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-&gt;2-&gt;3-&gt;3-&gt;4-&gt;4-&gt;5 处理后为 1-&gt;2-&gt;5</p><h3 id="解答-4"><a href="#解答-4" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deleteDuplication</span><span class="params">(self, pHead)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="comment"># 如果是空链表或只有一个结点，直接返回原链表</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pHead <span class="keyword">or</span> <span class="keyword">not</span> pHead.next:</span><br><span class="line">            <span class="keyword">return</span> pHead</span><br><span class="line">        <span class="comment"># 如果当前结点值与下个结点值相等，跳到下个结点（以下个结点为起始）</span></span><br><span class="line">        <span class="keyword">if</span> pHead.val==pHead.next.val:</span><br><span class="line">            tmp=pHead.next</span><br><span class="line">            <span class="comment"># 若还与接下来的结点值重复，继续跳到下个结点</span></span><br><span class="line">            <span class="keyword">while</span> tmp <span class="keyword">and</span> pHead.val==tmp.val:</span><br><span class="line">                tmp=tmp.next</span><br><span class="line">            <span class="keyword">return</span> self.deleteDuplication(tmp)</span><br><span class="line">        <span class="comment"># 否则，保留该节点，进行下个结点的递归</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pHead.next=self.deleteDuplication(pHead.next)</span><br><span class="line">            <span class="keyword">return</span> pHead</span><br></pre></td></tr></table></figure><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="替换空格"><a href="#替换空格" class="headerlink" title="替换空格"></a>替换空格</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p><h3 id="解答-5"><a href="#解答-5" class="headerlink" title="解答"></a>解答</h3><ul><li><p>方法1：replace函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s 源字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceSpace</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">return</span> s.replace(<span class="string">' '</span>,<span class="string">'%20'</span>)</span><br></pre></td></tr></table></figure></li><li><p>方法2：直接生成新的字符串（如果题目限制在原字符串上修改就GG）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s 源字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceSpace</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        s1=<span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> j==<span class="string">' '</span>:</span><br><span class="line">                s1=s1+<span class="string">'%20'</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                s1=s1+j</span><br><span class="line">        <span class="keyword">return</span> s1</span><br></pre></td></tr></table></figure><p>简化版</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s 源字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceSpace</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        l = [<span class="string">'%20'</span> <span class="keyword">if</span> x == <span class="string">' '</span> <span class="keyword">else</span> x <span class="keyword">for</span> x <span class="keyword">in</span> s]</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(l)</span><br></pre></td></tr></table></figure></li></ul><h2 id="正则表达式匹配"><a href="#正则表达式匹配" class="headerlink" title="正则表达式匹配"></a>正则表达式匹配</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数用来匹配包括’.’和’*‘的正则表达式。模式中的字符’.’表示任意一个字符，而’*‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串”aaa”与模式”a.a”和”ab*ac*a”匹配，但是与”aa.a”和”ab*a”均不匹配</p><h3 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h3><p>递归的思想：</p><ul><li><p>当模式中的第二个字符是“*”时：</p><ul><li><p>如果字符串第一个字符跟模式第一个字符不匹配，则模式后移2个字符，继续匹配。</p></li><li><p>如果字符串第一个字符跟模式第一个字符匹配，可以有3种匹配方式：</p><ol><li><p>模式后移2字符，相当于x*被忽略；</p></li><li><p>字符串后移1字符，模式后移2字符，相当于x*匹配一位；</p></li><li><p>字符串后移1字符，模式不变，即继续匹配字符下一位，相当于x*匹配多位；</p></li></ol><blockquote><p>情况1：”abcde”与”a*abcde”匹配</p><p>情况2：”abcde”与”a*bcde”匹配</p><p>情况3：”abcde”与”abcd<em>“匹配（递推与”abc\</em>“、”ab*“、”a*“均匹配）</p></blockquote></li></ul></li><li><p>当模式中的第二个字符不是“*”时：</p><ul><li>如果字符串第一个字符和模式中的第一个字符相匹配，那么字符串和模式都后移一个字符，然后匹配剩余的部分。</li><li>如果字符串第一个字符和模式中的第一个字符相不匹配，直接返回False。 </li></ul></li></ul><h3 id="解答-6"><a href="#解答-6" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s, pattern都是字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">match</span><span class="params">(self, s, pattern)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="comment"># 如果模式和字符串完全相等（包含均为空），则匹配成功</span></span><br><span class="line">        <span class="keyword">if</span> s == pattern:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 如果模式为空，字符串不为空，则匹配不成功</span></span><br><span class="line">        <span class="keyword">elif</span> (s != <span class="string">''</span> <span class="keyword">and</span> pattern == <span class="string">''</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果模式不为空，字符串为空</span></span><br><span class="line">        <span class="keyword">elif</span> (s == <span class="string">''</span> <span class="keyword">and</span> pattern != <span class="string">''</span>):</span><br><span class="line">            <span class="keyword">if</span> pattern[<span class="number">-1</span>] == <span class="string">'*'</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果模式不为空，字符串不为空</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> len(pattern) == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> pattern == <span class="string">'*'</span> <span class="keyword">or</span> (pattern == <span class="string">'.'</span> <span class="keyword">and</span> len(s) == <span class="number">1</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="comment"># 当模式中的第二个字符是'*'时</span></span><br><span class="line">            <span class="keyword">if</span> pattern[<span class="number">1</span>] == <span class="string">'*'</span> <span class="keyword">and</span> len(pattern) &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 如果字符串第一个字符跟模式第一个字符不匹配</span></span><br><span class="line">                <span class="keyword">if</span> s[<span class="number">0</span>] != pattern[<span class="number">0</span>] <span class="keyword">and</span> pattern[<span class="number">0</span>] != <span class="string">'.'</span> :</span><br><span class="line">                    <span class="keyword">return</span> self.match(s, pattern[<span class="number">2</span>:])</span><br><span class="line">                <span class="comment"># 如果字符串第一个字符跟模式第一个字符匹配</span></span><br><span class="line">                <span class="keyword">elif</span> len(s) &gt; <span class="number">1</span> <span class="keyword">and</span> len(pattern)&gt;<span class="number">1</span>:</span><br><span class="line">                    f1 = self.match(s, pattern[<span class="number">2</span>:]) <span class="comment"># 模式后移2字符</span></span><br><span class="line">                    f2 = self.match(s[<span class="number">1</span>:], pattern[<span class="number">2</span>:]) <span class="comment"># 字符串后移1字符，模式后移2字符</span></span><br><span class="line">                    f3 = self.match(s[<span class="number">1</span>:], pattern) <span class="comment"># 字符串后移1字符</span></span><br><span class="line">                    <span class="keyword">if</span> f1 <span class="keyword">or</span> f2 <span class="keyword">or</span> f3:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> pattern[<span class="number">-1</span>] == <span class="string">'*'</span> <span class="keyword">or</span> pattern[<span class="number">-1</span>] == s[<span class="number">-1</span>]:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="comment"># 当模式中的第二个字符不是'*'时</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 如果字符串第一个字符和模式中的第一个字符相匹配</span></span><br><span class="line">                <span class="keyword">if</span> s[<span class="number">0</span>] == pattern[<span class="number">0</span>] <span class="keyword">or</span> pattern[<span class="number">0</span>] == <span class="string">'.'</span>:</span><br><span class="line">                    <span class="keyword">return</span> self.match(s[<span class="number">1</span>:], pattern[<span class="number">1</span>:])</span><br><span class="line">                <span class="comment"># 如果字符串第一个字符和模式中的第一个字符相不匹配</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="表示数值的字符串"><a href="#表示数值的字符串" class="headerlink" title="表示数值的字符串"></a>表示数值的字符串</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串”+100”,”5e2”,”-123”,”3.1416”和”-1E-16”都表示数值。 但是”12e”,”1a3.14”,”1.2.3”,”+-5”和”12e+4.3”都不是。</p><h3 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h3><ul><li>正负号<ul><li>第二次出现正负号，则必须紧接在e之后</li><li>第一次出现正负号，且不是在字符串开头，则也必须紧接在e之后</li></ul></li><li>小数点<ul><li>e后面不能接小数点</li><li>小数点不能出现两次</li></ul></li><li>e/E<ul><li>e后面一定要接数字</li><li>不能同时存在两个e</li></ul></li></ul><h3 id="解答-7"><a href="#解答-7" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># s字符串</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isNumeric</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="comment"># 标记符号、小数点、e是否出现过</span></span><br><span class="line">        sign = <span class="literal">False</span></span><br><span class="line">        decimal = <span class="literal">False</span></span><br><span class="line">        hasE = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">'e'</span> <span class="keyword">or</span> s[i] == <span class="string">'E'</span>):</span><br><span class="line">                <span class="comment"># e后面一定要接数字</span></span><br><span class="line">                <span class="keyword">if</span> (i == len(s)<span class="number">-1</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="comment"># 不能同时存在两个e</span></span><br><span class="line">                <span class="keyword">if</span> (hasE == <span class="literal">True</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                hasE = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> (s[i] == <span class="string">'+'</span> <span class="keyword">or</span> s[i] == <span class="string">'-'</span>):</span><br><span class="line">                <span class="comment"># 第二次出现+-符号，则必须紧接在e之后</span></span><br><span class="line">                <span class="keyword">if</span> (sign <span class="keyword">and</span> s[i<span class="number">-1</span>] != <span class="string">'e'</span> <span class="keyword">and</span> s[i<span class="number">-1</span>] != <span class="string">'E'</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                <span class="comment"># 第一次出现+-符号，且不是在字符串开头，则也必须紧接在e之后</span></span><br><span class="line">                <span class="keyword">elif</span> (sign == <span class="literal">False</span> <span class="keyword">and</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> s[i<span class="number">-1</span>] != <span class="string">'e'</span> <span class="keyword">and</span> s[i<span class="number">-1</span>] != <span class="string">'E'</span>):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                sign = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> (s[i] == <span class="string">'.'</span>):</span><br><span class="line">                <span class="comment"># e后面不能接小数点，小数点不能出现两次</span></span><br><span class="line">                <span class="keyword">if</span> (hasE <span class="keyword">or</span> decimal):</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                decimal = <span class="literal">True</span></span><br><span class="line">            <span class="comment"># 非法字符</span></span><br><span class="line">            <span class="keyword">elif</span>(s[i] &lt; <span class="string">'0'</span> <span class="keyword">or</span> s[i] &gt; <span class="string">'9'</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="字符流中第一个不重复的字符"><a href="#字符流中第一个不重复的字符" class="headerlink" title="字符流中第一个不重复的字符"></a>字符流中第一个不重复的字符</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符”go”时，第一个只出现一次的字符是”g”。当从该字符流中读出前六个字符“google”时，第一个只出现一次的字符是”l”。如果当前字符流没有存在出现一次的字符，返回#字符。</p><h3 id="解答-8"><a href="#解答-8" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.s=<span class="string">''</span></span><br><span class="line">        self.dict=&#123;&#125; <span class="comment">#创建字典，key为读取的字符串中的每一个字符，val为每个字符出现的个数的计数值</span></span><br><span class="line">    <span class="comment"># 返回对应char</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FirstAppearingOnce</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> self.s: <span class="comment"># 遍历字符串s中的字符</span></span><br><span class="line">            <span class="keyword">if</span> self.dict[i]==<span class="number">1</span>: <span class="comment"># 如果某个字符对应的计数为1，则返回该字符</span></span><br><span class="line">                <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'#'</span> <span class="comment"># 不存在则返回'#'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Insert</span><span class="params">(self, char)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        self.s=self.s+char <span class="comment"># 从字符流中读入字符到字符串s中</span></span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> self.dict: <span class="comment"># 如果读入的字符在字符串中已存在</span></span><br><span class="line">            self.dict[char]=self.dict[char]+<span class="number">1</span> <span class="comment"># 在字典中对应的字符计数加一</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 如果读入的字符在字符串中不存在</span></span><br><span class="line">            self.dict[char]=<span class="number">1</span> <span class="comment"># 字典中对应的字符计数为一（即新增了一个新的字符）</span></span><br></pre></td></tr></table></figure><h1 id="树"><a href="#树" class="headerlink" title="树"></a>树</h1><h2 id="重建二叉树"><a href="#重建二叉树" class="headerlink" title="重建二叉树"></a>重建二叉树</h2><h3 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。</p><h3 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h3><blockquote><p>前序遍历：根左右</p><p>中序遍历：左根右</p><p>后序遍历：左右根</p></blockquote><p>步骤：</p><ol><li>根据前序序列第一个结点确定根结点 </li><li>根据根结点在中序序列中的位置分割出左右两个子序列 </li><li>对左子树和右子树分别递归使用同样的方法继续分解 </li></ol><p>例如：<br>前序序列{1,2,4,7,3,5,6,8} = pre<br>中序序列{4,7,2,1,5,3,8,6} = in</p><ol><li>根据当前前序序列的第一个结点确定根结点，为 1 </li><li>找到 1 在中序遍历序列中的位置，为 in[3] </li><li>切割左右子树，则 in[3] 前面的为左子树， in[3] 后面的为右子树 </li><li>则切割后的<strong>左子树前序序列</strong>为：{2,4,7}，切割后的<strong>左子树中序序列</strong>为：{4,7,2}；切割后的<strong>右子树前序序列</strong>为：{3,5,6,8}，切割后的<strong>右子树中序序列</strong>为：{5,3,8,6} </li><li>对子树分别使用同样的方法分解</li></ol><h3 id="解答-9"><a href="#解答-9" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回构造的TreeNode根节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reConstructBinaryTree</span><span class="params">(self, pre, tin)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> len(pre) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        root = TreeNode(pre[<span class="number">0</span>]) <span class="comment"># 用pop会更巧妙</span></span><br><span class="line">        index = tin.index(root.val)</span><br><span class="line">        root.left = self.reConstructBinaryTree(pre[<span class="number">1</span>:index+<span class="number">1</span>],tin[:index])</span><br><span class="line">        root.right = self.reConstructBinaryTree(pre[index+<span class="number">1</span>:],tin[index+<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><h2 id="二叉树的下一个结点"><a href="#二叉树的下一个结点" class="headerlink" title="二叉树的下一个结点"></a>二叉树的下一个结点</h2><h3 id="题目描述-11"><a href="#题目描述-11" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。</p><h3 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h3><p>首先明确中序遍历的规则是：左根右，然后作图 </p><p><img src="https://i.loli.net/2020/03/07/G5ANcsvSECuUxP6.png" alt="773262_1514198075109_20151104234034251.png"></p><p>结合图，可分成两大类：</p><ul><li>有右子树的，那么下个结点就是右子树最左边的点；（eg：D，B，E，A，C，G）</li><li>没有右子树的，也可以分成两类：<ul><li>是父节点的左孩子（eg：N，I，L） ，那么父节点就是下一个节点 ； </li><li>是父节点的右孩子（eg：H，J，K，M），不断向根节点遍历，直到当前节点是其父节点的左孩子位置。如果没有（eg：M），那么他就是尾节点。 </li></ul></li></ul><h3 id="解答-10"><a href="#解答-10" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeLinkNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetNext</span><span class="params">(self, pNode)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> pNode.right: <span class="comment"># 有右子树，则找右子树最左边的点</span></span><br><span class="line">            p = pNode.right</span><br><span class="line">            <span class="keyword">while</span> p.left:</span><br><span class="line">                p = p.left</span><br><span class="line">            <span class="keyword">return</span> p</span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 无右子树</span></span><br><span class="line">            <span class="keyword">while</span> pNode.next: </span><br><span class="line">                <span class="keyword">if</span> (pNode.next.left == pNode): <span class="comment"># 若当前节点是父节点的左孩子</span></span><br><span class="line">                    <span class="keyword">return</span> pNode.next</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                pNode = pNode.next <span class="comment"># 沿着父节点向上遍历</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span> <span class="comment">#到了根节点仍没找到，则返回空</span></span><br></pre></td></tr></table></figure><h2 id="对称的二叉树"><a href="#对称的二叉树" class="headerlink" title="对称的二叉树"></a>对称的二叉树</h2><h3 id="题目描述-12"><a href="#题目描述-12" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。</p><h3 id="解答-11"><a href="#解答-11" class="headerlink" title="解答"></a>解答</h3><p>使用递归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetrical</span><span class="params">(self, pRoot)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pRoot:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.compare(pRoot.left,pRoot.right)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compare</span><span class="params">(self,pRoot1,pRoot2)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pRoot1 <span class="keyword">and</span> <span class="keyword">not</span> pRoot2:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pRoot1 <span class="keyword">or</span> <span class="keyword">not</span> pRoot2:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> pRoot1.val==pRoot2.val:</span><br><span class="line">            <span class="keyword">return</span> self.compare(pRoot1.left,pRoot2.right) <span class="keyword">and</span> self.compare(pRoot1.right,pRoot2.left)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h2 id="按之字形顺序打印二叉树"><a href="#按之字形顺序打印二叉树" class="headerlink" title="按之字形顺序打印二叉树"></a>按之字形顺序打印二叉树</h2><h3 id="题目描述-13"><a href="#题目描述-13" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。</p><h3 id="解答-12"><a href="#解答-12" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Print</span><span class="params">(self, pRoot)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pRoot:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        current = [pRoot] <span class="comment"># 当前层</span></span><br><span class="line">        next_layer = [] <span class="comment"># 当前层子节点</span></span><br><span class="line">        result = [] <span class="comment"># 结果值</span></span><br><span class="line">        count = <span class="number">1</span> <span class="comment"># 用于判断正序或者逆序输出</span></span><br><span class="line">        <span class="keyword">while</span> current:</span><br><span class="line">            <span class="comment"># 把当前层的子节点都添加到下一层</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> current:</span><br><span class="line">                <span class="keyword">if</span> i.left:</span><br><span class="line">                    next_layer.append(i.left)</span><br><span class="line">                <span class="keyword">if</span> i.right:</span><br><span class="line">                    next_layer.append(i.right)</span><br><span class="line">            <span class="comment"># 如果取余2存在，则正序打印，反之亦然</span></span><br><span class="line">            <span class="keyword">if</span> count%<span class="number">2</span>:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                result.append([i.val <span class="keyword">for</span> i <span class="keyword">in</span> current])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                result.append([i.val <span class="keyword">for</span> i <span class="keyword">in</span> current[::<span class="number">-1</span>]])</span><br><span class="line">            <span class="comment">#把下一层换成当前层</span></span><br><span class="line">            current,next_layer = next_layer,[]</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="把二叉树打印成多行"><a href="#把二叉树打印成多行" class="headerlink" title="把二叉树打印成多行"></a>把二叉树打印成多行</h2><h3 id="题目描述-14"><a href="#题目描述-14" class="headerlink" title="题目描述"></a>题目描述</h3><p>从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。</p><h3 id="解答-13"><a href="#解答-13" class="headerlink" title="解答"></a>解答</h3><p>比上题更简单，不需要对偶数行进行逆序，对上题代码稍加修改即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回二维列表[[1,2],[4,5]]</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Print</span><span class="params">(self, pRoot)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pRoot:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        current = [pRoot]</span><br><span class="line">        next_layer = []</span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">while</span> current:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> current:</span><br><span class="line">                <span class="keyword">if</span> i.left:</span><br><span class="line">                    next_layer.append(i.left)</span><br><span class="line">                <span class="keyword">if</span> i.right:</span><br><span class="line">                    next_layer.append(i.right)</span><br><span class="line">            result.append([i.val <span class="keyword">for</span> i <span class="keyword">in</span> current])</span><br><span class="line">            current, next_layer = next_layer, []</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="序列化二叉树"><a href="#序列化二叉树" class="headerlink" title="序列化二叉树"></a>序列化二叉树</h2><h3 id="题目描述-15"><a href="#题目描述-15" class="headerlink" title="题目描述"></a>题目描述</h3><p>请实现两个函数，分别用来序列化和反序列化二叉树</p><p>二叉树的序列化是指：把一棵二叉树按照某种遍历方式的结果以某种格式保存为字符串，从而使得内存中建立起来的二叉树可以持久保存。序列化可以基于先序、中序、后序、层序的二叉树遍历方式来进行修改，序列化的结果是一个字符串，序列化时通过某种符号表示空节点（#），以 ！表示一个结点值的结束（value!）。</p><p>二叉树的反序列化是指：根据某种遍历顺序得到的序列化字符串结果str，重构二叉树。</p><h3 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h3><p><strong>二叉树的序列化</strong>：将二叉树转换成字符串</p><p><strong>二叉树的反序列化</strong>：通过字符串还原一棵二叉树，返回树的头节点</p><p>例如下面这颗树：</p><p><img src="https://i.loli.net/2020/03/07/xvbok3r1NK7zlSD.png" alt="未命名1583549530.png" style="zoom:67%;"></p><p>先序序列化二叉树：</p><p>上面这棵树的先序序列化结果为：<strong>5!3!2!1!#!#!#!4!#!#!8!7!6!#!#!#!10!9!#!#!11!#!#!</strong> </p><p><img src="https://i.loli.net/2020/03/07/dg17canVFS6EtXo.png" alt="未命名1583549545.png" style="zoom:67%;"></p><p> 从上图中我们可以看出在<strong>节点为空</strong>的位置使用”<strong>#!</strong>“来代替,每个<strong>节点后的数值</strong>都添加一个”<strong>!</strong>“ </p><h3 id="解答-14"><a href="#解答-14" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    ss = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Serialize</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">'#!'</span></span><br><span class="line">        <span class="keyword">return</span> str(root.val) +<span class="string">'!'</span>+ self.Serialize(root.left) + self.Serialize(root.right)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Deserialize</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(s)==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> s[<span class="number">0</span>] == <span class="string">'#'</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.ss = s.split(<span class="string">'!'</span>)</span><br><span class="line">        <span class="keyword">return</span> self.reconstruct()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self)</span>:</span></span><br><span class="line">        val = self.ss[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> val == <span class="string">'#'</span>:</span><br><span class="line">            self.ss = self.ss[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        val = int(val)</span><br><span class="line">        root = TreeNode(val)</span><br><span class="line">        self.ss = self.ss[<span class="number">1</span>:]</span><br><span class="line">        root.left = self.reconstruct()</span><br><span class="line">        root.right = self.reconstruct()</span><br><span class="line">        <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><p>(这里反序列化的原字符串没有’!’吗？)</p><h2 id="二叉搜索树的第k个结点"><a href="#二叉搜索树的第k个结点" class="headerlink" title="二叉搜索树的第k个结点"></a>二叉搜索树的第k个结点</h2><h3 id="题目描述-16"><a href="#题目描述-16" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一棵二叉搜索树，请找出其中的第k小的结点。例如，(5,3,7,2,4,6,8)中，按结点数值大小顺序第三小结点的值为4。</p><h3 id="思路-6"><a href="#思路-6" class="headerlink" title="思路"></a>思路</h3><p>二叉搜索树：</p><p>它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 它的左、右子树也分别为二叉排序树。 </p><p>中序遍历就是一个树从小到大的排列顺序，因此只需要求出中序遍历到第k个元素就是所需的节点</p><h3 id="解答-15"><a href="#解答-15" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回对应节点TreeNode</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">KthNode</span><span class="params">(self, pRoot, k)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        self.res=[]</span><br><span class="line">        self.mid(pRoot)</span><br><span class="line">        <span class="keyword">return</span> self.res[k<span class="number">-1</span>] <span class="keyword">if</span> <span class="number">0</span>&lt;k&lt;=len(self.res) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mid</span><span class="params">(self,root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self.mid(root.left)</span><br><span class="line">        self.res.append(root)</span><br><span class="line">        self.mid(root.right)</span><br></pre></td></tr></table></figure><h2 id="数据流中的中位数"><a href="#数据流中的中位数" class="headerlink" title="数据流中的中位数"></a>数据流中的中位数</h2><h3 id="题目描述-17"><a href="#题目描述-17" class="headerlink" title="题目描述"></a>题目描述</h3><p>如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。</p><h3 id="解答-16"><a href="#解答-16" class="headerlink" title="解答"></a>解答</h3><p>用一个数组存输入的所有数字，另一个数组存输入的所有数字排序的编号<br>求中位数时，找根据编号数组找中位数位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    arr = []</span><br><span class="line">    idx = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Insert</span><span class="params">(self, num)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(self.arr) == <span class="number">0</span>:</span><br><span class="line">            self.arr.append(num)</span><br><span class="line">            self.idx.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idx_tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.arr)):</span><br><span class="line">                <span class="keyword">if</span> self.arr[i] &lt; num:</span><br><span class="line">                    idx_tmp += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> self.arr[i] &gt; num:</span><br><span class="line">                    self.idx[i] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    idx_tmp = self.idx[i]</span><br><span class="line">            self.arr.append(num)</span><br><span class="line">            self.idx.append(idx_tmp)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetMedian</span><span class="params">(self, n=None)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        tmp = len(self.arr)/<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> len(self.arr)%<span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.idx)):</span><br><span class="line">                <span class="keyword">if</span> self.idx[i] == (tmp<span class="number">-1</span>):</span><br><span class="line">                    res += self.arr[i]</span><br><span class="line">                <span class="keyword">elif</span> self.idx[i] == tmp:</span><br><span class="line">                    res += self.arr[i]</span><br><span class="line">            res = res/<span class="number">2.0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.idx)):</span><br><span class="line">                <span class="keyword">if</span> self.idx[i] == tmp:</span><br><span class="line">                    res += self.arr[i]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h1 id="栈和队列"><a href="#栈和队列" class="headerlink" title="栈和队列"></a>栈和队列</h1><h2 id="用两个栈实现队列"><a href="#用两个栈实现队列" class="headerlink" title="用两个栈实现队列"></a>用两个栈实现队列</h2><h3 id="题目描述-18"><a href="#题目描述-18" class="headerlink" title="题目描述"></a>题目描述</h3><p>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。</p><h3 id="解答-17"><a href="#解答-17" class="headerlink" title="解答"></a>解答</h3><ul><li>栈A用来作入队列</li><li>栈B用来出队列，当栈B为空时，栈A全部出栈到栈B，栈B再出栈（即出队列）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.stackA = []</span><br><span class="line">        self.stackB = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        self.stackA.append(node)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># return xx</span></span><br><span class="line">        <span class="keyword">if</span> self.stackB: <span class="comment"># 如果栈B不为空</span></span><br><span class="line">            <span class="keyword">return</span> self.stackB.pop()</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> self.stackA: <span class="comment"># 如果两个都是空的</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment"># 如果栈B为空，栈A不为空</span></span><br><span class="line">            <span class="keyword">while</span> self.stackA:</span><br><span class="line">                self.stackB.append(self.stackA.pop())</span><br><span class="line">            <span class="keyword">return</span> self.stackB.pop()</span><br></pre></td></tr></table></figure><h2 id="滑动窗口的最大值"><a href="#滑动窗口的最大值" class="headerlink" title="滑动窗口的最大值"></a>滑动窗口的最大值</h2><h3 id="题目描述-19"><a href="#题目描述-19" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。</p><h3 id="解答-18"><a href="#解答-18" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxInWindows</span><span class="params">(self, num, size)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res, i = [], <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> size &gt; <span class="number">0</span> <span class="keyword">and</span> i + size - <span class="number">1</span> &lt; len(num):</span><br><span class="line">            res.append(max(num[i:i + size]))</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><h1 id="查找和排序"><a href="#查找和排序" class="headerlink" title="查找和排序"></a>查找和排序</h1><h2 id="旋转数组的最小数字"><a href="#旋转数组的最小数字" class="headerlink" title="旋转数组的最小数字"></a>旋转数组的最小数字</h2><h3 id="题目描述-20"><a href="#题目描述-20" class="headerlink" title="题目描述"></a>题目描述</h3><p>把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。<br>输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。<br>例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。<br>NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。</p><h3 id="思路-7"><a href="#思路-7" class="headerlink" title="思路"></a>思路</h3><p>二分查找变种，没有具体的值用来比较。那么用中间值和高低位进行比较，看处于递增还是递减序列，进行操作缩小范围。 </p><ul><li>处于递增：low上移</li><li>处于递减：high下移（如果是<code>high-1</code>，则可能会错过最小值，因为找的就是最小值）</li><li>其余情况：low++缩小范围</li></ul><p><img src="https://i.loli.net/2020/03/07/LlCRrYUq5Aeo8Qa.png" alt="807319133_1566217781994_E0D8DA4D79E73A35EB4365215E2F13BB.png"></p><p>特殊情况</p><p><img src="https://i.loli.net/2020/03/07/WbNYh9C2FAXErBo.png" alt="807319133_1566442034216_88777DA092B0D79C405886B923CA4344.png"></p><h3 id="解答-19"><a href="#解答-19" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minNumberInRotateArray</span><span class="params">(self, rotateArray)</span>:</span></span><br><span class="line">        <span class="comment"># write code here </span></span><br><span class="line">        low = <span class="number">0</span></span><br><span class="line">        high = len(rotateArray) - <span class="number">1</span></span><br><span class="line">        mid = low + (high - low) / <span class="number">2</span> <span class="comment">#Python2.7可以直接除得到整数，Python3需要加int()</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            <span class="keyword">if</span> rotateArray[low] &lt; rotateArray[high]:</span><br><span class="line">                <span class="keyword">return</span> rotateArray[low]</span><br><span class="line">            <span class="keyword">if</span> rotateArray[mid] &lt; rotateArray[high]:<span class="comment"># 位于递减数组</span></span><br><span class="line">                high = mid</span><br><span class="line">            <span class="keyword">elif</span> rotateArray[mid] &gt; rotateArray[low]: <span class="comment"># 位于递增数组</span></span><br><span class="line">                low = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                low += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> rotateArray[low]</span><br></pre></td></tr></table></figure><h1 id="递归和循环"><a href="#递归和循环" class="headerlink" title="递归和循环"></a>递归和循环</h1><h2 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h2><h3 id="题目描述-21"><a href="#题目描述-21" class="headerlink" title="题目描述"></a>题目描述</h3><p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。</p><p>n&lt;=39</p><h3 id="解答-20"><a href="#解答-20" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            a,b = b, a+b</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h2 id="跳台阶"><a href="#跳台阶" class="headerlink" title="跳台阶"></a>跳台阶</h2><h3 id="题目描述-22"><a href="#题目描述-22" class="headerlink" title="题目描述"></a>题目描述</h3><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p><h3 id="思路-8"><a href="#思路-8" class="headerlink" title="思路"></a>思路</h3><p>前提：只有一次一阶或者两阶的跳法</p><ol><li><p>通过实际的情况可以得出：只有一阶的时候f(1) = 1，只有两阶的时候f(2) = 2  </p></li><li><p>假定第一次跳的是一阶，那么剩下的是n-1个台阶，跳法是f(n-1)</p><p>假定第一次跳的是两阶，那么剩下的是n-2个台阶，跳法是f(n-2)  </p><p>可以得出总跳法为: f(n) = f(n-1) + f(n-2)  </p></li><li><p>可以发现最终得出的是一个斐波那契数列：</p><script type="math/tex; mode=display">f(n)=\left\{\begin{array}{**lr**} 1, &n=1\\2,&n=2\\f(n-1)+f(n-2),&n>2\end{array} \right.</script></li></ol><h3 id="解答-21"><a href="#解答-21" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jumpFloor</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        a, b = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(number<span class="number">-1</span>):</span><br><span class="line">            a, b = b, a+b</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p>（与上题的初始条件不同）</p><h2 id="变态跳台阶"><a href="#变态跳台阶" class="headerlink" title="变态跳台阶"></a>变态跳台阶</h2><h3 id="题目描述-23"><a href="#题目描述-23" class="headerlink" title="题目描述"></a>题目描述</h3><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><h3 id="思路-9"><a href="#思路-9" class="headerlink" title="思路"></a>思路</h3><p> 前提：n个台阶会有一次1,2,…,n阶的跳法 </p><ul><li><p>n = 1时，只有1种跳法，f(1) = 1（这里的f(n)代表的是n个台阶的跳法数）</p></li><li><p>n = 2时，会有两个跳的方式，一次1阶或者2阶，这回归到了问题(1)，f(2) = f(2-1) + f(2-2) </p></li><li><p>n = 3时，会有三种跳的方式：1阶、2阶、3阶， 那么就是第一次跳出1阶后面剩下：f(3-1)；第一次跳出2阶，剩下f(3-2)；第一次3阶，那么剩下f(3-3) 。因此，f(3) = f(3-1)+f(3-2)+f(3-3) </p></li><li><p>… </p></li><li><p>n = n时，会有n种跳的方式：1阶、2阶…n阶，得出结论： f(n) = f(n-1)+f(n-2)+…+f(n-(n-1)) + f(n-n) =&gt;  f(0) + f(1) + f(2) + f(3) + … + f(n-1) + f(n) = f(n-1) + f(n-2) + f(n-3) + … + f(n-(n-1)) + f(n-n) </p></li><li><p>可以继续简化： </p><p>f(n-1) = f(0) + f(1)+f(2)+f(3) + … + f((n-1)-1) =  f(0) + f(1) + f(2) + f(3) + … + f(n-2) </p><p>f(n) =[ f(0) + f(1) + f(2) + f(3) + … + f(n-2) ] +  f(n-1) = f(n-1) + f(n-1) = 2 * f(n-1) </p></li><li><p>得出最终结论，n阶台阶总的跳法为： </p><script type="math/tex; mode=display">f(n)=\left\{\begin{array}{**lr**} 1, &n=0\\1,&n=1\\2f(n-1),&n>1\end{array} \right.</script></li></ul><h3 id="解答-22"><a href="#解答-22" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jumpFloorII</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span>**(number<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><h2 id="矩形覆盖"><a href="#矩形覆盖" class="headerlink" title="矩形覆盖"></a>矩形覆盖</h2><h3 id="题目描述-24"><a href="#题目描述-24" class="headerlink" title="题目描述"></a>题目描述</h3><p>我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p><p>比如n=3时，2*3的矩形块有3种覆盖方法：</p><p> <img src="https://uploadfiles.nowcoder.com/images/20200218/6384065_1581999858239_64E40A35BE277D7E7C87D4DCF588BE84" alt="img"> </p><h3 id="思路-10"><a href="#思路-10" class="headerlink" title="思路"></a>思路</h3><p>依旧是斐波那契数列 </p><p>target*2为大矩阵的大小 </p><p>有以下几种情形： </p><ol><li><p>target &lt;= 0，大矩形为&lt;= 2*0，直接return 1； </p></li><li><p>target = 1，大矩形为2*1，只有一种摆放方法，return 1； </p></li><li><p>target = 2，大矩形为2*2，有两种摆放方法，return 2； </p></li><li><p>target = n，分为两步考虑： </p><p>第一次摆放一块 2*1 的小矩阵，则摆放方法总共为 f(target - 1)</p><p>第一次摆放一块 1*2 的小矩阵，则摆放方法总共为 f(target - 2)</p></li></ol><p>得到通项公式为</p><script type="math/tex; mode=display">f(n)=\left\{\begin{array}{**lr**} 1, &n=1\\2,&n=2\\f(n-1)+f(n-2),&n>2\end{array} \right.</script><h3 id="解答-23"><a href="#解答-23" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rectCover</span><span class="params">(self, number)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> number &lt;= <span class="number">0</span>: <span class="comment"># 考虑特殊情况</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        a, b = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(number<span class="number">-1</span>):</span><br><span class="line">            a, b = b,a+b</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h1 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h1><h2 id="二进制中1的个数"><a href="#二进制中1的个数" class="headerlink" title="二进制中1的个数"></a>二进制中1的个数</h2><h3 id="题目描述-25"><a href="#题目描述-25" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p><h3 id="思路-11"><a href="#思路-11" class="headerlink" title="思路"></a>思路</h3><p>如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。 </p><p>举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100&amp;1011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。</p><h3 id="解答-24"><a href="#解答-24" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">NumberOf1</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">0</span>:</span><br><span class="line">            n = n &amp; <span class="number">0xffffffff</span></span><br><span class="line">        <span class="keyword">while</span> n:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            n = (n - <span class="number">1</span>) &amp; n</span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure><p>【解释一下】n = n &amp; 0xffffffff，在Python中，数的大小是可以无限扩大的，所以对于一个负数而言，进行了这个操作，实际上是提取了负数的后32位（在计算机中以补码形式存在），前面的任意位呢，则变成了0，比如说 -1，一开始是 111…..(n个1)…11111111，进行与运算之后，得到，00….(n个0)….111111111（32个1），变成了含32个1的正数，然后就不用担心负数陷入死循环。 </p><h1 id="代码的完整性"><a href="#代码的完整性" class="headerlink" title="代码的完整性"></a>代码的完整性</h1><h2 id="数值的整数次方"><a href="#数值的整数次方" class="headerlink" title="数值的整数次方"></a>数值的整数次方</h2><h3 id="题目描述-26"><a href="#题目描述-26" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。</p><p>保证base和exponent不同时为0</p><h3 id="思路-12"><a href="#思路-12" class="headerlink" title="思路"></a>思路</h3><p>优化求幂函数：<br>当n为偶数，$a^n =a^{\frac{n}{2}}<em>a^{\frac{n}{2}}$<br>当n为奇数，$a^n = a^{\frac{n-1}{2}}</em>a^{\frac{n-1}{2}}* a$</p><h3 id="解答-25"><a href="#解答-25" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Power</span><span class="params">(self, base, exponent)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res = base</span><br><span class="line">        flag = <span class="number">1</span> <span class="comment"># 判断正负</span></span><br><span class="line">        <span class="keyword">if</span> exponent == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> exponent &lt; <span class="number">0</span>:</span><br><span class="line">            exponent *= <span class="number">-1</span> <span class="comment"># 将负数转化为正数</span></span><br><span class="line">            flag = <span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> exponent == <span class="number">1</span>:</span><br><span class="line">                res *= base</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            res *= res</span><br><span class="line">            exponent = exponent // <span class="number">2</span> <span class="comment"># 整除</span></span><br><span class="line">        <span class="keyword">return</span> res <span class="keyword">if</span> flag == <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> / res</span><br></pre></td></tr></table></figure><h2 id="调整数组顺序使奇数位于偶数前面"><a href="#调整数组顺序使奇数位于偶数前面" class="headerlink" title="调整数组顺序使奇数位于偶数前面"></a>调整数组顺序使奇数位于偶数前面</h2><h3 id="题目描述-27"><a href="#题目描述-27" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。</p><h3 id="解法-1"><a href="#解法-1" class="headerlink" title="解法"></a>解法</h3><p>方法1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reOrderArray</span><span class="params">(self, array)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        odd, even = [], []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> array:</span><br><span class="line">            odd.append(i) <span class="keyword">if</span> i%<span class="number">2</span> == <span class="number">1</span> <span class="keyword">else</span> even.append(i)</span><br><span class="line">        <span class="keyword">return</span> odd + even</span><br></pre></td></tr></table></figure><p>方法2（lambda）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reOrderArray</span><span class="params">(self, array)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">return</span> sorted(array, key = <span class="keyword">lambda</span> c:c%<span class="number">2</span>, reverse=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h1 id="代码的鲁棒性"><a href="#代码的鲁棒性" class="headerlink" title="代码的鲁棒性"></a>代码的鲁棒性</h1><h2 id="链表中倒数第k个结点"><a href="#链表中倒数第k个结点" class="headerlink" title="链表中倒数第k个结点"></a>链表中倒数第k个结点</h2><h3 id="题目描述-28"><a href="#题目描述-28" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个链表，输出该链表中倒数第k个结点。</p><h3 id="解答-26"><a href="#解答-26" class="headerlink" title="解答"></a>解答</h3><p>使用list遍历整个链表（Note：题目要求返回的是结点，非结点的值）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FindKthToTail</span><span class="params">(self, head, k)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">while</span> head:</span><br><span class="line">            res.append(head)</span><br><span class="line">            head = head.next</span><br><span class="line">        <span class="keyword">if</span> k &gt; len(res) <span class="keyword">or</span> k &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> res[-k]</span><br></pre></td></tr></table></figure><h2 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h2><h3 id="题目描述-29"><a href="#题目描述-29" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个链表，反转链表后，输出新链表的表头。</p><h3 id="思路-13"><a href="#思路-13" class="headerlink" title="思路"></a>思路</h3><p><img src="https://i.loli.net/2020/03/07/fXnZyFCOW3qE2Hr.png" alt="1583488259173.png"></p><p><img src="https://i.loli.net/2020/03/07/UftMyoHkPpsXhrw.png" alt="1583505544057.png"></p><h3 id="解答-27"><a href="#解答-27" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回ListNode</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ReverseList</span><span class="params">(self, pHead)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> pHead <span class="keyword">or</span> <span class="keyword">not</span> pHead.next:</span><br><span class="line">            <span class="keyword">return</span> pHead</span><br><span class="line">        pre = <span class="literal">None</span></span><br><span class="line">        cur = pHead</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            tmp = cur.next</span><br><span class="line">            cur.next = pre</span><br><span class="line">            pre = cur</span><br><span class="line">            cur = tmp</span><br><span class="line">        <span class="keyword">return</span> pre</span><br></pre></td></tr></table></figure><h2 id="合并两个排序的链表"><a href="#合并两个排序的链表" class="headerlink" title="合并两个排序的链表"></a>合并两个排序的链表</h2><h3 id="题目描述-30"><a href="#题目描述-30" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。</p><h3 id="思路-14"><a href="#思路-14" class="headerlink" title="思路"></a>思路</h3><p> <img src="https://uploadfiles.nowcoder.com/images/20170119/3111850_1484789893742_6903DA8DDE03E5B02CCB5F97FC3E53C2" alt="img" style="zoom:67%;"> </p><h3 id="解答-28"><a href="#解答-28" class="headerlink" title="解答"></a>解答</h3><p>方法1：非递归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回合并后列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(self, pHead1, pHead2)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        cur = ListNode(<span class="number">0</span>)</span><br><span class="line">        pHead = cur </span><br><span class="line">        <span class="keyword">while</span> pHead1 <span class="keyword">and</span> pHead2:</span><br><span class="line">            <span class="keyword">if</span> pHead1.val &lt; pHead2.val:</span><br><span class="line">                cur.next = pHead1</span><br><span class="line">                pHead1 = pHead1.next</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur.next = pHead2</span><br><span class="line">                pHead2 = pHead2.next</span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="keyword">if</span> pHead1:</span><br><span class="line">            cur.next = pHead1</span><br><span class="line">        <span class="keyword">elif</span> pHead2:</span><br><span class="line">            cur.next = pHead2</span><br><span class="line">        <span class="keyword">return</span> pHead.next <span class="comment"># pHead始终指向第一个节点</span></span><br></pre></td></tr></table></figure><p>方法2：递归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(self, pHead1, pHead2)</span>:</span></span><br><span class="line">    <span class="comment"># write code here</span></span><br><span class="line">    <span class="keyword">if</span> pHead1 <span class="keyword">and</span> pHead2:</span><br><span class="line">        <span class="keyword">if</span> pHead1.val &gt; pHead2.val:</span><br><span class="line">            pHead1, pHead2 = pHead2, pHead1</span><br><span class="line">        pHead1.next = self.Merge(pHead1.next, pHead2)</span><br><span class="line">    <span class="keyword">return</span> pHead1 <span class="keyword">or</span> pHead2</span><br></pre></td></tr></table></figure><h2 id="树的子结构"><a href="#树的子结构" class="headerlink" title="树的子结构"></a>树的子结构</h2><h3 id="题目描述-31"><a href="#题目描述-31" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构）</p><h3 id="解答-29"><a href="#解答-29" class="headerlink" title="解答"></a>解答</h3><p>使用前序遍历转化为字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">HasSubtree</span><span class="params">(self, pRoot1, pRoot2)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(p)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> p:</span><br><span class="line">                <span class="keyword">return</span> str(p.val) +  convert(p.left) + convert(p.right)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        <span class="keyword">return</span> convert(pRoot2) <span class="keyword">in</span> convert(pRoot1) <span class="keyword">if</span> pRoot2 <span class="keyword">else</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure><h1 id="面试思路"><a href="#面试思路" class="headerlink" title="面试思路"></a>面试思路</h1><h2 id="二叉树的镜像"><a href="#二叉树的镜像" class="headerlink" title="二叉树的镜像"></a>二叉树的镜像</h2><h3 id="题目描述-32"><a href="#题目描述-32" class="headerlink" title="题目描述"></a>题目描述</h3><p>操作给定的二叉树，将其变换为源二叉树的镜像。</p><h3 id="输入描述"><a href="#输入描述" class="headerlink" title="输入描述"></a>输入描述</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">二叉树的镜像定义：源二叉树 </span><br><span class="line">        8</span><br><span class="line">       /  \</span><br><span class="line">      6   10</span><br><span class="line">     / \  / \</span><br><span class="line">    5  7 9 11</span><br><span class="line">    镜像二叉树</span><br><span class="line">        8</span><br><span class="line">       /  \</span><br><span class="line">      10   6</span><br><span class="line">     / \  / \</span><br><span class="line">    11 9 7  5</span><br></pre></td></tr></table></figure><h3 id="解答-30"><a href="#解答-30" class="headerlink" title="解答"></a>解答</h3><p>采用递归的思想</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># 返回镜像树的根节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Mirror</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            root.left,root.right=self.Mirror(root.right),self.Mirror(root.left)</span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><h1 id="画图让抽象形象化"><a href="#画图让抽象形象化" class="headerlink" title="画图让抽象形象化"></a>画图让抽象形象化</h1><h2 id="顺时针打印矩阵"><a href="#顺时针打印矩阵" class="headerlink" title="顺时针打印矩阵"></a>顺时针打印矩阵</h2><h3 id="题目描述-33"><a href="#题目描述-33" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10.</p><h3 id="思路-15"><a href="#思路-15" class="headerlink" title="思路"></a>思路</h3><p>用旋转魔法的方式，一直取出第一行：</p><p>   例如  </p><p>   1 2 3  </p><p>   4 5 6  </p><p>   7 8 9  </p><p>   输出并删除第一行后，变为  </p><p>   4 5 6  </p><p>   7 8 9  </p><p>   再进行一次逆时针旋转，就变成：  </p><p>   6 9  </p><p>   5 8  </p><p>   4 7  </p><p>   继续重复上述操作即可。</p><h3 id="解答-31"><a href="#解答-31" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># matrix类型为二维列表，需要返回列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printMatrix</span><span class="params">(self, matrix)</span>:</span></span><br><span class="line">        s=[]</span><br><span class="line">        <span class="keyword">while</span> matrix:</span><br><span class="line">            s+=matrix[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">del</span> matrix[<span class="number">0</span>]</span><br><span class="line">            matrix=list(zip(*matrix))[::<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h1 id="举例让抽象具体化"><a href="#举例让抽象具体化" class="headerlink" title="举例让抽象具体化"></a>举例让抽象具体化</h1><h2 id="包含min函数的栈"><a href="#包含min函数的栈" class="headerlink" title="包含min函数的栈"></a>包含min函数的栈</h2><h3 id="题目描述-34"><a href="#题目描述-34" class="headerlink" title="题目描述"></a>题目描述</h3><p>定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O(1)）。</p><p>注意：保证测试中不会当栈为空的时候，对栈调用pop()或者min()或者top()方法。</p><h1 id="时间效率"><a href="#时间效率" class="headerlink" title="时间效率"></a>时间效率</h1><h2 id="数组中出现次数超过一半的数字"><a href="#数组中出现次数超过一半的数字" class="headerlink" title="数组中出现次数超过一半的数字"></a>数组中出现次数超过一半的数字</h2><h3 id="题目描述-35"><a href="#题目描述-35" class="headerlink" title="题目描述"></a>题目描述</h3><p> 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 </p><h3 id="解答-32"><a href="#解答-32" class="headerlink" title="解答"></a>解答</h3><p>用python的标准库collections </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">MoreThanHalfNum_Solution</span><span class="params">(self, numbers)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        c = Counter(numbers).most_common()</span><br><span class="line">        <span class="keyword">if</span> c[<span class="number">0</span>][<span class="number">1</span>] &gt; len(numbers)/<span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> c[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>此处使用了函数Counter().most_common()，使用方法如下</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试1：输入字符串</span></span><br><span class="line">str = <span class="string">"ashduioashfahuoif"</span></span><br><span class="line"></span><br><span class="line">Counter(str)</span><br><span class="line"><span class="comment">#Counter(&#123;'a': 3, 'h': 3, 's': 2, 'u': 2, 'i': 2, 'o': 2, 'f': 2, 'd': 1&#125;)</span></span><br><span class="line"></span><br><span class="line">Counter(str).most_common(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#[('a', 3), ('h', 3), ('s', 2)]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试2：输入list</span></span><br><span class="line">list_test = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">Counter(list_test).most_common(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#[(1, 4), (2, 3), (3, 2)]</span></span><br></pre></td></tr></table></figure><h2 id="最小的K个数"><a href="#最小的K个数" class="headerlink" title="最小的K个数"></a>最小的K个数</h2><h3 id="题目描述-36"><a href="#题目描述-36" class="headerlink" title="题目描述"></a>题目描述</h3><p> 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 </p><h3 id="解答-33"><a href="#解答-33" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetLeastNumbers_Solution</span><span class="params">(self, tinput, k)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> k &gt; len(tinput):</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        tinput.sort()</span><br><span class="line">        <span class="keyword">return</span> tinput[:k]</span><br></pre></td></tr></table></figure><blockquote><p>主要考察排序算法，常见排序算法整理如下（替换sort()）</p><p>待补充……</p></blockquote><h2 id="连续子数组的最大和"><a href="#连续子数组的最大和" class="headerlink" title="连续子数组的最大和"></a>连续子数组的最大和</h2><h3 id="题目描述-37"><a href="#题目描述-37" class="headerlink" title="题目描述"></a>题目描述</h3><p>HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) </p><h3 id="思路-16"><a href="#思路-16" class="headerlink" title="思路"></a>思路</h3><p>动态规划：</p><p>dp[i]表示以元素array[i]结尾的最大连续子数组和 </p><p>以[-2,-3,4,-1,-2,1,5,-3]为例，可以发现：</p><p>dp[0] = -2 </p><p>dp[1] = -3 </p><p>dp[2] = 4 </p><p>dp[3] = 3 </p><p>以此类推，会发现 </p><p>dp[i] = max(dp[i-1]+array[i], array[i])</p><h3 id="解答-34"><a href="#解答-34" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FindGreatestSumOfSubArray</span><span class="params">(self, array)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        x = [i <span class="keyword">for</span> i <span class="keyword">in</span> array]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(x)):</span><br><span class="line">            x[i] = max(array[i],array[i]+x[i<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">return</span> max(x)</span><br></pre></td></tr></table></figure><h2 id="整数中1出现的次数"><a href="#整数中1出现的次数" class="headerlink" title="整数中1出现的次数"></a>整数中1出现的次数</h2><h3 id="题目描述-38"><a href="#题目描述-38" class="headerlink" title="题目描述"></a>题目描述</h3><p>求出1-13的整数中1出现的次数，并算出100-1300的整数中1出现的次数？为此他特别数了一下1-13中包含1的数字有1、10、11、12、13，因此共出现6次，但是对于后面问题他就没辙了。ACMer希望你们帮帮他，并把问题更加普遍化，可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 </p><h3 id="解答-35"><a href="#解答-35" class="headerlink" title="解答"></a>解答</h3><p>暴力法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">NumberOf1Between1AndN_Solution</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            cnt += str(i).count(<span class="string">'1'</span>)</span><br><span class="line">        <span class="keyword">return</span> cnt</span><br></pre></td></tr></table></figure><h2 id="把数组排成最小的数"><a href="#把数组排成最小的数" class="headerlink" title="把数组排成最小的数"></a>把数组排成最小的数</h2><h3 id="题目描述-39"><a href="#题目描述-39" class="headerlink" title="题目描述"></a>题目描述</h3><p>输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 </p><h3 id="解答-36"><a href="#解答-36" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">PrintMinNumber</span><span class="params">(self, numbers)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> numbers:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        numbers = list(map(str,numbers))</span><br><span class="line">        numbers.sort(cmp = <span class="keyword">lambda</span> x,y: cmp(x+y,y+x))</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(numbers).lstrip(<span class="string">'0'</span>)</span><br></pre></td></tr></table></figure><h1 id="时间空间效率的平衡"><a href="#时间空间效率的平衡" class="headerlink" title="时间空间效率的平衡"></a>时间空间效率的平衡</h1><h2 id="丑数"><a href="#丑数" class="headerlink" title="丑数"></a>丑数</h2><h3 id="题目描述-40"><a href="#题目描述-40" class="headerlink" title="题目描述"></a>题目描述</h3><p>把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 </p><h3 id="解答-37"><a href="#解答-37" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GetUglyNumber_Solution</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        res=[<span class="number">2</span>**i*<span class="number">3</span>**j*<span class="number">5</span>**k <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">30</span>) <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">30</span>)]</span><br><span class="line">        res.sort()</span><br><span class="line">        <span class="keyword">return</span> res[index<span class="number">-1</span>] <span class="keyword">if</span> index <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="第一个只出现一次的字符"><a href="#第一个只出现一次的字符" class="headerlink" title="第一个只出现一次的字符"></a>第一个只出现一次的字符</h2><h3 id="题目描述-41"><a href="#题目描述-41" class="headerlink" title="题目描述"></a>题目描述</h3><p>在一个字符串（0&lt;=字符串长度&lt;=10000，全部由字母组成）中找到第一个只出现一次的字符，并返回它的位置，如果没有则返回 -1（需要区分大小写）. </p><h3 id="解答-38"><a href="#解答-38" class="headerlink" title="解答"></a>解答</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">FirstNotRepeatingChar</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> s==<span class="string">''</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> s.count(i)==<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> s.index(i)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h2 id="数组中的逆序对"><a href="#数组中的逆序对" class="headerlink" title="数组中的逆序对"></a>数组中的逆序对</h2><h3 id="题目描述-42"><a href="#题目描述-42" class="headerlink" title="题目描述"></a>题目描述</h3><p> 在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 </p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Coding </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列8——指数族分布</title>
      <link href="/2020/02/11/bai-ban-tui-dao-xi-lie-8-zhi-shu-zu-fen-bu/"/>
      <url>/2020/02/11/bai-ban-tui-dao-xi-lie-8-zhi-shu-zu-fen-bu/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>指数族分布包括：</p><p>正态分布、伯努利分布、二项分布、泊松分布、Beta分布、Gamma分布等</p><h2 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h2><script type="math/tex; mode=display">P(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))</script><p>其中，包含以下部分：</p><ul><li><p>$\eta$：参数向量</p></li><li><p>$A(\eta)$：对数配分函数log partition function</p><blockquote><p>partition function：配分函数</p><p>如$P(x|\theta)=\frac{1}{z}\hat{P}(x|\theta)$，其中分母$z=\int \hat{P}(x|\theta)dx$是归一化因子，也称配分函数</p><p>此处$P(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))=\frac{1}{\exp(A(\eta))}[h(x)\exp(\eta^T\phi(x))]$</p><p>可以认为$z=\exp(A(\eta))$，从而$A(\eta)=\log z$，从而称为对数配分函数</p></blockquote></li><li><p>$\phi(x)$：充分统计量</p><blockquote><p>充分统计量：对样本的加工。数学形式上，就是关于样本的一个函数，包括均值，方差等</p><p>该统计量可以完整地包含样本所包含的信息，可以起到压缩数据的作用。</p><p>比如对于正态分布来说，样本均值和样本方差所组成的向量即为充分统计量。</p></blockquote></li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li><p>充分统计量</p></li><li><p>共轭</p><p>对于$P(z|x)\propto P(x|z)P(z)$，先验$P(z)$与后验$P(z|x)$有相同的形式（如：同是Beta分布但参数不同）</p><p>好处在于：避免通过式$P(z|x)=\frac{P(x|z)P(z)}{\int_z{P(x|z)P(z)}}$去复杂地求分母</p></li><li><p>最大熵：因为先验分布未知，因此通过令熵最大的原则，假定等可能</p><p>如何确定先验$P(z)$：</p><ol><li>共轭（计算上的方便）</li><li>最大熵（无信息先验）</li><li>Jeffreys prior</li></ol></li></ul><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h3><ul><li>线性组合：$w^Tx$</li><li>link function：$(激活函数)^{-1}$</li><li>指数族分布：$y|x\sim 指数族分布$<ul><li>线性回归：$y|x\sim N(\mu,\Sigma)$</li><li>二分类模型：$y|x\sim  Bernoulli $</li><li>泊松回归：$y|x\sim Poisson$</li></ul></li></ul><h3 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h3><ul><li>无向图</li></ul><h3 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h3><p>……</p><h1 id="高斯分布的指数族形式"><a href="#高斯分布的指数族形式" class="headerlink" title="高斯分布的指数族形式"></a>高斯分布的指数族形式</h1><p>一维高斯分布</p><script type="math/tex; mode=display">\begin{align}P(x|\theta)&=\frac{1}{\sqrt{2\pi}\sigma}\exp \{-\frac{(x-\mu)^2}{2\sigma^2}\}\\&=\exp \{\log(2\pi\sigma^2)^{-\frac{1}{2}}\}·\exp\{-\frac{1}{2\sigma^2}(x^2-2\mu x)-\frac{\mu^2}{2\sigma^2}\}\\&=\exp\{\begin{pmatrix} \frac{\mu}{\sigma^2} & -\frac{1}{2\sigma^2} \end{pmatrix}\begin{pmatrix} x \\ x^2 \end{pmatrix}-[\frac{\mu^2}{2\sigma^2}+\frac{1}{2}\log(2\pi\sigma^2)]\}\\&=\exp(\eta^T\phi(x)-A(\eta))\end{align}</script><p>因此</p><script type="math/tex; mode=display">\eta=\begin{pmatrix} \eta_1 \\ \eta_2\end{pmatrix}=\begin{pmatrix} \frac{\mu}{\sigma^2} \\ -\frac{1}{2\sigma^2} \end{pmatrix} \quad \phi(x)=\begin{pmatrix} x \\ x^2\end{pmatrix}</script><p>从而</p><script type="math/tex; mode=display">\left\{ \begin{array}{**rcl**}\mu&=\sigma^2\eta_1\\\sigma^2&=-\frac{1}{2\eta_2}\end{array}\right.</script><script type="math/tex; mode=display">A(\eta)=-\frac{\eta_1^2}{4\eta_2^2}+\frac{1}{2}\log(-\frac{\pi}{\eta_2})</script><h1 id="对数配分函数与充分统计量的关系"><a href="#对数配分函数与充分统计量的关系" class="headerlink" title="对数配分函数与充分统计量的关系"></a>对数配分函数与充分统计量的关系</h1><script type="math/tex; mode=display">P(x|\eta)=h(x)\exp(\eta^T\phi(x)-A(\eta))\\=\frac{1}{\exp(A(\eta))}h(x)\exp(\eta^T\phi(x))</script><p>由于对于概率密度函数，对整个概率空间进行积分值为1，因此</p><script type="math/tex; mode=display">\exp(A(\eta))=\int h(x)\exp(\eta^T\phi(x))dx</script><p>两边同时对$\eta$进行求导，得到</p><script type="math/tex; mode=display">\exp(A(\eta))A'(\eta)=\int h(x)\exp(\eta^T\phi(x))\phi(x)dx\\\begin{align}A'(\eta)&=\frac{\int h(x)\exp(\eta^T\phi(x))\phi(x)dx}{\exp(A(\eta))}\\&=\int h(x)\exp(\eta^T\phi(x)-A(\eta))\phi(x)dx\\&=\int P(x|\eta)\phi(x)dx\\&=E_{P(x|\eta)}[\phi(x)]\end{align}</script><p>同理可得</p><script type="math/tex; mode=display">A''(\eta)=Var[\phi(x)]</script><p>由于$A’’(\eta)$大于0，因此$A(\eta)$是凸函数</p><blockquote><p>简单验证：</p><script type="math/tex; mode=display">E[\phi(x)]=\begin{pmatrix} E(x) \\ E(x^2)\end{pmatrix}</script><p>求$\frac{\partial A(\eta)}{\partial \eta_1}$验证其是否等于$\mu$：</p><script type="math/tex; mode=display">\frac{\partial A(\eta)}{\partial \eta_1}=-\frac{2\eta_1}{4\eta_2}=-\frac{\eta_1}{2\eta_2}=-\frac{\frac{\mu}{\sigma^2}}{-2\frac{1}{2\sigma^2}}=\mu</script><p>得证。</p></blockquote><h1 id="极大似然估计与充分统计量"><a href="#极大似然估计与充分统计量" class="headerlink" title="极大似然估计与充分统计量"></a>极大似然估计与充分统计量</h1><p>样本$D=\{x_1,x_2.\cdots,x_N\}$，参数$\eta$的极大似然估计为</p><script type="math/tex; mode=display">\begin{align}\eta_{MLE}&=\arg\max\log P(D|\eta)\\&=\arg\max\log \prod_{i=1}^{N}P(x_i|\eta)\\&=\arg\max\sum_{i=1}^{N}[\log h(x_i)+\eta^T\phi(x_i)-A(\eta)]\\&=\arg\max\sum_{i=1}^{N}[\eta^T\phi(x_i)-A(\eta)]\end{align}</script><p>为了求最大值，对其进行求导</p><script type="math/tex; mode=display">\frac{\partial}{\partial\eta}\sum_{i=1}^{N}[\eta^T\phi(x_i)-A(\eta)]=\sum_{i=1}^{N}[\phi(x_i)-A'(\eta)]=\sum_{i=1}^{N}\phi(x_i)-NA'(\eta)=0\\A'(\eta_{MLE})=\frac{1}{N}\sum_{i=1}^{N}\phi(x_i)</script><p>因此，已知$A(\eta)$，可以对其求导得到$A’(\eta)$，从而得到$\eta_{MLE}=A^{(-1)}{‘}(\eta)$</p><p>可以看出：不需要整个样本，只需要$\phi(x_i)$（充分统计量）就能得到$\eta_{MLE}$</p><h1 id="最大熵角度"><a href="#最大熵角度" class="headerlink" title="最大熵角度"></a>最大熵角度</h1><p>信息量：$-\log(p)$，与概率$p$成反比</p><p>熵：$E_{p(x)}[-\log(p)]=\int -p(x)\log p(x)dx=-\sum_{x}p(x)\log p(x)$</p><p>最大熵$\Longleftrightarrow$等可能（定量地表达等可能的概念）</p><p>假设$x$是离散的，熵$H(P)=-\sum_{x}p(x)\log p(x)$</p><p>若取值分布是未知的（没有任何已知的情况下），使熵取最大的求解问题为</p><script type="math/tex; mode=display">\max H(P)=\min\sum_{i=1}^{K}p_i\log p_i\\s.t. \sum_{i=1}^{K}p_i=1</script><script type="math/tex; mode=display">\hat{p}_i=\arg\max H(P)=\arg\min\sum_{i=1}^{K}p_i\log p_i</script><p>定义拉格朗日函数</p><script type="math/tex; mode=display">L(p,\lambda)=\sum_{i=1}^{K}p_i\log p_i+\lambda(1-\sum_{i=1}^{K}p_i)\\\frac{\part L}{\part p_i}=\log p_i+p_i\frac{1}{p_i}-\lambda=0\\\hat{p}_i=\exp(\lambda-1)</script><p>因此$\hat{p}_i$为常数，$\hat{p}_1=\hat{p}_2=\cdots=\hat{p}_K=\frac{1}{K}$，$p(x)$是均匀分布。即在没有任何已知的情况下，均匀分布可以使熵达到最大。</p><h2 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h2><p>样本$D=\{x_1,x_2.\cdots,x_N\}$</p><p>最大熵原理：在满足已知事实（约束）的情况下，使熵达到最大</p><p>经验分布：</p><script type="math/tex; mode=display">\hat{P}(X=x)=\hat{P}(x)=\frac{count(x)}{N}</script><p>（相当于频率视为概率？）</p><p>满足已知事实（即数据），但仅有数据难以形成约束，而经验分布又是对于数据的描述</p><script type="math/tex; mode=display">E_{\hat{p}}[f(x)]=\Delta</script><p>其中$f(x)$是任意关于$x$的函数向量，$f(x)=(f_1,f_2,\cdots,f_Q)^T$，因此满足已知事实可以转化为满足上式约束（为了定量地描述已知事实），求解可以表达为</p><script type="math/tex; mode=display">\min \sum_{x}p(x)\log p(x)\\s.t. \sum_{x}p(x)=1, E_{p}[f(x)]=E_{\hat{p}}[f(x)]=\Delta</script><p>拉格朗日函数定义为</p><script type="math/tex; mode=display">L(p,\lambda_0,\lambda)=\sum_{x}p(x)\log p(x)+\lambda_0(1-\sum_{x}p(x))+\lambda^T(\Delta-E_{p}[f(x)])</script><script type="math/tex; mode=display">\frac{\part L}{\part p(x)}=\sum_{x}[\log p(x)+1]-\sum_{x}\lambda_0-\sum_{x}\lambda^Tf(x)=0\\\log p(x)+1-\lambda_0-\lambda^Tf(x)=0\\\log p(x)=\lambda^Tf(x)+\lambda_0-1\\p(x)=\exp\{\lambda^Tf(x)-(1-\lambda_0)\}=\exp\{\eta^T\phi(x)-A(\eta)\}</script><p>其中$\lambda=\eta,f(x)=\phi(x),A(\eta)=1-\lambda_0$</p><p>因此$p(x)$满足指数族分布</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基础知识及调优</title>
      <link href="/2020/02/09/hive-ji-chu-zhi-shi-ji-diao-you/"/>
      <url>/2020/02/09/hive-ji-chu-zhi-shi-ji-diao-you/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考：尚硅谷大数据技术之Hive（<a href="https://www.bilibili.com/video/av65556024?from=search&amp;seid=1273196552526002153" target="_blank" rel="noopener">b站教学视频</a>）</p></blockquote><h1 id="Hive入门"><a href="#Hive入门" class="headerlink" title="Hive入门"></a>Hive入门</h1><h2 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h2><ul><li><p>概述：Hive是<strong>基于Hadoop</strong>的一个<strong>数据仓库工具</strong>，可以将结构化的数据文件映射为一张表，并提供<strong>类SQL</strong>查询功能</p></li><li><p>本质：<strong>将HiveQL转化为MapReduce程序</strong>，是一个分析引擎，相当于一个客户端</p><p>SQL与MapReduce的关系：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200209/QhHPHWrRFcws.png" alt="mark"></p></li><li><p>Hive是基于Hadoop的体现在：</p><ol><li>数据存储在HDFS上</li><li>数据底层实现用MapReduce（默认使用MR，也可以使用Spark）</li><li>执行程序运行在Yarn上</li></ol></li></ul><h2 id="Hive的优缺点"><a href="#Hive的优缺点" class="headerlink" title="Hive的优缺点"></a>Hive的优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>操作接口采用<strong>类SQL语法</strong>，<strong>提供快速开发的能力</strong>（简单、容易上手）。提供 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运行，使不熟悉MapReduce 的用户也能很方便地对数据进行查询、汇总、分析。</li><li>Hive优势在于处理大数据，因此Hive常用于数据分析，对实时性要求不高的场合。  </li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li><p><strong>Hive的HQL表达能力有限</strong>。迭代式算法无法表达，因此数据挖掘方面不擅长（需要不断地对结果进行迭代，但MR在迭代方面很慢）。</p></li><li><p><strong>Hive的效率比较低</strong>。Hive自动生成的MapReduce作业，通常情况下不够智能化；Hive调优（包含SQL代码调优、资源调优）比较困难，粒度较粗（依赖模板，无法像MR精细化管理）；Hive因为Hive的执行延迟比较高，对于处理小数据没有优势。</p></li></ol><h2 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h2><h3 id="查询语言"><a href="#查询语言" class="headerlink" title="查询语言"></a>查询语言</h3><p>由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。</p><blockquote><p><strong>Hive与SQL的区别</strong></p><p>Hive是一种基于Hadoop的数据仓库架构，定义了简单的类SQL查询语句（HQL），当输入HQL时，Hive会处理SQL将其转换为MapReduce。</p><p>Hive的表其实是HDFS的目录，Hive的数据对应目录下的文件。</p><p>SQL是一种查询语言的标准，Hive是基于Hadoop的数据仓库架构，提供了类SQL的查询接口。</p></blockquote><h3 id="数据存储位置"><a href="#数据存储位置" class="headerlink" title="数据存储位置"></a>数据存储位置</h3><ul><li>Hive是建立在 Hadoop 之上的，所有Hive 的数据都是存储在 HDFS 中的。</li><li>数据库可以将数据保存在块设备或者本地文件系统中。</li></ul><h3 id="数据更新"><a href="#数据更新" class="headerlink" title="数据更新"></a>数据更新</h3><ul><li>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。</li><li>数据库中的数据通常是需要经常进行修改的，需要实时地进行增删改查。</li></ul><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><ul><li>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。但由于 MapReduce 的引入，Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。</li><li>数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。</li></ul><h3 id="执行"><a href="#执行" class="headerlink" title="执行"></a>执行</h3><ul><li>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。</li><li>数据库通常有自己的执行引擎。</li></ul><h3 id="执行延迟"><a href="#执行延迟" class="headerlink" title="执行延迟"></a>执行延迟</h3><ul><li>Hive在查询数据的时候，由于没有索引需要扫描整个表，因此延迟较高。另外一个导致Hive执行延迟高的因素是 MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。</li><li>数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。</li></ul><h3 id="可扩展性"><a href="#可扩展性" class="headerlink" title="可扩展性"></a>可扩展性</h3><ul><li>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的。</li><li>数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。</li></ul><h3 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h3><ul><li>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据。</li><li>数据库可以支持的数据规模较小。</li></ul><h1 id="Hive数据类型"><a href="#Hive数据类型" class="headerlink" title="Hive数据类型"></a>Hive数据类型</h1><h2 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h2><div class="table-container"><table><thead><tr><th>Hive数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>2byte有符号整数</td><td>20</td></tr><tr><td><strong>INT</strong></td><td>4byte有符号整数</td><td>20</td></tr><tr><td><strong>BIGINT</strong></td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>布尔类型，true或者false</td><td>TRUE FALSE</td></tr><tr><td>FLOAT</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td><strong>DOUBLE</strong></td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td><strong>STRING </strong></td><td>字符系列。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td>字节数组</td></tr></tbody></table></div><h2 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h2><div class="table-container"><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,  ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table></div><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>Hive的原子数据类型是可以进行隐式转换的，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，</p><p>但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。</p><ul><li><p><strong>隐式类型转换规则如下</strong></p><ol><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。</li><li>TINYINT、SMALLINT、INT都可以转换为FLOAT。</li><li>BOOLEAN类型不可以转换为任何其它的类型。</li></ol></li><li><p><strong>可以使用CAST()操作显式地进行数据类型转换</strong></p><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；</p><p>如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式<strong>返回空值 NULL</strong>。</p></li></ul><h1 id="DDL数据定义"><a href="#DDL数据定义" class="headerlink" title="DDL数据定义"></a>DDL数据定义</h1><p>DDL(Data Definition Language)，数据定义语言</p><p>适用范围：对数据库中的某些对象（例如: database,table）进行管理（不会对具体的数据进行操作），如Create,Alter, Drop</p><ul><li><p><strong>DDL的操作对象</strong>： 包括数据库本身，以及数据库对象，如表、视图等等 </p></li><li><p><strong>DDL的主要语句</strong>： </p><ul><li>Create语句：可以创建数据库和数据库的一些对象。</li><li>Drop语句：可以删除数据表、索引、触发程序、条件约束以及数据表的权限等。</li><li>Alter语句：修改数据表定义及属性。</li></ul></li></ul><h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>默认存储路径是/user/hive/warehouse/*.db </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span> <span class="keyword">like</span> <span class="string">'db_hive*'</span>;</span><br><span class="line">desc database db_hive;</span><br><span class="line">desc database extended db_hive;  <span class="comment">--显示数据库详细信息</span></span><br></pre></td></tr></table></figure><h2 id="切换当前数据库"><a href="#切换当前数据库" class="headerlink" title="切换当前数据库"></a>切换当前数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> db_hive;</span><br></pre></td></tr></table></figure><h2 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h2><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。</p><p><strong>数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">database</span> db_hive <span class="keyword">set</span> dbproperties(<span class="string">'createtime'</span>=<span class="string">'20170830'</span>);</span><br></pre></td></tr></table></figure><h2 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [<span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name </span><br><span class="line">[(col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] </span><br><span class="line">[<span class="keyword">COMMENT</span> table_comment] <span class="comment">--为表和列添加注释</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [<span class="keyword">COMMENT</span> col_comment], ...)] <span class="comment">--创建分区表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...) <span class="comment">--创建分桶表</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span>|<span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS] <span class="comment">--不常用</span></span><br><span class="line">[<span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="built_in">char</span>] <span class="comment">--列分隔符</span></span><br><span class="line">[<span class="keyword">STORED</span> <span class="keyword">AS</span> file_format] <span class="comment">--指定存储文件类型</span></span><br><span class="line">[LOCATION hdfs_path] <span class="comment">--指定表在HDFS上的存储位置</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="内部表与外部表"><a href="#内部表与外部表" class="headerlink" title="内部表与外部表"></a>内部表与外部表</h3><p>默认创建的表都是内部表。<br>Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(如/user/hive/warehouse)所定义的目录的子目录下。<br>当我们删除一个内部表时，Hive也会删除这个表中数据。内部表不适合和其他工具共享数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;  <span class="comment">--以'\t'结尾的行格式分隔字段</span></span><br></pre></td></tr></table></figure><p><strong>被external修饰的为外部表（external table）</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">EXTERNAL</span> <span class="keyword">table</span> xxx.xxxx</span><br><span class="line">(</span><br><span class="line"><span class="keyword">id</span> <span class="keyword">string</span>,</span><br><span class="line"><span class="built_in">number</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>  <span class="comment">--以'\t'结尾的行格式分隔字段 </span></span><br><span class="line">location <span class="string">'/user/t2'</span>;</span><br></pre></td></tr></table></figure><p><strong>区别：</strong> </p><ol><li>内部表数据由Hive自身管理，外部表数据由HDFS管理 </li><li>内部表数据存储的位置默认是/user/hive/warehouse，会将数据移动到数据仓库指向的路径；外部表数据的存储位置由自己制定，仅记录数据所在的路径，不移动数据</li><li>删除内部表会直接删除元数据及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除，这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</li></ol><p><strong>相互转换：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'TRUE'</span>); <span class="comment">--转为外部表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">'EXTERNAL'</span>=<span class="string">'FALSE'</span>); <span class="comment">--转为内部表</span></span><br></pre></td></tr></table></figure><p>注意：(‘EXTERNAL’=’TRUE’)和(‘EXTERNAL’=’FALSE’)为固定写法，区分大小写！</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><p>分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，按分区键的列值存储在表目录的子目录中，针对的是<strong>数据的存储路径</strong>，提供一个隔离数据和优化查询的便利方式。</p><p>Hive中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集（通常会按照时间日/月进行分区）。</p><p>好处：<strong>可以更快地执行查询</strong>。使用分区列的名称创建一个子目录，当使用where子句进行查询时，<strong>只扫描特定子目录，而不是扫描整个表</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition   </span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">dname <span class="keyword">string</span>, </span><br><span class="line">loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建二级分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partition2</span><br><span class="line">(</span><br><span class="line">deptno <span class="built_in">int</span>, </span><br><span class="line">    dname <span class="keyword">string</span>, </span><br><span class="line">    loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>, <span class="keyword">day</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.dept_partition <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 增加（多个）分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201705'</span>) <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 删除分区</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">drop</span> <span class="keyword">partition</span> (<span class="keyword">month</span>=<span class="string">'201704'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看分区</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partitions</span> dept_partition;</span><br><span class="line"></span><br><span class="line"><span class="comment">--查看分区表结构</span></span><br><span class="line">desc formatted dept_partition;</span><br></pre></td></tr></table></figure><h2 id="分桶表"><a href="#分桶表" class="headerlink" title="分桶表"></a>分桶表</h2><p>将表中记录按分桶键的哈希值分散进多个文件中，针对的是<strong>数据文件</strong>，将数据集分解成更容易管理的若干部分</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分桶表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu_buck(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</span><br><span class="line">clustered <span class="keyword">by</span> (<span class="keyword">id</span>) </span><br><span class="line"><span class="keyword">into</span> <span class="number">4</span> buckets</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置参数（否则导入数据后不分桶）</span></span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing=<span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 导入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu_buck <span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> stu;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 使用分桶抽样查询（对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。）</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> stu_buck <span class="keyword">tablesample</span>(<span class="keyword">bucket</span> <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">4</span> <span class="keyword">on</span> <span class="keyword">id</span>);</span><br></pre></td></tr></table></figure><p>注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。</p><p>y必须是table总bucket数的倍数或者因子。hive根据y的大小，<strong>决定抽样的比例</strong>。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。</p><p>x表示<strong>从哪个bucket开始抽取</strong>，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。</p><p>注意：x的值必须小于等于y的值，否则报错</p><p>FAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck</p><blockquote><p><strong>分区与分桶的区别</strong></p><p>分区和分桶的区别除了存储的格式不同外，最主要的是作用：</p><ul><li>分区表：细化数据管理，缩小MapReduce程序需要<strong>扫描的数据量</strong>。</li><li>分桶表：<strong>提高join查询的效率</strong>，在一份数据会被经常用来做连接查询的时候建立分桶，分桶字段就是连接字段，从而<strong>提高采样的效率</strong>。</li></ul></blockquote><h2 id="修改表"><a href="#修改表" class="headerlink" title="修改表"></a>修改表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 重命名表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> old_table <span class="keyword">RENAME</span> <span class="keyword">TO</span> new_table;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 添加列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">add</span> <span class="keyword">columns</span>(deptdesc <span class="keyword">string</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 更新列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">change</span> <span class="keyword">column</span> dept_old dept_new <span class="built_in">int</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 替换列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> dept_partition <span class="keyword">replace</span> <span class="keyword">columns</span>(deptno <span class="keyword">string</span>, dname</span><br><span class="line"> <span class="keyword">string</span>, loc <span class="keyword">string</span>);</span><br></pre></td></tr></table></figure><h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> dept_partition;</span><br></pre></td></tr></table></figure><h1 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h1><p>DML(Data Manipulation Language，数据操控语言)</p><p>用于操作数据库对象中包含的数据，也就是说操作的单位是记录。</p><ul><li><p><strong>DML的操作对象：记录</strong></p></li><li><p><strong>DML的主要语句</strong>：</p><ul><li>Insert：向数据表张插入一条记录。</li><li>Delete：删除数据表中的一条或多条记录，也可以删除数据表中的所有记录，但操作对象仍是记录。</li><li>Update：用于修改已存在表中的记录的内容。</li></ul></li></ul><h2 id="数据导入"><a href="#数据导入" class="headerlink" title="数据导入"></a>数据导入</h2><ul><li><strong>方法1：使用load语句向表中装载数据</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> [<span class="keyword">local</span>] inpath <span class="string">'/opt/module/datas/student.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> student [<span class="keyword">partition</span> (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><ol><li>load data：表示加载数据</li><li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li><li>inpath：表示加载数据的路径</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>into table：表示加载到哪张表</li><li>student：表示具体的表</li><li>partition：表示上传到指定分区</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建一张空表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(<span class="keyword">id</span> <span class="keyword">string</span>, <span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 加载本地文件到hive</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/student.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.student;</span><br></pre></td></tr></table></figure><ul><li><strong>方法2：通过查询语句向表中插入数据（Insert）</strong> </li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建一张分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基本插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span>  student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201709'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'wangwu'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基本模式插入（根据单张表查询结果）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201708'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">from</span> student <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多插入模式（根据多张表查询结果）</span></span><br><span class="line">from student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201707'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span>=<span class="string">'201706'</span>)</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>, <span class="keyword">name</span> <span class="keyword">where</span> <span class="keyword">month</span>=<span class="string">'201709'</span>;</span><br></pre></td></tr></table></figure><ul><li><strong>方法3：创建表时通过Location指定加载数据路径</strong></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建表，并指定在hdfs上的位置</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">location <span class="string">'/user/hive/warehouse/student5'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 上传数据到上述hdfs的指定位置</span></span><br><span class="line">dfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;</span><br></pre></td></tr></table></figure><ul><li><p><strong>方法4：Import数据到指定Hive表中</strong>  </p><p>注意：先用export导出后，再将数据导入</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import table student2 partition(month='201709') from '/user/hive/warehouse/export/student';</span><br></pre></td></tr></table></figure><h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><ul><li><strong>方法1：Hive Shell 命令导出</strong>  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在linux环境下执行</span></span><br><span class="line">hive -e <span class="string">'select * from default.student;'</span> &gt;/opt/module/datas/<span class="built_in">export</span>/student4.txt</span><br></pre></td></tr></table></figure><ul><li><strong>方法2：Insert导出</strong>  </li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 三种导出方式：</span></span><br><span class="line"><span class="comment">-- 将查询的结果导出到本地</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/export/student'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将查询的结果格式化导出到本地</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/export/student1'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 将查询的结果导出到HDFS上(没有local)</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/user/atguigu/student2'</span></span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\t'</span> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><ul><li><strong>方法3：Hadoop命令导出到本地</strong>  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -get /user/hive/warehouse/student/month=201709/000000_0</span><br><span class="line">/opt/module/datas/export/student3.txt;</span><br></pre></td></tr></table></figure><ul><li><strong>方法4：Export导出到HDFS上</strong>  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export table default.student to &apos;/user/hive/warehouse/export/student&apos;;</span><br></pre></td></tr></table></figure><h2 id="清除表中数据"><a href="#清除表中数据" class="headerlink" title="清除表中数据"></a>清除表中数据</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><p>注意：Truncate只能删除管理表，不能删除外部表中数据</p><blockquote><p><strong>DROP、TRUNCATE和DELETE的区别</strong></p><ol><li><p>TRUNCATE和DELETE只删除数据，DROP则删除整个表（结构和数据）。</p></li><li><p>表和索引所占空间。当表被TRUNCATE后，这个表和索引所占用的空间会恢复到初始大小；DELETE操作不会减少表或索引所占用的空间；DROP语句将表所占用的空间全释放掉。</p></li><li><p>DELETE语句为DML，这个操作会被放到rollback segment中，事务提交后才生效。如果有相应的tigger，执行的时候将被触发；TRUNCATE、DROP是DDL，操作立即生效，原数据不放到rollback segment中，不能回滚。</p></li><li><p>在没有备份情况下，谨慎使用DROP与TRUNCATE。删除部分数据行采用DELETE时，要注意结合where来约束影响范围。删除整个表用DROP。若想保留表而将表中数据删除，用TRUNCATE即可实现。</p></li><li><p>TRUNCATE TABLE 表名速度快，而且效率高，因为TRUNCATE TABLE在功能上与不带 WHERE 子句的 DELETE 语句相同：二者均删除表中的全部行。但 TRUNCATE TABLE比 DELETE 速度快，且使用的系统和事务日志资源少。DELETE 语句每次删除一行，并在事务日志中为所删除的每行记录一项。TRUNCATE TABLE 通过释放存储表数据所用的数据页来删除数据，并且只在事务日志中记录页的释放。 </p></li></ol></blockquote><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">WITH</span> CommonTableExpression (, CommonTableExpression)*] </span><br><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> | <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">  | [<span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> col_list] [<span class="keyword">SORT</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">]</span><br><span class="line">[<span class="keyword">LIMIT</span> <span class="built_in">number</span>]</span><br></pre></td></tr></table></figure><h2 id="基本查询"><a href="#基本查询" class="headerlink" title="基本查询"></a>基本查询</h2><p>注意：</p><ol><li>SQL 语言大小写不敏感</li><li>SQL 可以写在一行或者多行</li><li>关键字不能被缩写也不能分行</li><li>各子句一般要分行写，使用缩进提高语句的可读性</li></ol><h3 id="算术运算符"><a href="#算术运算符" class="headerlink" title="算术运算符"></a>算术运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>A+B</td><td>A和B相加</td></tr><tr><td>A-B</td><td>A减去B</td></tr><tr><td>A*B</td><td>A和B相乘</td></tr><tr><td>A/B</td><td>A除以B</td></tr><tr><td>A%B</td><td>A对B取余</td></tr><tr><td>A&amp;B</td><td>A和B按位取与</td></tr><tr><td>A\</td><td>B</td><td>A和B按位取或</td></tr><tr><td>A^B</td><td>A和B按位取异或</td></tr><tr><td>~A</td><td>A按位取反</td></tr></tbody></table></div><h3 id="比较运算符"><a href="#比较运算符" class="headerlink" title="比较运算符"></a>比较运算符</h3><div class="table-container"><table><thead><tr><th>操作符</th><th>支持的数据类型</th><th>描述</th></tr></thead><tbody><tr><td>A=B</td><td>基本数据类型</td><td>如果A等于B则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;=&gt;B</td><td>基本数据类型</td><td>如果A和B都为NULL，则返回TRUE，其他的和等号(=)操作符的结果一致，如果任一为NULL则结果为NULL</td></tr><tr><td>A&lt;&gt;B, A!=B</td><td>基本数据类型</td><td>A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A&lt;(=)B</td><td>基本数据类型</td><td>A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE</td></tr><tr><td>A [NOT] BETWEEN B AND C</td><td>基本数据类型</td><td>如果A，B或者C任一为NULL，则结果为NULL。如果A的值<strong>大于等于</strong>B而且<strong>小于或等于</strong>C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A IS [NOT] NULL</td><td>所有数据类型</td><td>如果A等于NULL，则返回TRUE，反之返回FALSE</td></tr><tr><td>IN(数值1, 数值2)</td><td>所有数据类型</td><td>使用IN运算显示列表中的值</td></tr><tr><td>A [NOT] LIKE B</td><td>STRING 类型</td><td>B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。</td></tr><tr><td>A RLIKE B, A REGEXP B</td><td>STRING 类型</td><td>B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。</td></tr></tbody></table></div><h3 id="Like和RLike"><a href="#Like和RLike" class="headerlink" title="Like和RLike"></a>Like和RLike</h3><ol><li>使用LIKE运算选择类似的值。</li><li>选择条件可以包含字符或数字：% 代表零个或多个字符(任意个字符)，_ 代表一个字符。</li><li>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查找薪水中含有2的员工信息</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="keyword">RLIKE</span> <span class="string">'[2]'</span>;</span><br></pre></td></tr></table></figure><h2 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h2><h3 id="Having语句"><a href="#Having语句" class="headerlink" title="Having语句"></a>Having语句</h3><p>having与where不同点：</p><ol><li>where针对表中的列发挥作用，查询数据；having针对查询结果中的列发挥作用，筛选数据。</li><li>where后面不能写分组函数，而having后面可以使用分组函数。</li><li>having只用于group by分组统计语句。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 求每个部门的平均薪水大于2000的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="keyword">avg</span>(sal) avg_sal </span><br><span class="line"><span class="keyword">from</span> emp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> deptno </span><br><span class="line"><span class="keyword">having</span> avg_sal &gt; <span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><p>Hive支持通常的SQL JOIN语句，但是：</p><ul><li><p>只支持等值连接，<strong>不支持非等值连接</strong></p></li><li><p>连接谓词中<strong>不支持or</strong>，如</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno </span><br><span class="line"><span class="keyword">from</span> emp e </span><br><span class="line"><span class="keyword">join</span> dept d </span><br><span class="line"><span class="keyword">on</span> e.deptno= d.deptno <span class="keyword">or</span> e.ename=d.ename; <span class="comment">--错误</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="多种连接"><a href="#多种连接" class="headerlink" title="多种连接"></a>多种连接</h3><ol><li><p>内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。    </p></li><li><p>左（右）外连接：JOIN操作符左（右）边表中符合WHERE子句的所有记录将会被返回。</p></li><li><p>满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。</p></li></ol><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>注意：连接 n个表，至少需要n-1个连接条件。例如：连接三个表，至少需要两个连接条件。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1.创建位置表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> default.location</span><br><span class="line">(</span><br><span class="line">    loc <span class="built_in">int</span>,  </span><br><span class="line">    loc_name <span class="keyword">string</span>  </span><br><span class="line">)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.导入数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/opt/module/datas/location.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> default.location;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.多表连接查询</span></span><br><span class="line"><span class="keyword">SELECT</span> e.ename, d.deptno, l. loc_name</span><br><span class="line"><span class="keyword">FROM</span> emp e </span><br><span class="line"><span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> d.deptno = e.deptno </span><br><span class="line"><span class="keyword">JOIN</span> location l</span><br><span class="line"><span class="keyword">ON</span> d.loc = l.loc;</span><br></pre></td></tr></table></figure><p>大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。</p><p>注意：为什么不是表d和表l先进行连接操作呢？这是因为<strong>Hive总是按照从左到右的顺序执行的</strong>。</p><h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><p>笛卡尔集会在下面条件下产生：</p><ol><li>省略连接条件</li><li>连接条件无效</li><li>所有表中的所有行互相连接</li></ol><p><strong>在大型数据集上使用笛卡尔积会造成非常严重的生产事故！</strong></p><p>可使用以下选项进行限制（切换严格模式）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.mode=<span class="keyword">strict</span>;</span><br></pre></td></tr></table></figure><blockquote><p>严格模式：</p><p>防止用户执行那些可能意想不到的不好的影响的查询，开启严格模式可以禁止3种类型的查询。</p><ol><li><strong>对于分区表，分区表必须指定要查询的分区，否则不允许执行。</strong>换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</li><li><strong>对于使用了order by语句的查询，要求必须使用limit语句。</strong>因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。</li><li><strong>限制笛卡尔积的查询。</strong></li></ol></blockquote><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="全局排序（Order-By）"><a href="#全局排序（Order-By）" class="headerlink" title="全局排序（Order By）"></a>全局排序（Order By）</h3><p>Order By：全局排序，一个Reducer</p><ul><li><p>ASC（ascend）: 升序（默认）</p></li><li><p>DESC（descend）: 降序</p></li></ul><p>ORDER BY 子句在SELECT语句的<strong>结尾</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 可以按照别名排序：按照员工薪水的2倍降序排序</span></span><br><span class="line"><span class="keyword">select</span> ename, sal*<span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 多个列排序：按照部门和工资升序排序</span></span><br><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal;</span><br></pre></td></tr></table></figure><h3 id="每个MapReduce内部排序（Sort-By）"><a href="#每个MapReduce内部排序（Sort-By）" class="headerlink" title="每个MapReduce内部排序（Sort By）"></a>每个MapReduce内部排序（Sort By）</h3><p>Sort By：每个Reducer内部进行排序，对全局结果集来说不是排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 1.设置reduce个数(可以通过set mapreduce.job.reduces;查看设定的reduce个数)</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 2.根据部门编号降序查看员工信息</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 3.将查询结果导入到文件中（按照部门编号降序排序）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/sortby-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">sort</span> <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="分区排序（Distribute-By）"><a href="#分区排序（Distribute-By）" class="headerlink" title="分区排序（Distribute By）"></a>分区排序（Distribute By）</h3><p>Distribute By：类似MR中partition，进行分区，结合sort by使用。</p><p>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。</p><p>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 先按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces=<span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> <span class="keyword">directory</span> <span class="string">'/opt/module/datas/distribute-result'</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="Cluster-By"><a href="#Cluster-By" class="headerlink" title="Cluster By"></a>Cluster By</h3><p>当distribute by和sorts by字段相同时，可以使用cluster by方式。</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p><p>以下两种写法等价</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> emp <span class="keyword">distribute</span> <span class="keyword">by</span> deptno <span class="keyword">sort</span> <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><p>注意：按照部门编号分区，不一定就是固定的数值，可以是20号和30号部门分到一个分区里面去。</p><blockquote><p><strong>order by，sort by，distribute by，cluster by 的区别</strong></p><ul><li><p>order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，会导致当输入规模较大时，消耗较长的计算时间。 </p></li><li><p>sort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则<strong>sort by只会保证同一个reducer的输出有序，并不保证全局有序</strong>。sort by不同于order by，它不受hive.mapred.mode属性的影响。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序，即可得到全部结果。</p></li><li><p>distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by再为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。  </p><ul><li><p>Distribute by和sort by的使用场景：</p><p>Map输出的文件大小不均/Reduce输出文件大小不均/小文件过多/文件超大</p></li></ul><ul><li>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。 如果distribute by和sort by中所指定的列相同，可以缩写为cluster by该列以便同时指定两者所用的列。</li></ul></li></ul></blockquote><h2 id="其他查询"><a href="#其他查询" class="headerlink" title="其他查询"></a>其他查询</h2><h3 id="union和union-all"><a href="#union和union-all" class="headerlink" title="union和union all"></a>union和union all</h3><p>1、对<strong>重复</strong>结果的处理：union在进行表链接后会筛选掉重复的记录，union all不会去除重复记录。</p><p>2、对<strong>排序</strong>的处理：union将会按照字段的顺序进行排序；union all只是简单的将两个结果合并后就返回。</p><p>3、从<strong>效率</strong>上说，union all要比union快很多，所以，如果可以确认合并的两个结果集中不包含重复数据且不需要排序时的话，那么就使用union all。 </p><h3 id="count-count-1-和count-字段-的区别"><a href="#count-count-1-和count-字段-的区别" class="headerlink" title="count(*), count(1)和count(字段)的区别"></a>count(*), count(1)和count(字段)的区别</h3><ul><li><p><strong>count(1)和count(*)</strong>：都会对全表进行扫描，统计所有记录的条数，包括那些为null的记录，count(1)会比count(*)更快，查询结果是完全一致的。</p></li><li><p><strong>count(1) and count(字段)</strong>：</p><ul><li>count(1)会统计表中的所有的记录数，包含字段为null的记录</li><li>count(字段)会统计该字段在表中出现的次数，不统计字段为null的记录。</li></ul></li></ul><h1 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h1><h2 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h2><p>NVL：给值为NULL的数据赋值，它的格式是<code>NVL(string1, replace_with)</code>。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询：如果员工的comm为NULL，则用-1代替</span></span><br><span class="line"><span class="keyword">select</span> nvl(comm,<span class="number">-1</span>) <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询：如果员工的comm为NULL，则用领导id代替</span></span><br><span class="line"><span class="keyword">select</span> nvl(comm,mgr) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure><h2 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h2><p><code>CONCAT(string A/col, string B/col…)</code>：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p><code>CONCAT_WS(separator, str1, str2,...)</code>：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p><code>COLLECT_SET(col)</code>：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。（COLLECT_LIST类似，不去重）</p><p><strong>e.g. </strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>name</th><th>constellation</th><th>blood_type</th></tr></thead><tbody><tr><td>孙悟空</td><td>白羊座</td><td>A</td></tr><tr><td>大海</td><td>射手座</td><td>A</td></tr><tr><td>宋宋</td><td>白羊座</td><td>B</td></tr><tr><td>猪八戒</td><td>白羊座</td><td>A</td></tr><tr><td>凤姐</td><td>射手座</td><td>A</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    <span class="keyword">concat_ws</span>(<span class="string">','</span>, collect_set(t1.name)) <span class="keyword">name</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        <span class="keyword">name</span>,</span><br><span class="line">        <span class="keyword">concat</span>(constellation, <span class="string">","</span>, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th>base</th><th>name</th></tr></thead><tbody><tr><td>射手座,A</td><td>大海,凤姐</td></tr><tr><td>白羊座,A</td><td>孙悟空,猪八戒</td></tr><tr><td>白羊座,B</td><td>宋宋</td></tr></tbody></table></div><h2 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h2><p><code>EXPLODE(col)</code>：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：<code>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</code></p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p><strong>e.g.</strong></p><p>原数据：</p><div class="table-container"><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑,动作,科幻,剧情</td></tr><tr><td>《Lie  to me》</td><td>悬疑,警匪,动作,心理,剧情</td></tr><tr><td>《战狼2》</td><td>战争,动作,灾难</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">category</span>) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><p>结果：</p><div class="table-container"><table><thead><tr><th>movie</th><th>category</th></tr></thead><tbody><tr><td>《疑犯追踪》</td><td>悬疑</td></tr><tr><td>《疑犯追踪》</td><td>动作</td></tr><tr><td>《疑犯追踪》</td><td>科幻</td></tr><tr><td>《疑犯追踪》</td><td>剧情</td></tr><tr><td>《Lie to me》</td><td>悬疑</td></tr><tr><td>《Lie to me》</td><td>警匪</td></tr><tr><td>《Lie to me》</td><td>动作</td></tr><tr><td>《Lie to me》</td><td>心理</td></tr><tr><td>《Lie to me》</td><td>剧情</td></tr><tr><td>《战狼2》</td><td>战争</td></tr><tr><td>《战狼2》</td><td>动作</td></tr><tr><td>《战狼2》</td><td>灾难</td></tr></tbody></table></div><h2 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h2><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化</p><p>CURRENT ROW：当前行</p><p>n PRECEDING：往前n行数据</p><p>n FOLLOWING：往后n行数据</p><p>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点</p><p>LAG(col,n)：往前第n行数据</p><p>LEAD(col,n)：往后第n行数据</p><p>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。<strong>注意：n必须为int类型</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  orderdate,</span><br><span class="line">  <span class="keyword">cost</span>, </span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span>) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">and</span> <span class="keyword">current</span> <span class="keyword">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">PRECEDING</span> <span class="keyword">AND</span> <span class="number">1</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行 </span></span><br><span class="line">  <span class="keyword">sum</span>(<span class="keyword">cost</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="keyword">row</span> <span class="keyword">and</span> <span class="keyword">UNBOUNDED</span> <span class="keyword">FOLLOWING</span> ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行 </span></span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">  business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查看顾客上次购买时间</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  <span class="keyword">name</span>,</span><br><span class="line">  orderdate,</span><br><span class="line">  <span class="keyword">cost</span>, </span><br><span class="line">  lag(orderdate,<span class="number">1</span>,<span class="string">'1900-01-01'</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1,     </span><br><span class="line">  lag(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2 </span><br><span class="line"><span class="keyword">from</span> business;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询前20%时间的订单信息</span></span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">  * </span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span> </span><br><span class="line">    <span class="keyword">name</span>,orderdate,<span class="keyword">cost</span>,ntile(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">  <span class="keyword">from</span></span><br><span class="line">    business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> </span><br><span class="line">  sorted = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h2 id="rank函数"><a href="#rank函数" class="headerlink" title="rank函数"></a>rank函数</h2><p>RANK()：排序相同时会重复，总数不会变</p><p>DENSE_RANK()：排序相同时会重复，总数会减少</p><p>ROW_NUMBER()：会根据顺序计算</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line"><span class="keyword">name</span>,subject,score,</span><br><span class="line"><span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">rank</span>,</span><br><span class="line"><span class="keyword">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">dense_rank</span>,</span><br><span class="line">row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) row_number</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">score;</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>name</th><th>subject</th><th>score</th><th>rank</th><th>dense_rank</th><th>row_number</th></tr></thead><tbody><tr><td>宋宋</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>1</strong></td></tr><tr><td>大海</td><td>英语</td><td>84</td><td><strong>1</strong></td><td><strong>1</strong></td><td><strong>2</strong></td></tr><tr><td>婷婷</td><td>英语</td><td>78</td><td><strong>3</strong></td><td><strong>2</strong></td><td><strong>3</strong></td></tr><tr><td>孙悟空</td><td>英语</td><td>68</td><td><strong>4</strong></td><td><strong>3</strong></td><td><strong>4</strong></td></tr></tbody></table></div><h1 id="Hive调优"><a href="#Hive调优" class="headerlink" title="Hive调优"></a>Hive调优</h1><h2 id="Fetch抓取"><a href="#Fetch抓取" class="headerlink" title="Fetch抓取"></a>Fetch抓取</h2><p>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：<code>SELECT * FROM employees;</code>在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。</p><p>在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.fetch.task.conversion=more;</span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h2><p>有时Hive的输入数据量是非常小的。在这种情况下，<strong>为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多</strong>。对于大多数这种情况，<strong>Hive可以通过本地模式在单台机器上处理所有的任务</strong>。对于小数据集，执行时间可以明显被缩短。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启本地MR</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto=<span class="literal">true</span>;  </span><br><span class="line"></span><br><span class="line"><span class="comment">--设置local MR的最大输入数据量，当输入数据量小于这个值时采用local MR的方式，默认为134217728，即128M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto.inputbytes.max=<span class="number">50000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">from</span> emp <span class="keyword">limit</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure><h2 id="表的优化"><a href="#表的优化" class="headerlink" title="表的优化"></a>表的优化</h2><h3 id="小表、大表join"><a href="#小表、大表join" class="headerlink" title="小表、大表join"></a>小表、大表join</h3><p><strong>将key相对分散，并且数据量小的表放在join的左边</strong>，这样可以有效减少内存溢出错误发生的几率；</p><p>再进一步，可以使用MapJoin让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。</p><p><strong>实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。</strong></p><h3 id="大表join小表"><a href="#大表join小表" class="headerlink" title="大表join小表"></a>大表join小表</h3><h4 id="空KEY过滤"><a href="#空KEY过滤" class="headerlink" title="空KEY过滤"></a>空KEY过滤</h4><p>有时join超时是因为<strong>某些key对应的数据太多</strong>，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable </span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> (<span class="keyword">select</span> * <span class="keyword">from</span> nullidtable <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span> ) n  <span class="keyword">left</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> n.id = o.id; <span class="comment">-- 更快</span></span><br></pre></td></tr></table></figure><h4 id="空KEY转换"><a href="#空KEY转换" class="headerlink" title="空KEY转换"></a>空KEY转换</h4><p>有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以<strong>表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上</strong>。  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> jointable</span><br><span class="line"><span class="keyword">select</span> n.* <span class="keyword">from</span> nullidtable n <span class="keyword">full</span> <span class="keyword">join</span> ori o <span class="keyword">on</span> </span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> n.id <span class="keyword">is</span> <span class="literal">null</span> <span class="keyword">then</span> <span class="keyword">concat</span>(<span class="string">'hive'</span>, <span class="keyword">rand</span>()) <span class="keyword">else</span> n.id <span class="keyword">end</span> = o.id;</span><br></pre></td></tr></table></figure><h3 id="MapJoin"><a href="#MapJoin" class="headerlink" title="MapJoin"></a>MapJoin</h3><p>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。<strong>可以用MapJoin把小表全部加载到内存在map端进行join</strong>，避免reducer处理。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置自动选择Mapjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 大表小表的阈值设置（默认25M一下认为是小表）</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize=<span class="number">25000000</span>;</span><br></pre></td></tr></table></figure><blockquote><p><strong>MapJoin工作机制</strong></p><p>Hive的Join连接总是按照<strong>从前到后</strong>的顺序执行的。</p><p>当Hive执行Join时，需要选择哪个表被流式传输(steam)，哪个表被缓存(cache)。Hive将Join语句中最后一个表用于流式传输，因此我们需要确保这个流表在两者之间是最大的。</p><p>如果要在不同的key上Join更多的表，那么对于每个Join集，只需在on条件右侧指定较大的表。</p><p>将小表放在左边，大表放到join的右边，这样可以提高性能。更准确的说法：把重复关联键少的表放在join前面，做关联可以提高join的效率。写在关联左侧的表每有1条重复的关联键时底层就会多1次运算处理。</p></blockquote><h3 id="Group-By"><a href="#Group-By" class="headerlink" title="Group By"></a>Group By</h3><p>默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。</p><p>并不是所有的聚合操作都需要在Reduce端完成，<strong>很多聚合操作都可以先在Map端进行部分聚合</strong>，最后在Reduce端得出最终结果。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 是否在Map端进行聚合，默认为True</span></span><br><span class="line">hive.map.aggr = true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在Map端进行聚合操作的条目数目</span></span><br><span class="line">hive.groupby.mapaggr.checkinterval = 100000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 有数据倾斜的时候进行负载均衡（默认是false）</span></span><br><span class="line">hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure><p>当选项设定为 true，生成的查询计划会有两个MR Job。</p><p>第一个MR Job中，<strong>Map的输出结果会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的Reduce中</strong>，从而达到负载均衡的目的；</p><p>第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证<strong>相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。  </p><h3 id="Count-Distinct"><a href="#Count-Distinct" class="headerlink" title="Count(Distinct)"></a>Count(Distinct)</h3><p>数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用<strong>先GROUP BY再COUNT</strong>的方式替换  </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 原始</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">distinct</span> <span class="keyword">id</span>) <span class="keyword">from</span> bigtable;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 改进</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="keyword">id</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> bigtable <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) a;</span><br></pre></td></tr></table></figure><p>虽然会多用一个Job来完成，但在数据量大的情况下，绝对是值得的。</p><h3 id="动态分区"><a href="#动态分区" class="headerlink" title="动态分区"></a>动态分区</h3><p>对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启动态分区功能（默认true，开启）</span></span><br><span class="line">hive.exec.dynamic.partition=true</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）</span></span><br><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在所有执行MR的节点上，最大一共可以创建多少个动态分区。</span></span><br><span class="line">hive.exec.max.dynamic.partitions=1000</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。</span></span><br><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure><h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><p>数据倾斜：由于数据分布不均匀，造成数据大量的集中到一点，造成数据热点</p><p>主要表现：任务进度长时间维持在99%的附近，只有少量reduce子任务未完成，因为其处理的数据量和其他的 reduce 差异过大。 </p><p>数据倾斜的原因：</p><ul><li>key 分布不均匀</li><li>业务数据本身的特性（小表join大表）</li><li>建表考虑不周全</li><li>某些 HQL 语句本身就存在数据倾斜（count(distinct)，group by不和聚集函数搭配使用的时候）</li></ul><p><strong>目的：使map的输出数据更均匀的分布到reduce中去</strong></p><p><strong>在hive中产生数据倾斜的原因和解决方法：</strong></p><ul><li><strong>group by</strong><ul><li>使用Hive对数据做一些类型统计的时候遇到过<strong>某种类型的数据量特别多，而其他类型数据的数据量特别少</strong>。当按照类型进行group by的时候，会将相同的group by字段的reduce任务<strong>需要的数据拉取到同一个节点进行聚合</strong>，而当其中每一组的数据量过大时，会出现其他组的计算已经完成而这里还没计算完成，其他节点的一直等待这个节点的任务执行完成，所以会看到一直map 100% reduce 99%的情况。</li><li>解决方法：设置参数<code>set hive.map.aggr=true; set hive.groupby.skewindata=true;</code></li><li>原理：<code>set hive.map.aggr=true;</code>这个配置项代表<strong>是否在map端进行聚合</strong>。<code>set hive.groupby.skwindata=true;</code> 当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，<strong>Map 的输出结果集合会随机分布到Reduce中</strong>，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是<strong>相同的Group By Key有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的</strong>；第二个 MR Job 再根据预处理的数据结果按照 Group By Key分布到 Reduce 中（这个过程可以<strong>保证相同的Group By Key被分布到同一个Reduce中</strong>），最后完成最终的聚合操作。</li></ul></li><li><strong>map和reduce优化</strong></li><li>当出现小文件过多，需要合并小文件。可以通过<code>set hive.merge.mapfiles=true</code>来解决。<ul><li>单个文件大小稍稍大于配置的block块的大写，此时需要适当增加map的个数。解决方法：set mapred.map.tasks个数</li></ul></li><li>文件大小适中，但map端计算量非常大，如select id,count(*),sum(case when…),sum(case when…)…需要增加map个数。解决方法：set mapred.map.tasks个数，set mapred.reduce.tasks个数<ul><li>大表和小表join。解决方法：使用<strong>MapJoin</strong> 将小表加载到内存中（在Map阶段进行表之间的连接。而不需要进入到Reduce阶段才进行连接。这样就节省了在Shuffle阶段时要进行的大量数据传输。从而起到了优化作业的作用）。set hive.auto.convert.join=true;</li></ul></li><li><p><strong>count(distinct)</strong></p><ul><li>如果数据量非常大，执行如<code>select a, count(distinct b) from t group by a;</code>类型的SQL时，会出现数据倾斜的问题。</li><li>解决方法：使用sum…group by代替。如select a, sum(1) from (select a, b from t group by a, b) group by a;</li></ul></li><li><p><strong>遇到需要进行join的但是关联字段有数据为空</strong></p><ul><li>解决方法1：id为空的不参与关联</li><li>解决方法2：给空值分配随机的key值，其核心是将这些引起倾斜的值随机分发到Reduce</li></ul></li></ul><h3 id="合理设置Map数"><a href="#合理设置Map数" class="headerlink" title="合理设置Map数"></a>合理设置Map数</h3><ul><li><p>如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。因此需要<strong>减少map数</strong>。</p></li><li><p>比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。因此<strong>增加map数</strong>。</p></li><li><p>复杂文件增加Map数</p><ul><li>当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。</li><li>增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。</li></ul></li></ul><h3 id="合理设置Reduce数"><a href="#合理设置Reduce数" class="headerlink" title="合理设置Reduce数"></a>合理设置Reduce数</h3><ul><li><p>调整reduce个数方法一</p><ul><li>每个Reduce处理的数据量默认是256MB。<code>hive.exec.reducers.bytes.per.reducer=256000000</code></li><li>每个任务最大的reduce数默认为1009。<code>hive.exec.reducers.max=1009</code></li><li>计算reducer数的公式：N=min(参数2，总输入数据量/参数1)</li></ul></li><li><p>调整reduce个数方法二</p><ul><li>在hadoop的mapred-default.xml文件中修改</li><li>设置每个job的Reduce个数<code>set mapreduce.job.reduces = 15;</code></li></ul></li><li><p>reduce个数并不是越多越好</p><ul><li>过多的启动和初始化reduce也会消耗时间和资源；</li><li>另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</li><li>在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；</li></ul></li></ul><h3 id="小文件进行合并"><a href="#小文件进行合并" class="headerlink" title="小文件进行合并"></a>小文件进行合并</h3><p>在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。</p><p>set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</p><h3 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p><strong>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。</strong></p><p>不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。在系统资源比较空闲的时候才有优势。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel=<span class="literal">true</span>;       <span class="comment">-- 打开任务并行执行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number=<span class="number">16</span>; <span class="comment">-- 同一个sql允许最大并行度，默认为8。</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Hadoop </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列3——线性回归</title>
      <link href="/2020/02/07/bai-ban-tui-dao-xi-lie-3-xian-xing-hui-gui/"/>
      <url>/2020/02/07/bai-ban-tui-dao-xi-lie-3-xian-xing-hui-gui/</url>
      
        <content type="html"><![CDATA[<ul><li>最小二乘法（矩阵表达，几何意义）</li><li>概率角度：最小二乘法$\Longleftrightarrow$noise为正态分布的MLE</li><li>正则化（L1:Lasso；L2：Ridge）</li></ul><h1 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h1><p>Data：$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in R,i=1,\cdots,N$</p><p>$X_{N<em>p}=(x_1,x_2,…,x_N)^T,Y_{N</em>1}=(y_1,y_2,…,y_N)^T$</p><p>定义最小二乘法的损失函数</p><script type="math/tex; mode=display">\begin{align}L(w)&=\sum_{i=1}^{N}(w^Tx_i-y_i)^2\\&=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\\&=[w^T(x_1,\cdots,x_N)-(y_1,\cdots,y_N)][w^T(x_1,\cdots,x_N)-(y_1,\cdots,y_N)]^T\\&=(w^TX^T-Y^T)(Xw-Y)\\&=w^TX^TXw-w^TX^TY-Y^TXw+Y^TY\\&=w^TX^TXw-2w^TX^TY+Y^TY\end{align}</script><p>求解参数$w$</p><script type="math/tex; mode=display">\hat{w}=\arg\min L(w)\\\frac{\partial L(w)}{\partial w}=2X^TXw-2X^TY=0\\\Rightarrow w=(X^TX)^{-1}X^TY</script><h2 id="几何意义"><a href="#几何意义" class="headerlink" title="几何意义"></a>几何意义</h2><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200208/UJ34aJcsDTUb.jpg" alt="mark"></p><h2 id="概率角度"><a href="#概率角度" class="headerlink" title="概率角度"></a>概率角度</h2><p>假定</p><script type="math/tex; mode=display">\varepsilon \sim N(0,\sigma^2)\\y=f(w)+\varepsilon=w^Tx+\varepsilon\\y|x;w\sim N(w^Tx,\sigma^2)\\P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\frac{-(y_i-w^Tx_i)^2}{2\sigma^2}}</script><p>定义对数似然函数为</p><script type="math/tex; mode=display">l(w)=\log P(Y|X;w)=\log \prod_{i=1}^{N}P(y_i|x_i;w)=\sum_{i=1}^{N}\log P(y_i|x_i;w)\\=\sum_{i=1}^{N}[\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}]</script><script type="math/tex; mode=display">\begin{align}\hat{w}&=\arg\max \limits_{w}l(w)\\&=\arg\max \limits_{w}\sum_{i=1}^{N}[-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}]\\&=\arg\min \limits_{w}\sum_{i=1}^{N}(y_i-w^Tx_i)^2\end{align}</script><p>可以得到极大似然估计（noise服从正态分布的条件下）与最小二乘估计等价</p><p>即LSE $\Longleftrightarrow$ MLE​(noise is Gaussian Dist)</p><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>过拟合解决方法：</p><ol><li>加数据</li><li>特征选择/特征提取（如PCA）</li><li>正则化（对参数空间的约束）</li></ol><h2 id="正则化框架"><a href="#正则化框架" class="headerlink" title="正则化框架"></a>正则化框架</h2><script type="math/tex; mode=display">\arg\min \limits_{w}J(w)=\arg\min \limits_{w}[L(w)+\lambda P(w)]</script><p>其中$L(w)$代表损失函数，$P(w)$代表penalty</p><ul><li>L1：Lasso，$P(w)=||w||$</li><li>L2：Ridge岭回归（权值衰减），$P(w)=w^Tw$</li></ul><h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>令</p><script type="math/tex; mode=display">\begin{align}J(w)&=L(w)+\lambda P(w)\\&=\sum_{i=1}^{N}||w^Tx_i-y_i||^2+\lambda w^Tw\\&=(w^TX^T-Y^T)(XW-Y)+\lambda w^Tw\\&=w^TX^TXW-w^TX^TY-Y^TXW+Y^TY+\lambda w^Tw\\&=w^TX^TXW-2w^TX^TY+Y^TY+\lambda w^Tw\\&=w^T(X^TX+\lambda I)W-2w^TX^TY+Y^TY\end{align}</script><script type="math/tex; mode=display">\hat{w}=\arg\max \limits_{w}J(w)\\\frac{\partial J(w)}{\partial w}=2(X^TX+\lambda I)W-2X^TY=0\\\hat{w}=(X^TX+\lambda I)^{-1}X^TY</script><p>$X^TX$是半正定矩阵，从而保证$X^TX+\lambda I$一定是一个可逆矩阵，从而一定程度抑制过拟合</p><h2 id="贝叶斯角度"><a href="#贝叶斯角度" class="headerlink" title="贝叶斯角度"></a>贝叶斯角度</h2><script type="math/tex; mode=display">w \sim N(0,\sigma_0^2)\\P(w|y)=\frac{P(y|w)P(w)}{P(y)}</script><p>从MAP（最大后验估计）来估计$w$：</p><script type="math/tex; mode=display">\hat{w}=\arg\max \limits_{w}P(w|y)=\arg\max \limits_{w}P(y|w)P(w)</script><p>其中，由于$y|x;w\sim N(w^Tx,\sigma^2)$（条件同最小二乘法-概率角度，即noise服从$N(0,\sigma^2)$），则</p><script type="math/tex; mode=display">P(y|x;w)=\frac{1}{\sqrt{2\pi}\sigma}\exp{\frac{-(y_i-w^Tx_i)^2}{2\sigma^2}}</script><p>又因为$w \sim N(0,\sigma_0^2)$，则</p><script type="math/tex; mode=display">P(w)=\frac{1}{\sqrt{2\pi}\sigma_0}\exp{\frac{-||w||^2}{2\sigma^2_0}}</script><p>从而（为简化过程省略求和符号）</p><script type="math/tex; mode=display">\begin{align}\hat{w}&=\arg\max \limits_{w}P(y|w)P(w)\\&=\arg\max \limits_{w}\log [P(y|w)P(w)]\\&=\arg\max \limits_{w}\log\exp[-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}-\frac{||w||^2}{2\sigma^2_0}]\\&=\arg\min \limits_{w}[\frac{(y_i-w^Tx_i)^2}{2\sigma^2}+\frac{||w||^2}{2\sigma^2_0}]\\&=\arg\min \limits_{w}[(y_i-w^Tx_i)^2+\frac{\sigma^2}{\sigma^2_0}||w||^2]\end{align}</script><p>因此最终的MAP估计为</p><script type="math/tex; mode=display">\hat{w}=\arg\min \limits_{w}[\sum_{i=1}^{N}(y_i-w^Tx_i)^2+\frac{\sigma^2}{\sigma^2_0}||w||^2]</script><p>因此加上正则化的最小二乘估计 $\Longleftrightarrow$ MAP(noise服从高斯分布，先验也服从高斯分布)</p><blockquote><p>总结：</p><p>LSE $\Longleftrightarrow$ MLE(noise is Gaussian Dist)</p><p>Regularized LSE $\Longleftrightarrow$ MAP(noise is Gaussian Dist, prior is Gaussian Dist)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
            <tag> Regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列6——支持向量机SVM</title>
      <link href="/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/"/>
      <url>/2020/02/06/bai-ban-tui-dao-xi-lie-6-zhi-chi-xiang-liang-ji-svm/</url>
      
        <content type="html"><![CDATA[<blockquote><p>b站up主： <strong>shuhuai008</strong> </p><p><a href="https://www.bilibili.com/video/av70839977" target="_blank" rel="noopener">机器学习-白板推导系列-合集</a> 学习笔记</p></blockquote><p>SVM有三宝：间隔，对偶，核技巧~</p><ul><li>hard-margin SVM</li><li>soft-margin SVM</li><li>kernel SVM</li></ul><h1 id="硬间隔SVM"><a href="#硬间隔SVM" class="headerlink" title="硬间隔SVM"></a>硬间隔SVM</h1><h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><p>找到一个超平面$w^Tx+b=0$，使得两类能够完全分开，且间隔最大。</p><p>Data：$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{+1,-1\}$</p><p>最大间隔分类器</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max margin(w,b)\\s.t.y_i(w^Tx+b)>0,\forall i=1,…,N\end{array} \right.</script><p>其中定义$N$个样本点到直线的最小距离为</p><script type="math/tex; mode=display">margin(w,b)=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}distance=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}\frac{1}{||w||}|w^Tx_i+b|=\mathop{\min}\limits_{w,b,x_i\\i=1,\cdots,N}\frac{1}{||w||}y_i(w^Tx_i+b)</script><blockquote><p>上式用到了点到直线的距离公式</p><script type="math/tex; mode=display">distance=\frac{1}{||w||}|w^Tx_i+b|</script></blockquote><p>因此最大间隔分类器可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{w,b} \mathop{\min}\limits_{x_i,i=1,\cdots,N}\frac{1}{||w||}y_i(w^Tx_i+b) \\s.t.y_i(w^Tx+b)>0,\forall i=1,…,N\end{array} \right.</script><p>可以转化为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{w,b} \frac{1}{||w||}\mathop{\min}\limits_{x_i,i=1,\cdots,N}y_i(w^Tx_i+b)\\\exists \gamma>0,s.t.\mathop{\min}\limits_{x_i,y_i,i=1,\cdots,N}y_i(w^Tx_i+b)=\gamma\end{array} \right.</script><p>可以令$\gamma=1$（无论$\gamma$为多少都可以进行缩放至1）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \frac{1}{2}w^Tw\\s.t.y_i(w^Tx_i+b)\ge1,\forall i=1,…,N\end{array} \right.</script><p>（使用$\frac{1}{2}$作为系数仅为了求导方便）</p><p>可以看出该模型是凸二次规划问题，有$N$个约束。</p><h2 id="模型求解"><a href="#模型求解" class="headerlink" title="模型求解"></a>模型求解</h2><h3 id="原问题primal-problem"><a href="#原问题primal-problem" class="headerlink" title="原问题primal problem"></a>原问题primal problem</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \frac{1}{2}w^Tw\\s.t. 1-y_i(w^Tx_i+b)\le0,\forall i=1,…,N\end{array} \right.</script><p>使用拉格朗日乘子法，定义拉格朗日函数为</p><script type="math/tex; mode=display">L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b))</script><p>其中$\lambda_i\ge0$，将求解问题转化为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)\\s.t. \lambda_i\ge0\end{array} \right.</script><p>从而将带约束的问题转化成无约束的（对$w,b$无约束）</p><blockquote><p><strong>Q：如何证明带约束的和无约束的是等价的？</strong></p><p>如果$1-y_i(w^Tx_i+b)&gt;0$，则$\max\limits_{\lambda } L(w,b,\lambda)=\frac{1}{2}w^Tw+\infty=\infty$</p><p>如果$1-y_i(w^Tx_i+b)\le0$，则$\max\limits_{\lambda } L(w,b,\lambda)=\frac{1}{2}w^Tw+0=\frac{1}{2}w^Tw$，$\min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)=\min\limits_{w,b}\frac{1}{2}w^Tw$</p><p>因此$\min\limits_{w,b} \max\limits_{\lambda } L(w,b,\lambda)=\min\limits_{w,b}\{\infty,\frac{1}{2}w^Tw\}=\min\limits_{w,b}\frac{1}{2}w^Tw $</p><p>且去除了$1-y_i(w^Tx_i+b)&gt;0$部分，即最优解一定满足$1-y_i(w^Tx_i+b)\le0$</p><p>（奇妙！！）</p></blockquote><h3 id="对偶问题dual-problem"><a href="#对偶问题dual-problem" class="headerlink" title="对偶问题dual problem"></a>对偶问题dual problem</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{\lambda }\min\limits_{w,b}  L(w,b,\lambda)\\s.t. \lambda_i\ge0\end{array} \right.</script><p>经过某种神秘的证明再结合本身成立的“凤尾$\ge$鸡头”弱对偶关系（$\min\max L\ge\max\min L$），可以证明该情况下（凸优化二次问题）满足强对偶关系，即弱对偶关系“$\ge$”等价于强对偶关系“$=$”（$\min\max L=\max\min L$），因此原问题等价于对偶问题。</p><p>对$b$求偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial b}=\frac{\partial }{\partial b}[\sum_{i=1}^{N}\lambda_i-\sum_{i=1}^{N}\lambda_iy_i(w^Tx_i+b)]=-\sum_{i=1}^{N}\lambda_iy_i=0\\\Rightarrow\sum_{i=1}^{N}\lambda_iy_i=0</script><p>代入到拉格朗日函数中，从而转化为</p><script type="math/tex; mode=display">L(w,b,\lambda)=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_i(w^Tx_i+b))=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)</script><p>再对$w$求偏导</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w}=\frac{\partial }{\partial w}[\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)]=w-\sum_{i=1}^{N}\lambda_iy_ix_i=0\\\Rightarrow w=\sum_{i=1}^{N}\lambda_iy_ix_i</script><p>再代入到拉格朗日函数中，从而转化为</p><script type="math/tex; mode=display">\begin{align}L(w,b,\lambda)&=\frac{1}{2}w^Tw+\sum_{i=1}^{N}\lambda_i(1-y_iw^Tx_i)\\&=\frac{1}{2}(\sum_{i=1}^{N}\lambda_iy_ix_i)^T(\sum_{j=1}^{N}\lambda_jy_jx_j)+\sum_{i=1}^{N}\lambda_i-\sum_{i=1}^{N}\lambda_iy_i(\sum_{i=1}^{N}\lambda_iy_ix_i)^Tx_i\\&=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i\lambda_jy_iy_jx_i^Tx_j+\sum_{i=1}^{N}\lambda_i\end{align}</script><p>最终的优化问题可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{\lambda } \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\lambda_i\lambda_jy_iy_jx_i^Tx_j-\sum_{i=1}^{N}\lambda_i\\s.t. \lambda_i\ge0,\sum_{i=1}^{N}\lambda_iy_i=0\end{array} \right.</script><h3 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h3><script type="math/tex; mode=display">\left\{\begin{array}{**lr**}  \frac{\partial L}{\partial w}=0,\frac{\partial L}{\partial b}=0,\frac{\partial L}{\partial \lambda}=0\\\lambda_i(1-y_i(w^Tx_i+b))=0\\\lambda_i \ge 0\\1-y_i(w^Tx_i+b)\le0\end{array} \right.</script><p>原对偶问题具有强对偶关系（默认情况下满足弱对偶关系且是凸二次规划问题且约束是线性的）$\Longleftrightarrow$满足KKT条件</p><blockquote><p>KKT条件（后有详细）</p><p>第一行：梯度条件</p><p>第二行$\lambda_i(1-y_i(w^Tx_i+b))=0$：松弛互补条件</p><p>第三四行：可行条件</p></blockquote><p>已经求出最优的$w$解为</p><script type="math/tex; mode=display">w^*=\sum_{i=1}^{N}\lambda_iy_ix_i</script><p>又因为松弛互补条件，一定存在一个样本点$(x_k,y_k)$使得$1-y_k(w^Tx_k+b)=0$</p><blockquote><p>若不存在，则只能所有$\lambda_i=0$，则目标函数$L(w,b,\lambda)=\frac{1}{2}w^Tw$，没有任何限制</p><p>对于少数$\lambda_i\neq 0$的样本称为支持向量 ，实际上只有这些样本在真正起作用</p></blockquote><p>对其进行化简得到$b^*$</p><script type="math/tex; mode=display">y_k(w^Tx_k+b)=1\\y_k^2(w^Tx_k+b)=y_k\\\Rightarrow b^*=y_k-w^Tx_k=y_k-\sum_{i=1}^{N}\lambda_iy_ix_i^Tx_k</script><p>因此决策超平面为</p><script type="math/tex; mode=display">f(x)=sign({w^*}^Tx+b^*)</script><h1 id="软间隔SVM"><a href="#软间隔SVM" class="headerlink" title="软间隔SVM"></a>软间隔SVM</h1><p>数据是不可分的，或数据是可分的但存在一定噪声，此时应使用软间隔SVM</p><p>soft：允许一点点错误，用loss来表达，加到损失函数上</p><ul><li>定义$loss=\sum_{i=1}^{N}I\{y_i(w^Tx_i+b)&lt;1\}$，但缺点在于该loss function不连续，不能求导，因此不使用该loss function</li><li>定义loss为距离（合页损失hinge loss），$loss=\sum_{i=1}^{N}\max\{0,1-y_i(w^Tx_i+b)\}$，函数是连续的√<ul><li>如果$y_i(w^Tx_i+b)\ge1,loss=0$</li><li>如果$y_i(w^Tx_i+b)&lt;1,loss=1-y_i(w^Tx_i+b)$</li></ul></li></ul><p>引入$\xi_i=1-y_i(w^Tx_i+b),\xi_i\ge0$，得到软间隔SVM的最终形式</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{\lambda } \frac{1}{2}w^Tw+C\sum_{i=1}^{N}\xi_i\\s.t. y_i(w^Tx_i+b)\ge1-\xi_i\end{array} \right.</script><h1 id="约束优化问题"><a href="#约束优化问题" class="headerlink" title="约束优化问题"></a>约束优化问题</h1><blockquote><p>这一节实际和SVM没有直接关系，是最优化的内容</p></blockquote><ul><li>原问题（Primal Problem）</li></ul><p>原问题的一般表达形式（原问题的有约束形式）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x\in R^p }f(x)\\s.t. &m_i(x)\le0, i=1,\cdots,M\\&n_j(x)\le0, j=1,\cdots,N\end{array} \right.</script><p>写成拉格朗日函数的形式</p><script type="math/tex; mode=display">L(x,\lambda,\eta)=f(x)+\sum_{i=1}^{M}\lambda_im_i(x)+\sum_{j=1}^{N}\eta_in_i(x)</script><p>可以将原问题转化为（原问题的无约束形式）</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x}\max\limits_{\lambda,\eta} L(x,\lambda,\eta)\\s.t. \lambda_i\ge0\end{array} \right.</script><blockquote><p>Q：为何两者是等价的？</p><p>如果$x_i$违反约束$m_i(x)\le0$，即$m_i(x)\gt0$，则$\max\limits_{\lambda} L\rightarrow\infty$</p><p>如果$x_i$符合约束$m_i(x)\le0$，则$\max\limits_{\lambda} L\nrightarrow\infty$</p><p>$\min\limits_{x}\max\limits_{\lambda} L=\min\limits_{x}\{\max\limits_{\lambda} L,\infty\}=\min\limits_{x}\max\limits_{\lambda}  L$</p><p>实际上是进行了过滤，其中蕴含了约束$m_i(x)\le0$，自动去掉了$m_i(x)\gt0$的部分</p></blockquote><ul><li>对偶问题（Dual Problem）</li></ul><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \max\limits_{\lambda,\eta} \min\limits_{x}L(x,\lambda,\eta)\\s.t. \lambda_i\ge0\end{array} \right.</script><blockquote><p>原问题是关于$x$的函数，对偶问题是关于$\lambda,\eta$的函数</p></blockquote><h2 id="弱对偶性"><a href="#弱对偶性" class="headerlink" title="弱对偶性"></a>弱对偶性</h2><p>弱对偶性：对偶问题(d)$\le$原问题(p)</p><p>之前使用了“凤尾$\ge$鸡头”的比喻来说明，此处在理论上进行证明：</p><script type="math/tex; mode=display">\max\limits_{\lambda,\eta} \min\limits_{x} L(x,\lambda,\eta)\le \min\limits_{x}\max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><p>证：</p><p>由于</p><script type="math/tex; mode=display">\min\limits_{x} L(x,\lambda,\eta) \le L(x,\lambda,\eta) \le \max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><blockquote><p>可以理解为下面只是变量而已，好比一个多元函数，在某处取最大值还是最小值时，这时你可以只看成关于不同变量的也是可以的，因为函数最大最小值是确定的</p></blockquote><p>设</p><script type="math/tex; mode=display">A(\lambda,\eta)=\min\limits_{x} L(x,\lambda,\eta) \\B(x)=\max\limits_{\lambda,\eta} L(x,\lambda,\eta)</script><p>则上式转化为</p><script type="math/tex; mode=display">A(\lambda,\eta) \le B(x)\\\Rightarrow A(\lambda,\eta) \le \min\limits_{x}  B(x)\\\Rightarrow \max\limits_{\lambda,\eta} A(\lambda,\eta) \le \min\limits_{x}  B(x)\\\Rightarrow \max\limits_{\lambda,\eta} \min\limits_{x} L(x,\lambda,\eta) \le \min\limits_{x}  \max\limits_{\lambda,\eta} L(x,\lambda,\eta)\\</script><p>从而得证。</p><h2 id="对偶性的几何解释"><a href="#对偶性的几何解释" class="headerlink" title="对偶性的几何解释"></a>对偶性的几何解释</h2><p>简化后的优化问题可以表达为</p><script type="math/tex; mode=display">\left\{\begin{array}{**lr**} \min\limits_{x\in R^p }f(x)\\s.t. m_i(x)\le0, i=1,\cdots,M\end{array} \right.</script><p>其中定义域$D=\mathrm{dom}f\cap \mathrm{dom} m_i$</p><p>拉格朗日函数定义为</p><script type="math/tex; mode=display">L(x,\lambda)=f(x)+\lambda m_1(x),\lambda\ge0</script><p>原问题最优解定义为</p><script type="math/tex; mode=display">p^*=\min f(x)</script><p>对偶问题最优解定义为</p><script type="math/tex; mode=display">d^*=\max\limits_{\lambda} \min\limits_{x} L(x,\lambda)</script><p>定义区域$G$为（一般化区域$G$认为其为非凸集）</p><script type="math/tex; mode=display">G=\{(m_1(x),f(x))|x\in D\}=\{(u,t)|x\in D\}</script><p>因此$p^*$可以表达为</p><script type="math/tex; mode=display">p^*=\inf\{t|(u,t)\in G,u\le 0\}</script><p>（集合中的下确界相当于集合中的最小值）</p><p>$d^*$可以表达为</p><script type="math/tex; mode=display">\begin{align}d^*&=\max\limits_{\lambda} \min\limits_{x} L(x,\lambda)\\&= \max\limits_{\lambda} \min\limits_{x} (t+\lambda u)\\&= \max\limits_{\lambda} g(\lambda)\end{align}</script><p>其中$g(\lambda)=\min\limits_{x} (t+\lambda u)=\inf\{t+\lambda u|(u,t)\in G\}$</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200207/dEyLlYLvko39.jpg" alt="mark"></p><h2 id="slater-condition"><a href="#slater-condition" class="headerlink" title="slater condition"></a>slater condition</h2><p>凸优化 + slater条件 $\Rightarrow$ 强对偶条件</p><p>slater condition定义：</p><script type="math/tex; mode=display">\exists \hat{x} \in reliant D\\s.t. \forall i=1,\cdots,M, m_i(\hat{x})<0</script><p>其中reliant为relative interior（相对内部），去除边界的部分，内点的集合</p><ol><li>对于大多数凸优化，slater条件成立</li><li>放松的slater条件：如果$M$中有$K$个仿射函数，只需要校验剩余的$M-K$个函数是否满足上述条件。（仿射函数：一阶的多项式函数）</li></ol><p>凸二次规划：目标函数$f$是凸函数，限制条件$m_i$是仿射函数（线性函数一定是仿射函数），$n_j$是仿射函数。</p><p><strong>SVM是凸二次规划问题，满足强对偶条件，因此可以使用KKT条件直接求解。</strong></p><h2 id="KKT条件-1"><a href="#KKT条件-1" class="headerlink" title="KKT条件"></a>KKT条件</h2><ul><li><p><strong>可行条件</strong></p><script type="math/tex; mode=display">m_i(x^{*})\le0, n_j(x^{*})=0, \lambda^{*} \ge0</script></li><li><p><strong>互补松弛条件</strong>：$\lambda^*_im_i=0, \forall i=1,\cdots,M$</p></li></ul><script type="math/tex; mode=display">\begin{align}d^*&=\max\limits_{\lambda,\eta}g(\lambda,\eta)=g(\lambda^*,\eta^*)\\&=\min\limits_{x}L(x,\lambda^*,\eta^*)\\&\le L(x^*,\lambda^*,\eta^*)\\&=f(x^*)+\sum_{i}\lambda^*_im_i+\sum_{j}\eta^*_jn_j\\&\le f(x^*)=p^*\end{align}</script><p>且又因为强对偶关系</p><script type="math/tex; mode=display">d^*=p^*</script><p>因此不等号只能取等号，且$\sum_{j}\eta^*_jn_j=0$，因此由(10)-(11)得</p><script type="math/tex; mode=display">\sum_{i}\lambda^*_im_i=0\\\Rightarrow \lambda^*_im_i=0, \forall i=1,\cdots,M</script><p>（若存在有一个$\lambda^*_im_i&lt;0$，由于不存在小于0的部分，无法抵消，和无法等于0）</p><ul><li><strong>梯度为0</strong>：<script type="math/tex; mode=display">\frac{\partial L(x,\lambda^{*},\eta^{*})}{\partial x}|_{x=x^{*}}=0</script>由(8)-(9)可得<script type="math/tex; mode=display">\min\limits_{x}L(x,\lambda^*,\eta^*)=L(x^*,\lambda^*,\eta^*)</script>因此<script type="math/tex; mode=display">\frac{\partial L}{\partial x}|_{x=x^*}=0</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree 决策树算法及实现</title>
      <link href="/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-shi-xian/"/>
      <url>/2020/02/05/decision-tree-jue-ce-shu-suan-fa-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="分类树理论"><a href="#分类树理论" class="headerlink" title="分类树理论"></a>分类树理论</h1><p>递归地选择最优特征，并根据该特征对训练数据进行分割，使得各个子数据集有一个最好的分类的过程。</p><ul><li><strong>信息熵</strong>：表示随机变量不确定性的度量</li></ul><script type="math/tex; mode=display">Info(D)=-\sum_{i=1}^{n}{p_ilog_2p_i}</script><p>​        其中$p_i$指样本集合$D$中第$i$类样本所占比例。</p><p>​        信息熵描述样本集合$D$携带的信息量。 信息量越大（值变化越多），则越不确定，越不容易被预测。</p><ul><li><strong>信息熵特点</strong>： <ol><li>不同类别的概率分布越均匀，信息熵越大</li><li>类别个数越多，信息熵越大</li><li>信息熵越大，越不容易被预测（变化个数多，变化之间区分小，则越不容易被预测；对于确定性问题，信息熵为0）</li></ol></li></ul><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><p><strong>特征选择</strong>：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何该选择特征有着很多不同评估标准，从而衍生出不同的决策树算法。</p><p>特征选择的关键是如何选择最优特征对数据集进行划分，随着划分过程的进行，希望决策树的分支结点所包含的样本尽可能属于同一类别，即节点的纯度越来越高。</p><p>根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分此时决策树停止生长。</p><p>有三种选择最优特征的标准：信息增益、增益率和基尼指数，分别对应了三种决策树算法：ID3，C4.5，CART。</p><h3 id="ID3算法：信息增益"><a href="#ID3算法：信息增益" class="headerlink" title="ID3算法：信息增益"></a>ID3算法：信息增益</h3><p>计算每个特征的信息增益，并比较它们的大小，每一次都选择使得<strong>信息增益最大</strong>的特征进行分裂，递归地构建决策树。信息增益越大，意味着使用某个特征进行划分所获得的纯度的提升越大。</p><p>信息增益$Gain(A)$：由于特征$A$使数据集$D$的分类不确定性减少的程度</p><script type="math/tex; mode=display">Gain(A)=Info(D)-Info_A(D)</script><p>缺点：</p><ol><li><p>选择取值较多的特征往往会具有较大的信息增益（取值越多意味着确定性更高，条件熵越小，信息增益越大），所以<strong>ID3偏向于选择取值较多的特征</strong>。</p></li><li><p>仅支持分类不支持回归、不支持连续型变量、只有树的生成没有剪枝（容易过拟合）、没有缺失值处理方法。</p></li></ol><h3 id="C4-5算法：信息增益率"><a href="#C4-5算法：信息增益率" class="headerlink" title="C4.5算法：信息增益率"></a>C4.5算法：信息增益率</h3><p>针对ID3算法的不足，C4.5算法根据信息增益比来选择特征</p><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。</p><p>信息增益率$GainRate(A)$：特征$A$的信息增益$Gain(A)$与训练数据集$D$关于特征$A$的值的熵$SplitInfo(A)$之比</p><script type="math/tex; mode=display">GainRate(A)=\frac{Gain(A)}{SplitInfo(A)}</script><p>其中$SplitInfo(A)=-\sum_{k=1}^{K}\frac{|D_k|}{|D|}\log_2\frac{|D_k|}{|D|}$，其中$K$是特征$A$取值的个数，$|D|$表示样本$D$的样本个数。</p><p>特征数越多的特征对应的特征熵越大，它作为分母，一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升泛化能力。</p><p>信息增益率准则<strong>对可取值数目较少</strong>的特征有所偏好，因此C4.5算法不是直接选取增益率最大的候选划分特征，<strong>而是先从候选划分特征中找出信息增益高于平均水平的特征，再从中选择增益率最高的</strong>。</p><p>过拟合策略：C4.5引入了正则化系数进行剪枝</p><h3 id="CART算法：基尼指数"><a href="#CART算法：基尼指数" class="headerlink" title="CART算法：基尼指数"></a>CART算法：基尼指数</h3><p>基尼指数也是度量数据集纯度的指标，CART是使用基尼指数来选择最优特征的。基尼指数越小，代表数据集的纯度越高。</p><p>假设有$K$个类，第$k$个类别的概率为$p_k$，对于样本$D$，基尼指数定义为</p><script type="math/tex; mode=display">Gini(D)=1-\sum_{k=1}^{K}p_k^2</script><p>对于样本$D$，样本个数为$|D|$。根据特征$A$的某个值$a$，把$D$分成$D_1$和$D_2$，则在特征$A$的条件下，样本$D$的基尼系数表达式为</p><script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p>表示经过特征$A$分割之后集合$D$的不确定性，因此选<strong>基尼指数最小</strong>的特征的作为最优划分特征。</p><p>CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样<strong>CART分类树算法所建立起来的是二叉树，而不是多叉树</strong>。</p><blockquote><p><strong>CART分类树对连续特征和离散特征的处理</strong></p><ul><li>CART分类树对连续特征的处理：连续特征离散化</li></ul><p>$m$个样本的连续特征$A$从小到大排列$a_1,a_2,\cdots,a_m$，CART取相邻两样本值的平均数作为划分点，一共有$m-1$个，其中第$i$个划分点$T_i$表示为：$T_i=(a_i+a_{i+1})/2$。</p><p>分别计算这$m-1$个划分点作为二元分类点时的基尼系数，选择基尼系数最小的点为该连续特征的划分点。比如取到使基尼系数最小的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样就做到了连续特征的离散化。</p><ul><li>CART分类树对离散特征的处理：不停地二分离散特征</li></ul><p>在ID3、C4.5算法中，比如特征$A$被选取建立决策树节点，若它有三个取值$A_1,A_2,A_3$，会在决策树上建立一个三叉点，这样的决策树是多叉树。</p><p>CART采用的是不停地二分。对于特征$A$会考虑把其分成$\{A_1\}$和$\{A_2,A_3\}$、$\{A_2\}$和$\{A_1,A_3\}$、$\{A_3\}$和$\{A_1,A_2\}$三种情况，找到基尼系数最小的组合，比如$\{A_1\}$和$\{A_2,A_3\}$，一个节点是特征$A$取值为$A_1$对应的样本，另一个节点是取值为$A_2$或$A_3$对应的样本。</p><p>由于CART分类树是二叉树，与ID3和C4.5不同，在对某特征进行划分后，该特征在后面还可以参与子节点的划分过程。</p></blockquote><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：</p><p>易于理解和解释，可视化分析，容易提取出规则</p><p>可同时处理分类型和数值型变量</p><p>缺点：</p><p>容易过拟合</p><p>通常情况下精确度不如其他算法好</p><h3 id="ID3-C4-5-CART对比"><a href="#ID3-C4-5-CART对比" class="headerlink" title="ID3/C4.5/CART对比"></a>ID3/C4.5/CART对比</h3><div class="table-container"><table><thead><tr><th>算法</th><th>支持模型</th><th>树结构</th><th>特征选择</th><th>连续值处理</th><th>缺失值</th><th>剪枝</th></tr></thead><tbody><tr><td>ID3</td><td>分类</td><td>多叉树</td><td>信息增益</td><td>不支持</td><td>不支持</td><td>不支持</td></tr><tr><td>C4.5</td><td>分类</td><td>多叉树</td><td>信息增益比</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td>CART</td><td>分类，回归</td><td>二叉树</td><td>基尼指数，均方误差</td><td>支持</td><td>支持</td><td>支持</td></tr></tbody></table></div><p>CART指的是分类回归树，它既可以用来分类，又可以被用来进行回归。</p><p>回归树：用平方误差最小化作为选择特征的准则</p><p>分类树：采用基尼指数最小化原则进行特征选择，递归地生成二叉树。</p><p>也提供了优化的剪枝策略</p><p><strong>从样本类型的角度：</strong></p><p>ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。</p><p>C4.5会排序找到切分点，将连续变量转换为多个取值区间的离散型变量；</p><p>CART每次会对特征进行二值划分，适用于连续变量。</p><p><strong>从应用角度：</strong></p><p>ID3和C4.5只能用于分类，CART树可以用于分类和回归。</p><p><strong>从细节、优化过程角度：</strong></p><p>ID3对样本特征缺失值比较敏感，而C4.5和CART树都可以对缺失值进行不同方式的处理。</p><p>ID3和C4.5可以产生多叉分支，且每个特征在层级之间不会复用。CART树是二叉树，<strong>每个特征可以被重复利用</strong>。</p><p>ID3和C4.5通过剪枝来权衡树的准确性和泛化性能，CART树枝节利用全部数据发现所有可能的树结构进行对比。</p><h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>剪枝是将子树删除，用一个叶子结点代替，节点类别取多数类。为缓解决策树过拟合，需要对决策树进行剪枝。往往通过极小化决策树整体的损失函数或代价函数来实现。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p><h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>在生成决策树的过程中提前停止树的增长。核心思想是在树中节点进行扩展之前，先计算当前的划分是否能带来模型泛化性能的提升，如果不能则不再继续生成子树。</p><p>预剪枝停止决策树生长的几种方法：</p><ol><li><p>当树达到一定深度时停止生长。</p></li><li><p>当到达当前节点的样本数量小于某个阈值时停止生长。</p></li><li><p>计算每次分类对测试集的准确率提升，当小于某个阈值时停止生长。</p></li></ol><p>预剪枝的优缺点：</p><p>优点：简单高效，适合解决大规模问题。</p><p>缺点：深度和阈值这些参数很难准确估计，针对不同问题会有很大差别。前剪枝存在一定局限性，有<strong>欠拟合的风险</strong>。</p><h3 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h3><p>核心思想是让算法生成一棵完全生长的决策树，然后从底层向上计算是否剪枝。如果剪枝之后准确率有提升，则剪枝。</p><p>后剪枝的优缺点：</p><p>优点：通常可以得到泛化能力更强的决策树。</p><p>缺点：时间开销大。</p><h4 id="代价复杂性剪枝算法-CCP"><a href="#代价复杂性剪枝算法-CCP" class="headerlink" title="代价复杂性剪枝算法(CCP)"></a>代价复杂性剪枝算法(CCP)</h4><p>CART决策树所采用的剪枝方法，是后剪枝方法的一个实例。</p><p>算法分为两步：</p><ol><li>先从自由生成的决策树$T_0$底端开始不断剪枝，直到$T_0$的根节点，形成一个子树序列$\{T_0,T_1,\cdots,T_n\}$。</li><li>通过交叉验证的方式，在验证集中对这$n$个树序列进行评价，选择最优的子树作为最终剪枝结果。</li></ol><p>因此关键问题在于：如何构造步骤1中的子树序列$\{T_0,T_1,\cdots,T_n\}$。</p><p>首先给出子树$T$的损失函数</p><script type="math/tex; mode=display">C_\alpha(T)=C(T)+\alpha|T|</script><p>其中$C(T)$为训练集的预测误差，误差率的计算是认为较少的一部分样本数量作为误差样本（对每个叶子节点中的两个分类，个数少的是误差），$|T|$为叶节点个数，$\alpha$为剪枝系数（作为平衡代价和复杂度两者的参数），$\alpha=0$时，损失函数等于预测误差，相当于不进行剪枝；$\alpha$越大，惩罚越大，会得到更简单的树，即剪枝幅度更大。</p><p><strong>对于固定的$\alpha$，一定存在一个使得损失函数达到最小的子树$T_\alpha$，即对于$\alpha$序列$\{\alpha_0,\alpha_1,\cdots,\alpha_n\}$，有最优子树序列$\{T_0,T_1,\cdots,T_n\}$与其一一对应</strong>，因此我们只需要在最优子树系列中寻找交叉验证集效果最好的那个作为最终的剪枝结果即可。因此子树序列$\{T_0,T_1,\cdots,T_n\}$的构造可以转变为$\alpha$序列$\{\alpha_0,\alpha_1,\cdots,\alpha_n\}$的构造。</p><p>将$\alpha$序列构造为递增的序列，则子树序列$T$是满树到根节点树的递减树序列。 </p><p>首先令$\alpha_0=0$，决策树本身为$T_0$，在$T_0$上进行第一次剪枝（构造$\alpha_1$），只需要将问题聚焦于节点$t$以及对应的子树$T_t$上：</p><ul><li><p>若在节点$t$处进行剪枝，则节点$t$就变成了单节点树，对应的损失函数为</p><script type="math/tex; mode=display">C_\alpha(t)=C(t)+\alpha</script></li><li><p>若不进行剪枝，节点$t$及以下部分构成的子树$T_t$的损失函数为</p><script type="math/tex; mode=display">C_\alpha(T_t)=C(T_t)+\alpha|T_t|</script></li></ul><p>已知剪枝后的损失函数$\le$剪枝前的损失函数，则有</p><script type="math/tex; mode=display">\begin{align}C_\alpha(t) &\le C_\alpha(T_t)\\C(t)+\alpha &\le C(T_t)+\alpha|T_t|\\\alpha &\ge \frac{C(t)-C(T_t)}{|T_t|-1}\end{align}</script><p>因此对于$T_0$，能够在节点$t$处剪枝的最小$\alpha$为$\alpha_{min}=\frac{C(t)-C(T_t)}{|T_t|-1}$；当$\alpha\in[0,\frac{C(t)-C(T_t)}{|T_t|-1})$时，此时进行剪枝会使损失函数增大，不能进行剪枝。</p><p>自下而上地对每个内部节点$t$计算可以剪枝的最小$\alpha$（即$\alpha_{min}$），取$\alpha_{min}$中最小的值作为$\alpha_1$（$\alpha$序列逐渐增大，树序列逐渐更简单；最小的$\alpha_{min}$可以保证没有遗漏可以剪枝的子树），对应的子树为$T_1$，剪切点为$t_1$，这样得到了$\alpha$序列中的$\alpha_1$以及对应的子树$T_1$。</p><p>不断重复以上过程，可以得到最优子树序列$\{T_0,T_1,\cdots,T_n\}$，再将每棵树进行交叉验证，交叉验证结果最好的那颗子树便是最终的剪枝结果。</p><blockquote><p>CART决策树剪枝过程如下：</p><p>输入：CART算法生成的决策树$T_0$</p><p>输出：最优的决策树$T_\alpha$</p><ol><li>设$k=0,T=T_0,\alpha=+\infty$</li><li>自下而上地对各内部节点$t$计算$C(T_t)$，$|T_t|$以及$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1}$，$\alpha=\min\{\alpha,g(t)\}$，这里$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的测试误差，$|T_t|$是$T_t$的叶节点个数。</li><li>自上而下地访问内部节点$t$，如果有$g(t)=\alpha$则进行剪枝，并对叶节点$t$以多数表决法决定其类，得到树$T$。</li><li>设$\alpha_k=\alpha,T_k=T,k=k+1$。</li><li>如果$T$不是由根节点单独构成的树，则回到步骤3。</li><li>采用交叉验证法在子树序列$\{T_0,T_1,\cdots,T_n\}$中选取最优子树$T_\alpha$。</li></ol></blockquote><h1 id="回归树理论"><a href="#回归树理论" class="headerlink" title="回归树理论"></a>回归树理论</h1><h1 id="单机python"><a href="#单机python" class="headerlink" title="单机python"></a>单机python</h1><p><em>class</em> <code>sklearn.tree.DecisionTreeClassifier</code>(<em>criterion=’gini’</em>, <em>splitter=’best’</em>, <em>max_depth=None</em>, <em>min_samples_split=2</em>, <em>min_samples_leaf=1</em>, <em>min_weight_fraction_leaf=0.0</em>, <em>max_features=None</em>, <em>random_state=None</em>, <em>max_leaf_nodes=None</em>, <em>class_weight=None</em>, <em>presort=False</em>)</p><ul><li>criterion：特征选择标准，{‘entropy’,’gini’}，默认’gini’，即CART算法。</li><li>splitter：特征划分标准，{‘best’, ‘random’}，默认’best’。’best’在特征的所有划分点中找出最优的划分点，’random’随机的在部分划分点中找局部最优的划分点。’best’适合样本量不大的时候，而如果样本数据量非常大，此时决策树构建推荐’random’。</li><li>max_depth：决策树最大深度，int/None，默认值是‘None’。一般数据比较少或者特征少的时候可以不用管这个值，如果模型样本数量多，特征也多时，推荐限制这个最大深度，具体取值取决于数据的分布。常用的可以取值10-100之间，常用来解决过拟合。</li><li>min_samples_split：内部节点（即判断条件）再划分所需最小样本数，int/float，默认值为2。如果是int，则取传入值本身作为最小样本数；如果是float，则取<code>ceil(min_samples_split*样本数量)</code>作为最小样本数（向上取整）。</li><li>min_samples_leaf：叶子节点（即分类）最少样本数，int/float，默认值为1。如果是int，则取传入值本身作为最小样本数；如果是float，则取<code>ceil(min_samples_leaf*样本数量)</code>的值作为最小样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。</li><li>min_weight_fraction_leaf：叶子节点（即分类）最小的样本权重和，float。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，就是不考虑权重问题，所有样本的权重相同。一般来说如果我们有较多样本有缺失值或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意此值。</li><li>max_features：在划分数据集时考虑的最多的特征值数量，int/float/None，默认值为None。int值表示在每次split时最大特征数；float值表示百分数，即<code>max_features*n_features</code>。</li><li>random_state：int/None，默认是None</li><li>max_leaf_nodes：最大叶子节点数。int/None，通过设置最大叶子节点数，可以防止过拟合。默认值None，默认情况下不设置最大叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征多，可以加限制，具体的值可以通过交叉验证得到。</li><li>class_weight：类别权重，dict/list of dicts/balanced，默认为None。（不适用于回归树DecisionTreeRegressor）指定样本各类别的权重，主要是为了防止训练集某些类别的样本过多，导致训练的决策树过于偏向这些类别。balanced，算法自己计算权重，样本量少的类别所对应的样本权重会更高。如果样本类别分布没有明显的偏倚，则可以不管这个参数。</li><li>presort：bool，默认是False，表示在进行拟合之前，是否预分数据来加快树的构建。对于数据集非常庞大的分类，presort=true将导致整个分类变得缓慢；当数据集较小，且树的深度有限制，presort=true才会加速分类。</li></ul><h1 id="集群pyspark"><a href="#集群pyspark" class="headerlink" title="集群pyspark"></a>集群pyspark</h1>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> DecisionTree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列5——降维</title>
      <link href="/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/"/>
      <url>/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>ML中相比训练误差，更加关注的是泛化误差。过拟合问题就是造成泛化误差大的一个原因。</p><p>解决过拟合的方法：</p><ul><li><p>增加样本数量</p></li><li><p>正则化（Ridge/Lasso）</p></li><li><p>降维</p><ul><li>直接降维——特征选择<em>（不是本节的关注点，本节关注线性和非线性降维）</em></li><li>线性降维——PCA，MDS（多维空间缩放）</li><li>非线性降维——流形（Isomap，LLE）</li></ul><blockquote><p>特征维度高往往会造成<strong>维度灾难( Curse of Dimensionality )</strong></p><ul><li>从数学角度来看，每增加一个特征（属性），为了布满它所有的样本空间，所需要的样本数量呈指数级增长（例如对于一个二值变量，则至少需要$2^n$个样本数）</li><li>从几何角度来看，比如对于一个体积为1的超立方体，内接一个超球体，$V_{超球体}=C(0.5)^d$，其中$d$为维度，当$d\rightarrow\infty $时，超球体的体积无限趋于0，那么会造成<strong>数据的稀疏性</strong>，且大部分集中在一起，很难进行分类。</li></ul></blockquote></li></ul><h1 id="概率相关知识"><a href="#概率相关知识" class="headerlink" title="概率相关知识"></a>概率相关知识</h1><p>Data：$X=(x_1,x_2,…,x_N)^T_{N*P}$，$N$个样本，每个样本是$P$维的</p><p>样本均值和样本方差的矩阵表示：</p><script type="math/tex; mode=display">\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_i=\frac{1}{N}(x_1,x_2,…,x_N)1_N=\frac{1}{N}X^T1_N\\\begin{align}S_{p*p}&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T\\&=\frac{1}{N}(X^T-\bar{x}1_N^T)(X^T-\bar{x}1_N^T)^T\\&=\frac{1}{N}(X^T-\frac{1}{N}X^T1_N1_N^T)(X^T-\frac{1}{N}X^T1_N1_N^T)^T\\&=\frac{1}{N}X^T(I_N-\frac{1}{N}1_N1_N^T)(I_N-\frac{1}{N}1_N1_N^T)^TX\\&=\frac{1}{N}X^THH^TX\\&=\frac{1}{N}X^THX\end{align}</script><p>其中$H=I_N-\frac{1}{N}1_N1_N^T$，且具有性质$H^T=H,H^n=H$，称为中心矩阵( centering matrix )</p><h1 id="主成分分析PCA"><a href="#主成分分析PCA" class="headerlink" title="主成分分析PCA"></a>主成分分析PCA</h1><ul><li><p>一个中心：<strong>原始特征空间的重构</strong>（将一组可能<strong>线性相关</strong>的变量通过正交变换，变换成<strong>线性无关</strong>的变量）</p></li><li><p>两个基本点（两个角度是一个方法）：<strong>最大投影方差，最小重构距离</strong>（在该方向上投影的方差最大，把投影上的点重构回去的代价最小）</p></li></ul><h2 id="最大投影方差"><a href="#最大投影方差" class="headerlink" title="最大投影方差"></a>最大投影方差</h2><ol><li><p>中心化（将中心平移到原点，即减去均值$x_i-\bar{x}$）</p></li><li><p>令被投影的向量的模为1，$||u_1||=1$</p></li><li><p>投影方差为（向量$a$在向量$b$上的投影为$a^Tb$）</p><script type="math/tex; mode=display">J=\frac{1}{N}\sum_{i=1}^{N}((x_i-\bar{x})^Tu_1)^2=u_1^T[\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T]u_1=u_1^TSu_1</script><p>从而求解目标为</p><script type="math/tex; mode=display">\hat{u}_1=\mathop{\arg\max}\limits_{u_1} u_1^TSu_1\\s.t. u_1^Tu_1=1</script><p>使用拉格朗日乘子法</p><script type="math/tex; mode=display">L(u_1,\lambda)=u_1^TSu_1+\lambda(1-u_1^Tu_1)\\\frac{\partial L}{\partial u_1}=2Su_1-\lambda2u_1=0\\Su_1=\lambda u_1</script><p>从而$\lambda$是特征值，$u_1$是特征向量（主成分）</p></li></ol><h2 id="最小重构距离"><a href="#最小重构距离" class="headerlink" title="最小重构距离"></a>最小重构距离</h2><p>PCA可以视为以下两个部分：</p><ul><li>先进行特征空间的重构，得到$\{u_1,u_2,…,u_p\}$共$p$个特征向量</li><li>再对这$p$个特征向量进行筛选，选出前$q$个特征向量，从而实现降维</li></ul><p>原先样本为（用新的坐标轴去重构，并认为$x_i$已经是中心化后的）</p><script type="math/tex; mode=display">x_i=\sum_{k=1}^{p}(x_i^Tu_k)u_k</script><p>重构回来的样本为</p><script type="math/tex; mode=display">\hat{x}_i=\sum_{k=1}^{q}(x_i^Tu_k)u_k</script><p>重构代价为（目标是希望重构代价最小）</p><script type="math/tex; mode=display">\begin{align}J&=\frac{1}{N}\sum_{i=1}^{N}||x_i-\hat{x}_i||^2\\&=\frac{1}{N}\sum_{i=1}^{N}||\sum_{k=q+1}^{p}(x_i^Tu_k)u_k||^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}(x_i^Tu_k)^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}((x_i-\bar{x})^Tu_k)^2\\&=\sum_{k=q+1}^{p}[\sum_{i=1}^{N}\frac{1}{N}((x_i-\bar{x})^Tu_k)^2]\\&=\sum_{k=q+1}^{p}u_k^T·S·u_k\end{align}</script><p>其中坐标轴为$u_{q+1},…,u_{p}$，坐标为$x_i^Tu_{q+1},…,x_i^Tu_{p}$，对向量求模的平方即对各个坐标轴下的坐标求平方和</p><p>因此目标函数为</p><script type="math/tex; mode=display">\hat{u}_k=\mathop{\arg\max}\limits_{u_k} u_k^TSu_k\\s.t. u_k^Tu_k=1</script><h2 id="从SVD角度看PCA"><a href="#从SVD角度看PCA" class="headerlink" title="从SVD角度看PCA"></a>从SVD角度看PCA</h2><p>奇异值分解SVD：</p><script type="math/tex; mode=display">S=GKG^T\\G^TG=I</script><p>$K$为由特征值从大到小排列构成的对角矩阵</p><p>对中心化后的数据矩阵（原数据矩阵$X_{N·p}$）进行奇异值分解（任何实数矩阵可以进行奇异值分解）</p><script type="math/tex; mode=display">HX=U\Sigma V^T</script><p>样本方差矩阵为（忽略$\frac{1}{N}$）</p><script type="math/tex; mode=display">S=X^THX=X^TH^THX=V\Sigma U ^TU\Sigma V^T</script><p>由于SVD的性质</p><script type="math/tex; mode=display">U^TU=I\\V^TV=VV^T=I</script><p>且$\Sigma$为对角矩阵</p><p>则可以转换为</p><script type="math/tex; mode=display">S=V\Sigma U ^TU\Sigma V^T=V\Sigma ^2 V^T</script><p>因此不需要求样本方差矩阵$S$，可以对中心化后的数据矩阵$HX$进行奇异值分解，同样可以求得$V$和$\Sigma$，从而得到特征向量和特征值。</p><p>定义矩阵</p><script type="math/tex; mode=display">T=HXX^TH=U\Sigma V^TV\Sigma U^T=U\Sigma^2 U^T</script><p>因此$T$和$S$有相同的特征值（$\Sigma^2$）</p><ul><li><p>对$S$做特征分解，得到方向（主成分）$V$，然后通过将数据矩阵乘以方向$V$</p><script type="math/tex; mode=display">HX·V=U\Sigma V^TV=U\Sigma</script><p>从而得到在新的方向下的坐标矩阵$U\Sigma$</p></li><li><p>对$T$做特征分解（<strong>主坐标分析</strong>，Principle Coordinate Analysis，PCoA），可以直接得到坐标</p><script type="math/tex; mode=display">T=U\Sigma^2 U^T\\TU\Sigma=U\Sigma^2 U^TU\Sigma=U\Sigma^3=U\Sigma\Sigma^2</script><p>得到特征向量组成的矩阵$U\Sigma$和特征值组成的矩阵$\Sigma^2$</p></li></ul><blockquote><p>Q：为什么$U\Sigma$是$T$的特征向量组成的矩阵？$T$的特征向量组成的矩阵不应该直接是$U$吗？ </p><p>$U\Sigma$是将$T$的每个特征向量依据$HX$的相应特征值大小做缩放之后的矩阵 </p><p><strong>？？？……没懂</strong></p><p>PCA的目的是求出在新的投影方向上的坐标。PCA先通过SVD找到主成分（方向）$u_1$ ，然后对于样本点$x_i$来说，先进行中心化再乘上主成分$(x_i-\bar{x})u_i=z_i$，从而得到该样本点在新坐标轴$u_1$上的坐标$z_i$。即先求的是方向，再对样本进行投影才能得到坐标。</p><p>PCoA没有通过求方向再进行投影得到坐标的方式，而是通过对矩阵$T$进行分解，直接求出坐标</p></blockquote><p>PCoA好处：</p><p>维度方面：$S_{p<em>p},T_{N</em>N}$，当维度高时可以简化运算</p><h1 id="概率角度P-PCA"><a href="#概率角度P-PCA" class="headerlink" title="概率角度P-PCA"></a>概率角度P-PCA</h1><p>原始样本$x\in R^p$(observed data)，降维后的样本$z\in R^q$(latent data)，且$q&lt;p$</p><p>令</p><script type="math/tex; mode=display">z \sim N(0_q,I_q)\\x=w_{p*q}z+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I_p)</script><p>是线性高斯模型(Linear Gaussian Model)，$\sigma^2I_p$矩阵各向同性。</p><p>P-PCA关注两个问题：</p><ul><li>Inference：$P(z|x)$</li><li>Learning：$w,\mu,\sigma^2$——可使用EM算法求解，较复杂，此处省略</li></ul><p>条件：</p><script type="math/tex; mode=display">z \sim N(0,I)\\x=wz+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I)，\varepsilon \perp x\\E[x|z]=E[wz+\mu+\varepsilon|z]=wz+\mu\\Var[x|z]=Var[wz+\mu+\varepsilon|z]=\sigma^2I \\\Rightarrow x|z \sim N(wz+\mu,\sigma^2I)</script><p>则</p><script type="math/tex; mode=display">E[x]=E[wz+\mu+\varepsilon]=E[wz+\mu]+E[\varepsilon]=\mu\\Var[x]=Var[wz+\mu+\varepsilon]=Var[wz]+Var[\varepsilon]=ww^T+\sigma^2I\\\Rightarrow x \sim N(\mu,ww^T+\sigma^2I)</script><p>构造$x$和$z$的联合概率：</p><script type="math/tex; mode=display">Cov(x,z)=E[(x-\mu)z^T]=E[(wz+\varepsilon)z^T]=wE[zz^T]+E[\varepsilon]E[z^T]=w</script><p>则</p><script type="math/tex; mode=display">\begin{pmatrix} x  \\ z  \end{pmatrix}\sim N\begin{pmatrix} ww^T+\sigma^2I & w \\ w & I \end{pmatrix}</script><p>再由公式$x_b|x_a \sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\Sigma_{bb·a})$，从而得到$z|x$的条件概率分布</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列4——线性分类</title>
      <link href="/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/"/>
      <url>/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>b站up主： <strong>shuhuai008</strong> </p><p><a href="https://www.bilibili.com/video/av70839977" target="_blank" rel="noopener">机器学习-白板推导系列-合集</a> 学习笔记</p></blockquote><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>对线性回归的条件逐一打破，从而引申到其它模型：</p><ul><li><p>线性：</p><ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性（激活函数是非线性，如逻辑回归）</li><li>系数非线性：神经网络，感知机</li></ul></li><li><p>全局性（全部是由一条线拟合）：线性样条回归、决策树</p></li><li>数据未加工：PCA，流形</li></ul><p>线性回归通过激活函数进行降维，达到线性分类。</p><script type="math/tex; mode=display">y=f(w^Tx+b),x\in R^p</script><p>$f$是激活函数（activation function），$f^{-1}$是链接函数（link function）。</p><p>激活函数$f$将数据的线性组合作为输入，映射到{0,1}或[0,1]区间上；</p><p>链接函数$f^{-1}$将{0,1}或[0,1]区间映射到线性组合上。</p><p>线性分类分为两大类：</p><ol><li>硬分类：$y\in \{0,1\}$，代表模型有线性判别分析(Fisher判别分析)、感知机</li><li>软分类：$y\in [0,1]$，分为生成式模型和判别式模型。判别式直接对$P(Y|X)$进行求解，如逻辑回归；生成式不直接求解$P(Y|X)$，而是通过贝叶斯定理，即通过式$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$进行求解，如高斯判别分析(GDA)（假设数据本身是连续的），朴素贝叶斯（假设数据本身是离散的）。</li></ol><h1 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机(Perceptron)"></a>感知机(Perceptron)</h1><p>思想：错误驱动——不断向正确分类的方向移动</p><p>模型</p><script type="math/tex; mode=display">f(x)=sign(w^Tx),x\in R^p,w\in R^p\\</script><p>前提：假定模型是线性可分的（若不满足可使用pocket algorithm）</p><p>策略：假设样本集$\{(x_i,y_i)\}_{i=1}^{N}$，将损失函数定义为</p><script type="math/tex; mode=display">L(w)=\sum_{i=1}^{N}I\{y_iw^Tx_i\lt 0\}</script><p>但该损失函数不可导不连续，难以求解</p><p>因此采用新的损失函数</p><script type="math/tex; mode=display">L(w)=\sum_{x_i\in D}-y_iw^Tx_i</script><p>是连续函数且可导，对其进行求导$\triangledown_wL=\sum_{x_i\in D}-y_ix_i$，可采用算法SGD（随机梯度下降）求解</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\lambda\triangledown_wL=w^t+\lambda y_ix_i</script><h1 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h1><p><strong>思想：类内小，类间大。</strong>（高类聚，低耦合）</p><p>将点投影到一维的坐标轴上，每个点都对应该坐标轴上的一个值，并设定一个阈值，根据值与阈值大小进行分类。</p><p>重点：找到一个合适的投影方向，使得类内方差小，类间方差大</p><h2 id="目标函数推导"><a href="#目标函数推导" class="headerlink" title="目标函数推导"></a>目标函数推导</h2><p>给定样本$X=(x_1,x_2,…,x_N)^T,Y=(y_1,y_2,…,y_N)^T,\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{+1,-1\}$</p><p>定义样本集合$x_{c_1}=\{x_i|y_i=+1\},x_{c_2}=\{x_i|y_i=-1\}$</p><p>令$x_i$在$w$方向上的投影为$z_i=w^Tx_i$(假设$||w||=1)$</p><script type="math/tex; mode=display">\begin{align} \bar{z}&=\frac{1}{N}\sum_{i=1}^{N}z_i=\frac{1}{N}\sum_{i=1}^{N}w^Tx_i\\S_z  &=\frac{1}{N}\sum_{i=1}^{N}(z_i-\bar{z})(z_i-\bar{z})^T     =\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar{z})(w^Tx_i-\bar{z})^T\\c_1: \bar{z}_1&=\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i\\S_1  &=\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T     =\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\     c_2: \bar{z}_2&=\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i\\S_2  &=\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T     =\frac{1}{N_2}\sum_{i=1}^{N_2}(w^Tx_i-\bar{z}_2)(w^Tx_i-\bar{z}_2)^T\end{align}</script><p>则类间用$(\bar{z}_1-\bar{z}_2)^2$来表达，类内用$S_1+S_2$来表达</p><p>目标函数为</p><script type="math/tex; mode=display">J(w)=\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\\\hat{w}=\mathop{\arg\max}\limits_{w} J(w)</script><p>则分子部分为</p><script type="math/tex; mode=display">(\bar{z_1}-\bar{z_2})^2=(\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i)^2=(w^T(\bar{x}_{c_1}-\bar{x}_{c_2}))^2=w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw</script><p>由于$S_1$可表达为</p><script type="math/tex; mode=display">\begin{align}S_1 &=\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\&=w^T[\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{c_1})(x_i-\bar{x}_{c_2})^T]w\\&=w^T*S_{c_1}*w\end{align}</script><p>则分母部分为</p><script type="math/tex; mode=display">S_1+S_2=w^TS_{c_1}w+w^TS_{c_2}w=w^T(S_{c_1}+S_{c_2})w</script><p>因此</p><script type="math/tex; mode=display">\begin{align}J(w) &=\frac{w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw}{w^T(S_{c_1}+S_{c_2})w}\\     &=\frac{w^TS_bw}{w^TS_ww}=w^TS_bw(w^TS_ww)^{-1}\end{align}</script><p>其中$S_b=(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^T$为类间方差（between-class），$S_w=S_{c_1}+S_{c_2}$为类内方差（within-class），再对$w$求偏导</p><script type="math/tex; mode=display">\frac{\partial J(w)}{\partial w}=2S_bw(w^TS_ww)^{-1}+w^TS_bw(-1)(w^TS_ww)^{-2}2S_ww=0\\S_bw(w^TS_ww)=(w^TS_bw)S_ww\\S_ww=\frac{w^TS_ww}{w^TS_bw}S_bw</script><p>只需要求$w$的方向不需要求大小，因此常数不影响，因此</p><script type="math/tex; mode=display">\begin{align}w=\frac{w^TS_ww}{w^TS_bw}{S_w}^{-1}S_bw & \propto{S_w}^{-1}S_bw={S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})[(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw]\\&\propto{S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})\end{align}</script><p>若$S_w^{-1}$是对角矩阵，且各向同性，则$S_w^{-1}\propto I$，那么</p><script type="math/tex; mode=display">w\propto(\bar{x}_{c_1}-\bar{x}_{c_2})</script><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>是判别式模型，直接对$P(Y|X)$进行建模，采用极大似然估计求解参数。</p><p>sigmoid函数：</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}\\\sigma:R \longmapsto (0,1)\\w^Tx \longmapsto Probability</script><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><script type="math/tex; mode=display">\begin{align}p_1&:P(y=1|x)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},y=1\\p_0&:P(y=0|x)=1-P(y=1|x)=\frac{e^{-w^Tx}}{1+e^{-w^Tx}},y=0\\\end{align}</script><script type="math/tex; mode=display">\Rightarrow P(y|x)=p_1^yp_0^{1-y}</script><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>MLE:</p><script type="math/tex; mode=display">\begin{align} \hat{w}&=\mathop{\arg\max}\limits_{w} \log P(Y|X)\\&=\mathop{\arg\max}\limits_{w} \log \prod_{i=1}^{N}P(y_i|x_i) \\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}\log P(y_i|x_i)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log p_i+(1-y_i)\log p_0)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log \psi(x_i;w)+(1-y_i)\log (1-\psi(x_i;w)))\end{align}</script><p>MLE是最大化问题，可以导出一个Loss function，转化为最小化问题，等价于最小化cross entropy</p><h1 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>高斯判别分析是生成式模型，<strong>生成式模型并不是求取$P(Y|X)$，而是利用贝叶斯定理，比较$P(y=0|x)$与$P(y=1|x)$的大小，选择较大者作为预测的类</strong>。</p><p>由贝叶斯定理</p><script type="math/tex; mode=display">P(y|x)=\frac{P(x|y)P(y)}{P(x)}\propto P(x|y)P(y)</script><p>由于$P(x|y)P(y)=P(x,y)$，因此我们主要是对于联合概率进行建模。</p><p>其中$P(y)$是先验（prior），$p(x|y)$是似然（likelihood），$P(y|x)$是后验（posterior），因此求解可以表达为</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p><strong>假设$y$服从伯努利分布$Bernoulli(\phi)$，$x|y$服从高斯分布</strong></p><script type="math/tex; mode=display">\begin{align}&x|y=1 \sim N(\mu_1,\Sigma)\\&x|y=0 \sim N(\mu_2,\Sigma)\\\Rightarrow &x|y \sim N(\mu_1,\Sigma)^y*N(\mu_2,\Sigma)^{1-y}\end{align}</script><p>令参数部分</p><script type="math/tex; mode=display">\theta = (\mu_1,\mu_2,\Sigma,\phi)\\\hat{\theta}=\mathop{\arg\max}\limits_{\theta} l(\theta)</script><p>定义对数似然函数为</p><script type="math/tex; mode=display">\begin{align}l(\theta)&=\log \prod_{i=1}^{N}p(x_i,y_i)\\&=\sum_{i=1}^{N}\log(P(x_i|y_i)P(y_i))\\&=\sum_{i=1}^{N}[\log P(x_i|y_i)+\log P(y_i)]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}*N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}+\log N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\\end{align}</script><h2 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h2><p>将上式分为三个部分①+②+③，且有$N_1$个样本标签为1，有$N_2$个样本标签为0（$N_1+N_2=N$）</p><p>求$\phi$：</p><script type="math/tex; mode=display">③=\sum_{i=1}^{N}[y_i\log \phi+(1-y_i)\log(1-\phi)]\\\frac{\partial ③}{\partial \phi}=\sum_{i=1}^{N}[\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}]=0\\\phi=\frac{1}{N}\sum_{i=1}^{N}y_i=\frac{N_1}{N}</script><p>求$\mu_1$：</p><script type="math/tex; mode=display">\begin{align}①&=\sum_{i=1}^{N}\log N(\mu_1,\Sigma)^{y_i}\\&=\sum_{i=1}^{N}y_i\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\end{align}</script><p>去除与$\mu_1$无关的常数</p><script type="math/tex; mode=display">\hat{\mu}_1=\mathop{\arg\max}\limits_{\mu_1} ①=\mathop{\arg\max}\limits_{\mu_1}\sum_{i=1}^{N}y_i[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\\-\frac{1}{2}\sum_{i=1}^{N}y_i[(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]=-\frac{1}{2}\sum_{i=1}^{N}y_i[x_i^T\Sigma^{-1}x_i-2\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1=\bigtriangleup\\\frac{\partial \bigtriangleup}{\partial \mu_1}=\sum_{i=1}^{N}y_i(\Sigma^{-1}x_i-\Sigma^{-1}\mu_1)=0\\\hat{\mu}_1=\frac{\sum_{i=1}^{N}y_ix_i}{\sum_{i=1}^{N}y_i}=\frac{\sum_{i=1}^{N}y_ix_i}{N_1}</script><p>求$\Sigma$：</p><p>首先令</p><script type="math/tex; mode=display">C_1=\{x_i|y_i=1,i=1,…,N\}\\C_2=\{x_i|y_i=0,i=1,…,N\}\\|C_1|=N_1,|C_2|=N_2,N_1+N_2=N</script><p>则</p><script type="math/tex; mode=display">\hat{\Sigma}_1=\mathop{\arg\max}\limits_{\Sigma}(①+②)\\①+②=\sum_{x_i\in C_1}\log N(\mu_1,\Sigma)+\sum_{x_i\in C_2}\log N(\mu_2,\Sigma)\\\begin{align}\log N(\mu,\Sigma)&=\sum_{i=1}^{N}\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[\log\frac{1}{(2\pi)^\frac{p}{2}}+\log{|\Sigma|^\frac{1}{2}}-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[C-\frac{1}{2}\log|\Sigma|-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}tr[(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}tr[\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T\Sigma^{-1}]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}Ntr(S\Sigma^{-1})\\\end{align}</script><p>其中$S=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T$为样本方差</p><p>则</p><script type="math/tex; mode=display">\begin{align}①+②&=-\frac{N_1}{2}\log|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})-\frac{N_2}{2}\log|\Sigma|-\frac{1}{2}N_2tr(S_2\Sigma^{-1})+C\\&=-\frac{1}{2}[N\log|\Sigma|+N_1tr(S_1\Sigma^{-1})+N_2tr(S_2\Sigma^{-1})]+C\\\frac{\partial (①+②)}{\partial \Sigma}&=-\frac{1}{2}(N\Sigma^{-1}-N_1S_1\Sigma^{-2}-N_2S_2\Sigma^{-2})=0\\\end{align}</script><script type="math/tex; mode=display">N\Sigma-N_1S_1-N_2S_2=0\\ \Sigma=\frac{1}{N}(N_1S_1+N_2S_2)</script><p>其中用到求导$\frac{tr(\Sigma^{-1}S)}{\partial \Sigma}=S^T(-1)\Sigma^{-2}$</p><h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>朴素贝叶斯假设：条件独立性假设</p><script type="math/tex; mode=display">x_i \perp x_j | y(i\neq j)\\P(x|y)=\prod_{j=1}^{p}P(x_j|y)</script><p>最简单的概率图（有向图）模型</p><p>动机：简化运算</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p>若$x$离散，则认为$x_j$服从伯努利分布/多项分布</p><p>若$x$连续，则认为$x_j$服从正态分布$x_j\sim N(\mu_j,\sigma^2_j)$</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression 逻辑回归算法及实现</title>
      <link href="/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/"/>
      <url>/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><p>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到二分类的目的。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>逻辑回归是在线性回归的基础上，利用sigmoid函数（或称为logistic函数）</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>进行映射，代入线性回归部分</p><script type="math/tex; mode=display">z=\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n</script><p>得到二元逻辑回归模型的一般形式：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的$h_\theta(x)$就是逻辑回归返回的值，介于0到1之间，可以将其当做样本取正类的“概率”。因此对于样本$x$分类结果为正类1和负类0的概率分别为</p><script type="math/tex; mode=display">\begin{cases}P(y=1|x;\theta)=h_\theta(x)\\P(y=0|x;\theta)=1-h_\theta(x)\end{cases}</script><p>对$h_\theta(x)$进行变换可以得到对数几率的表达式</p><script type="math/tex; mode=display">ln\frac{h_\theta(x)}{1-h_\theta(x)}=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})=ln(\frac{1}{e^{-\theta^Tx}})=ln(e^{\theta^Tx})=\theta^Tx</script><p>从上式可以看出，<strong>逻辑回归的本质是在对线性回归模型的预测去逼近真实标记的对数几率</strong>。求解的关注点在于求解参数$\theta$上，通常使用极大似然估计的方法对$\theta$进行估计。</p><p>令</p><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的似然函数为</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^{m}P(y^i|x^i;\theta)=\prod_{i=1}^{m}h_\theta(x^i)^{y^i}*(1-h_\theta(x^i))^{1-y^i}</script><p>其中，$x^i$为第$i$个样本的特征做构成的向量（每个向量$n+1$维，共$m$个向量），$y^i$为第$i$个样本的标签，$m$为样本量。实际中为了简化计算，同时防止连乘所造成的浮点数下溢，通常会转化为对数似然函数</p><script type="math/tex; mode=display">l(\theta)=\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>逻辑回归所要解决的问题即为找到参数$\theta$，使得对数似然函数达到最大。</p><p>令损失函数为（忽略正则化项）</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}l(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>利用梯度下降求解参数</p><script type="math/tex; mode=display">\begin{align}\frac{∂J(\theta)}{∂\theta_j}& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{h_\theta(x^i)}-\frac{1-y^i}{1-h_\theta(x^i)}]\frac{∂h_\theta(x^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]\frac{∂g(\theta^Tx^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))\frac{∂\theta^Tx^i}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i(1-g(\theta^Tx^i))-(1-y^i)g(\theta^Tx^i)]x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i-g(\theta^Tx^i)]x^i_j\\& =\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j\\\end{align}</script><p>因此最终得到参数迭代式</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>（参考：<a href="https://www.cnblogs.com/Luv-GEM/p/10674719.html" target="_blank" rel="noopener">Logistic回归（逻辑回归）和softmax回归</a>）</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点：</strong></p><ol><li><p>速度快，在时间和内存需求上相当高效，它可以应用于分布式数据和在线算法实现，用较少的资源处理大型数据</p></li><li><p>对线性分类问题拟合很好</p></li><li>简单易于理解，直接看到各个特征的权重</li></ol><p><strong>缺点：</strong></p><ol><li>分类精度可能不高，在非线性分类问题上表现不好</li><li>数据特征有缺失或者特征空间很大时表现效果并不好，受异常值影响大</li></ol><h2 id="与线性回归的异同"><a href="#与线性回归的异同" class="headerlink" title="与线性回归的异同"></a>与线性回归的异同</h2><p>本质是线性的，只是特征到结果映射用的是sigmoid函数，属于广义线性模型（GLM）</p><ul><li><p>相同</p><ol><li><p>都使用极大似然估计对训练样本进行建模；求解超参数时都可以使用梯度下降。</p></li><li><p>都是广义线性模型，逻辑回归本质上是一个线性回归模型，LR是以线性回归为理论支持的。</p></li></ol></li><li><p>不同</p><ol><li>本质：逻辑回归是分类，线性回归是回归，逻辑回归中$y$是因变量而非$\frac{p}{1-p}$，因变量是离散而非连续</li><li>LR形式上是线性回归，实质上是在求取输入空间到输出空间的非线性函数映射（对率函数起到将线性回归模型的预测值与真实标记联系起来的作用）</li><li>LR是直接对分类可能性进行建模，无需事先假设数据分布，而线性回归需要假设数据分布</li></ol></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数是它的极大似然函数取对数再除以样本量的相反数</strong></p><p>极大似然函数：</p><script type="math/tex; mode=display">L_\theta(x)=\prod_{i=1}^{m}h_\theta(x^i;\theta)^{y^i}*(1-h_\theta(x^i;\theta))^{1-y^i}</script><p>损失函数：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]</script><p>除以样本量$m$并不改变最终求导极值结果，通过除以$m$可以得到<strong>平均损失值</strong>，避免<strong>样本数量对于损失值的影响</strong></p><p>（但是也有不除以样本量的，比如sklearn中的损失函数就不除以样本量）</p><p>（乘上样本量的倒数也并不影响梯度下降的过程┓( ´∀` )┏ ）</p><blockquote><p><strong>Q：为什么要用极大似然函数作为损失函数？</strong></p><p>损失函数一般有四种：平方损失函数，对数损失函数，HingeLoss损失函数，绝对值损失函数。</p><p>将极大似然函数取对数以后等同于对数损失函数。</p><p><strong>在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</strong></p><p>梯度更新公式：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>这个式子的更新速度只和$x^i_j$和$y^i$相关，和sigmoid函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 </p><p><strong>Q：为什么不选平方损失函数？</strong></p><p>其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmoid函数本身的梯度是很相关的。</p><script type="math/tex; mode=display">θ_j=θ_j-2(sigmoid(x)*(1-sigmoid(x)))x^i_j</script><p>sigmoid函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p><p>实际上也可以用最小二乘，但是最小二乘得到的权重效果比较差。</p><p>如果用最小二乘法，目标函数就是差值的平方和，<strong>是非凸的，不容易求解，很容易陷入到局部最优</strong>。</p><p>如果用极大似然估计，目标函数就是对数似然函数，是关于$(w,b)$的高阶<strong>连续可导凸函数</strong>，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。</p></blockquote><h2 id="参数求解方法"><a href="#参数求解方法" class="headerlink" title="参数求解方法"></a>参数求解方法</h2><ul><li>梯度下降法</li></ul><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼近最优解。</p><blockquote><p>梯度下降：随机梯度下降，批梯度下降，small-batch梯度下降</p><p>Q：三种方式的优劣以及如何选择最合适的梯度下降方式</p><ol><li><p>批梯度下降(BGD)：每次迭代使用所有样本来进行梯度的更新，能得到全局最优解。缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降(SGD)：每次迭代随机使用一个样本来对参数进行更新，优点是每一轮参数的更新速度大大加快，缺点是准确度下降，可能会收敛到局部最优（单个样本不能代表全体样本的趋势）。</p></li><li><p>小批量梯度下降：结合了BGD和SGD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p></li></ol></blockquote><ul><li><p>牛顿法</p><p>(待更新)</p></li><li><p>拟牛顿法</p><p>(待更新)</p></li></ul><blockquote><p>牛顿法与梯度下降法求解参数的区别：</p><p>两种方法不同在于牛顿法中<strong>多了一项二阶导数</strong>，这项二阶导数对参数更新的影响主要体现在<strong>改变参数更新方向上</strong>。如下图所示，红色是牛顿法参数更新的方向，绿色为梯度下降法参数更新方向，因为牛顿法考虑了二阶导数，因而可以<strong>找到更优的参数更新方向</strong>，在每次更新的步幅相同的情况下，可以<strong>比梯度下降法节省很多的迭代次数</strong>。</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/zdow4EB72J3r.png" alt="mark"></p></blockquote><h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>作用：需要一个单调可微的函数，把分类任务的真实标记与线性回归模型的预测值联系起来。</p><p>对于二分类问题，由线性回归得来的启发是根据特征的加权平均进行预测。很自然地想到设定一个阈值，如果加权平均大于该阈值就判为正类，反之判为负类。但<strong>阶跃函数不可导</strong>，所以<strong>引入Sigmoid函数，将样本的加权平均代入函数得到的值就是样本属于正类的概率，即将输入空间到输出空间作非线性函数映射</strong>。</p><p>Sigmoid函数形式：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid函数是一个S型的函数，函数图像：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><p>当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0。</p><p>它能够<strong>将任何实数映射到(0,1)区间</strong>（开区间，不可等于0或1），使其可用于将任意值函数转换为更适合二分类的函数。 </p><p>因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。</p><p>区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p><h2 id="共线性问题"><a href="#共线性问题" class="headerlink" title="共线性问题"></a>共线性问题</h2><p>对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。</p><p>通常做法为：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF， VIF越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><ol><li><p>多项逻辑回归(Softmax Regression)</p><p>（二分类逻辑回归在多标签分类下的一种拓展）</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/4HnDRP6ijvH6.png" alt="mark"></p></li><li><p>one v.s. rest</p><p>k个二分类LR分类器，把标签重新整理为“第i类标签”与“非第i类标签”</p></li></ol><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a> </p><blockquote><p><em>class</em> <code>sklearn.linear_model.LogisticRegression</code>(<em>penalty=’l2’</em>, <em>dual=False</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>random_state=None</em>, <em>solver=’lbfgs’</em>, <em>max_iter=100</em>, <em>multi_class=’auto’</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>n_jobs=None</em>, <em>l1_ratio=None</em>) </p></blockquote><h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数"><a href="#1-基本模型参数" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fit_intercept</strong>：bool，指定是否将截距项添加到线性回归部分中。默认True。</p></li><li><p><strong>intercept_scaling</strong>：float，默认1。仅在solver=’liblinear’且fit_intercept=True时有用。 在这种情况下原本的向量是[x]就变成[x,intercept_scaling]，即具有等于设定的intercept_scaling值的“合成”特征会被添加到实例矢量。截距会变为intercept_scaling * synthetic_feature_weight(合成特征权重)。synthetic_feature_weight会与其他特征经历l1和l2正则化，为减小正则化对synthetic_feature_weight（并因此对截距）的影响，必须增加intercept_scaling。</p><blockquote><p>因为本身截距项是不需要进行正则化的，当采用fit_intcept时相当于人造一个特征出来，特征恒为1，权重为b。在计算正则化项的时候，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 (O_o)??</p></blockquote></li><li><p><strong>multi_class</strong>：str，’auto’（默认）/‘ovr’/‘multinomial’，表示要预测的分类是二分类或一对多形式的多分类问题，还是多对多形式的多分类问题。</p><ul><li><p><strong>‘auto’</strong>：表示自动选择，会根据数据的分类情况和其他参数确定模型要处理的分类问题的类型。</p><blockquote><p>根据源码得到判定方法如下：</p><p>step 1：if solver = ‘liblinear’: multi_class = ‘ovr’</p><p>step 2：elif n_classes &gt; 2: multi_class = ‘multinomial’</p><p>step 3：else: multi_class = ‘ovr’</p></blockquote></li><li><p><strong>‘ovr’</strong>：表示当前处理的是二分类或一对多形式的多分类问题</p></li><li><p><strong>‘multinomial’</strong>：表示当前处理的是多对多形式的多分类问题</p></li></ul></li><li><p><strong>class_weight</strong>：None（默认）/‘balanced’/dict，标签(label)的权重。</p><ul><li><p><strong>None</strong>：所有的label持有相同的权重， 所有类别的权值为1 </p></li><li><p><strong>‘balanced’</strong>：自动调整与样本中类频率成反比的权重，即<code>n_samples/(n_classes*np.bincount(y))</code></p><blockquote><p><strong>‘balanced’如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line"></span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment"># 标签值，一共16个样本</span></span><br><span class="line">np.bincount(y)</span><br><span class="line"><span class="comment"># array([8, 6, 2], dtype=int64) 计算每个类别的样本数量，顺序按类别的出现次序</span></span><br><span class="line">class_weight = <span class="string">'balanced'</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight) <span class="comment"># [0.66666667 0.88888889 2.66666667]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">8</span>))  <span class="comment">#输出 0.6666666666666666</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">6</span>))  <span class="comment">#输出 0.8888888888888888</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">2</span>))  <span class="comment">#输出 2.6666666666666665</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>dict类型</strong></p><p>对于二分类问题，可定义class_weight = {0:0.9, 1:0.1}，这样类别0的权重为0.9，类别1的权重为0.1。</p><p>对于多分类问题，定义的权重必须具体到每个标签下的每个类，其中类是key-value中的key，权重是value。</p><blockquote><p><strong>dict类型如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment">#标签值，一共16个样本</span></span><br><span class="line"></span><br><span class="line">class_weight = &#123;<span class="number">0</span>:<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>,<span class="number">2</span>:<span class="number">5</span>&#125;   <span class="comment"># 设置</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight)   <span class="comment"># 输出：[1. 3. 5.]，也就是字典中设置的值</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><p><strong>class_weight如何体现在逻辑回归的损失函数上？</strong></p></blockquote><p>  class_weight给每个类别分别设置不同的<strong>惩罚参数C</strong>。</p><p>  惩罚项C会相应的放大或者缩小某一类的损失，如果某一类C越大，这一类的损失也被（相对于其他类来说）放大，那么系统会把本次学习重点放在这一类上，使得系统尽可能的预测对这一类的输入，所以惩罚项C不会影响计算的损失，但反向学习时会相应的放大或缩小损失，间接影响学习的方向。</p><p>  (参考：<a href="https://www.zhihu.com/question/265420166/answer/293896934" target="_blank" rel="noopener">https://www.zhihu.com/question/265420166/answer/293896934</a>)</p><blockquote><p><strong>源码关于class_weight与sample_weight在LR损失函数上的具体计算方式？</strong></p></blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sample_weight *= class_weight_[le.fit_transform(y_bin)] </span><br><span class="line"><span class="comment"># 将class_weight乘到每个样本的sample_weight上</span></span><br><span class="line"><span class="comment"># sample_weight : shape (n_samples,)</span></span><br><span class="line"><span class="comment"># le即LabelEncoder，将标签标准化为0/1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">out = -np.sum(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure><h3 id="2-求解算法参数"><a href="#2-求解算法参数" class="headerlink" title="2. 求解算法参数"></a>2. 求解算法参数</h3><ul><li><p><strong>solver</strong>：str，用于求解模型最优化问题的算法，可选{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认’lbfgs’。</p><ul><li><strong>‘liblinear’</strong>：使用坐标轴下降法来迭代优化损失函数。</li><li><strong>‘lbfgs’</strong>：拟牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。</li><li><strong>‘newton-cg’</strong>：牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。？</li><li><strong>‘sag’</strong>：随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅用一部分的样本来计算梯度，适用于样本量大的情况。</li><li><strong>‘saga’</strong>：线性收敛的随机优化算法的变种。</li></ul><blockquote><ul><li><p>对于数据量大小方面，’liblinear’仅限于处理二分类和一对多（OvR）问题，适用于小型数据集。’sag’和’saga’对于大型数据集来说更快（快速收敛仅在量纲大致相同的数据上得到保证），’sag’每次仅使用了部分样本进行梯度迭代，样本量少时不适合。</p></li><li><p>对于多分类问题来说，’liblinear’只能用于一对多（OvR），其它算法还可处理多对多（MvM），而多对多一般比一对多分类相对更准确一些。</p></li><li><p>对于正则化方法来说，’newton-cg’,’sag’,’lbfgs’这三种算法计算时都需要涉及到损失函数的一阶导或二阶导，因此不能用于没有连续导数的l1正则化，只能用于l2正则化。其他两种算法均可使用l1和l2正则化。</p></li></ul></blockquote><p>（这部分还不是很了解……待补充）</p></li><li><p><strong>dual</strong>：bool，是否使用对偶或原始计算方式。对偶方式仅在solver=’liblinear’与penalty=’l2’连用的情况下有小。如果样本量大于特征的数目，这个参数设置为False会更好。（逻辑回归的对偶形式是什么？……待补充）</p></li></ul><h3 id="3-正则化参数"><a href="#3-正则化参数" class="headerlink" title="3. 正则化参数"></a>3. 正则化参数</h3><p>损失函数</p><script type="math/tex; mode=display">\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>其中$C$为正则化参数（$\lambda\ge0$)，$\alpha$为l1正则化的占比（$\alpha\in[0,1]$）。</p><blockquote><p>这里用的是<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" rel="noopener">sklearn官网</a>给出的损失函数形式， sklearn中假设y正负label定义为1和-1，与之前1/0不一样</p><p>损失函数除去正则化和求和部分剩余部分为：$\log(\exp(- y_i (X_i^T w + c)) + 1)$</p><p>当$y_i=+1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp(- (X_i^T w + c)) + 1)$</p><p>当$y_i=-1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp((X_i^T w + c)) + 1)$</p><p>上式难以继续化简，因此对比之前的损失函数形式，观测结果是否一致</p><p>之前的表达式对应的部分为$-[y^i\log(h_\theta(x^i;\theta))+(1-y^i)\log(1-h_\theta(x^i;\theta))]$</p><p>当$y_i=1$时，$-y^i\log(h_\theta(x^i;\theta))=-\log(\frac{1}{1+\exp(-(X_i^T w + c))})=\log(\exp(-(X_i^T w + c))+1)$</p><p>当$y_i=0$时，$-(1-y^i)\log(1-h_\theta(x^i;\theta))=-\log(1-\frac{1}{\exp(-(X_i^T w + c))+1})=\log(\exp((X_i^T w + c)) + 1)$</p><p>从而证明这两种表达形式是等价的。</p><p>（注意sklearn损失函数里没有除以样本量）</p></blockquote><ul><li><strong>penalty</strong>：str，指定正则化策略 ， {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}，默认’l2’。’elasticnet’同时包含‘l1’和‘l2’正则化。</li><li><strong>C</strong>：float，正则化系数$\lambda$的倒数（乘在损失函数的前面，与乘在正则化部分前效果相同，均用来平衡两个部分的比重），必须是一个大于0的浮点数，默认值1.0，即默认正则项与损失函数的比值是1:1。</li><li><strong>l1_ratio</strong>：float，l1正则化的占比$\rho$，取值范围[0,1]，默认为None。仅当penalty=’elasticnet’时使用，对于0&lt; l1_ratio &lt;1，惩罚是l1和l2正则化的组合。</li></ul><h3 id="4-控制迭代次数参数"><a href="#4-控制迭代次数参数" class="headerlink" title="4. 控制迭代次数参数"></a>4. 控制迭代次数参数</h3><ul><li><strong>max_iter</strong>：int，控制梯度下降的迭代次数（仅适用于solver=’newton-cg’, ‘lbfgs’, ‘sag’）。默认值为100。值过小损失函数可能会没有收敛到最小值，值过大会使得梯度下降迭代次数过多，模型运行时间缓慢。</li><li><strong>tol</strong>：float，让迭代停下的最小值。默认1e-4。数字越大迭代越早停下。</li></ul><h3 id="5-其他参数"><a href="#5-其他参数" class="headerlink" title="5. 其他参数"></a>5. 其他参数</h3><ul><li><strong>random_state</strong>：int(可选)，随机数种子，可选参数（仅适用于solver=’liblinear’, ‘sag’）。默认为无。</li><li><strong>verbose</strong>：int，日志冗长度。对于solver=’liblinear’, ‘lbfgs’，当设置为大于等于1的任何整数时，输出训练的详细过程 。默认为0，不输出训练过程。</li><li><strong>warm_start</strong>：bool，是否进行热启动，默认为False。若设置为True，则以上一次fit的结果作为此次的初始化，如果”solver”参数为”liblinear”时无效。 </li><li><strong>n_jobs</strong>：int，并行数。int类型，默认为1。等于1时用CPU的一个内核运行程序，等于-1时用所有CPU的内核运行程序。 </li></ul><h2 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h2><ul><li><strong>coef_</strong>：预测函数中特征对应的系数$w$。</li><li><strong>intercept_</strong>：预测函数中的截距$c$。</li><li><strong>n_iter_</strong>：实际迭代次数。</li></ul><h2 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h2><ul><li><strong>fit(x,y[,sample_weight])</strong>：训练模型。</li><li><strong>predict(x)</strong>：用模型进行训练，返回预测值。</li><li><strong>predict_log_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率的对数值。</li><li><strong>predict_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率值。</li><li><strong>score(x,y[,sample_weight])</strong>：返回在(x,y)上预测的准确率。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line">clf.predict(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本的类别</span></span><br><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本属于各个类的概率</span></span><br><span class="line">clf.score(X, y) <span class="comment"># 返回准确率</span></span><br><span class="line"></span><br><span class="line">clf.coef_ <span class="comment"># 系数</span></span><br><span class="line">clf.intercept_ <span class="comment"># 截距</span></span><br><span class="line">clf.n_iter_ <span class="comment"># 迭代次数</span></span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression</a> </p><p><em>class</em> <code>pyspark.ml.classification.LogisticRegression</code>(<em>featuresCol=’features’</em>, <em>labelCol=’label’</em>, <em>predictionCol=’prediction’</em>, <em>maxIter=100</em>, <em>regParam=0.0</em>, <em>elasticNetParam=0.0</em>, <em>tol=1e-06</em>, <em>fitIntercept=True</em>, <em>threshold=0.5</em>, <em>thresholds=None</em>, <em>probabilityCol=’probability’</em>, <em>rawPredictionCol=’rawPrediction’</em>, <em>standardization=True</em>, <em>weightCol=None</em>, <em>aggregationDepth=2</em>, <em>family=’auto’</em>, <em>lowerBoundsOnCoefficients=None</em>, <em>upperBoundsOnCoefficients=None</em>, <em>lowerBoundsOnIntercepts=None</em>, <em>upperBoundsOnIntercepts=None</em>) </p><h2 id="参数列表-1"><a href="#参数列表-1" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数-1"><a href="#1-基本模型参数-1" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fitIntercept</strong>：bool，是否包含截距项，默认True。</p></li><li><p><strong>family</strong>：str，表示分类是二分类还是多分类，默认’auto’，还可选’binomial’和’multinomial’。</p><blockquote><p>spark中处理多分类问题的’multinomial’使用的是<strong>softmax回归</strong></p><p>(参考：<a href="https://blog.csdn.net/u013855234/article/details/84343963" target="_blank" rel="noopener">spark 2.x 源码分析 之 Logistic Regression 逻辑回归</a>)</p><p>在多分类问题中，假设有$C$个类，即类别标签$y\in\{1,2,…,C\}$，则给定一个样本$x$，softmax回归预测样本$x$属于类别$c$的后验概率为</p><script type="math/tex; mode=display">P(y=c|x;\theta)=\frac{\exp(\theta^T_cx)}{\sum_{c=1}^{C}\exp(\theta^T_cx)}</script><p>其中$\theta^T_c$是第$c$类的权重向量，则样本$x$属于每个类别的概率可以由向量表示，向量的第$c$个元素就是样本被预测为第$c$类的概率。</p></blockquote></li><li><p><strong>threshold</strong>：float，分类中的阈值，默认为0.5。</p></li><li><p><strong>thresholds</strong>：list，分类中的阈值，默认为0.5。多元分类中的thresholds是为了调整预测每个类别时的概率。数组长度必须和类别数目相等，且值都大于0。若thresholds长度为2（即对于二分类问题），要满足<code>threshold = 1/(1+threholds[0]/threholds[1])</code></p><blockquote><p>这两个阈值看上去很迷惑，因此直接进行测试……</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类的情况</span></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThreshold(<span class="number">0.8</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment"># 会认为Thresholds = [1,6], Threshold = 0.8</span></span><br><span class="line"><span class="comment"># 报错，因为不满足threshold = 1/(1+threholds[0]/threholds[1])</span></span><br><span class="line"></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>]).setThreshold(<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># 会认为Threshold = 0.8, 忽略Thresholds(被覆盖，之前定义无效)</span></span><br><span class="line"><span class="comment"># 不报错，将超过0.8的类概率预测为该类</span></span><br></pre></td></tr></table></figure><p>对于多分类的情况比较简单，无论thresholds和threshold如何设定，仍会按照概率最高类进行预测（迷惑行为）</p><p>（参考：<a href="https://stackoverflow.com/questions/47325607/set-thresholds-in-pyspark-multinomial-logistic-regression" target="_blank" rel="noopener">Set thresholds in PySpark multinomial logistic regression</a>）</p></li><li><p><strong>standardization</strong>：bool，是否在训练模型之前对特征进行标准化，默认True。</p><blockquote><p>这里体现出spark与python的不同，spark会默认对特征进行标准化。</p><p>但如果设置<code>standardization = False</code>，仍会将数据进行标准化以提高收敛速度（又一迷惑行为），从而获得相同效果的目标函数。</p><p>这里的标准化是直接除以变量的标准差，没有减去均值的部分。</p></blockquote><p><a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L683" target="_blank" rel="noopener">源码标准化部分</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> standardizationParam = $(standardization)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regParamL1Fun</span> </span>= (index: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">// Remove the L1 penalization on the intercept</span></span><br><span class="line">  <span class="keyword">val</span> isIntercept = $(fitIntercept) &amp;&amp; index &gt;= numFeatures * numCoefficientSets</span><br><span class="line">  <span class="keyword">if</span> (isIntercept) &#123;</span><br><span class="line">    <span class="number">0.0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (standardizationParam) &#123;</span><br><span class="line">      regParamL1</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> featureIndex = index / numCoefficientSets</span><br><span class="line">      <span class="comment">// If `standardization` is false, we still standardize the data</span></span><br><span class="line">      <span class="comment">// to improve the rate of convergence; as a result, we have to</span></span><br><span class="line">      <span class="comment">// perform this reverse standardization by penalizing each component</span></span><br><span class="line">      <span class="comment">// differently to get effectively the same objective function when</span></span><br><span class="line">      <span class="comment">// the training dataset is not standardized.</span></span><br><span class="line">      <span class="keyword">if</span> (featuresStd(featureIndex) != <span class="number">0.0</span>) &#123;</span><br><span class="line">        regParamL1 / featuresStd(featureIndex)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="number">0.0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>aggregationDepth</strong>：int，树聚合所建议的深度，默认为2</p></li><li><p><strong>lowerBoundsOnCoefficients(upperBoundsOnCoefficients)</strong>：？</p></li><li><p><strong>lowerBoundsOnIntercepts(upperBoundsOnIntercepts)</strong>：？</p></li></ul><h3 id="2-正则化参数"><a href="#2-正则化参数" class="headerlink" title="2. 正则化参数"></a>2. 正则化参数</h3><p>正则化部分</p><script type="math/tex; mode=display">\lambda[\frac{1 - \rho}{2}w^T w + \rho \|w\|_1], \rho\in[0,1], \lambda\ge0</script><p>其中$\lambda$为regParam，$\rho$为elasticNetParam。</p><ul><li><p><strong>regParam</strong>：float，正则化参数，默认0.0，即不进行正则化。</p></li><li><p><strong>elasticNetParam</strong>：float，正则化范式比，即l1正则化的占比。默认0.0，即只使用l2正则化。</p><blockquote><p>与sklearn的正则化参数$C$不同，这里的$\lambda$是乘在正则化部分，而$C$乘在损失部分</p><p>仅表达方式不同，改变了参数的位置</p><p>以l2正则为例</p><script type="math/tex; mode=display">J(\theta)+\lambda L_2 \Longleftrightarrow CJ(\theta)+L_2</script><p>$\lambda$越大$C$越小，正则项的地位越高，优化时集中优化$L_2$，从而使参数$\theta$中的元素尽量小</p></blockquote></li></ul><h3 id="3-控制迭代次数参数"><a href="#3-控制迭代次数参数" class="headerlink" title="3. 控制迭代次数参数"></a>3. 控制迭代次数参数</h3><ul><li><strong>maxIter</strong>：int，最大迭代次数，默认100。（与sklearn完全一致）</li><li><strong>tol</strong>：float，让迭代停下的最小值，数字越大迭代越早停下，默认1e-6。（sklearn默认1e-4)</li></ul><h3 id="4-设定列名参数"><a href="#4-设定列名参数" class="headerlink" title="4. 设定列名参数"></a>4. 设定列名参数</h3><p>这部分参数仅用于指定列名和设置输出的列名</p><ul><li><p><strong>featuresCol</strong>：str，输入的数据集中的特征列名（一个合并后的列名），默认’features’</p><blockquote><p>spark输入到模型训练的数据需要把多列的特征合并成一列（测试集也一样）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">model_train_df = model_train.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line">model_test_df = model_test.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line"><span class="comment"># 这里标签列均在最后一列</span></span><br><span class="line"><span class="comment"># 然后就可以在训练模型时令参数featuresCol = 'features'（也可以不用设定，默认值即为'features'）</span></span><br></pre></td></tr></table></figure></li><li><p><strong>labelCol</strong>：str，输入的数据集中的标签列名（同featuresCol），默认’label’</p></li><li><p><strong>predictionCol</strong>：str，输出的模型预测结果中样本预测类的列名，默认’prediction’</p></li><li><p><strong>rawPredictionCol</strong>：str，输出的模型预测结果中原始概率的列名，默认’rawPrediction’</p></li><li><p><strong>probabilityCol</strong>：str，输出的模型预测结果中最终预测概率的列名，默认’probability’</p></li><li><p><strong>weightCol</strong>：str，样本权重列名，默认None，即所有样本的权重均视为等权重。</p></li></ul><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression, LogisticRegressionModel</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">df = sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line"><span class="comment"># 转换为features和label两列的形式</span></span><br><span class="line">model_df = df.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>])</span><br><span class="line"><span class="comment"># 划分训练集、测试集</span></span><br><span class="line">train, test = model_df.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>], seed = <span class="number">123</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lrModel = lr.fit(train)</span><br><span class="line"><span class="comment"># 查看模型系数和截距</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'coefficient'</span>:list(lrModel.coefficientMatrix.toArray()[<span class="number">0</span>])&#125;, index = df.columns[:<span class="number">-1</span>]).sort_values(by = <span class="string">'coefficient'</span>, ascending = <span class="literal">False</span>)</span><br><span class="line">lrModel.interceptVector</span><br><span class="line"><span class="comment"># 在测试集上预测</span></span><br><span class="line">lr_test = lrModel.transform(test)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator()</span><br><span class="line">print(evaluator.evaluate(lr_test, &#123;evaluator.matricName: <span class="string">'areaUnderROC'</span>&#125;))</span><br></pre></td></tr></table></figure><p>模型输出结果（仅取前两行为例）</p><div class="table-container"><table><thead><tr><th>features</th><th>label</th><th>rawPrediction</th><th>probability</th><th>prediction</th></tr></thead><tbody><tr><td>[4.9,3.1,1.5,0.1]</td><td>0</td><td>[60.297,-7.393,-52.905]</td><td>[1,0,0]</td><td>0</td></tr><tr><td>[5.0,3.2,1.2,0.2]</td><td>0</td><td>[64.815,-8.896,-55.919]</td><td>[1,0,0]</td><td>0</td></tr></tbody></table></div><p>该模型为多分类情况，其中rawPrediction为线性回归模型输出结果，probability为经过softmax过后得到的逻辑回归结果。具体来说，以第一行为例，根据rawPrediction输出probability，并以最大值对应的类作为最终预测的类</p><script type="math/tex; mode=display">\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 1\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0</script><h1 id="集群scala实现"><a href="#集群scala实现" class="headerlink" title="集群scala实现"></a>集群scala实现</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.param.<span class="type">ParamMap</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.&#123;<span class="type">Vector</span>, <span class="type">Vectors</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// basic variable settings</span></span><br><span class="line"><span class="keyword">val</span> train_tbl = <span class="string">"test.sklearn_dataset_iris_train"</span></span><br><span class="line"><span class="keyword">val</span> test_tbl = <span class="string">"test.sklearn_dataset_iris_test"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hc = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">//data processing</span></span><br><span class="line"><span class="keyword">val</span> train_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$train_tbl</span>"</span>).cache())</span><br><span class="line"><span class="keyword">val</span> test_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$test_tbl</span>"</span>).cache())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> featureCols = train_dataset.columns.filter(x=&gt;x.split(<span class="string">"_"</span>)(x.split(<span class="string">"_"</span>).length<span class="number">-1</span>)!=<span class="string">"label"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> assembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>().setInputCols(featureCols).setOutputCol(<span class="string">"features"</span>))</span><br><span class="line"><span class="keyword">val</span> train_data = assembler.transform(train_dataset)</span><br><span class="line"><span class="keyword">val</span> test_data = assembler.transform(test_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line">lr.setMaxIter(<span class="number">10</span>).setRegParam(<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">val</span> lr_model = lr.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model evaluation</span></span><br><span class="line"><span class="keyword">val</span> auc_calculator = lr_model.transform(test_data)</span><br><span class="line"><span class="keyword">val</span> evaluator = (<span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>())</span><br><span class="line"><span class="keyword">val</span> auc = evaluator.evaluate(auc_calculator)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> LogisticRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WOE与IV理论介绍及实现</title>
      <link href="/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="WOE定义"><a href="#WOE定义" class="headerlink" title="WOE定义"></a>WOE定义</h2><p>全称：Weight of Evidence</p><p>前提：计算之前需要是离散化后的连续变量or离散变量。</p><p>对于变量第$i$个取值的$WOE_i$的计算公式为</p><script type="math/tex; mode=display">WOE_i = ln(\frac{py_i} {pn_i})=ln(\frac{ \frac{ \#y_i }{ \#y_T } } { \frac{ \#n_i } { \#n_T }})=ln(\frac{\frac{ \#y_i }{ \#n_i } } {\frac{ \#y_T } { \#n_T } })</script><p>其中 #$y_i $表示在第$i$个取值的样本中（或第$i$个箱内）的正样本个数，#$ y_T $表示所有正样本的个数，$py_i$表示在第$i$个取值的样本中（或第$i$个箱内）正样本占所有正样本的比例。</p><p>从最后一个等号后的表达式可以看出，$WOE_i$表示的是某个取值下正样本和负样本的比值，与所有样本中这个比值的差异。这个差异是用比值再取对数来表示的。$WOE_i$越大，表示差异越大，则这个取值下的样本为正的可能性越大。</p><h2 id="WOE编码作用"><a href="#WOE编码作用" class="headerlink" title="WOE编码作用"></a>WOE编码作用</h2><ol><li>标准化功能。编码过后的自变量其实具备了某种标准化的性质，自变量内部的各个取值之间都可以直接进行比较，且不同自变量之间的各个取值也可以通过$WOE_i$进行直接的比较。</li><li>可以反映出自变量的贡献情况（？）。自变量内部$WOE_i$值的变异（波动）情况，结合模型拟合出的系数，构造出各个自变量的贡献率和相对重要性。一般地，系数越大，$WOE_i$的方差越大，则自变量的贡献率越大。</li></ol><h2 id="IV​"><a href="#IV​" class="headerlink" title="IV​"></a>IV​</h2><p>全程：Information Value</p><p>在$WOE_i$的基础上，$IV$在$WOE_i$的前面乘以一个系数$(py_i-pn_i)$作为各箱$WOE_i$的权重，并进行加权求和，其计算公式如下</p><script type="math/tex; mode=display">IV=\sum_{i=1}^{N}(py_i-pn_i)*WOE_i=\sum_{i=1}^{N}(py_i-pn_i)*ln(\frac{py_i}{pn_i})</script><p>其中$N$为分箱的个数（变量的取值个数），系数$(py_i-pn_i)$为箱内正样本占比与负样本占比的差。该系数不仅保证了每个箱乘积的值为非负数，更重要的是考虑了变量当前箱中样本的数量占整体样本数量的比例，比例越高，该箱对变量整体预测能力的贡献越高。</p><p>$IV$衡量某个特征对目标的影响程度，通过该特征中正负样本的比例与总体正负样本的比例，来对比和计算其关联程度，因此可以代表<strong>该特征上的信息量</strong>以及<strong>该特征对模型的贡献</strong>。</p><p>$IV$是对于整个特征来说的，代表的意义由下表来控制：</p><div class="table-container"><table><thead><tr><th>IV</th><th>特征对预测函数的贡献</th></tr></thead><tbody><tr><td>&lt;0.03</td><td>特征几乎不含有效信息，对模型没有贡献，可以删除</td></tr><tr><td>[0.03, 0.10)</td><td>有效信息很少，对模型的贡献度低</td></tr><tr><td>[0.10, 0.30)</td><td>有效信息一般，对模型的贡献度中等</td></tr><tr><td>[0.30, 0.50)</td><td>有效信息较多，对模型的贡献度较高</td></tr><tr><td>&gt;=0.50</td><td>有效信息非常多，对模型的贡献极高且可疑</td></tr></tbody></table></div><p>因此通常会选择$IV$值在0.1~0.5范围内的特征。</p><h1 id="单机python"><a href="#单机python" class="headerlink" title="单机python"></a>单机python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcWOE</span><span class="params">(df, col, target)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得某个变量的分箱操作后每个箱体对应的WOE值，并且基于WOE值计算该变量的IV值</span></span><br><span class="line"><span class="string">    :param df: 包含需要计算WOE的变量和目标变量</span></span><br><span class="line"><span class="string">    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的离散型变量</span></span><br><span class="line"><span class="string">    :param target: 目标变量，0、1表示好、坏</span></span><br><span class="line"><span class="string">    :return: 返回WOE和IV</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    total = df.groupby([col])[target].count()  <span class="comment"># 每个箱体的总数</span></span><br><span class="line">    total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)    </span><br><span class="line">    bad = df.groupby([col])[target].sum()      <span class="comment"># 每个箱体的坏样本数</span></span><br><span class="line">    bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line">    regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>)</span><br><span class="line">    regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    N = sum(regroup[<span class="string">'total'</span>]) <span class="comment"># 总体样本数</span></span><br><span class="line">    B = sum(regroup[<span class="string">'bad'</span>])   <span class="comment"># 总体坏样本数</span></span><br><span class="line">    regroup[<span class="string">'good'</span>] = regroup[<span class="string">'total'</span>] - regroup[<span class="string">'bad'</span>]  <span class="comment"># 每个箱体的好样本数</span></span><br><span class="line">    G = N - B                 <span class="comment"># 总体好样本数</span></span><br><span class="line">    regroup[<span class="string">'bad_pcnt'</span>] = regroup[<span class="string">'bad'</span>].map(<span class="keyword">lambda</span> x: x/B)    <span class="comment"># 每个箱体的坏样本数占总体的坏样本数的比例</span></span><br><span class="line">    regroup[<span class="string">'good_pcnt'</span>] = regroup[<span class="string">'good'</span>].map(<span class="keyword">lambda</span> x: x/G)  <span class="comment"># 每个箱体的好样本数占总体的好样本数的比例</span></span><br><span class="line">    <span class="comment"># WOEi计算公式(不含系数)： WOE(每个箱体) = ln(该箱体的好样本数占总体的好样本数的比例/该箱体的好样本数占总体的好样本数的比例）</span></span><br><span class="line">    regroup[<span class="string">'WOE'</span>] = regroup.apply(<span class="keyword">lambda</span> x: np.log(x.good_pcnt/x.bad_pcnt),axis=<span class="number">1</span>)   </span><br><span class="line">    WOE_dict = regroup[[col,<span class="string">'WOE'</span>]].set_index(col).to_dict(orient=<span class="string">'index'</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> WOE_dict.items(): <span class="comment"># k代表箱体名，v代表了以WOE为键，箱体的实际WOE值为value的字典</span></span><br><span class="line">        WOE_dict[k] = v[<span class="string">'WOE'</span>]</span><br><span class="line">    <span class="comment"># 计算该变量的IV值：sum((某箱体的好样本数占总体的好样本数的比例 - 该箱体的坏样本数占总体的坏样本数的比例)*WOE(某个箱体))</span></span><br><span class="line">    IV = regroup.apply(<span class="keyword">lambda</span> x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt/x.bad_pcnt),axis = <span class="number">1</span>)</span><br><span class="line">    IV = sum(IV)</span><br><span class="line">    <span class="keyword">return</span> &#123;col: &#123;<span class="string">"WOE"</span>: WOE_dict, <span class="string">'IV'</span>:IV&#125;&#125;</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先进行等频分箱和编码（用到的函数见'分箱'）</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">     df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后循环计算每个变量的各箱WOE和IV，输出result</span></span><br><span class="line">result = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     woe_iv_dict = CalcWOE(df_cut, col, target = <span class="string">'label'</span>)</span><br><span class="line">     result = pd.concat([result, pd.DataFrame(woe_iv_dict).T])</span><br></pre></td></tr></table></figure><h1 id="集群pyspark"><a href="#集群pyspark" class="headerlink" title="集群pyspark"></a>集群pyspark</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_woe_iv</span><span class="params">(tableName, featureCol, labelCol)</span>:</span></span><br><span class="line">     <span class="string">'''</span></span><br><span class="line"><span class="string">     计算spark.sql.DataFrame的WOE和IV</span></span><br><span class="line"><span class="string">     :param tableName: 储存在HDFS上的表名</span></span><br><span class="line"><span class="string">     :param featureCol: 所需要计算的字段名</span></span><br><span class="line"><span class="string">     :param labelCol: 标签列名</span></span><br><span class="line"><span class="string">     :return WOE字典和IV值</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">     df = spark.sql(<span class="string">"select &#123;featureCol&#125;, &#123;labelCol&#125; from &#123;tableName&#125;"</span>.format(featureCol=featureCol, labelCol=labelCol, tableName=tableName))</span><br><span class="line">     <span class="comment"># 使用crosstab函数生成列联表，并对列进行命名</span></span><br><span class="line">     binCount = df.crosstab(featureCol,labelCol).toDF(<span class="string">'feature'</span>,<span class="string">'neg_cnt'</span>,<span class="string">'pos_cnt'</span>)</span><br><span class="line">     <span class="comment"># 注册临时表便于后续调用</span></span><br><span class="line">     binCount.registerTempTable(<span class="string">"binCount"</span>)</span><br><span class="line">     <span class="comment"># 计算主要步骤</span></span><br><span class="line">     woe_df = spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">        select</span></span><br><span class="line"><span class="string">          feature,</span></span><br><span class="line"><span class="string">          nvl(log(pos_per/neg_per), 0) as woe_i,</span></span><br><span class="line"><span class="string">          nvl((pos_per-neg_per)*log(pos_per/neg_per), 0) as iv_i</span></span><br><span class="line"><span class="string">        from</span></span><br><span class="line"><span class="string">        (</span></span><br><span class="line"><span class="string">          select</span></span><br><span class="line"><span class="string">            feature,</span></span><br><span class="line"><span class="string">            neg_cnt/(select sum(neg_cnt) from binCount) as neg_per,</span></span><br><span class="line"><span class="string">            pos_cnt/(select sum(pos_cnt) from binCount) as pos_per</span></span><br><span class="line"><span class="string">          from</span></span><br><span class="line"><span class="string">            binCount</span></span><br><span class="line"><span class="string">        ) t</span></span><br><span class="line"><span class="string">     """</span>)</span><br><span class="line">     <span class="comment"># 将计算结果转化为pandas.DataFrame</span></span><br><span class="line">     woe_df_pandas = woe_df.toPandas()</span><br><span class="line">     woe = dict(zip(woe_df_pandas[<span class="string">"feature"</span>].values.reshape(<span class="number">-1</span>,), woe_df_pandas[<span class="string">"woe_i"</span>].values.reshape(<span class="number">-1</span>,),))</span><br><span class="line">     iv = sum(woe_df_pandas[<span class="string">"iv_i"</span>].values)</span><br><span class="line">     <span class="keyword">return</span> woe, iv</span><br></pre></td></tr></table></figure><p>调用并测试运算时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    spark = SparkSession.builder.enableHiveSupport().getOrCreate()</span><br><span class="line">    tableName = <span class="string">"test.calc_woe_iv_sample"</span></span><br><span class="line">    featureCol = <span class="string">"age_level"</span></span><br><span class="line">    labelCol = <span class="string">"label"</span></span><br><span class="line">    time_cost = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        woe, iv = cal_woe_iv(tableName, featureCol, labelCol)</span><br><span class="line">        end = time.time()</span><br><span class="line">        time_cost.append(end-start)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'time cost: %.5f sec'</span> % (np.mean(time_cost)))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'woe:'</span>, woe)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'iv:'</span>, iv)</span><br></pre></td></tr></table></figure><p>集群资源配置：</p><blockquote><p>spark.driver.memory=5G<br>spark.driver.maxResultSize=5G<br>num-executors=20<br>executor-cores=6<br>executor-memory=4G</p></blockquote><p>根据样本量的不同，测试计算时间结果（循环30次取平均）</p><div class="table-container"><table><thead><tr><th></th><th>10w</th><th>100w</th><th>1000w</th><th>1e</th></tr></thead><tbody><tr><td>计算时间(s)</td><td>0.872</td><td>1.819</td><td>3.172</td><td>25.793</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> FeatureEngineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分箱基本方法介绍及实现</title>
      <link href="/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将连续型的数据分为几个数据段，即特征离散化。</p><p>把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率。（百度百科） </p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li><p><strong>离散化后的特征对异常数据有很强的鲁棒性，模型更加稳定。</strong>比如年龄&gt;70岁为一个分箱，异常数据如年龄为300岁同样会划分到该箱，不会给模型造成很大的干扰；比如将20岁-30岁划分为一个区间，不会因为年龄增长一岁就变成一个完全不同的人。而对于处于区间相邻处的样本会刚好相反，因此如何划分区间也很重要。</p></li><li><p>对于如逻辑回归的广义线性模型，表达能力受限，单变量离散化为$N$个变量后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。（？）</p></li><li><p>离散化后可以进行特征交叉，进一步<strong>引入非线性，提升模型表达能力</strong>。</p></li><li><p>可以将缺失值作为独立的箱代入变量。</p></li><li><p>可以将所有变量转换到相似的尺度上（不需要进行归一化）。</p></li></ol><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="无监督分箱"><a href="#无监督分箱" class="headerlink" title="无监督分箱"></a>无监督分箱</h3><h4 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h4><p>每个箱包含大致相等的样本数量，可以根据分位数进行划分，如四分位数等。</p><h4 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h4><p>从变量的最小值到最大值之间均等分为$N$等份。若$m$和$M$分别代表最小值和最大值，则每个区间的长度为$w=\frac{M-m}{N}$，区间的边界值为$m+w,m+2w,…,m+(N-1)w$。这里只考虑边界，每个等份的样本数量一般不等。</p><h3 id="有监督分箱"><a href="#有监督分箱" class="headerlink" title="有监督分箱"></a>有监督分箱</h3><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p><strong>初始化</strong></p><ul><li><p>根据连续变量值的大小进行排序。</p></li><li><p>把每一个单独的值视为一个箱体，构建最初的离散化。目的是想从每个单独的箱体开始逐渐合并。</p></li></ul></li><li><p><strong>合并</strong>：在初始化构建完毕后，该步就是不断地进行自底向上的合并，直到满足停止条件。</p><ul><li><p><strong>计算所有相邻分箱的卡方值</strong>。比如有1,2,3,4这4个分箱，绑定相邻的两个分箱，一共有3组：12,23,34，然后分别计算三个绑定组的卡方值。卡方值的计算公式为</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}</script><p>其中，$m$表示要合并相邻分箱的数目（$m=2$表示对两个箱进行合并），$k$表示目标变量的类别数（两分类或多分类），$A_{ij}$表示实际频数（即第$i$个分箱第$j$类别的频数），$E_{ij}$表示期望频数，计算公式如下（可根据公式$P(AB)=P(A)P(B)$推导出来）</p><script type="math/tex; mode=display">E_{ij}=\frac{R_i*C_j}{N}</script><p>其中$R_i$和$C_j$分别是实际频数整行和整列的加和，$N$为总样本量。</p></li><li><p><strong>从计算的卡方值中找出最小的一个，并把这两个分箱合并</strong>。比如23是卡方值最小的一个，那么就将2和3合并，经过本轮的计算后分箱就变为1,23,4。</p><p>从合并的方法可以看出，卡方分箱的基本思想是，<strong>如果两个相邻的区间具有非常类似的类分布（即低卡方值，低卡方值表明它们具有类似的类分布），那么这两个区间可以合并，否则应该分开</strong>。</p></li></ul></li><li><p><strong>停止条件</strong>：以上仅是每一轮需要计算的内容，若不设置停止条件，算法会一直运行。一般从以下两个方面设置停止条件：</p><ul><li>卡方停止的阈值</li><li>分箱数目的限制</li></ul><p>即当所有分箱对的卡方值均大于阈值，且分箱数大于分箱数时，计算就会继续，直到不满足。</p><p>以上两个阈值一般根据经验来定义，来作为分箱函数的参数进行设置，一般使用0.90,0.95,0.99的置信度，分箱数一般可以设置为5。</p></li></ul><p><em>例如：</em></p><p>对于某两个箱（分箱1和分箱2），实际频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th><th>行频数和</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$A_{11}$</td><td>$A_{12}$</td><td>$R_1$</td></tr><tr><td><strong>分箱2</strong></td><td>$A_{21}$</td><td>$A_{22}$</td><td>$R_2$</td></tr><tr><td><strong>列频数和</strong></td><td>$C_1$</td><td>$C_2$</td><td>$N$</td></tr></tbody></table></div><p>期望频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$E_{11}=\frac{R_1*C_1}{N}$</td><td>$E_{12}=\frac{R_1*C_2}{N}$</td></tr><tr><td><strong>分箱2</strong></td><td>$E_{21}=\frac{R_2*C_1}{N}$</td><td>$E_{22}=\frac{R_2*C_2}{N}$</td></tr></tbody></table></div><p>代入卡方公式求解，过程如下：</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}=[\frac{(A_{11}-E_{11})^2}{E_{11}}+\frac{(A_{12}-E_{12})^2}{E_{12}}]+[\frac{(A_{21}-E_{21})^2}{E_{21}}+\frac{(A_{22}-E_{22})^2}{E_{22}}]</script><p>如果计算结果是所有卡方值中最小的，说明这两个分箱具有最相似的类分布，因此把它们合并。</p><p>（参考 <a href="https://cloud.tencent.com/developer/article/1418720" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1418720</a> ）</p><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><h2 id="无监督分箱-1"><a href="#无监督分箱-1" class="headerlink" title="无监督分箱"></a>无监督分箱</h2><ul><li>UnsupervisedSplitBin函数：对数值型变量进行分组，分组的依据有等频和等距，最后返回划分点列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UnsupervisedSplitBin</span><span class="params">(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对数值型变量进行划分，最后返回划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: 数据集</span></span><br><span class="line"><span class="string">:param col: 需要分箱的变量。仅限数值型变量</span></span><br><span class="line"><span class="string">:param numOfSplit: 需要分箱个数，默认是5</span></span><br><span class="line"><span class="string">:param method: 分箱方法，'equalFreq'：默认是等频，否则是等距</span></span><br><span class="line"><span class="string">:return: 返回划分点组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'equalFreq'</span>:</span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = [np.percentile(df[col], <span class="number">100</span>/numOfSplit*i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line">splitPoint = sorted(list(set(frequency_cutoff)))</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">'equalDis'</span>:</span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">var_max, var_min = max(df[col]), min(df[col])</span><br><span class="line">interval_len = (var_max-var_min)/numOfSplit</span><br><span class="line">splitPoint = [var_min+i*interval_len <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="string">'Sorry, no such method'</span></span><br></pre></td></tr></table></figure><h2 id="有监督分箱：卡方分箱"><a href="#有监督分箱：卡方分箱" class="headerlink" title="有监督分箱：卡方分箱"></a>有监督分箱：卡方分箱</h2><ul><li><p>SplitData函数：用于对某变量进行划分，根据划分的组数，返回划分点数值组成的列表</p></li><li><p>Chi2函数：用于计算卡方值，返回卡方值（按照卡方检验的原理进行计算）</p></li><li><p>BinBadRate函数：按某变量进行分组，计算分组后每组的坏样本率，返回的有，字典形式，数据框，总体坏样本率（可选）</p></li><li><p>AssignGroup函数：根据分组后的划分点列表，给某个需分箱的变量的每个取值进行分箱前的匹配，形成对应箱的映射</p></li><li><p>AssignBin函数：将某列的每个取值进行分箱编号</p></li><li><p>ChiMerge函数：卡方分箱的主体函数，其中调用了前面五个基础函数，返回的是最终的满足所有限制条件的划分点列表</p><ul><li><p>其中需要满足：</p><p>（1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</p><p>（2）每个箱体必须同时包含好坏样本</p><p>（3）每个箱体的占比不低于预设值（可选）</p><p>（4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(df,col,numOfSplit,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得数据切分时对应位置的数值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame</span></span><br><span class="line"><span class="string">:param col: str 选择数据集中的某列进行操作</span></span><br><span class="line"><span class="string">:param numOfSplit: int 划分的组数</span></span><br><span class="line"><span class="string">:param special_attribute: 用于过滤掉一些特殊的值，不参与数据划分（可选，默认不过滤）</span></span><br><span class="line"><span class="string">:return: 返回划分点位置对应的数值组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"><span class="keyword">if</span> special_attribute != []:</span><br><span class="line">df2 = df2.loc[~df2[col].isin(special_attribute)]  <span class="comment"># 排除有特殊值的样本行</span></span><br><span class="line">N = len(df2) <span class="comment"># 样本总数</span></span><br><span class="line">n = int(N/numOfSplit) <span class="comment"># 每组包含的样本量</span></span><br><span class="line">SplitPointIndex = [i*n <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)] <span class="comment"># 不包含numOfSplit</span></span><br><span class="line"><span class="comment"># 如总样本N=20，numofsplit为5组，则每组包含4个元素(n=4)</span></span><br><span class="line"><span class="comment"># 则最后得到的SplitPointIndex=[4,8,12,16]，即切分点位置，各组样本为0-3/4-7/8-11/12-15</span></span><br><span class="line">rawValues = sorted(list(df[col])) <span class="comment"># sorted返回一个新的排列后的列表</span></span><br><span class="line">SplitPoint = [rawValues[i] <span class="keyword">for</span> i <span class="keyword">in</span> SplitPointIndex] <span class="comment"># 返回位置索引上对应的数值</span></span><br><span class="line"><span class="comment"># 为了以防万一，去重再排序</span></span><br><span class="line">SplitPoint = sorted(list(set(SplitPoint)))</span><br><span class="line"><span class="keyword">return</span> SplitPoint </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignGroup</span><span class="params">(x,bin)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于：根据分组后的划分点列表（bin)，给某个需要分箱的变量进行分箱前的匹配，形成对应箱的映射，以便后期分箱操作，将值划分到不同箱体</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param bin: 上述变量分组后（通过SplitData函数）对应的划分位置的数值组成的列表</span></span><br><span class="line"><span class="string">:return: x在分箱结果下的映射 </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">N = len(bin)            <span class="comment"># 划分点的长度</span></span><br><span class="line"><span class="keyword">if</span> x&lt;=min(bin):         <span class="comment"># 如果某个取值小于等于最小划分点，则返回最小划分点</span></span><br><span class="line"><span class="keyword">return</span> min(bin)</span><br><span class="line"><span class="keyword">elif</span> x&gt;max(bin):        <span class="comment"># 如果某个取值大于最小划分点，则返回10e10</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">10e10</span></span><br><span class="line"><span class="keyword">else</span>:                   <span class="comment"># 除此之外，返回其他对应的划分点</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> bin[i] &lt; x &lt;= bin[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> bin[i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BinBadRate</span><span class="params">(df,col,target,grantRateIndicator=False)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对变量按照取值（每个取值都是唯一的）进行分组，获得每箱的坏样本率，后期基于该值判断是否需要合并操作</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 需要计算好坏比率的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要计算好坏比率的特征</span></span><br><span class="line"><span class="string">:param target: str 好坏标签</span></span><br><span class="line"><span class="string">:param grantRateIndicator: bool True返回总体的坏样本率，False不返回（可选，默认不返回）</span></span><br><span class="line"><span class="string">:return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator==True时）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">total = df.groupby([col])[target].count()</span><br><span class="line">total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)</span><br><span class="line">bad = df.groupby([col])[target].sum()</span><br><span class="line">bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line"></span><br><span class="line">regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>) <span class="comment"># 每箱的坏样本数，总样本数</span></span><br><span class="line">regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regroup[<span class="string">'bad_rate'</span>] = regroup.apply(<span class="keyword">lambda</span> x: x.bad / x.total, axis=<span class="number">1</span>) <span class="comment"># 加上一列坏样本率</span></span><br><span class="line">dicts = dict(zip(regroup[col],regroup[<span class="string">'bad_rate'</span>])) <span class="comment"># 每箱对应的坏样本率组成的字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> grantRateIndicator:</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">N = sum(regroup[<span class="string">'total'</span>])</span><br><span class="line">B = sum(regroup[<span class="string">'bad'</span>])</span><br><span class="line">overallRate = B / N</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup, overallRate)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Chi2</span><span class="params">(df,totalCol,badCol)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得卡方值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含了每个分组下的样本总数和坏样本数</span></span><br><span class="line"><span class="string">:param totalCol: str 列名，元素由各个分组下的样本总数构成</span></span><br><span class="line"><span class="string">:param badCol: str 列名，元素由各个分组下的坏样本数构成</span></span><br><span class="line"><span class="string">:return: 卡方值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算该特征各个分组下的好样本数</span></span><br><span class="line">df2[<span class="string">'good'</span>] = df2.apply(<span class="keyword">lambda</span> x: x[totalCol]-x[badCol], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求出期望值E</span></span><br><span class="line"><span class="comment"># 1.求出总体的坏样本率和好样本率</span></span><br><span class="line">badRate = sum(df2[badCol]) / sum(df2[totalCol])</span><br><span class="line">goodRate = sum(df2[<span class="string">'good'</span>]) / sum(df2[totalCol])</span><br><span class="line"><span class="comment"># 特殊情况：当全部样本只有好或者坏样本时，卡方值为0</span></span><br><span class="line"><span class="keyword">if</span> badRate <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.根据总体的好坏样本率算出期望的好坏样本数，计算公式为：</span></span><br><span class="line"><span class="comment"># 期望坏（好）样本个数 ＝ 全部样本个数 * 总体的坏（好）样本率</span></span><br><span class="line">df2[<span class="string">'badExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*badRate)</span><br><span class="line">df2[<span class="string">'goodExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*goodRate)</span><br><span class="line"></span><br><span class="line">badCombined = zip(df2[<span class="string">'badExpected'</span>], df2[badCol])</span><br><span class="line">goodCombined = zip(df2[<span class="string">'goodExpected'</span>], df2[<span class="string">'good'</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 按照卡方计算的公式计算卡方值：  ∑ (O - E)^2 / E, O代表实际值，E代表期望值     </span></span><br><span class="line">badChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> badCombined]</span><br><span class="line">goodChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> goodCombined]</span><br><span class="line">chi2 = sum(badChi) + sum(goodChi)</span><br><span class="line"><span class="keyword">return</span> chi2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignBin</span><span class="params">(x,cutOffPoints,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对某个列的某个取值进行分箱编号（编码）</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param cutOffPoints: 上述变量的分组结果，用切分点表示，列表形式</span></span><br><span class="line"><span class="string">:param special_attribute: 不参与分箱的特殊取值（可选）</span></span><br><span class="line"><span class="string">:return: 分箱后的对应的第几个箱，从0开始</span></span><br><span class="line"><span class="string">比如，若cutOffPoints=[10,20,30]，当x=7，返回0；当x=35，返回3</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">numBin = len(cutOffPoints) + <span class="number">1</span> + len(special_attribute)</span><br><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> special_attribute:</span><br><span class="line">i = special_attribute.index(x)+<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> (<span class="number">0</span>-i)</span><br><span class="line"><span class="keyword">if</span> x &lt;= cutOffPoints[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">elif</span> x &gt; cutOffPoints[<span class="number">-1</span>]:</span><br><span class="line"><span class="keyword">return</span> (numBin<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,numBin<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> cutOffPoints[i] &lt; x &lt;= cutOffPoints[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> (i+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ChiMerge</span><span class="params">(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于实际的卡方分箱算法操作，最后返回有实际的划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含目标变量与需要分箱变量的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要分箱的属性</span></span><br><span class="line"><span class="string">:param target: str 目标变量，取值0或1</span></span><br><span class="line"><span class="string">:param max_interval: int 最大分箱数（默认5）。如果原始属性的取值个数低于该参数，不执行这段函数</span></span><br><span class="line"><span class="string">:param special_attribute: list 不参与分箱的属性取值（可选）</span></span><br><span class="line"><span class="string">:param minBinPcnt：最小箱的占比（默认0），如果不满足最小分箱占比继续进行组别合并</span></span><br><span class="line"><span class="string">:return: 分箱结果</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">colLevels = sorted(list(set(df[col])))  <span class="comment"># 某列的不重复值</span></span><br><span class="line">N_distinct = len(colLevels)             <span class="comment"># 某列的不重复值计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> N_distinct &lt;= max_interval:  <span class="comment">#如果原始属性的取值个数低于max_interval，不执行这段函数（不参与分箱）</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The number of original levels for '&#123;col&#125;' is &#123;dis_cnt&#125;, which is less than or equal to max intervals"</span>.format(col=col, dis_cnt=N_distinct))</span><br><span class="line"><span class="keyword">return</span> colLevels[:<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">if</span> len(special_attribute)&gt;=<span class="number">1</span>:</span><br><span class="line">df2 = df.loc[~df[col].isin(special_attribute)] <span class="comment"># 去掉special_attribute后的df</span></span><br><span class="line">N_distinct = len(list(set(df2[col])))  <span class="comment"># 去掉special_attribute后的非重复值计数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤一: 通过col对数据进行分组，求出每组的总样本数和坏样本数</span></span><br><span class="line"><span class="keyword">if</span> N_distinct &gt; <span class="number">100</span>:</span><br><span class="line">split_x = SplitData(df2,col,<span class="number">100</span>) <span class="comment"># 若非重复值计数超过100组，均将其转化成100组，将多余样本都划分到最后一个箱中</span></span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col].map(<span class="keyword">lambda</span> x: AssignGroup(x,split_x))</span><br><span class="line"><span class="comment"># Assgingroup函数：每一行的数值和切分点做对比，返回原值在切分后的映射，</span></span><br><span class="line"><span class="comment"># 经过map以后，生成该特征的值对象的"分箱"后的值        </span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col] <span class="comment"># 不重复值计数不超过100时，不需要进行上述步骤</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过上述过程，我们现在可以将该列进行BadRate计算，用来计算每个箱体的坏样本率 以及总体的坏样本率      </span></span><br><span class="line">(binBadRate, regroup, overallRate) = BinBadRate(df2, <span class="string">'temp_cutoff'</span>, target, grantRateIndicator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在此，我们将每个单独的属性值分成单独一组</span></span><br><span class="line"><span class="comment"># 对属性值进行去重排序，然后两两组别进行合并，用于后续的卡方值计算</span></span><br><span class="line">colLevels = sorted(list(set(df2[<span class="string">'temp_cutoff'</span>])))</span><br><span class="line">groupIntervals = [[i] <span class="keyword">for</span> i <span class="keyword">in</span> colLevels] <span class="comment"># 把每个箱的值打包成[[],[]]的形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤二：通过循环的方式，不断的合并相邻的两个组别，直到：</span></span><br><span class="line"><span class="comment"># （1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</span></span><br><span class="line"><span class="comment"># （2）每个箱体必须同时包含好坏样本</span></span><br><span class="line"><span class="comment"># （3）每个箱体的占比不低于预设值（可选）</span></span><br><span class="line"><span class="comment"># （4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</span></span><br><span class="line">split_intervals = max_interval - len(special_attribute)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次循环时, 计算合并相邻组别后的卡方值。当组别数大于预设的分箱数时，持续选择最小卡方值的组合并</span></span><br><span class="line"><span class="keyword">while</span> len(groupIntervals)&gt;split_intervals:</span><br><span class="line">chisqList = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(len(groupIntervals)<span class="number">-1</span>):</span><br><span class="line">temp_group = groupIntervals[k] + groupIntervals[k+<span class="number">1</span>]  <span class="comment"># 返回的是，这两个值组成的列表</span></span><br><span class="line"><span class="comment"># 因此，可以通过temp_group，每次只选相邻的两组进行相关操作</span></span><br><span class="line">df2b = regroup.loc[regroup[<span class="string">'temp_cutoff'</span>].isin(temp_group)]</span><br><span class="line"><span class="comment"># 计算相邻两组的卡方值(通过调用Chi2函数)</span></span><br><span class="line">chisq = Chi2(df2b,<span class="string">'total'</span>,<span class="string">'bad'</span>)</span><br><span class="line">chisqList.append(chisq)</span><br><span class="line">best_comnbined = chisqList.index(min(chisqList)) <span class="comment"># 检索最小卡方值所在的索引    </span></span><br><span class="line"><span class="comment"># 把groupIntervals的值改成类似的值改成类似从[[1],[2],[3]]到[[1,2],[3]]</span></span><br><span class="line">groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+<span class="number">1</span>]</span><br><span class="line">groupIntervals.remove(groupIntervals[best_comnbined+<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述循环结束后，即可获得预设的分箱数，并且可以得到各分箱的划分点</span></span><br><span class="line">groupIntervals = [sorted(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals]</span><br><span class="line">cutOffPoints = [max(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals[:<span class="number">-1</span>]] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后，我们将某列进行分箱编号</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp_cutoff'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line"><span class="comment"># AssignBin函数：每一行的数值和切分点做对比，返回原值所在的分箱编号（形成新列）    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步，我们想要保证每个箱体都有好坏样本数</span></span><br><span class="line"><span class="comment"># 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本</span></span><br><span class="line">(binBadRate,regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)    <span class="comment"># 返回每箱坏样本率字典，和包含'分箱号、总样本数、坏样本数、坏样本率的数据框'）</span></span><br><span class="line">minBadRate, maxBadRate = min(binBadRate.values()), max(binBadRate.values())</span><br><span class="line"><span class="keyword">while</span> minBadRate ==<span class="number">0</span> <span class="keyword">or</span> maxBadRate == <span class="number">1</span>:</span><br><span class="line"><span class="comment"># 找出全部为好／坏样本的箱</span></span><br><span class="line">indexForBad01 = regroup[regroup[<span class="string">'bad_rate'</span>].isin([<span class="number">0</span>,<span class="number">1</span>])].temp_Bin.tolist()    </span><br><span class="line">bin = indexForBad01[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个划分点需要移除</span></span><br><span class="line"><span class="keyword">if</span> bin == max(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> bin == min(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果是中间的某一箱，则需要和前后中的一个箱体进行合并，具体选择哪个箱体，要依据前后箱体哪个卡方值较小</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(regroup.temp_Bin).index(bin)</span><br><span class="line">prevIndex = list(regroup.temp_Bin)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(regroup.temp_Bin)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成此项合并后，需要再次计算，在新的分箱准则下，每箱是否同时包含好坏样本，</span></span><br><span class="line"><span class="comment"># 如何仍然出现不能同时包含好坏样本的箱体，继续循坏，直到好坏样本同时出现在每个箱体后，跳出</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">(binBadRate, regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">[minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们来检查分箱后的最小占比</span></span><br><span class="line"><span class="keyword">if</span> minBinPcnt &gt; <span class="number">0</span>: <span class="comment"># 如果函数调用初期给的minBinPct不是零，则进一步对箱体进行合并</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>]) <span class="comment"># 得到箱体的最小占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果箱体最小占比小于给定的分箱占比阈值 且 划分点大于2，进入合并循环中</span></span><br><span class="line"><span class="keyword">while</span> minPcnt &lt; minBinPcnt <span class="keyword">and</span> len(cutOffPoints) &gt; <span class="number">2</span>: </span><br><span class="line"><span class="comment"># 找出占比最小的箱</span></span><br><span class="line">indexForMinPcnt = valueCounts[valueCounts[<span class="string">'pcnt'</span>] == minPcnt].index.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除</span></span><br><span class="line"><span class="keyword">if</span> indexForMinPcnt == max(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> indexForMinPcnt == min(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，合并依据是较小的卡方值</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(valueCounts.index).index(indexForMinPcnt)</span><br><span class="line">prevIndex = list(valueCounts.index)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(valueCounts.index)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])        </span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x * <span class="number">1.0</span> / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>])</span><br><span class="line"></span><br><span class="line">cutOffPoints = special_attribute + cutOffPoints</span><br><span class="line"><span class="keyword">return</span> cutOffPoints</span><br></pre></td></tr></table></figure><p>(参考 <a href="https://blog.csdn.net/LuLuYao9494/article/details/92083755" target="_blank" rel="noopener">https://blog.csdn.net/LuLuYao9494/article/details/92083755</a> )</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置分箱参数</span></span><br><span class="line">bins = <span class="number">5</span> <span class="comment"># 分箱数</span></span><br><span class="line">col = <span class="string">'mean radius'</span> <span class="comment"># 分箱字段名</span></span><br><span class="line">target = <span class="string">'label'</span> <span class="comment"># 标签字段名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line"><span class="comment"># [11.366, 12.726, 14.058000000000002, 17.067999999999998]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">distance_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalDis'</span>)</span><br><span class="line"><span class="comment"># [11.2068, 15.432599999999999, 19.6584, 23.8842]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卡方分箱</span></span><br><span class="line">chiMerge_cutoff = ChiMerge(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># [12.46, 13.38, 15.0, 16.84]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以等频分箱为例在数据集上进行编码</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">    <span class="comment"># 得到切分点（获取切分点的方法可替换）</span></span><br><span class="line">    cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">    <span class="comment"># 进行编码</span></span><br><span class="line">    df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p>pyspark.ml.feature提供等频分箱的API可以直接调用</p><p><em>class</em> <code>pyspark.ml.feature.QuantileDiscretizer</code>(<em>numBuckets=2</em>, <em>inputCol=None</em>, <em>outputCol=None</em>, <em>relativeError=0.001</em>, <em>handleInvalid=’error’</em>)<a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer" target="_blank" rel="noopener"><a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/feature.html#QuantileDiscretizer" target="_blank" rel="noopener">source\</a></a></p><p>参数：</p><ol><li>numBuckets：分箱数，但若样本数据只有3个取值，但numBuckets=4，则仍只划分为3个箱</li><li>relativeError：用于控制近似的精度，取值范围为[0,1]，当设置为0时会计算精确的分位数（计算代价较高）</li><li>handleInvalid：选择处理空值的方式，有三种选项：’keep’将空值放入专门的箱中，例如如果使用4个箱，则将非空数据放入箱0-3中，将空值放入特殊的箱4中；’skip’：过滤掉含有空值的行；’error’报错。（？实验了一下这三种选项并没有区别？？？结果没有处理缺失值仍输出空值，也没有报错……？？）</li><li>inputCol：输入要分箱的列名，outputCol：输出分箱后新特征的列名</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line">df = spark.sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line">col = <span class="string">'sepal length (cm)'</span> <span class="comment"># 对该列进行等频分箱</span></span><br><span class="line"></span><br><span class="line">qd = QuantileDiscretizer(numBuckets=<span class="number">5</span>, inputCol=col, outputCol=col+<span class="string">'_bin'</span>)</span><br><span class="line">qd_model = qd.fit(df)</span><br><span class="line"><span class="keyword">print</span> (qd_model.getSplits()) <span class="comment"># 打印分箱的节点</span></span><br><span class="line">df_new = qd_model.transform(df)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> FeatureEngineering </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
