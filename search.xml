<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>白板推导系列5——降维</title>
      <link href="/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/"/>
      <url>/2020/02/05/bai-ban-tui-dao-xi-lie-5-jiang-wei/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>ML中相比训练误差，更加关注的是泛化误差。过拟合问题就是造成泛化误差大的一个原因。</p><p>解决过拟合的方法：</p><ul><li><p>增加样本数量</p></li><li><p>正则化（Ridge/Lasso）</p></li><li><p>降维</p><ul><li>直接降维——特征选择<em>（不是本节的关注点，本节关注线性和非线性降维）</em></li><li>线性降维——PCA，MDS（多维空间缩放）</li><li>非线性降维——流形（Isomap，LLE）</li></ul><blockquote><p>特征维度高往往会造成<strong>维度灾难( Curse of Dimensionality )</strong></p><ul><li>从数学角度来看，每增加一个特征（属性），为了布满它所有的样本空间，所需要的样本数量呈指数级增长（例如对于一个二值变量，则至少需要$2^n$个样本数）</li><li>从几何角度来看，比如对于一个体积为1的超立方体，内接一个超球体，$V_{超球体}=C(0.5)^d$，其中$d$为维度，当$d\rightarrow\infty $时，超球体的体积无限趋于0，那么会造成<strong>数据的稀疏性</strong>，且大部分集中在一起，很难进行分类。</li></ul></blockquote></li></ul><h1 id="概率相关知识"><a href="#概率相关知识" class="headerlink" title="概率相关知识"></a>概率相关知识</h1><p>Data：$X=(x_1,x_2,…,x_N)^T_{N*P}$，$N$个样本，每个样本是$P$维的</p><p>样本均值和样本方差的矩阵表示：</p><script type="math/tex; mode=display">\bar{x}=\frac{1}{N}\sum_{i=1}^{N}x_i=\frac{1}{N}(x_1,x_2,…,x_N)1_N=\frac{1}{N}X^T1_N\\\begin{align}S_{p*p}&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T\\&=\frac{1}{N}(X^T-\bar{x}1_N^T)(X^T-\bar{x}1_N^T)^T\\&=\frac{1}{N}(X^T-\frac{1}{N}X^T1_N1_N^T)(X^T-\frac{1}{N}X^T1_N1_N^T)^T\\&=\frac{1}{N}X^T(I_N-\frac{1}{N}1_N1_N^T)(I_N-\frac{1}{N}1_N1_N^T)^TX\\&=\frac{1}{N}X^THH^TX\\&=\frac{1}{N}X^THX\end{align}</script><p>其中$H=I_N-\frac{1}{N}1_N1_N^T$，且具有性质$H^T=H,H^n=H$，称为中心矩阵( centering matrix )</p><h1 id="主成分分析PCA"><a href="#主成分分析PCA" class="headerlink" title="主成分分析PCA"></a>主成分分析PCA</h1><ul><li><p>一个中心：<strong>原始特征空间的重构</strong>（将一组可能<strong>线性相关</strong>的变量通过正交变换，变换成<strong>线性无关</strong>的变量）</p></li><li><p>两个基本点（两个角度是一个方法）：<strong>最大投影方差，最小重构距离</strong>（在该方向上投影的方差最大，把投影上的点重构回去的代价最小）</p></li></ul><h2 id="最大投影方差"><a href="#最大投影方差" class="headerlink" title="最大投影方差"></a>最大投影方差</h2><ol><li><p>中心化（将中心平移到原点，即减去均值$x_i-\bar{x}$）</p></li><li><p>令被投影的向量的模为1，$||u_1||=1$</p></li><li><p>投影方差为（向量$a$在向量$b$上的投影为$a^Tb$）</p><script type="math/tex; mode=display">J=\frac{1}{N}\sum_{i=1}^{N}((x_i-\bar{x})^Tu_1)^2=u_1^T[\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})^T]u_1=u_1^TSu_1</script><p>从而求解目标为</p><script type="math/tex; mode=display">\hat{u}_1=\mathop{\arg\max}\limits_{u_1} u_1^TSu_1\\s.t. u_1^Tu_1=1</script><p>使用拉格朗日乘子法</p><script type="math/tex; mode=display">L(u_1,\lambda)=u_1^TSu_1+\lambda(1-u_1^Tu_1)\\\frac{\partial L}{\partial u_1}=2Su_1-\lambda2u_1=0\\Su_1=\lambda u_1</script><p>从而$\lambda$是特征值，$u_1$是特征向量（主成分）</p></li></ol><h2 id="最小重构距离"><a href="#最小重构距离" class="headerlink" title="最小重构距离"></a>最小重构距离</h2><p>PCA可以视为以下两个部分：</p><ul><li>先进行特征空间的重构，得到$\{u_1,u_2,…,u_p\}$共$p$个特征向量</li><li>再对这$p$个特征向量进行筛选，选出前$q$个特征向量，从而实现降维</li></ul><p>原先样本为（用新的坐标轴去重构，并认为$x_i$已经是中心化后的）</p><script type="math/tex; mode=display">x_i=\sum_{k=1}^{p}(x_i^Tu_k)u_k</script><p>重构回来的样本为</p><script type="math/tex; mode=display">\hat{x}_i=\sum_{k=1}^{q}(x_i^Tu_k)u_k</script><p>重构代价为（目标是希望重构代价最小）</p><script type="math/tex; mode=display">\begin{align}J&=\frac{1}{N}\sum_{i=1}^{N}||x_i-\hat{x}_i||^2\\&=\frac{1}{N}\sum_{i=1}^{N}||\sum_{k=q+1}^{p}(x_i^Tu_k)u_k||^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}(x_i^Tu_k)^2\\&=\frac{1}{N}\sum_{i=1}^{N}\sum_{k=q+1}^{p}((x_i-\bar{x})^Tu_k)^2\\&=\sum_{k=q+1}^{p}[\sum_{i=1}^{N}\frac{1}{N}((x_i-\bar{x})^Tu_k)^2]\\&=\sum_{k=q+1}^{p}u_k^T·S·u_k\end{align}</script><p>其中坐标轴为$u_{q+1},…,u_{p}$，坐标为$x_i^Tu_{q+1},…,x_i^Tu_{p}$，对向量求模的平方即对各个坐标轴下的坐标求平方和</p><p>因此目标函数为</p><script type="math/tex; mode=display">\hat{u}_k=\mathop{\arg\max}\limits_{u_k} u_k^TSu_k\\s.t. u_k^Tu_k=1</script><h2 id="从SVD角度看PCA"><a href="#从SVD角度看PCA" class="headerlink" title="从SVD角度看PCA"></a>从SVD角度看PCA</h2><p>奇异值分解SVD：</p><script type="math/tex; mode=display">S=GKG^T\\G^TG=I</script><p>$K$为由特征值从大到小排列构成的对角矩阵</p><p>对中心化后的数据矩阵（原数据矩阵$X_{N·p}$）进行奇异值分解（任何实数矩阵可以进行奇异值分解）</p><script type="math/tex; mode=display">HX=U\Sigma V^T</script><p>样本方差矩阵为（忽略$\frac{1}{N}$）</p><script type="math/tex; mode=display">S=X^THX=X^TH^THX=V\Sigma U ^TU\Sigma V^T</script><p>由于SVD的性质</p><script type="math/tex; mode=display">U^TU=I\\V^TV=VV^T=I</script><p>且$\Sigma$为对角矩阵</p><p>则可以转换为</p><script type="math/tex; mode=display">S=V\Sigma U ^TU\Sigma V^T=V\Sigma ^2 V^T</script><p>因此不需要求样本方差矩阵$S$，可以对中心化后的数据矩阵$HX$进行奇异值分解，同样可以求得$V$和$\Sigma$，从而得到特征向量和特征值。</p><p>定义矩阵</p><script type="math/tex; mode=display">T=HXX^TH=U\Sigma V^TV\Sigma U^T=U\Sigma^2 U^T</script><p>因此$T$和$S$有相同的特征值（$\Sigma^2$）</p><ul><li><p>对$S$做特征分解，得到方向（主成分）$V$，然后通过将数据矩阵乘以方向$V$</p><script type="math/tex; mode=display">HX·V=U\Sigma V^TV=U\Sigma</script><p>从而得到在新的方向下的坐标矩阵$U\Sigma$</p></li><li><p>对$T$做特征分解（<strong>主坐标分析</strong>，Principle Coordinate Analysis，PCoA），可以直接得到坐标</p><script type="math/tex; mode=display">T=U\Sigma^2 U^T\\TU\Sigma=U\Sigma^2 U^TU\Sigma=U\Sigma^3=U\Sigma\Sigma^2</script><p>得到特征向量组成的矩阵$U\Sigma$和特征值组成的矩阵$\Sigma^2$</p></li></ul><blockquote><p>Q：为什么$U\Sigma$是$T$的特征向量组成的矩阵？$T$的特征向量组成的矩阵不应该直接是$U$吗？ </p><p>$U\Sigma$是将$T$的每个特征向量依据$HX$的相应特征值大小做缩放之后的矩阵 </p><p><strong>？？？……没懂</strong></p><p>PCA的目的是求出在新的投影方向上的坐标。PCA先通过SVD找到主成分（方向）$u_1$ ，然后对于样本点$x_i$来说，先进行中心化再乘上主成分$(x_i-\bar{x})u_i=z_i$，从而得到该样本点在新坐标轴$u_1$上的坐标$z_i$。即先求的是方向，再对样本进行投影才能得到坐标。</p><p>PCoA没有通过求方向再进行投影得到坐标的方式，而是通过对矩阵$T$进行分解，直接求出坐标</p></blockquote><p>PCoA好处：</p><p>维度方面：$S_{p<em>p},T_{N</em>N}$，当维度高时可以简化运算</p><h1 id="概率角度P-PCA"><a href="#概率角度P-PCA" class="headerlink" title="概率角度P-PCA"></a>概率角度P-PCA</h1><p>原始样本$x\in R^p$(observed data)，降维后的样本$z\in R^q$(latent data)，且$q&lt;p$</p><p>令</p><script type="math/tex; mode=display">z \sim N(0_q,I_q)\\x=w_{p*q}z+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I_p)</script><p>是线性高斯模型(Linear Gaussian Model)，$\sigma^2I_p$矩阵各向同性。</p><p>P-PCA关注两个问题：</p><ul><li>Inference：$P(z|x)$</li><li>Learning：$w,\mu,\sigma^2$——可使用EM算法求解，较复杂，此处省略</li></ul><p>条件：</p><script type="math/tex; mode=display">z \sim N(0,I)\\x=wz+\mu+\varepsilon\\\varepsilon \sim N(0,\sigma^2I)，\varepsilon \perp x\\E[x|z]=E[wz+\mu+\varepsilon|z]=wz+\mu\\Var[x|z]=Var[wz+\mu+\varepsilon|z]=\sigma^2I \\\Rightarrow x|z \sim N(wz+\mu,\sigma^2I)</script><p>则</p><script type="math/tex; mode=display">E[x]=E[wz+\mu+\varepsilon]=E[wz+\mu]+E[\varepsilon]=\mu\\Var[x]=Var[wz+\mu+\varepsilon]=Var[wz]+Var[\varepsilon]=ww^T+\sigma^2I\\\Rightarrow x \sim N(\mu,ww^T+\sigma^2I)</script><p>构造$x$和$z$的联合概率：</p><script type="math/tex; mode=display">Cov(x,z)=E[(x-\mu)z^T]=E[(wz+\varepsilon)z^T]=wE[zz^T]+E[\varepsilon]E[z^T]=w</script><p>则</p><script type="math/tex; mode=display">\begin{pmatrix} x  \\ z  \end{pmatrix}\sim N\begin{pmatrix} ww^T+\sigma^2I & w \\ w & I \end{pmatrix}</script><p>再由公式$x_b|x_a \sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\Sigma_{bb·a})$，从而得到$z|x$的条件概率分布</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>白板推导系列4——线性分类</title>
      <link href="/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/"/>
      <url>/2020/02/04/bai-ban-tui-dao-xi-lie-4-xian-xing-fen-lei/</url>
      
        <content type="html"><![CDATA[<blockquote><p>b站up主： <strong>shuhuai008</strong> </p><p><a href="https://www.bilibili.com/video/av70839977" target="_blank" rel="noopener">机器学习-白板推导系列-合集</a> 学习笔记</p></blockquote><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>对线性回归的条件逐一打破，从而引申到其它模型：</p><ul><li><p>线性：</p><ul><li>属性非线性：特征转换（多项式回归）</li><li>全局非线性（激活函数是非线性，如逻辑回归）</li><li>系数非线性：神经网络，感知机</li></ul></li><li><p>全局性（全部是由一条线拟合）：线性样条回归、决策树</p></li><li>数据未加工：PCA，流形</li></ul><p>线性回归通过激活函数进行降维，达到线性分类。</p><script type="math/tex; mode=display">y=f(w^Tx+b),x\in R^p</script><p>$f$是激活函数（activation function），$f^{-1}$是链接函数（link function）。</p><p>激活函数$f$将数据的线性组合作为输入，映射到{0,1}或[0,1]区间上；</p><p>链接函数$f^{-1}$将{0,1}或[0,1]区间映射到线性组合上。</p><p>线性分类分为两大类：</p><ol><li>硬分类：$y\in \{0,1\}$，代表模型有线性判别分析(Fisher判别分析)、感知机</li><li>软分类：$y\in [0,1]$，分为生成式模型和判别式模型。判别式直接对$P(Y|X)$进行求解，如逻辑回归；生成式不直接求解$P(Y|X)$，而是通过贝叶斯定理，即通过式$P(Y|X)=\frac{P(X|Y)P(Y)}{P(X)}$进行求解，如高斯判别分析(GDA)（假设数据本身是连续的），朴素贝叶斯（假设数据本身是离散的）。</li></ol><h1 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机(Perceptron)"></a>感知机(Perceptron)</h1><p>思想：错误驱动——不断向正确分类的方向移动</p><p>模型</p><script type="math/tex; mode=display">f(x)=sign(w^Tx),x\in R^p,w\in R^p\\</script><p>前提：假定模型是线性可分的（若不满足可使用pocket algorithm）</p><p>策略：假设样本集$\{(x_i,y_i)\}_{i=1}^{N}$，将损失函数定义为</p><script type="math/tex; mode=display">L(w)=\sum_{i=1}^{N}I\{y_iw^Tx_i\lt 0\}</script><p>但该损失函数不可导不连续，难以求解</p><p>因此采用新的损失函数</p><script type="math/tex; mode=display">L(w)=\sum_{x_i\in D}-y_iw^Tx_i</script><p>是连续函数且可导，对其进行求导$\triangledown_wL=\sum_{x_i\in D}-y_ix_i$，可采用算法SGD（随机梯度下降）求解</p><script type="math/tex; mode=display">w^{t+1}\leftarrow w^t-\lambda\triangledown_wL=w^t+\lambda y_ix_i</script><h1 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h1><p><strong>思想：类内小，类间大。</strong>（高类聚，低耦合）</p><p>将点投影到一维的坐标轴上，每个点都对应该坐标轴上的一个值，并设定一个阈值，根据值与阈值大小进行分类。</p><p>重点：找到一个合适的投影方向，使得类内方差小，类间方差大</p><h2 id="目标函数推导"><a href="#目标函数推导" class="headerlink" title="目标函数推导"></a>目标函数推导</h2><p>给定样本$X=(x_1,x_2,…,x_N)^T,Y=(y_1,y_2,…,y_N)^T,\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{+1,-1\}$</p><p>定义样本集合$x_{c_1}=\{x_i|y_i=+1\},x_{c_2}=\{x_i|y_i=-1\}$</p><p>令$x_i$在$w$方向上的投影为$z_i=w^Tx_i$(假设$||w||=1)$</p><script type="math/tex; mode=display">\begin{align} \bar{z}&=\frac{1}{N}\sum_{i=1}^{N}z_i=\frac{1}{N}\sum_{i=1}^{N}w^Tx_i\\S_z  &=\frac{1}{N}\sum_{i=1}^{N}(z_i-\bar{z})(z_i-\bar{z})^T     =\frac{1}{N}\sum_{i=1}^{N}(w^Tx_i-\bar{z})(w^Tx_i-\bar{z})^T\\c_1: \bar{z}_1&=\frac{1}{N_1}\sum_{i=1}^{N_1}z_i=\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i\\S_1  &=\frac{1}{N_1}\sum_{i=1}^{N_1}(z_i-\bar{z}_1)(z_i-\bar{z}_1)^T     =\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\     c_2: \bar{z}_2&=\frac{1}{N_2}\sum_{i=1}^{N_2}z_i=\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i\\S_2  &=\frac{1}{N_2}\sum_{i=1}^{N_2}(z_i-\bar{z}_2)(z_i-\bar{z}_2)^T     =\frac{1}{N_2}\sum_{i=1}^{N_2}(w^Tx_i-\bar{z}_2)(w^Tx_i-\bar{z}_2)^T\end{align}</script><p>则类间用$(\bar{z}_1-\bar{z}_2)^2$来表达，类内用$S_1+S_2$来表达</p><p>目标函数为</p><script type="math/tex; mode=display">J(w)=\frac{(\bar{z}_1-\bar{z}_2)^2}{S_1+S_2}\\\hat{w}=\mathop{\arg\max}\limits_{w} J(w)</script><p>则分子部分为</p><script type="math/tex; mode=display">(\bar{z_1}-\bar{z_2})^2=(\frac{1}{N_1}\sum_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum_{i=1}^{N_2}w^Tx_i)^2=(w^T(\bar{x}_{c_1}-\bar{x}_{c_2}))^2=w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw</script><p>由于$S_1$可表达为</p><script type="math/tex; mode=display">\begin{align}S_1 &=\frac{1}{N_1}\sum_{i=1}^{N_1}(w^Tx_i-\bar{z}_1)(w^Tx_i-\bar{z}_1)^T\\&=w^T[\frac{1}{N_1}\sum_{i=1}^{N_1}(x_i-\bar{x}_{c_1})(x_i-\bar{x}_{c_2})^T]w\\&=w^T*S_{c_1}*w\end{align}</script><p>则分母部分为</p><script type="math/tex; mode=display">S_1+S_2=w^TS_{c_1}w+w^TS_{c_2}w=w^T(S_{c_1}+S_{c_2})w</script><p>因此</p><script type="math/tex; mode=display">\begin{align}J(w) &=\frac{w^T(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw}{w^T(S_{c_1}+S_{c_2})w}\\     &=\frac{w^TS_bw}{w^TS_ww}=w^TS_bw(w^TS_ww)^{-1}\end{align}</script><p>其中$S_b=(\bar{x}_{c_1}-\bar{x}_{c_2})(\bar{x}_{c_1}-\bar{x}_{c_2})^T$为类间方差（between-class），$S_w=S_{c_1}+S_{c_2}$为类内方差（within-class），再对$w$求偏导</p><script type="math/tex; mode=display">\frac{\partial J(w)}{\partial w}=2S_bw(w^TS_ww)^{-1}+w^TS_bw(-1)(w^TS_ww)^{-2}2S_ww=0\\S_bw(w^TS_ww)=(w^TS_bw)S_ww\\S_ww=\frac{w^TS_ww}{w^TS_bw}S_bw</script><p>只需要求$w$的方向不需要求大小，因此常数不影响，因此</p><script type="math/tex; mode=display">\begin{align}w=\frac{w^TS_ww}{w^TS_bw}{S_w}^{-1}S_bw & \propto{S_w}^{-1}S_bw={S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})[(\bar{x}_{c_1}-\bar{x}_{c_2})^Tw]\\&\propto{S_w}^{-1}(\bar{x}_{c_1}-\bar{x}_{c_2})\end{align}</script><p>若$S_w^{-1}$是对角矩阵，且各向同性，则$S_w^{-1}\propto I$，那么</p><script type="math/tex; mode=display">w\propto(\bar{x}_{c_1}-\bar{x}_{c_2})</script><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>是判别式模型，直接对$P(Y|X)$进行建模，采用极大似然估计求解参数。</p><p>sigmoid函数：</p><script type="math/tex; mode=display">\sigma(z)=\frac{1}{1+e^{-z}}\\\sigma:R \longmapsto (0,1)\\w^Tx \longmapsto Probability</script><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><script type="math/tex; mode=display">\begin{align}p_1&:P(y=1|x)=\sigma(w^Tx)=\frac{1}{1+e^{-w^Tx}},y=1\\p_0&:P(y=0|x)=1-P(y=1|x)=\frac{e^{-w^Tx}}{1+e^{-w^Tx}},y=0\\\end{align}</script><script type="math/tex; mode=display">\Rightarrow P(y|x)=p_1^yp_0^{1-y}</script><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>MLE:</p><script type="math/tex; mode=display">\begin{align} \hat{w}&=\mathop{\arg\max}\limits_{w} \log P(Y|X)\\&=\mathop{\arg\max}\limits_{w} \log \prod_{i=1}^{N}P(y_i|x_i) \\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}\log P(y_i|x_i)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log p_i+(1-y_i)\log p_0)\\&=\mathop{\arg\max}\limits_{w} \sum_{i=1}^{N}(y_i\log \psi(x_i;w)+(1-y_i)\log (1-\psi(x_i;w)))\end{align}</script><p>MLE是最大化问题，可以导出一个Loss function，转化为最小化问题，等价于最小化cross entropy</p><h1 id="高斯判别分析"><a href="#高斯判别分析" class="headerlink" title="高斯判别分析"></a>高斯判别分析</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>给定样本$\{(x_i,y_i)\}_{i=1}^{N},x_i\in R^p,y_i\in \{0,1\}$</p><p>高斯判别分析是生成式模型，<strong>生成式模型并不是求取$P(Y|X)$，而是利用贝叶斯定理，比较$P(y=0|x)$与$P(y=1|x)$的大小，选择较大者作为预测的类</strong>。</p><p>由贝叶斯定理</p><script type="math/tex; mode=display">P(y|x)=\frac{P(x|y)P(y)}{P(x)}\propto P(x|y)P(y)</script><p>由于$P(x|y)P(y)=P(x,y)$，因此我们主要是对于联合概率进行建模。</p><p>其中$P(y)$是先验（prior），$p(x|y)$是似然（likelihood），$P(y|x)$是后验（posterior），因此求解可以表达为</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p><strong>假设$y$服从伯努利分布$Bernoulli(\phi)$，$x|y$服从高斯分布</strong></p><script type="math/tex; mode=display">\begin{align}&x|y=1 \sim N(\mu_1,\Sigma)\\&x|y=0 \sim N(\mu_2,\Sigma)\\\Rightarrow &x|y \sim N(\mu_1,\Sigma)^y*N(\mu_2,\Sigma)^{1-y}\end{align}</script><p>令参数部分</p><script type="math/tex; mode=display">\theta = (\mu_1,\mu_2,\Sigma,\phi)\\\hat{\theta}=\mathop{\arg\max}\limits_{\theta} l(\theta)</script><p>定义对数似然函数为</p><script type="math/tex; mode=display">\begin{align}l(\theta)&=\log \prod_{i=1}^{N}p(x_i,y_i)\\&=\sum_{i=1}^{N}\log(P(x_i|y_i)P(y_i))\\&=\sum_{i=1}^{N}[\log P(x_i|y_i)+\log P(y_i)]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}*N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\&=\sum_{i=1}^{N}[\log N(\mu_1,\Sigma)^{y_i}+\log N(\mu_2,\Sigma)^{1-y_i}+\log \phi^{y_i}(1-\phi)^{1-y_i}]\\\end{align}</script><h2 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h2><p>将上式分为三个部分①+②+③，且有$N_1$个样本标签为1，有$N_2$个样本标签为0（$N_1+N_2=N$）</p><p>求$\phi$：</p><script type="math/tex; mode=display">③=\sum_{i=1}^{N}[y_i\log \phi+(1-y_i)\log(1-\phi)]\\\frac{\partial ③}{\partial \phi}=\sum_{i=1}^{N}[\frac{y_i}{\phi}-\frac{1-y_i}{1-\phi}]=0\\\phi=\frac{1}{N}\sum_{i=1}^{N}y_i=\frac{N_1}{N}</script><p>求$\mu_1$：</p><script type="math/tex; mode=display">\begin{align}①&=\sum_{i=1}^{N}\log N(\mu_1,\Sigma)^{y_i}\\&=\sum_{i=1}^{N}y_i\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\end{align}</script><p>去除与$\mu_1$无关的常数</p><script type="math/tex; mode=display">\hat{\mu}_1=\mathop{\arg\max}\limits_{\mu_1} ①=\mathop{\arg\max}\limits_{\mu_1}\sum_{i=1}^{N}y_i[-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]\\-\frac{1}{2}\sum_{i=1}^{N}y_i[(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)]=-\frac{1}{2}\sum_{i=1}^{N}y_i[x_i^T\Sigma^{-1}x_i-2\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1=\bigtriangleup\\\frac{\partial \bigtriangleup}{\partial \mu_1}=\sum_{i=1}^{N}y_i(\Sigma^{-1}x_i-\Sigma^{-1}\mu_1)=0\\\hat{\mu}_1=\frac{\sum_{i=1}^{N}y_ix_i}{\sum_{i=1}^{N}y_i}=\frac{\sum_{i=1}^{N}y_ix_i}{N_1}</script><p>求$\Sigma$：</p><p>首先令</p><script type="math/tex; mode=display">C_1=\{x_i|y_i=1,i=1,…,N\}\\C_2=\{x_i|y_i=0,i=1,…,N\}\\|C_1|=N_1,|C_2|=N_2,N_1+N_2=N</script><p>则</p><script type="math/tex; mode=display">\hat{\Sigma}_1=\mathop{\arg\max}\limits_{\Sigma}(①+②)\\①+②=\sum_{x_i\in C_1}\log N(\mu_1,\Sigma)+\sum_{x_i\in C_2}\log N(\mu_2,\Sigma)\\\begin{align}\log N(\mu,\Sigma)&=\sum_{i=1}^{N}\log\frac{1}{(2\pi)^\frac{p}{2}|\Sigma|^\frac{1}{2}}\exp[-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[\log\frac{1}{(2\pi)^\frac{p}{2}}+\log{|\Sigma|^\frac{1}{2}}-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=\sum_{i=1}^{N}[C-\frac{1}{2}\log|\Sigma|-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}\sum_{i=1}^{N}tr[(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}tr[\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T\Sigma^{-1}]\\&=C-\frac{N}{2}\log|\Sigma|-\frac{1}{2}Ntr(S\Sigma^{-1})\\\end{align}</script><p>其中$S=\frac{1}{N}\sum_{i=1}^{N}(x_i-\mu)(x_i-\mu)^T$为样本方差</p><p>则</p><script type="math/tex; mode=display">\begin{align}①+②&=-\frac{N_1}{2}\log|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})-\frac{N_2}{2}\log|\Sigma|-\frac{1}{2}N_2tr(S_2\Sigma^{-1})+C\\&=-\frac{1}{2}[N\log|\Sigma|+N_1tr(S_1\Sigma^{-1})+N_2tr(S_2\Sigma^{-1})]+C\\\frac{\partial (①+②)}{\partial \Sigma}&=-\frac{1}{2}(N\Sigma^{-1}-N_1S_1\Sigma^{-2}-N_2S_2\Sigma^{-2})=0\\\end{align}</script><script type="math/tex; mode=display">N\Sigma-N_1S_1-N_2S_2=0\\ \Sigma=\frac{1}{N}(N_1S_1+N_2S_2)</script><p>其中用到求导$\frac{tr(\Sigma^{-1}S)}{\partial \Sigma}=S^T(-1)\Sigma^{-2}$</p><h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><p>朴素贝叶斯假设：条件独立性假设</p><script type="math/tex; mode=display">x_i \perp x_j | y(i\neq j)\\P(x|y)=\prod_{j=1}^{p}P(x_j|y)</script><p>最简单的概率图（有向图）模型</p><p>动机：简化运算</p><script type="math/tex; mode=display">\hat{y}=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y|x)=\mathop{\arg\max}\limits_{y\in \{0,1\}} P(y)P(x|y)</script><p>若$x$离散，则认为$x_j$服从伯努利分布/多项分布</p><p>若$x$连续，则认为$x_j$服从正态分布$x_j\sim N(\mu_j,\sigma^2_j)$</p>]]></content>
      
      
      <categories>
          
          <category> Notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning, Classification, Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression 算法基础及实现</title>
      <link href="/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/"/>
      <url>/2020/02/02/logistic-regression-suan-fa-ji-qi-ying-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><p>假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到二分类的目的。</p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>逻辑回归是在线性回归的基础上，利用sigmoid函数（或称为logistic函数）</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>进行映射，代入线性回归部分</p><script type="math/tex; mode=display">z=\theta^Tx=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n</script><p>得到二元逻辑回归模型的一般形式：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的$h_\theta(x)$就是逻辑回归返回的值，介于0到1之间，可以将其当做样本取正类的“概率”。因此对于样本$x$分类结果为正类1和负类0的概率分别为</p><script type="math/tex; mode=display">\begin{cases}P(y=1|x;\theta)=h_\theta(x)\\P(y=0|x;\theta)=1-h_\theta(x)\end{cases}</script><p>对$h_\theta(x)$进行变换可以得到对数几率的表达式</p><script type="math/tex; mode=display">ln\frac{h_\theta(x)}{1-h_\theta(x)}=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{1-\frac{1}{1+e^{-\theta^Tx}}})=ln(\frac{\frac{1}{1+e^{-\theta^Tx}}}{\frac{e^{-\theta^Tx}}{1+e^{-\theta^Tx}}})=ln(\frac{1}{e^{-\theta^Tx}})=ln(e^{\theta^Tx})=\theta^Tx</script><p>从上式可以看出，<strong>逻辑回归的本质是在对线性回归模型的预测去逼近真实标记的对数几率</strong>。求解的关注点在于求解参数$\theta$上，通常使用极大似然估计的方法对$\theta$进行估计。</p><p>令</p><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>得到的似然函数为</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^{m}P(y^i|x^i;\theta)=\prod_{i=1}^{m}h_\theta(x^i)^{y^i}*(1-h_\theta(x^i))^{1-y^i}</script><p>其中，$x^i$为第$i$个样本的特征做构成的向量（每个向量$n+1$维，共$m$个向量），$y^i$为第$i$个样本的标签，$m$为样本量。实际中为了简化计算，同时防止连乘所造成的浮点数下溢，通常会转化为对数似然函数</p><script type="math/tex; mode=display">l(\theta)=\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>逻辑回归所要解决的问题即为找到参数$\theta$，使得对数似然函数达到最大。</p><p>令损失函数为（忽略正则化项）</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}l(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script><p>利用梯度下降求解参数</p><script type="math/tex; mode=display">\begin{align}\frac{∂J(\theta)}{∂\theta_j}& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{h_\theta(x^i)}-\frac{1-y^i}{1-h_\theta(x^i)}]\frac{∂h_\theta(x^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]\frac{∂g(\theta^Tx^i)}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))\frac{∂\theta^Tx^i}{∂\theta_j}\\& =-\frac{1}{m}\sum_{i=1}^{m}[\frac{y^i}{g(\theta^Tx^i)}-\frac{1-y^i}{1-g(\theta^Tx^i)}]g(\theta^Tx^i)(1-g(\theta^Tx^i))x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i(1-g(\theta^Tx^i))-(1-y^i)g(\theta^Tx^i)]x^i_j\\& =-\frac{1}{m}\sum_{i=1}^{m}[y^i-g(\theta^Tx^i)]x^i_j\\& =\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j\\\end{align}</script><p>因此最终得到参数迭代式</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>（参考：<a href="https://www.cnblogs.com/Luv-GEM/p/10674719.html" target="_blank" rel="noopener">Logistic回归（逻辑回归）和softmax回归</a>）</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p><strong>优点：</strong></p><ol><li><p>速度快，在时间和内存需求上相当高效，它可以应用于分布式数据和在线算法实现，用较少的资源处理大型数据</p></li><li><p>对线性分类问题拟合很好</p></li><li>简单易于理解，直接看到各个特征的权重</li></ol><p><strong>缺点：</strong></p><ol><li>分类精度可能不高，在非线性分类问题上表现不好</li><li>数据特征有缺失或者特征空间很大时表现效果并不好，受异常值影响大</li></ol><h2 id="与线性回归的异同"><a href="#与线性回归的异同" class="headerlink" title="与线性回归的异同"></a>与线性回归的异同</h2><p>本质是线性的，只是特征到结果映射用的是sigmoid函数，属于广义线性模型（GLM）</p><ul><li><p>相同</p><ol><li><p>都使用极大似然估计对训练样本进行建模；求解超参数时都可以使用梯度下降。</p></li><li><p>都是广义线性模型，逻辑回归本质上是一个线性回归模型，LR是以线性回归为理论支持的。</p></li></ol></li><li><p>不同</p><ol><li>本质：逻辑回归是分类，线性回归是回归，逻辑回归中$y$是因变量而非$\frac{p}{1-p}$，因变量是离散而非连续</li><li>LR形式上是线性回归，实质上是在求取输入空间到输出空间的非线性函数映射（对率函数起到将线性回归模型的预测值与真实标记联系起来的作用）</li><li>LR是直接对分类可能性进行建模，无需事先假设数据分布，而线性回归需要假设数据分布</li></ol></li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数是它的极大似然函数取对数再除以样本量的相反数</strong></p><p>极大似然函数：</p><script type="math/tex; mode=display">L_\theta(x)=\prod_{i=1}^{m}h_\theta(x^i;\theta)^{y^i}*(1-h_\theta(x^i;\theta))^{1-y^i}</script><p>损失函数：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^ilog(h_\theta(x^i;\theta))+(1-y^i)log(1-h_\theta(x^i;\theta))]</script><p>除以样本量$m$并不改变最终求导极值结果，通过除以$m$可以得到<strong>平均损失值</strong>，避免<strong>样本数量对于损失值的影响</strong></p><p>（但是也有不除以样本量的，比如sklearn中的损失函数就不除以样本量）</p><p>（乘上样本量的倒数也并不影响梯度下降的过程┓( ´∀` )┏ ）</p><blockquote><p><strong>Q：为什么要用极大似然函数作为损失函数？</strong></p><p>损失函数一般有四种：平方损失函数，对数损失函数，HingeLoss损失函数，绝对值损失函数。</p><p>将极大似然函数取对数以后等同于对数损失函数。</p><p><strong>在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。</strong></p><p>梯度更新公式：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\eta\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^i)-y^i]x^i_j</script><p>这个式子的更新速度只和$x^i_j$和$y^i$相关，和sigmoid函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。 </p><p><strong>Q：为什么不选平方损失函数？</strong></p><p>其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmoid函数本身的梯度是很相关的。</p><script type="math/tex; mode=display">θ_j=θ_j-2(sigmoid(x)*(1-sigmoid(x)))x^i_j</script><p>sigmoid函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p><p>实际上也可以用最小二乘，但是最小二乘得到的权重效果比较差。</p><p>如果用最小二乘法，目标函数就是差值的平方和，<strong>是非凸的，不容易求解，很容易陷入到局部最优</strong>。</p><p>如果用极大似然估计，目标函数就是对数似然函数，是关于$(w,b)$的高阶<strong>连续可导凸函数</strong>，可以方便通过一些凸优化算法求解，比如梯度下降法、牛顿法等。</p></blockquote><h2 id="参数求解方法"><a href="#参数求解方法" class="headerlink" title="参数求解方法"></a>参数求解方法</h2><ul><li>梯度下降法</li></ul><p>由于该极大似然函数无法直接求解，我们一般通过对该函数进行<strong>梯度下降</strong>来不断逼近最优解。</p><blockquote><p>梯度下降：随机梯度下降，批梯度下降，small-batch梯度下降</p><p>Q：三种方式的优劣以及如何选择最合适的梯度下降方式</p><ol><li><p>批梯度下降(BGD)：每次迭代使用所有样本来进行梯度的更新，能得到全局最优解。缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。</p></li><li><p>随机梯度下降(SGD)：每次迭代随机使用一个样本来对参数进行更新，优点是每一轮参数的更新速度大大加快，缺点是准确度下降，可能会收敛到局部最优（单个样本不能代表全体样本的趋势）。</p></li><li><p>小批量梯度下降：结合了BGD和SGD的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。</p></li></ol></blockquote><ul><li><p>牛顿法</p><p>(待更新)</p></li><li><p>拟牛顿法</p><p>(待更新)</p></li></ul><blockquote><p>牛顿法与梯度下降法求解参数的区别：</p><p>两种方法不同在于牛顿法中<strong>多了一项二阶导数</strong>，这项二阶导数对参数更新的影响主要体现在<strong>改变参数更新方向上</strong>。如下图所示，红色是牛顿法参数更新的方向，绿色为梯度下降法参数更新方向，因为牛顿法考虑了二阶导数，因而可以<strong>找到更优的参数更新方向</strong>，在每次更新的步幅相同的情况下，可以<strong>比梯度下降法节省很多的迭代次数</strong>。</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/zdow4EB72J3r.png" alt="mark"></p></blockquote><h2 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h2><p>作用：需要一个单调可微的函数，把分类任务的真实标记与线性回归模型的预测值联系起来。</p><p>对于二分类问题，由线性回归得来的启发是根据特征的加权平均进行预测。很自然地想到设定一个阈值，如果加权平均大于该阈值就判为正类，反之判为负类。但<strong>阶跃函数不可导</strong>，所以<strong>引入Sigmoid函数，将样本的加权平均代入函数得到的值就是样本属于正类的概率，即将输入空间到输出空间作非线性函数映射</strong>。</p><p>Sigmoid函数形式：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>Sigmoid函数是一个S型的函数，函数图像：</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/hMNRkXsKheJ9.png" alt="mark" style="zoom:67%;"></p><p>当自变量z趋近正无穷时，因变量g(z)趋近于1，而当z趋近负无穷时，g(z)趋近于0。</p><p>它能够<strong>将任何实数映射到(0,1)区间</strong>（开区间，不可等于0或1），使其可用于将任意值函数转换为更适合二分类的函数。 </p><p>因为这个性质，Sigmoid函数也被当作是归一化的一种方法，与MinMaxSclaer同理，是属于数据预处理中的“缩放”功能，可以将数据压缩到[0,1]之内。</p><p>区别在于，MinMaxScaler归一化之后，是可以取到0和1的（最大值归一化后就是1，最小值归一化后就是0），但<strong>Sigmoid函数只是无限趋近于0和1</strong>。</p><h2 id="共线性问题"><a href="#共线性问题" class="headerlink" title="共线性问题"></a>共线性问题</h2><p>对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。</p><p>通常做法为：将所有回归中要用到的变量依次作为因变量、其他变量作为自变量进行回归分析，可以得到各个变量的膨胀系数VIF， VIF越大共线性越严重，通常VIF小于5可以认为共线性不严重，宽泛一点的标准小于10即可。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><ol><li><p>多项逻辑回归(Softmax Regression)</p><p>（二分类逻辑回归在多标签分类下的一种拓展）</p><p><img src="http://q4ws08qse.bkt.clouddn.com/blog/20200130/4HnDRP6ijvH6.png" alt="mark"></p></li><li><p>one v.s. rest</p><p>k个二分类LR分类器，把标签重新整理为“第i类标签”与“非第i类标签”</p></li></ol><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank" rel="noopener">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression</a> </p><blockquote><p><em>class</em> <code>sklearn.linear_model.LogisticRegression</code>(<em>penalty=’l2’</em>, <em>dual=False</em>, <em>tol=0.0001</em>, <em>C=1.0</em>, <em>fit_intercept=True</em>, <em>intercept_scaling=1</em>, <em>class_weight=None</em>, <em>random_state=None</em>, <em>solver=’lbfgs’</em>, <em>max_iter=100</em>, <em>multi_class=’auto’</em>, <em>verbose=0</em>, <em>warm_start=False</em>, <em>n_jobs=None</em>, <em>l1_ratio=None</em>) </p></blockquote><h2 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数"><a href="#1-基本模型参数" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fit_intercept</strong>：bool，指定是否将截距项添加到线性回归部分中。默认True。</p></li><li><p><strong>intercept_scaling</strong>：float，默认1。仅在solver=’liblinear’且fit_intercept=True时有用。 在这种情况下原本的向量是[x]就变成[x,intercept_scaling]，即具有等于设定的intercept_scaling值的“合成”特征会被添加到实例矢量。截距会变为intercept_scaling * synthetic_feature_weight(合成特征权重)。synthetic_feature_weight会与其他特征经历l1和l2正则化，为减小正则化对synthetic_feature_weight（并因此对截距）的影响，必须增加intercept_scaling。</p><blockquote><p>因为本身截距项是不需要进行正则化的，当采用fit_intcept时相当于人造一个特征出来，特征恒为1，权重为b。在计算正则化项的时候，该人造特征也被考虑了，因此为了降低这个人造特征的影响，需要提供intercept_scaling。 (O_o)??</p></blockquote></li><li><p><strong>multi_class</strong>：str，’auto’（默认）/‘ovr’/‘multinomial’，表示要预测的分类是二分类或一对多形式的多分类问题，还是多对多形式的多分类问题。</p><ul><li><p><strong>‘auto’</strong>：表示自动选择，会根据数据的分类情况和其他参数确定模型要处理的分类问题的类型。</p><blockquote><p>根据源码得到判定方法如下：</p><p>step 1：if solver = ‘liblinear’: multi_class = ‘ovr’</p><p>step 2：elif n_classes &gt; 2: multi_class = ‘multinomial’</p><p>step 3：else: multi_class = ‘ovr’</p></blockquote></li><li><p><strong>‘ovr’</strong>：表示当前处理的是二分类或一对多形式的多分类问题</p></li><li><p><strong>‘multinomial’</strong>：表示当前处理的是多对多形式的多分类问题</p></li></ul></li><li><p><strong>class_weight</strong>：None（默认）/‘balanced’/dict，标签(label)的权重。</p><ul><li><p><strong>None</strong>：所有的label持有相同的权重， 所有类别的权值为1 </p></li><li><p><strong>‘balanced’</strong>：自动调整与样本中类频率成反比的权重，即<code>n_samples/(n_classes*np.bincount(y))</code></p><blockquote><p><strong>‘balanced’如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line"></span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment"># 标签值，一共16个样本</span></span><br><span class="line">np.bincount(y)</span><br><span class="line"><span class="comment"># array([8, 6, 2], dtype=int64) 计算每个类别的样本数量，顺序按类别的出现次序</span></span><br><span class="line">class_weight = <span class="string">'balanced'</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight) <span class="comment"># [0.66666667 0.88888889 2.66666667]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">8</span>))  <span class="comment">#输出 0.6666666666666666</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">6</span>))  <span class="comment">#输出 0.8888888888888888</span></span><br><span class="line">print(<span class="number">16</span>/(<span class="number">3</span>*<span class="number">2</span>))  <span class="comment">#输出 2.6666666666666665</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><ul><li><p><strong>dict类型</strong></p><p>对于二分类问题，可定义class_weight = {0:0.9, 1:0.1}，这样类别0的权重为0.9，类别1的权重为0.1。</p><p>对于多分类问题，定义的权重必须具体到每个标签下的每个类，其中类是key-value中的key，权重是value。</p><blockquote><p><strong>dict类型如何计算class_weight？</strong></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight </span><br><span class="line">  </span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]  <span class="comment">#标签值，一共16个样本</span></span><br><span class="line"></span><br><span class="line">class_weight = &#123;<span class="number">0</span>:<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>,<span class="number">2</span>:<span class="number">5</span>&#125;   <span class="comment"># 设置</span></span><br><span class="line">classes = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])  <span class="comment">#标签类别</span></span><br><span class="line">weight = compute_class_weight(class_weight, classes, y)</span><br><span class="line">print(weight)   <span class="comment"># 输出：[1. 3. 5.]，也就是字典中设置的值</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><p><strong>class_weight如何体现在逻辑回归的损失函数上？</strong></p></blockquote><p>  class_weight给每个类别分别设置不同的<strong>惩罚参数C</strong>。</p><p>  惩罚项C会相应的放大或者缩小某一类的损失，如果某一类C越大，这一类的损失也被（相对于其他类来说）放大，那么系统会把本次学习重点放在这一类上，使得系统尽可能的预测对这一类的输入，所以惩罚项C不会影响计算的损失，但反向学习时会相应的放大或缩小损失，间接影响学习的方向。</p><p>  (参考：<a href="https://www.zhihu.com/question/265420166/answer/293896934" target="_blank" rel="noopener">https://www.zhihu.com/question/265420166/answer/293896934</a>)</p><blockquote><p><strong>源码关于class_weight与sample_weight在LR损失函数上的具体计算方式？</strong></p></blockquote>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sample_weight *= class_weight_[le.fit_transform(y_bin)] </span><br><span class="line"><span class="comment"># 将class_weight乘到每个样本的sample_weight上</span></span><br><span class="line"><span class="comment"># sample_weight : shape (n_samples,)</span></span><br><span class="line"><span class="comment"># le即LabelEncoder，将标签标准化为0/1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic loss is the negative of the log of the logistic function</span></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">out = -np.sum(sample_weight * log_logistic(yz)) + <span class="number">.5</span> * alpha * np.dot(w, w)</span><br></pre></td></tr></table></figure><h3 id="2-求解算法参数"><a href="#2-求解算法参数" class="headerlink" title="2. 求解算法参数"></a>2. 求解算法参数</h3><ul><li><p><strong>solver</strong>：str，用于求解模型最优化问题的算法，可选{‘newton-cg’,’lbfgs’,’liblinear’,’sag’,’saga’}，默认’lbfgs’。</p><ul><li><strong>‘liblinear’</strong>：使用坐标轴下降法来迭代优化损失函数。</li><li><strong>‘lbfgs’</strong>：拟牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。</li><li><strong>‘newton-cg’</strong>：牛顿法的一种，利用损失函数二阶导数矩阵（即海森矩阵）来迭代优化损失函数。？</li><li><strong>‘sag’</strong>：随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅用一部分的样本来计算梯度，适用于样本量大的情况。</li><li><strong>‘saga’</strong>：线性收敛的随机优化算法的变种。</li></ul><blockquote><ul><li><p>对于数据量大小方面，’liblinear’仅限于处理二分类和一对多（OvR）问题，适用于小型数据集。’sag’和’saga’对于大型数据集来说更快（快速收敛仅在量纲大致相同的数据上得到保证），’sag’每次仅使用了部分样本进行梯度迭代，样本量少时不适合。</p></li><li><p>对于多分类问题来说，’liblinear’只能用于一对多（OvR），其它算法还可处理多对多（MvM），而多对多一般比一对多分类相对更准确一些。</p></li><li><p>对于正则化方法来说，’newton-cg’,’sag’,’lbfgs’这三种算法计算时都需要涉及到损失函数的一阶导或二阶导，因此不能用于没有连续导数的l1正则化，只能用于l2正则化。其他两种算法均可使用l1和l2正则化。</p></li></ul></blockquote><p>（这部分还不是很了解……待补充）</p></li><li><p><strong>dual</strong>：bool，是否使用对偶或原始计算方式。对偶方式仅在solver=’liblinear’与penalty=’l2’连用的情况下有小。如果样本量大于特征的数目，这个参数设置为False会更好。（逻辑回归的对偶形式是什么？……待补充）</p></li></ul><h3 id="3-正则化参数"><a href="#3-正则化参数" class="headerlink" title="3. 正则化参数"></a>3. 正则化参数</h3><p>损失函数</p><script type="math/tex; mode=display">\min_{w, c} \frac{1 - \rho}{2}w^T w + \rho \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1)</script><p>其中$C$为正则化参数（$\lambda\ge0$)，$\alpha$为l1正则化的占比（$\alpha\in[0,1]$）。</p><blockquote><p>这里用的是<a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" rel="noopener">sklearn官网</a>给出的损失函数形式， sklearn中假设y正负label定义为1和-1，与之前1/0不一样</p><p>损失函数除去正则化和求和部分剩余部分为：$\log(\exp(- y_i (X_i^T w + c)) + 1)$</p><p>当$y_i=+1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp(- (X_i^T w + c)) + 1)$</p><p>当$y_i=-1$时，$\log(\exp(- y_i (X_i^T w + c)) + 1)=\log(\exp((X_i^T w + c)) + 1)$</p><p>上式难以继续化简，因此对比之前的损失函数形式，观测结果是否一致</p><p>之前的表达式对应的部分为$-[y^i\log(h_\theta(x^i;\theta))+(1-y^i)\log(1-h_\theta(x^i;\theta))]$</p><p>当$y_i=1$时，$-y^i\log(h_\theta(x^i;\theta))=-\log(\frac{1}{1+\exp(-(X_i^T w + c))})=\log(\exp(-(X_i^T w + c))+1)$</p><p>当$y_i=0$时，$-(1-y^i)\log(1-h_\theta(x^i;\theta))=-\log(1-\frac{1}{\exp(-(X_i^T w + c))+1})=\log(\exp((X_i^T w + c)) + 1)$</p><p>从而证明这两种表达形式是等价的。</p><p>（注意sklearn损失函数里没有除以样本量）</p></blockquote><ul><li><strong>penalty</strong>：str，指定正则化策略 ， {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}，默认’l2’。’elasticnet’同时包含‘l1’和‘l2’正则化。</li><li><strong>C</strong>：float，正则化系数$\lambda$的倒数（乘在损失函数的前面，与乘在正则化部分前效果相同，均用来平衡两个部分的比重），必须是一个大于0的浮点数，默认值1.0，即默认正则项与损失函数的比值是1:1。</li><li><strong>l1_ratio</strong>：float，l1正则化的占比$\rho$，取值范围[0,1]，默认为None。仅当penalty=’elasticnet’时使用，对于0&lt; l1_ratio &lt;1，惩罚是l1和l2正则化的组合。</li></ul><h3 id="4-控制迭代次数参数"><a href="#4-控制迭代次数参数" class="headerlink" title="4. 控制迭代次数参数"></a>4. 控制迭代次数参数</h3><ul><li><strong>max_iter</strong>：int，控制梯度下降的迭代次数（仅适用于solver=’newton-cg’, ‘lbfgs’, ‘sag’）。默认值为100。值过小损失函数可能会没有收敛到最小值，值过大会使得梯度下降迭代次数过多，模型运行时间缓慢。</li><li><strong>tol</strong>：float，让迭代停下的最小值。默认1e-4。数字越大迭代越早停下。</li></ul><h3 id="5-其他参数"><a href="#5-其他参数" class="headerlink" title="5. 其他参数"></a>5. 其他参数</h3><ul><li><strong>random_state</strong>：int(可选)，随机数种子，可选参数（仅适用于solver=’liblinear’, ‘sag’）。默认为无。</li><li><strong>verbose</strong>：int，日志冗长度。对于solver=’liblinear’, ‘lbfgs’，当设置为大于等于1的任何整数时，输出训练的详细过程 。默认为0，不输出训练过程。</li><li><strong>warm_start</strong>：bool，是否进行热启动，默认为False。若设置为True，则以上一次fit的结果作为此次的初始化，如果”solver”参数为”liblinear”时无效。 </li><li><strong>n_jobs</strong>：int，并行数。int类型，默认为1。等于1时用CPU的一个内核运行程序，等于-1时用所有CPU的内核运行程序。 </li></ul><h2 id="属性列表"><a href="#属性列表" class="headerlink" title="属性列表"></a>属性列表</h2><ul><li><strong>coef_</strong>：预测函数中特征对应的系数$w$。</li><li><strong>intercept_</strong>：预测函数中的截距$c$。</li><li><strong>n_iter_</strong>：实际迭代次数。</li></ul><h2 id="接口列表"><a href="#接口列表" class="headerlink" title="接口列表"></a>接口列表</h2><ul><li><strong>fit(x,y[,sample_weight])</strong>：训练模型。</li><li><strong>predict(x)</strong>：用模型进行训练，返回预测值。</li><li><strong>predict_log_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率的对数值。</li><li><strong>predict_proba(x)</strong>：返回一个数组，数组的元素依次是x预测为各个类别的概率值。</li><li><strong>score(x,y[,sample_weight])</strong>：返回在(x,y)上预测的准确率。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line">clf.predict(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本的类别</span></span><br><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :]) <span class="comment"># 预测前两个样本属于各个类的概率</span></span><br><span class="line">clf.score(X, y) <span class="comment"># 返回准确率</span></span><br><span class="line"></span><br><span class="line">clf.coef_ <span class="comment"># 系数</span></span><br><span class="line">clf.intercept_ <span class="comment">#截距</span></span><br><span class="line">clf.n_iter_ <span class="comment"># 迭代次数</span></span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p><a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegression</a> </p><p><em>class</em> <code>pyspark.ml.classification.LogisticRegression</code>(<em>featuresCol=’features’</em>, <em>labelCol=’label’</em>, <em>predictionCol=’prediction’</em>, <em>maxIter=100</em>, <em>regParam=0.0</em>, <em>elasticNetParam=0.0</em>, <em>tol=1e-06</em>, <em>fitIntercept=True</em>, <em>threshold=0.5</em>, <em>thresholds=None</em>, <em>probabilityCol=’probability’</em>, <em>rawPredictionCol=’rawPrediction’</em>, <em>standardization=True</em>, <em>weightCol=None</em>, <em>aggregationDepth=2</em>, <em>family=’auto’</em>, <em>lowerBoundsOnCoefficients=None</em>, <em>upperBoundsOnCoefficients=None</em>, <em>lowerBoundsOnIntercepts=None</em>, <em>upperBoundsOnIntercepts=None</em>) </p><h2 id="参数列表-1"><a href="#参数列表-1" class="headerlink" title="参数列表"></a>参数列表</h2><h3 id="1-基本模型参数-1"><a href="#1-基本模型参数-1" class="headerlink" title="1. 基本模型参数"></a>1. 基本模型参数</h3><ul><li><p><strong>fitIntercept</strong>：bool，是否包含截距项，默认True。</p></li><li><p><strong>family</strong>：str，表示分类是二分类还是多分类，默认’auto’，还可选’binomial’和’multinomial’。</p><blockquote><p>spark中处理多分类问题的’multinomial’使用的是<strong>softmax回归</strong></p><p>(参考：<a href="https://blog.csdn.net/u013855234/article/details/84343963" target="_blank" rel="noopener">spark 2.x 源码分析 之 Logistic Regression 逻辑回归</a>)</p><p>在多分类问题中，假设有$C$个类，即类别标签$y\in\{1,2,…,C\}$，则给定一个样本$x$，softmax回归预测样本$x$属于类别$c$的后验概率为</p><script type="math/tex; mode=display">P(y=c|x;\theta)=\frac{\exp(\theta^T_cx)}{\sum_{c=1}^{C}\exp(\theta^T_cx)}</script><p>其中$\theta^T_c$是第$c$类的权重向量，则样本$x$属于每个类别的概率可以由向量表示，向量的第$c$个元素就是样本被预测为第$c$类的概率。</p></blockquote></li><li><p><strong>threshold</strong>：float，分类中的阈值，默认为0.5。</p></li><li><p><strong>thresholds</strong>：list，分类中的阈值，默认为0.5。多元分类中的thresholds是为了调整预测每个类别时的概率。数组长度必须和类别数目相等，且值都大于0。若thresholds长度为2（即对于二分类问题），要满足<code>threshold = 1/(1+threholds[0]/threholds[1])</code></p><blockquote><p>这两个阈值看上去很迷惑，因此直接进行测试……</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二分类的情况</span></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThreshold(<span class="number">0.8</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>])</span><br><span class="line"><span class="comment"># 会认为Thresholds = [1,6], Threshold = 0.8</span></span><br><span class="line"><span class="comment"># 报错，因为不满足threshold = 1/(1+threholds[0]/threholds[1])</span></span><br><span class="line"></span><br><span class="line">lr = LogisticRegression().setFamily(<span class="string">"binomial"</span>).setThresholds([<span class="number">1</span>,<span class="number">6</span>]).setThreshold(<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># 会认为Threshold = 0.8, 忽略Thresholds(被覆盖，之前定义无效)</span></span><br><span class="line"><span class="comment"># 不报错，将超过0.8的类概率预测为该类</span></span><br></pre></td></tr></table></figure><p>对于多分类的情况比较简单，无论thresholds和threshold如何设定，仍会按照概率最高类进行预测（迷惑行为）</p><p>（参考：<a href="https://stackoverflow.com/questions/47325607/set-thresholds-in-pyspark-multinomial-logistic-regression" target="_blank" rel="noopener">Set thresholds in PySpark multinomial logistic regression</a>）</p></li><li><p><strong>standardization</strong>：bool，是否在训练模型之前对特征进行标准化，默认True。</p><blockquote><p>这里体现出spark与python的不同，spark会默认对特征进行标准化。</p><p>但如果设置<code>standardization = False</code>，仍会将数据进行标准化以提高收敛速度（又一迷惑行为），从而获得相同效果的目标函数。</p><p>这里的标准化是直接除以变量的标准差，没有减去均值的部分。</p></blockquote><p><a href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L683" target="_blank" rel="noopener">源码标准化部分</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> standardizationParam = $(standardization)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regParamL1Fun</span> </span>= (index: <span class="type">Int</span>) =&gt; &#123;</span><br><span class="line">  <span class="comment">// Remove the L1 penalization on the intercept</span></span><br><span class="line">  <span class="keyword">val</span> isIntercept = $(fitIntercept) &amp;&amp; index &gt;= numFeatures * numCoefficientSets</span><br><span class="line">  <span class="keyword">if</span> (isIntercept) &#123;</span><br><span class="line">    <span class="number">0.0</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (standardizationParam) &#123;</span><br><span class="line">      regParamL1</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> featureIndex = index / numCoefficientSets</span><br><span class="line">      <span class="comment">// If `standardization` is false, we still standardize the data</span></span><br><span class="line">      <span class="comment">// to improve the rate of convergence; as a result, we have to</span></span><br><span class="line">      <span class="comment">// perform this reverse standardization by penalizing each component</span></span><br><span class="line">      <span class="comment">// differently to get effectively the same objective function when</span></span><br><span class="line">      <span class="comment">// the training dataset is not standardized.</span></span><br><span class="line">      <span class="keyword">if</span> (featuresStd(featureIndex) != <span class="number">0.0</span>) &#123;</span><br><span class="line">        regParamL1 / featuresStd(featureIndex)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="number">0.0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>aggregationDepth</strong>：int，树聚合所建议的深度，默认为2</p></li><li><p><strong>lowerBoundsOnCoefficients(upperBoundsOnCoefficients)</strong>：？</p></li><li><p><strong>lowerBoundsOnIntercepts(upperBoundsOnIntercepts)</strong>：？</p></li></ul><h3 id="2-正则化参数"><a href="#2-正则化参数" class="headerlink" title="2. 正则化参数"></a>2. 正则化参数</h3><p>正则化部分</p><script type="math/tex; mode=display">\lambda[\frac{1 - \rho}{2}w^T w + \rho \|w\|_1], \rho\in[0,1], \lambda\ge0</script><p>其中$\lambda$为regParam，$\rho$为elasticNetParam。</p><ul><li><p><strong>regParam</strong>：float，正则化参数，默认0.0，即不进行正则化。</p></li><li><p><strong>elasticNetParam</strong>：float，正则化范式比，即l1正则化的占比。默认0.0，即只使用l2正则化。</p><blockquote><p>与sklearn的正则化参数$C$不同，这里的$\lambda$是乘在正则化部分，而$C$乘在损失部分</p><p>仅表达方式不同，改变了参数的位置</p><p>以l2正则为例</p><script type="math/tex; mode=display">J(\theta)+\lambda L_2 \Longleftrightarrow CJ(\theta)+L_2</script><p>$\lambda$越大$C$越小，正则项的地位越高，优化时集中优化$L_2$，从而使参数$\theta$中的元素尽量小</p></blockquote></li></ul><h3 id="3-控制迭代次数参数"><a href="#3-控制迭代次数参数" class="headerlink" title="3. 控制迭代次数参数"></a>3. 控制迭代次数参数</h3><ul><li><strong>maxIter</strong>：int，最大迭代次数，默认100。（与sklearn完全一致）</li><li><strong>tol</strong>：float，让迭代停下的最小值，数字越大迭代越早停下，默认1e-6。（sklearn默认1e-4)</li></ul><h3 id="4-设定列名参数"><a href="#4-设定列名参数" class="headerlink" title="4. 设定列名参数"></a>4. 设定列名参数</h3><p>这部分参数仅用于指定列名和设置输出的列名</p><ul><li><p><strong>featuresCol</strong>：str，输入的数据集中的特征列名（一个合并后的列名），默认’features’</p><blockquote><p>spark输入到模型训练的数据需要把多列的特征合并成一列（测试集也一样）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"></span><br><span class="line">model_train_df = model_train.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line">model_test_df = model_test.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>]))</span><br><span class="line"><span class="comment"># 这里标签列均在最后一列</span></span><br><span class="line"><span class="comment"># 然后就可以在训练模型时令参数featuresCol = 'features'（也可以不用设定，默认值即为'features'）</span></span><br></pre></td></tr></table></figure></li><li><p><strong>labelCol</strong>：str，输入的数据集中的标签列名（同featuresCol），默认’label’</p></li><li><p><strong>predictionCol</strong>：str，输出的模型预测结果中样本预测类的列名，默认’prediction’</p></li><li><p><strong>rawPredictionCol</strong>：str，输出的模型预测结果中原始概率的列名，默认’rawPrediction’</p></li><li><p><strong>probabilityCol</strong>：str，输出的模型预测结果中最终预测概率的列名，默认’probability’</p></li><li><p><strong>weightCol</strong>：str，样本权重列名，默认None，即所有样本的权重均视为等权重。</p></li></ul><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.classification <span class="keyword">import</span> LogisticRegression, LogisticRegressionModel</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.evaluation <span class="keyword">import</span> BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入鸢尾花数据集</span></span><br><span class="line">df = sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line"><span class="comment"># 转换为features和label两列的形式</span></span><br><span class="line">model_df = df.rdd.map(<span class="keyword">lambda</span> x:(Vectors.dense(x[<span class="number">0</span>:<span class="number">-1</span>], x[<span class="number">-1</span>])).toDF([<span class="string">'features'</span>,<span class="string">'label'</span>])</span><br><span class="line"><span class="comment"># 划分训练集、测试集</span></span><br><span class="line">train, test = model_df.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>], seed = <span class="number">123</span>)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lrModel = lr.fit(train)</span><br><span class="line"><span class="comment"># 查看模型系数和截距</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'coefficient'</span>:list(lrModel.coefficientMatrix.toArray()[<span class="number">0</span>])&#125;, index = df.columns[:<span class="number">-1</span>]).sort_values(by = <span class="string">'coefficient'</span>, ascending = <span class="literal">False</span>)</span><br><span class="line">lrModel.interceptVector</span><br><span class="line"><span class="comment"># 在测试集上预测</span></span><br><span class="line">lr_test = lrModel.transform(test)</span><br><span class="line"><span class="comment"># 模型评价</span></span><br><span class="line">evaluator = BinaryClassificationEvaluator()</span><br><span class="line">print(evaluator.evaluate(lr_test, &#123;evaluator.matricName: <span class="string">'areaUnderROC'</span>&#125;))</span><br></pre></td></tr></table></figure><p>模型输出结果（仅取前两行为例）</p><div class="table-container"><table><thead><tr><th>features</th><th>label</th><th>rawPrediction</th><th>probability</th><th>prediction</th></tr></thead><tbody><tr><td>[4.9,3.1,1.5,0.1]</td><td>0</td><td>[60.297,-7.393,-52.905]</td><td>[1,0,0]</td><td>0</td></tr><tr><td>[5.0,3.2,1.2,0.2]</td><td>0</td><td>[64.815,-8.896,-55.919]</td><td>[1,0,0]</td><td>0</td></tr></tbody></table></div><p>该模型为多分类情况，其中rawPrediction为线性回归模型输出结果，probability为经过softmax过后得到的逻辑回归结果。具体来说，以第一行为例，根据rawPrediction输出probability，并以最大值对应的类作为最终预测的类</p><script type="math/tex; mode=display">\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 1\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0\\\frac{e^{60.297}}{e^{60.297}+e^{-7.393}+e^{-52.905}}\approx 0</script><h1 id="集群scala实现"><a href="#集群scala实现" class="headerlink" title="集群scala实现"></a>集群scala实现</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkContext</span>, <span class="type">SparkConf</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.param.<span class="type">ParamMap</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.&#123;<span class="type">Vector</span>, <span class="type">Vectors</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.hive.<span class="type">HiveContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">BinaryClassificationEvaluator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// basic variable settings</span></span><br><span class="line"><span class="keyword">val</span> train_tbl = <span class="string">"test.sklearn_dataset_iris_train"</span></span><br><span class="line"><span class="keyword">val</span> test_tbl = <span class="string">"test.sklearn_dataset_iris_test"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> hc = <span class="keyword">new</span> <span class="type">HiveContext</span>(sc)</span><br><span class="line"></span><br><span class="line"><span class="comment">//data processing</span></span><br><span class="line"><span class="keyword">val</span> train_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$train_tbl</span>"</span>).cache())</span><br><span class="line"><span class="keyword">val</span> test_dataset = (hc.sql(<span class="string">s"select * from <span class="subst">$test_tbl</span>"</span>).cache())</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> featureCols = train_dataset.columns.filter(x=&gt;x.split(<span class="string">"_"</span>)(x.split(<span class="string">"_"</span>).length<span class="number">-1</span>)!=<span class="string">"label"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> assembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>().setInputCols(featureCols).setOutputCol(<span class="string">"features"</span>))</span><br><span class="line"><span class="keyword">val</span> train_data = assembler.transform(train_dataset)</span><br><span class="line"><span class="keyword">val</span> test_data = assembler.transform(test_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>()</span><br><span class="line">lr.setMaxIter(<span class="number">10</span>).setRegParam(<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">val</span> lr_model = lr.fit(train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// model evaluation</span></span><br><span class="line"><span class="keyword">val</span> auc_calculator = lr_model.transform(test_data)</span><br><span class="line"><span class="keyword">val</span> evaluator = (<span class="keyword">new</span> <span class="type">BinaryClassificationEvaluator</span>())</span><br><span class="line"><span class="keyword">val</span> auc = evaluator.evaluate(auc_calculator)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> Classification </tag>
            
            <tag> LogisticRegression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WOE与IV理论介绍及实现</title>
      <link href="/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/27/woe-yu-iv-li-lun-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="WOE定义"><a href="#WOE定义" class="headerlink" title="WOE定义"></a>WOE定义</h2><p>全称：Weight of Evidence</p><p>前提：计算之前需要是离散化后的连续变量or离散变量。</p><p>对于变量第$i$个取值的$WOE_i$的计算公式为</p><script type="math/tex; mode=display">WOE_i = ln(\frac{py_i} {pn_i})=ln(\frac{ \frac{ \#y_i }{ \#y_T } } { \frac{ \#n_i } { \#n_T }})=ln(\frac{\frac{ \#y_i }{ \#n_i } } {\frac{ \#y_T } { \#n_T } })</script><p>其中 #$y_i $表示在第$i$个取值的样本中（或第$i$个箱内）的正样本个数，#$ y_T $表示所有正样本的个数，$py_i$表示在第$i$个取值的样本中（或第$i$个箱内）正样本占所有正样本的比例。</p><p>从最后一个等号后的表达式可以看出，$WOE_i$表示的是某个取值下正样本和负样本的比值，与所有样本中这个比值的差异。这个差异是用比值再取对数来表示的。$WOE_i$越大，表示差异越大，则这个取值下的样本为正的可能性越大。</p><h2 id="WOE编码作用"><a href="#WOE编码作用" class="headerlink" title="WOE编码作用"></a>WOE编码作用</h2><ol><li>标准化功能。编码过后的自变量其实具备了某种标准化的性质，自变量内部的各个取值之间都可以直接进行比较，且不同自变量之间的各个取值也可以通过$WOE_i$进行直接的比较。</li><li>可以反映出自变量的贡献情况（？）。自变量内部$WOE_i$值的变异（波动）情况，结合模型拟合出的系数，构造出各个自变量的贡献率和相对重要性。一般地，系数越大，$WOE_i$的方差越大，则自变量的贡献率越大。</li></ol><h2 id="IV​"><a href="#IV​" class="headerlink" title="IV​"></a>IV​</h2><p>全程：Information Value</p><p>在$WOE_i$的基础上，$IV$在$WOE_i$的前面乘以一个系数$(py_i-pn_i)$作为各箱$WOE_i$的权重，并进行加权求和，其计算公式如下</p><script type="math/tex; mode=display">IV=\sum_{i=1}^{N}(py_i-pn_i)*WOE_i=\sum_{i=1}^{N}(py_i-pn_i)*ln(\frac{py_i}{pn_i})</script><p>其中$N$为分箱的个数（变量的取值个数），系数$(py_i-pn_i)$为箱内正样本占比与负样本占比的差。该系数不仅保证了每个箱乘积的值为非负数，更重要的是考虑了变量当前箱中样本的数量占整体样本数量的比例，比例越高，该箱对变量整体预测能力的贡献越高。</p><p>$IV$衡量某个特征对目标的影响程度，通过该特征中正负样本的比例与总体正负样本的比例，来对比和计算其关联程度，因此可以代表<strong>该特征上的信息量</strong>以及<strong>该特征对模型的贡献</strong>。</p><p>$IV$是对于整个特征来说的，代表的意义由下表来控制：</p><div class="table-container"><table><thead><tr><th>IV</th><th>特征对预测函数的贡献</th></tr></thead><tbody><tr><td>&lt;0.03</td><td>特征几乎不含有效信息，对模型没有贡献，可以删除</td></tr><tr><td>[0.03, 0.10)</td><td>有效信息很少，对模型的贡献度低</td></tr><tr><td>[0.10, 0.30)</td><td>有效信息一般，对模型的贡献度中等</td></tr><tr><td>[0.30, 0.50)</td><td>有效信息较多，对模型的贡献度较高</td></tr><tr><td>&gt;=0.50</td><td>有效信息非常多，对模型的贡献极高且可疑</td></tr></tbody></table></div><p>因此通常会选择$IV$值在0.1~0.5范围内的特征。</p><h1 id="单机python"><a href="#单机python" class="headerlink" title="单机python"></a>单机python</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcWOE</span><span class="params">(df, col, target)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    获得某个变量的分箱操作后每个箱体对应的WOE值，并且基于WOE值计算该变量的IV值</span></span><br><span class="line"><span class="string">    :param df: 包含需要计算WOE的变量和目标变量</span></span><br><span class="line"><span class="string">    :param col: 需要计算WOE、IV的变量，必须是分箱后的变量，或者不需要分箱的离散型变量</span></span><br><span class="line"><span class="string">    :param target: 目标变量，0、1表示好、坏</span></span><br><span class="line"><span class="string">    :return: 返回WOE和IV</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    total = df.groupby([col])[target].count()  <span class="comment"># 每个箱体的总数</span></span><br><span class="line">    total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)    </span><br><span class="line">    bad = df.groupby([col])[target].sum()      <span class="comment"># 每个箱体的坏样本数</span></span><br><span class="line">    bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line">    regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>)</span><br><span class="line">    regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    N = sum(regroup[<span class="string">'total'</span>]) <span class="comment"># 总体样本数</span></span><br><span class="line">    B = sum(regroup[<span class="string">'bad'</span>])   <span class="comment"># 总体坏样本数</span></span><br><span class="line">    regroup[<span class="string">'good'</span>] = regroup[<span class="string">'total'</span>] - regroup[<span class="string">'bad'</span>]  <span class="comment"># 每个箱体的好样本数</span></span><br><span class="line">    G = N - B                 <span class="comment"># 总体好样本数</span></span><br><span class="line">    regroup[<span class="string">'bad_pcnt'</span>] = regroup[<span class="string">'bad'</span>].map(<span class="keyword">lambda</span> x: x/B)    <span class="comment"># 每个箱体的坏样本数占总体的坏样本数的比例</span></span><br><span class="line">    regroup[<span class="string">'good_pcnt'</span>] = regroup[<span class="string">'good'</span>].map(<span class="keyword">lambda</span> x: x/G)  <span class="comment"># 每个箱体的好样本数占总体的好样本数的比例</span></span><br><span class="line">    <span class="comment"># WOEi计算公式(不含系数)： WOE(每个箱体) = ln(该箱体的好样本数占总体的好样本数的比例/该箱体的好样本数占总体的好样本数的比例）</span></span><br><span class="line">    regroup[<span class="string">'WOE'</span>] = regroup.apply(<span class="keyword">lambda</span> x: np.log(x.good_pcnt/x.bad_pcnt),axis=<span class="number">1</span>)   </span><br><span class="line">    WOE_dict = regroup[[col,<span class="string">'WOE'</span>]].set_index(col).to_dict(orient=<span class="string">'index'</span>)</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> WOE_dict.items(): <span class="comment"># k代表箱体名，v代表了以WOE为键，箱体的实际WOE值为value的字典</span></span><br><span class="line">        WOE_dict[k] = v[<span class="string">'WOE'</span>]</span><br><span class="line">    <span class="comment"># 计算该变量的IV值：sum((某箱体的好样本数占总体的好样本数的比例 - 该箱体的坏样本数占总体的坏样本数的比例)*WOE(某个箱体))</span></span><br><span class="line">    IV = regroup.apply(<span class="keyword">lambda</span> x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt/x.bad_pcnt),axis = <span class="number">1</span>)</span><br><span class="line">    IV = sum(IV)</span><br><span class="line">    <span class="keyword">return</span> &#123;col: &#123;<span class="string">"WOE"</span>: WOE_dict, <span class="string">'IV'</span>:IV&#125;&#125;</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先进行等频分箱和编码（用到的函数见'分箱'）</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">     df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后循环计算每个变量的各箱WOE和IV，输出result</span></span><br><span class="line">result = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">     woe_iv_dict = CalcWOE(df_cut, col, target = <span class="string">'label'</span>)</span><br><span class="line">     result = pd.concat([result, pd.DataFrame(woe_iv_dict).T])</span><br></pre></td></tr></table></figure><h1 id="集群pyspark"><a href="#集群pyspark" class="headerlink" title="集群pyspark"></a>集群pyspark</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_woe_iv</span><span class="params">(tableName, featureCol, labelCol)</span>:</span></span><br><span class="line">     <span class="string">'''</span></span><br><span class="line"><span class="string">     计算spark.sql.DataFrame的WOE和IV</span></span><br><span class="line"><span class="string">     :param tableName: 储存在HDFS上的表名</span></span><br><span class="line"><span class="string">     :param featureCol: 所需要计算的字段名</span></span><br><span class="line"><span class="string">     :param labelCol: 标签列名</span></span><br><span class="line"><span class="string">     :return WOE字典和IV值</span></span><br><span class="line"><span class="string">     '''</span></span><br><span class="line">     df = spark.sql(<span class="string">"select &#123;featureCol&#125;, &#123;labelCol&#125; from &#123;tableName&#125;"</span>.format(featureCol=featureCol, labelCol=labelCol, tableName=tableName))</span><br><span class="line">     <span class="comment"># 使用crosstab函数生成列联表，并对列进行命名</span></span><br><span class="line">     binCount = df.crosstab(featureCol,labelCol).toDF(<span class="string">'feature'</span>,<span class="string">'neg_cnt'</span>,<span class="string">'pos_cnt'</span>)</span><br><span class="line">     <span class="comment"># 注册临时表便于后续调用</span></span><br><span class="line">     binCount.registerTempTable(<span class="string">"binCount"</span>)</span><br><span class="line">     <span class="comment"># 计算主要步骤</span></span><br><span class="line">     woe_df = spark.sql(<span class="string">"""</span></span><br><span class="line"><span class="string">        select</span></span><br><span class="line"><span class="string">          feature,</span></span><br><span class="line"><span class="string">          nvl(log(pos_per/neg_per), 0) as woe_i,</span></span><br><span class="line"><span class="string">          nvl((pos_per-neg_per)*log(pos_per/neg_per), 0) as iv_i</span></span><br><span class="line"><span class="string">        from</span></span><br><span class="line"><span class="string">        (</span></span><br><span class="line"><span class="string">          select</span></span><br><span class="line"><span class="string">            feature,</span></span><br><span class="line"><span class="string">            neg_cnt/(select sum(neg_cnt) from binCount) as neg_per,</span></span><br><span class="line"><span class="string">            pos_cnt/(select sum(pos_cnt) from binCount) as pos_per</span></span><br><span class="line"><span class="string">          from</span></span><br><span class="line"><span class="string">            binCount</span></span><br><span class="line"><span class="string">        ) t</span></span><br><span class="line"><span class="string">     """</span>)</span><br><span class="line">     <span class="comment"># 将计算结果转化为pandas.DataFrame</span></span><br><span class="line">     woe_df_pandas = woe_df.toPandas()</span><br><span class="line">     woe = dict(zip(woe_df_pandas[<span class="string">"feature"</span>].values.reshape(<span class="number">-1</span>,), woe_df_pandas[<span class="string">"woe_i"</span>].values.reshape(<span class="number">-1</span>,),))</span><br><span class="line">     iv = sum(woe_df_pandas[<span class="string">"iv_i"</span>].values)</span><br><span class="line">     <span class="keyword">return</span> woe, iv</span><br></pre></td></tr></table></figure><p>调用并测试运算时间</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    spark = SparkSession.builder.enableHiveSupport().getOrCreate()</span><br><span class="line">    tableName = <span class="string">"test.calc_woe_iv_sample"</span></span><br><span class="line">    featureCol = <span class="string">"age_level"</span></span><br><span class="line">    labelCol = <span class="string">"label"</span></span><br><span class="line">    time_cost = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        start = time.time()</span><br><span class="line">        woe, iv = cal_woe_iv(tableName, featureCol, labelCol)</span><br><span class="line">        end = time.time()</span><br><span class="line">        time_cost.append(end-start)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'time cost: %.5f sec'</span> % (np.mean(time_cost)))</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'woe:'</span>, woe)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'iv:'</span>, iv)</span><br></pre></td></tr></table></figure><p>集群资源配置：</p><blockquote><p>spark.driver.memory=5G<br>spark.driver.maxResultSize=5G<br>num-executors=20<br>executor-cores=6<br>executor-memory=4G</p></blockquote><p>根据样本量的不同，测试计算时间结果（循环30次取平均）</p><div class="table-container"><table><thead><tr><th></th><th>10w</th><th>100w</th><th>1000w</th><th>1e</th></tr></thead><tbody><tr><td>计算时间(s)</td><td>0.872</td><td>1.819</td><td>3.172</td><td>25.793</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FeatureEngineering </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分箱基本方法介绍及实现</title>
      <link href="/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/"/>
      <url>/2020/01/20/fen-xiang-ji-ben-fang-fa-jie-shao-ji-shi-xian/</url>
      
        <content type="html"><![CDATA[<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>将连续型的数据分为几个数据段，即特征离散化。</p><p>把无限空间中有限的个体映射到有限的空间中去，以此提高算法的时空效率。（百度百科） </p><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li><p><strong>离散化后的特征对异常数据有很强的鲁棒性，模型更加稳定。</strong>比如年龄&gt;70岁为一个分箱，异常数据如年龄为300岁同样会划分到该箱，不会给模型造成很大的干扰；比如将20岁-30岁划分为一个区间，不会因为年龄增长一岁就变成一个完全不同的人。而对于处于区间相邻处的样本会刚好相反，因此如何划分区间也很重要。</p></li><li><p>对于如逻辑回归的广义线性模型，表达能力受限，单变量离散化为$N$个变量后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。（？）</p></li><li><p>离散化后可以进行特征交叉，进一步<strong>引入非线性，提升模型表达能力</strong>。</p></li><li><p>可以将缺失值作为独立的箱代入变量。</p></li><li><p>可以将所有变量转换到相似的尺度上（不需要进行归一化）。</p></li></ol><h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="无监督分箱"><a href="#无监督分箱" class="headerlink" title="无监督分箱"></a>无监督分箱</h3><h4 id="等频分箱"><a href="#等频分箱" class="headerlink" title="等频分箱"></a>等频分箱</h4><p>每个箱包含大致相等的样本数量，可以根据分位数进行划分，如四分位数等。</p><h4 id="等距分箱"><a href="#等距分箱" class="headerlink" title="等距分箱"></a>等距分箱</h4><p>从变量的最小值到最大值之间均等分为$N$等份。若$m$和$M$分别代表最小值和最大值，则每个区间的长度为$w=\frac{M-m}{N}$，区间的边界值为$m+w,m+2w,…,m+(N-1)w$。这里只考虑边界，每个等份的样本数量一般不等。</p><h3 id="有监督分箱"><a href="#有监督分箱" class="headerlink" title="有监督分箱"></a>有监督分箱</h3><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p><strong>初始化</strong></p><ul><li><p>根据连续变量值的大小进行排序。</p></li><li><p>把每一个单独的值视为一个箱体，构建最初的离散化。目的是想从每个单独的箱体开始逐渐合并。</p></li></ul></li><li><p><strong>合并</strong>：在初始化构建完毕后，该步就是不断地进行自底向上的合并，直到满足停止条件。</p><ul><li><p><strong>计算所有相邻分箱的卡方值</strong>。比如有1,2,3,4这4个分箱，绑定相邻的两个分箱，一共有3组：12,23,34，然后分别计算三个绑定组的卡方值。卡方值的计算公式为</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}</script><p>其中，$m$表示要合并相邻分箱的数目（$m=2$表示对两个箱进行合并），$k$表示目标变量的类别数（两分类或多分类），$A_{ij}$表示实际频数（即第$i$个分箱第$j$类别的频数），$E_{ij}$表示期望频数，计算公式如下（可根据公式$P(AB)=P(A)P(B)$推导出来）</p><script type="math/tex; mode=display">E_{ij}=\frac{R_i*C_j}{N}</script><p>其中$R_i$和$C_j$分别是实际频数整行和整列的加和，$N$为总样本量。</p></li><li><p><strong>从计算的卡方值中找出最小的一个，并把这两个分箱合并</strong>。比如23是卡方值最小的一个，那么就将2和3合并，经过本轮的计算后分箱就变为1,23,4。</p><p>从合并的方法可以看出，卡方分箱的基本思想是，<strong>如果两个相邻的区间具有非常类似的类分布（即低卡方值，低卡方值表明它们具有类似的类分布），那么这两个区间可以合并，否则应该分开</strong>。</p></li></ul></li><li><p><strong>停止条件</strong>：以上仅是每一轮需要计算的内容，若不设置停止条件，算法会一直运行。一般从以下两个方面设置停止条件：</p><ul><li>卡方停止的阈值</li><li>分箱数目的限制</li></ul><p>即当所有分箱对的卡方值均大于阈值，且分箱数大于分箱数时，计算就会继续，直到不满足。</p><p>以上两个阈值一般根据经验来定义，来作为分箱函数的参数进行设置，一般使用0.90,0.95,0.99的置信度，分箱数一般可以设置为5。</p></li></ul><p><em>例如：</em></p><p>对于某两个箱（分箱1和分箱2），实际频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th><th>行频数和</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$A_{11}$</td><td>$A_{12}$</td><td>$R_1$</td></tr><tr><td><strong>分箱2</strong></td><td>$A_{21}$</td><td>$A_{22}$</td><td>$R_2$</td></tr><tr><td><strong>列频数和</strong></td><td>$C_1$</td><td>$C_2$</td><td>$N$</td></tr></tbody></table></div><p>期望频数如下表</p><div class="table-container"><table><thead><tr><th></th><th>类别1</th><th>类别2</th></tr></thead><tbody><tr><td><strong>分箱1</strong></td><td>$E_{11}=\frac{R_1*C_1}{N}$</td><td>$E_{12}=\frac{R_1*C_2}{N}$</td></tr><tr><td><strong>分箱2</strong></td><td>$E_{21}=\frac{R_2*C_1}{N}$</td><td>$E_{22}=\frac{R_2*C_2}{N}$</td></tr></tbody></table></div><p>代入卡方公式求解，过程如下：</p><script type="math/tex; mode=display">\chi^2=\sum_{i=1}^{m}\sum_{j=1}^{k}\frac{(A_{ij}-E_{ij})^2}{E_{ij}}=[\frac{(A_{11}-E_{11})^2}{E_{11}}+\frac{(A_{12}-E_{12})^2}{E_{12}}]+[\frac{(A_{21}-E_{21})^2}{E_{21}}+\frac{(A_{22}-E_{22})^2}{E_{22}}]</script><p>如果计算结果是所有卡方值中最小的，说明这两个分箱具有最相似的类分布，因此把它们合并。</p><p>（参考 <a href="https://cloud.tencent.com/developer/article/1418720" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1418720</a> ）</p><h1 id="单机python实现"><a href="#单机python实现" class="headerlink" title="单机python实现"></a>单机python实现</h1><h2 id="无监督分箱-1"><a href="#无监督分箱-1" class="headerlink" title="无监督分箱"></a>无监督分箱</h2><ul><li>UnsupervisedSplitBin函数：对数值型变量进行分组，分组的依据有等频和等距，最后返回划分点列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">UnsupervisedSplitBin</span><span class="params">(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对数值型变量进行划分，最后返回划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: 数据集</span></span><br><span class="line"><span class="string">:param col: 需要分箱的变量。仅限数值型变量</span></span><br><span class="line"><span class="string">:param numOfSplit: 需要分箱个数，默认是5</span></span><br><span class="line"><span class="string">:param method: 分箱方法，'equalFreq'：默认是等频，否则是等距</span></span><br><span class="line"><span class="string">:return: 返回划分点组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">if</span> method == <span class="string">'equalFreq'</span>:</span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = [np.percentile(df[col], <span class="number">100</span>/numOfSplit*i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line">splitPoint = sorted(list(set(frequency_cutoff)))</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">elif</span> method == <span class="string">'equalDis'</span>:</span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">var_max, var_min = max(df[col]), min(df[col])</span><br><span class="line">interval_len = (var_max-var_min)/numOfSplit</span><br><span class="line">splitPoint = [var_min+i*interval_len <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)]</span><br><span class="line"><span class="keyword">return</span> splitPoint</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="string">'Sorry, no such method'</span></span><br></pre></td></tr></table></figure><h2 id="有监督分箱：卡方分箱"><a href="#有监督分箱：卡方分箱" class="headerlink" title="有监督分箱：卡方分箱"></a>有监督分箱：卡方分箱</h2><ul><li><p>SplitData函数：用于对某变量进行划分，根据划分的组数，返回划分点数值组成的列表</p></li><li><p>Chi2函数：用于计算卡方值，返回卡方值（按照卡方检验的原理进行计算）</p></li><li><p>BinBadRate函数：按某变量进行分组，计算分组后每组的坏样本率，返回的有，字典形式，数据框，总体坏样本率（可选）</p></li><li><p>AssignGroup函数：根据分组后的划分点列表，给某个需分箱的变量的每个取值进行分箱前的匹配，形成对应箱的映射</p></li><li><p>AssignBin函数：将某列的每个取值进行分箱编号</p></li><li><p>ChiMerge函数：卡方分箱的主体函数，其中调用了前面五个基础函数，返回的是最终的满足所有限制条件的划分点列表</p><ul><li><p>其中需要满足：</p><p>（1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</p><p>（2）每个箱体必须同时包含好坏样本</p><p>（3）每个箱体的占比不低于预设值（可选）</p><p>（4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SplitData</span><span class="params">(df,col,numOfSplit,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得数据切分时对应位置的数值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame</span></span><br><span class="line"><span class="string">:param col: str 选择数据集中的某列进行操作</span></span><br><span class="line"><span class="string">:param numOfSplit: int 划分的组数</span></span><br><span class="line"><span class="string">:param special_attribute: 用于过滤掉一些特殊的值，不参与数据划分（可选，默认不过滤）</span></span><br><span class="line"><span class="string">:return: 返回划分点位置对应的数值组成的列表</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"><span class="keyword">if</span> special_attribute != []:</span><br><span class="line">df2 = df2.loc[~df2[col].isin(special_attribute)]  <span class="comment"># 排除有特殊值的样本行</span></span><br><span class="line">N = len(df2) <span class="comment"># 样本总数</span></span><br><span class="line">n = int(N/numOfSplit) <span class="comment"># 每组包含的样本量</span></span><br><span class="line">SplitPointIndex = [i*n <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,numOfSplit)] <span class="comment"># 不包含numOfSplit</span></span><br><span class="line"><span class="comment"># 如总样本N=20，numofsplit为5组，则每组包含4个元素(n=4)</span></span><br><span class="line"><span class="comment"># 则最后得到的SplitPointIndex=[4,8,12,16]，即切分点位置，各组样本为0-3/4-7/8-11/12-15</span></span><br><span class="line">rawValues = sorted(list(df[col])) <span class="comment"># sorted返回一个新的排列后的列表</span></span><br><span class="line">SplitPoint = [rawValues[i] <span class="keyword">for</span> i <span class="keyword">in</span> SplitPointIndex] <span class="comment"># 返回位置索引上对应的数值</span></span><br><span class="line"><span class="comment"># 为了以防万一，去重再排序</span></span><br><span class="line">SplitPoint = sorted(list(set(SplitPoint)))</span><br><span class="line"><span class="keyword">return</span> SplitPoint </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignGroup</span><span class="params">(x,bin)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于：根据分组后的划分点列表（bin)，给某个需要分箱的变量进行分箱前的匹配，形成对应箱的映射，以便后期分箱操作，将值划分到不同箱体</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param bin: 上述变量分组后（通过SplitData函数）对应的划分位置的数值组成的列表</span></span><br><span class="line"><span class="string">:return: x在分箱结果下的映射 </span></span><br><span class="line"><span class="string">'''</span> </span><br><span class="line">N = len(bin)            <span class="comment"># 划分点的长度</span></span><br><span class="line"><span class="keyword">if</span> x&lt;=min(bin):         <span class="comment"># 如果某个取值小于等于最小划分点，则返回最小划分点</span></span><br><span class="line"><span class="keyword">return</span> min(bin)</span><br><span class="line"><span class="keyword">elif</span> x&gt;max(bin):        <span class="comment"># 如果某个取值大于最小划分点，则返回10e10</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">10e10</span></span><br><span class="line"><span class="keyword">else</span>:                   <span class="comment"># 除此之外，返回其他对应的划分点</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> bin[i] &lt; x &lt;= bin[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> bin[i+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BinBadRate</span><span class="params">(df,col,target,grantRateIndicator=False)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对变量按照取值（每个取值都是唯一的）进行分组，获得每箱的坏样本率，后期基于该值判断是否需要合并操作</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 需要计算好坏比率的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要计算好坏比率的特征</span></span><br><span class="line"><span class="string">:param target: str 好坏标签</span></span><br><span class="line"><span class="string">:param grantRateIndicator: bool True返回总体的坏样本率，False不返回（可选，默认不返回）</span></span><br><span class="line"><span class="string">:return: 每箱的坏样本率，以及总体的坏样本率（当grantRateIndicator==True时）</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">total = df.groupby([col])[target].count()</span><br><span class="line">total = pd.DataFrame(&#123;<span class="string">'total'</span>: total&#125;)</span><br><span class="line">bad = df.groupby([col])[target].sum()</span><br><span class="line">bad = pd.DataFrame(&#123;<span class="string">'bad'</span>: bad&#125;)</span><br><span class="line"></span><br><span class="line">regroup = total.merge(bad, left_index=<span class="literal">True</span>, right_index=<span class="literal">True</span>, how=<span class="string">'left'</span>) <span class="comment"># 每箱的坏样本数，总样本数</span></span><br><span class="line">regroup.reset_index(level=<span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">regroup[<span class="string">'bad_rate'</span>] = regroup.apply(<span class="keyword">lambda</span> x: x.bad / x.total, axis=<span class="number">1</span>) <span class="comment"># 加上一列坏样本率</span></span><br><span class="line">dicts = dict(zip(regroup[col],regroup[<span class="string">'bad_rate'</span>])) <span class="comment"># 每箱对应的坏样本率组成的字典</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> grantRateIndicator:</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">N = sum(regroup[<span class="string">'total'</span>])</span><br><span class="line">B = sum(regroup[<span class="string">'bad'</span>])</span><br><span class="line">overallRate = B / N</span><br><span class="line"><span class="keyword">return</span> (dicts, regroup, overallRate)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Chi2</span><span class="params">(df,totalCol,badCol)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于获得卡方值</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含了每个分组下的样本总数和坏样本数</span></span><br><span class="line"><span class="string">:param totalCol: str 列名，元素由各个分组下的样本总数构成</span></span><br><span class="line"><span class="string">:param badCol: str 列名，元素由各个分组下的坏样本数构成</span></span><br><span class="line"><span class="string">:return: 卡方值</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算该特征各个分组下的好样本数</span></span><br><span class="line">df2[<span class="string">'good'</span>] = df2.apply(<span class="keyword">lambda</span> x: x[totalCol]-x[badCol], axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求出期望值E</span></span><br><span class="line"><span class="comment"># 1.求出总体的坏样本率和好样本率</span></span><br><span class="line">badRate = sum(df2[badCol]) / sum(df2[totalCol])</span><br><span class="line">goodRate = sum(df2[<span class="string">'good'</span>]) / sum(df2[totalCol])</span><br><span class="line"><span class="comment"># 特殊情况：当全部样本只有好或者坏样本时，卡方值为0</span></span><br><span class="line"><span class="keyword">if</span> badRate <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.根据总体的好坏样本率算出期望的好坏样本数，计算公式为：</span></span><br><span class="line"><span class="comment"># 期望坏（好）样本个数 ＝ 全部样本个数 * 总体的坏（好）样本率</span></span><br><span class="line">df2[<span class="string">'badExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*badRate)</span><br><span class="line">df2[<span class="string">'goodExpected'</span>] = df2[totalCol].apply(<span class="keyword">lambda</span> x: x*goodRate)</span><br><span class="line"></span><br><span class="line">badCombined = zip(df2[<span class="string">'badExpected'</span>], df2[badCol])</span><br><span class="line">goodCombined = zip(df2[<span class="string">'goodExpected'</span>], df2[<span class="string">'good'</span>])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 按照卡方计算的公式计算卡方值：  ∑ (O - E)^2 / E, O代表实际值，E代表期望值     </span></span><br><span class="line">badChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> badCombined]</span><br><span class="line">goodChi = [(i[<span class="number">0</span>]-i[<span class="number">1</span>])**<span class="number">2</span>/i[<span class="number">1</span>] <span class="keyword">if</span> i[<span class="number">1</span>]!=<span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> goodCombined]</span><br><span class="line">chi2 = sum(badChi) + sum(goodChi)</span><br><span class="line"><span class="keyword">return</span> chi2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">AssignBin</span><span class="params">(x,cutOffPoints,special_attribute=[])</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于对某个列的某个取值进行分箱编号（编码）</span></span><br><span class="line"><span class="string">:param x: 某个变量的某个取值</span></span><br><span class="line"><span class="string">:param cutOffPoints: 上述变量的分组结果，用切分点表示，列表形式</span></span><br><span class="line"><span class="string">:param special_attribute: 不参与分箱的特殊取值（可选）</span></span><br><span class="line"><span class="string">:return: 分箱后的对应的第几个箱，从0开始</span></span><br><span class="line"><span class="string">比如，若cutOffPoints=[10,20,30]，当x=7，返回0；当x=35，返回3</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">numBin = len(cutOffPoints) + <span class="number">1</span> + len(special_attribute)</span><br><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> special_attribute:</span><br><span class="line">i = special_attribute.index(x)+<span class="number">1</span></span><br><span class="line"><span class="keyword">return</span> (<span class="number">0</span>-i)</span><br><span class="line"><span class="keyword">if</span> x &lt;= cutOffPoints[<span class="number">0</span>]:</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="keyword">elif</span> x &gt; cutOffPoints[<span class="number">-1</span>]:</span><br><span class="line"><span class="keyword">return</span> (numBin<span class="number">-1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,numBin<span class="number">-1</span>):</span><br><span class="line"><span class="keyword">if</span> cutOffPoints[i] &lt; x &lt;= cutOffPoints[i+<span class="number">1</span>]:</span><br><span class="line"><span class="keyword">return</span> (i+<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ChiMerge</span><span class="params">(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span>:</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">该函数用于实际的卡方分箱算法操作，最后返回有实际的划分点组成的列表</span></span><br><span class="line"><span class="string">:param df: pandas.DataFrame 包含目标变量与需要分箱变量的数据集</span></span><br><span class="line"><span class="string">:param col: str 需要分箱的属性</span></span><br><span class="line"><span class="string">:param target: str 目标变量，取值0或1</span></span><br><span class="line"><span class="string">:param max_interval: int 最大分箱数（默认5）。如果原始属性的取值个数低于该参数，不执行这段函数</span></span><br><span class="line"><span class="string">:param special_attribute: list 不参与分箱的属性取值（可选）</span></span><br><span class="line"><span class="string">:param minBinPcnt：最小箱的占比（默认0），如果不满足最小分箱占比继续进行组别合并</span></span><br><span class="line"><span class="string">:return: 分箱结果</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">colLevels = sorted(list(set(df[col])))  <span class="comment"># 某列的不重复值</span></span><br><span class="line">N_distinct = len(colLevels)             <span class="comment"># 某列的不重复值计数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> N_distinct &lt;= max_interval:  <span class="comment">#如果原始属性的取值个数低于max_interval，不执行这段函数（不参与分箱）</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"The number of original levels for '&#123;col&#125;' is &#123;dis_cnt&#125;, which is less than or equal to max intervals"</span>.format(col=col, dis_cnt=N_distinct))</span><br><span class="line"><span class="keyword">return</span> colLevels[:<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">if</span> len(special_attribute)&gt;=<span class="number">1</span>:</span><br><span class="line">df2 = df.loc[~df[col].isin(special_attribute)] <span class="comment"># 去掉special_attribute后的df</span></span><br><span class="line">N_distinct = len(list(set(df2[col])))  <span class="comment"># 去掉special_attribute后的非重复值计数</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2 = df.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤一: 通过col对数据进行分组，求出每组的总样本数和坏样本数</span></span><br><span class="line"><span class="keyword">if</span> N_distinct &gt; <span class="number">100</span>:</span><br><span class="line">split_x = SplitData(df2,col,<span class="number">100</span>) <span class="comment"># 若非重复值计数超过100组，均将其转化成100组，将多余样本都划分到最后一个箱中</span></span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col].map(<span class="keyword">lambda</span> x: AssignGroup(x,split_x))</span><br><span class="line"><span class="comment"># Assgingroup函数：每一行的数值和切分点做对比，返回原值在切分后的映射，</span></span><br><span class="line"><span class="comment"># 经过map以后，生成该特征的值对象的"分箱"后的值        </span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">df2[<span class="string">'temp_cutoff'</span>] = df2[col] <span class="comment"># 不重复值计数不超过100时，不需要进行上述步骤</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过上述过程，我们现在可以将该列进行BadRate计算，用来计算每个箱体的坏样本率 以及总体的坏样本率      </span></span><br><span class="line">(binBadRate, regroup, overallRate) = BinBadRate(df2, <span class="string">'temp_cutoff'</span>, target, grantRateIndicator=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在此，我们将每个单独的属性值分成单独一组</span></span><br><span class="line"><span class="comment"># 对属性值进行去重排序，然后两两组别进行合并，用于后续的卡方值计算</span></span><br><span class="line">colLevels = sorted(list(set(df2[<span class="string">'temp_cutoff'</span>])))</span><br><span class="line">groupIntervals = [[i] <span class="keyword">for</span> i <span class="keyword">in</span> colLevels] <span class="comment"># 把每个箱的值打包成[[],[]]的形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤二：通过循环的方式，不断的合并相邻的两个组别，直到：</span></span><br><span class="line"><span class="comment"># （1）最后分裂出的分箱数 &lt;= 预设的最大分箱数</span></span><br><span class="line"><span class="comment"># （2）每个箱体必须同时包含好坏样本</span></span><br><span class="line"><span class="comment"># （3）每个箱体的占比不低于预设值（可选）</span></span><br><span class="line"><span class="comment"># （4）如果有特殊的属性值，则最终的分箱数 = 预设的最大分箱数 - 特殊值个数</span></span><br><span class="line">split_intervals = max_interval - len(special_attribute)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每次循环时, 计算合并相邻组别后的卡方值。当组别数大于预设的分箱数时，持续选择最小卡方值的组合并</span></span><br><span class="line"><span class="keyword">while</span> len(groupIntervals)&gt;split_intervals:</span><br><span class="line">chisqList = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(len(groupIntervals)<span class="number">-1</span>):</span><br><span class="line">temp_group = groupIntervals[k] + groupIntervals[k+<span class="number">1</span>]  <span class="comment"># 返回的是，这两个值组成的列表</span></span><br><span class="line"><span class="comment"># 因此，可以通过temp_group，每次只选相邻的两组进行相关操作</span></span><br><span class="line">df2b = regroup.loc[regroup[<span class="string">'temp_cutoff'</span>].isin(temp_group)]</span><br><span class="line"><span class="comment"># 计算相邻两组的卡方值(通过调用Chi2函数)</span></span><br><span class="line">chisq = Chi2(df2b,<span class="string">'total'</span>,<span class="string">'bad'</span>)</span><br><span class="line">chisqList.append(chisq)</span><br><span class="line">best_comnbined = chisqList.index(min(chisqList)) <span class="comment"># 检索最小卡方值所在的索引    </span></span><br><span class="line"><span class="comment"># 把groupIntervals的值改成类似的值改成类似从[[1],[2],[3]]到[[1,2],[3]]</span></span><br><span class="line">groupIntervals[best_comnbined] = groupIntervals[best_comnbined] + groupIntervals[best_comnbined+<span class="number">1</span>]</span><br><span class="line">groupIntervals.remove(groupIntervals[best_comnbined+<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上述循环结束后，即可获得预设的分箱数，并且可以得到各分箱的划分点</span></span><br><span class="line">groupIntervals = [sorted(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals]</span><br><span class="line">cutOffPoints = [max(i) <span class="keyword">for</span> i <span class="keyword">in</span> groupIntervals[:<span class="number">-1</span>]] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后，我们将某列进行分箱编号</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp_cutoff'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line"><span class="comment"># AssignBin函数：每一行的数值和切分点做对比，返回原值所在的分箱编号（形成新列）    </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进一步，我们想要保证每个箱体都有好坏样本数</span></span><br><span class="line"><span class="comment"># 检查是否有箱没有好或者坏样本。如果有，需要跟相邻的箱进行合并，直到每箱同时包含好坏样本</span></span><br><span class="line">(binBadRate,regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)    <span class="comment"># 返回每箱坏样本率字典，和包含'分箱号、总样本数、坏样本数、坏样本率的数据框'）</span></span><br><span class="line">minBadRate, maxBadRate = min(binBadRate.values()), max(binBadRate.values())</span><br><span class="line"><span class="keyword">while</span> minBadRate ==<span class="number">0</span> <span class="keyword">or</span> maxBadRate == <span class="number">1</span>:</span><br><span class="line"><span class="comment"># 找出全部为好／坏样本的箱</span></span><br><span class="line">indexForBad01 = regroup[regroup[<span class="string">'bad_rate'</span>].isin([<span class="number">0</span>,<span class="number">1</span>])].temp_Bin.tolist()    </span><br><span class="line">bin = indexForBad01[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个划分点需要移除</span></span><br><span class="line"><span class="keyword">if</span> bin == max(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> bin == min(regroup.temp_Bin):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果是中间的某一箱，则需要和前后中的一个箱体进行合并，具体选择哪个箱体，要依据前后箱体哪个卡方值较小</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(regroup.temp_Bin).index(bin)</span><br><span class="line">prevIndex = list(regroup.temp_Bin)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(regroup.temp_Bin)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, bin])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完成此项合并后，需要再次计算，在新的分箱准则下，每箱是否同时包含好坏样本，</span></span><br><span class="line"><span class="comment"># 如何仍然出现不能同时包含好坏样本的箱体，继续循坏，直到好坏样本同时出现在每个箱体后，跳出</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">(binBadRate, regroup) = BinBadRate(df2, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">[minBadRate, maxBadRate] = [min(binBadRate.values()), max(binBadRate.values())]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后，我们来检查分箱后的最小占比</span></span><br><span class="line"><span class="keyword">if</span> minBinPcnt &gt; <span class="number">0</span>: <span class="comment"># 如果函数调用初期给的minBinPct不是零，则进一步对箱体进行合并</span></span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>]) <span class="comment"># 得到箱体的最小占比</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果箱体最小占比小于给定的分箱占比阈值 且 划分点大于2，进入合并循环中</span></span><br><span class="line"><span class="keyword">while</span> minPcnt &lt; minBinPcnt <span class="keyword">and</span> len(cutOffPoints) &gt; <span class="number">2</span>: </span><br><span class="line"><span class="comment"># 找出占比最小的箱</span></span><br><span class="line">indexForMinPcnt = valueCounts[valueCounts[<span class="string">'pcnt'</span>] == minPcnt].index.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是最后一箱，则需要和上一个箱进行合并，也就意味着分裂点cutOffPoints中的最后一个需要移除</span></span><br><span class="line"><span class="keyword">if</span> indexForMinPcnt == max(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是第一箱，则需要和下一个箱进行合并，也就意味着分裂点cutOffPoints中的第一个需要移除</span></span><br><span class="line"><span class="keyword">elif</span> indexForMinPcnt == min(valueCounts.index):</span><br><span class="line">cutOffPoints = cutOffPoints[<span class="number">1</span>:]</span><br><span class="line"><span class="comment"># 如果占比最小的箱是中间的某一箱，则需要和前后中的一个箱进行合并，合并依据是较小的卡方值</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="comment"># 和前一箱进行合并，并且计算卡方值</span></span><br><span class="line">currentIndex = list(valueCounts.index).index(indexForMinPcnt)</span><br><span class="line">prevIndex = list(valueCounts.index)[currentIndex - <span class="number">1</span>]</span><br><span class="line">df3 = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([prevIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq1 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="comment"># 和后一箱进行合并，并且计算卡方值</span></span><br><span class="line">laterIndex = list(valueCounts.index)[currentIndex + <span class="number">1</span>]</span><br><span class="line">df3b = df2.loc[df2[<span class="string">'temp_Bin'</span>].isin([laterIndex, indexForMinPcnt])]</span><br><span class="line">(binBadRate, df2b) = BinBadRate(df3b, <span class="string">'temp_Bin'</span>, target)</span><br><span class="line">chisq2 = Chi2(df2b, <span class="string">'total'</span>, <span class="string">'bad'</span>)</span><br><span class="line"><span class="keyword">if</span> chisq1 &lt; chisq2:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex - <span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">cutOffPoints.remove(cutOffPoints[currentIndex])        </span><br><span class="line">groupedvalues = df2[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutOffPoints))</span><br><span class="line">df2[<span class="string">'temp_Bin'</span>] = groupedvalues</span><br><span class="line">valueCounts = groupedvalues.value_counts().to_frame()</span><br><span class="line">valueCounts[<span class="string">'pcnt'</span>] = valueCounts[<span class="string">'temp'</span>].apply(<span class="keyword">lambda</span> x: x * <span class="number">1.0</span> / sum(valueCounts[<span class="string">'temp'</span>]))</span><br><span class="line">valueCounts = valueCounts.sort_index()</span><br><span class="line">minPcnt = min(valueCounts[<span class="string">'pcnt'</span>])</span><br><span class="line"></span><br><span class="line">cutOffPoints = special_attribute + cutOffPoints</span><br><span class="line"><span class="keyword">return</span> cutOffPoints</span><br></pre></td></tr></table></figure><p>(参考 <a href="https://blog.csdn.net/LuLuYao9494/article/details/92083755" target="_blank" rel="noopener">https://blog.csdn.net/LuLuYao9494/article/details/92083755</a> )</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的乳腺癌数据集作为例子</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line">df = pd.DataFrame(cancer.data, columns = cancer.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = pd.DataFrame(cancer.target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置分箱参数</span></span><br><span class="line">bins = <span class="number">5</span> <span class="comment"># 分箱数</span></span><br><span class="line">col = <span class="string">'mean radius'</span> <span class="comment"># 分箱字段名</span></span><br><span class="line">target = <span class="string">'label'</span> <span class="comment"># 标签字段名</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等频分箱</span></span><br><span class="line">frequency_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line"><span class="comment"># [11.366, 12.726, 14.058000000000002, 17.067999999999998]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等距分箱</span></span><br><span class="line">distance_cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalDis'</span>)</span><br><span class="line"><span class="comment"># [11.2068, 15.432599999999999, 19.6584, 23.8842]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 卡方分箱</span></span><br><span class="line">chiMerge_cutoff = ChiMerge(df,col,target,max_interval=<span class="number">5</span>,special_attribute=[],minBinPcnt=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># [12.46, 13.38, 15.0, 16.84]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#以等频分箱为例在数据集上进行编码</span></span><br><span class="line">df_cut = df.copy()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> df_cut.columns[:<span class="number">-1</span>]:</span><br><span class="line">    <span class="comment"># 得到切分点（获取切分点的方法可替换）</span></span><br><span class="line">    cutoff = UnsupervisedSplitBin(df,col,numOfSplit=<span class="number">5</span>,method=<span class="string">'equalFreq'</span>)</span><br><span class="line">    <span class="comment"># 进行编码</span></span><br><span class="line">    df_cut[col] = df_cut[col].apply(<span class="keyword">lambda</span> x: AssignBin(x, cutoff))</span><br></pre></td></tr></table></figure><h1 id="集群pyspark实现"><a href="#集群pyspark实现" class="headerlink" title="集群pyspark实现"></a>集群pyspark实现</h1><p>pyspark.ml.feature提供等频分箱的API可以直接调用</p><p><em>class</em> <code>pyspark.ml.feature.QuantileDiscretizer</code>(<em>numBuckets=2</em>, <em>inputCol=None</em>, <em>outputCol=None</em>, <em>relativeError=0.001</em>, <em>handleInvalid=’error’</em>)<a href="http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer" target="_blank" rel="noopener"><a href="http://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/feature.html#QuantileDiscretizer" target="_blank" rel="noopener">source\</a></a></p><p>参数：</p><ol><li>numBuckets：分箱数，但若样本数据只有3个取值，但numBuckets=4，则仍只划分为3个箱</li><li>relativeError：用于控制近似的精度，取值范围为[0,1]，当设置为0时会计算精确的分位数（计算代价较高）</li><li>handleInvalid：选择处理空值的方式，有三种选项：’keep’将空值放入专门的箱中，例如如果使用4个箱，则将非空数据放入箱0-3中，将空值放入特殊的箱4中；’skip’：过滤掉含有空值的行；’error’报错。（？实验了一下这三种选项并没有区别？？？结果没有处理缺失值仍输出空值，也没有报错……？？）</li><li>inputCol：输入要分箱的列名，outputCol：输出分箱后新特征的列名</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="comment"># 导入数据集</span></span><br><span class="line">df = spark.sql(<span class="string">"select * from test.sklearn_dataset_iris"</span>)</span><br><span class="line">col = <span class="string">'sepal length (cm)'</span> <span class="comment"># 对该列进行等频分箱</span></span><br><span class="line"></span><br><span class="line">qd = QuantileDiscretizer(numBuckets=<span class="number">5</span>, inputCol=col, outputCol=col+<span class="string">'_bin'</span>)</span><br><span class="line">qd_model = qd.fit(df)</span><br><span class="line"><span class="keyword">print</span> (qd_model.getSplits()) <span class="comment"># 打印分箱的节点</span></span><br><span class="line">df_new = qd_model.transform(df)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> MachineLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FeatureEngineering </tag>
            
            <tag> MachineLearning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
